file_path,Name,full_name,Start Line,End Line,Comment,Pre_Comment,child Name,domain,inner_method,node_level
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int)",73,75,"/**
* Creates a FileRange instance with specified offset and length.
* @param offset starting file position
* @param length number of bytes in range
*/","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)",84,86,"/**
* Creates a FileRange object from given parameters.
* @param offset starting file offset
* @param length file range length
* @param reference arbitrary reference data
*/","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @param reference nullable reference to store in the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateRangeRequest,org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange),65,75,"/**
* Validates and returns a FileRange object.
* @param range FileRange object to validate
*/","* Validate a single range.
   * @param range range to validate.
   * @return the range.
   * @param <T> range type
   * @throws IllegalArgumentException the range length is negative or other invalid condition
   * is met other than the those which raise EOFException or NullPointerException.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNull,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)",45,47,"/**
* Validates object is not null and throws exception if true.
* @param obj Object to validate
* @param argName Name of argument being validated
*/","* Validates that the given reference argument is not null.
   * @param obj the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPositiveInteger,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)",54,56,"/**
* Validates and masks input value as a positive integer.
* @param value input value to validate
* @param argName name of the argument being validated
*/","* Validates that the given integer argument is not zero or negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNegative,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)",63,65,"/**
* Validates that the provided value is non-negative.
* @param value long integer to check
* @param argName name of the argument being validated
*/","* Validates that the given integer argument is not negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkRequired,"org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)",72,74,"/**
* Validates argument presence with error message.
* @param isPresent true if present, false otherwise
* @param argName name of the argument to validate
*/","* Validates that the expression (that checks a required field is present) is true.
   * @param isPresent indicates whether the given argument is present.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)",81,83,"/**
* Logs error message with the specified argument name if it's not valid.
* @param isValid true if valid, false otherwise
* @param argName name of the argument in question
*/","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)",91,96,"/**
* Logs an error message if the input value is invalid.
* @param isValid true if the value is valid; false otherwise
* @param argName name of the argument being validated
* @param validValues comma-separated list of allowed values
*/","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.
   * @param validValues the list of values that are allowed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValuesEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)",201,213,"/**
* Validates equality between two values with specified names.
* @param value1 first value to compare
* @param value1Name name of the first value
* @param value2 second value to compare
* @param value2Name name of the second value
*/","* Validates that the given two values are equal.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkIntegerMultiple,"org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)",222,234,"/**
* Validates if one number is a multiple of another.
* @param value1 the base value
* @param value1Name name of the base value
* @param value2 the divisor value
* @param value2Name name of the divisor value
*/","* Validates that the first value is an integer multiple of the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreater,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)",243,255,"/**
* Validates that the first value is greater than the second.
* @param value1 first value to compare
* @param value1Name name of the first value
* @param value2 second value to compare
* @param value2Name name of the second value
*/","* Validates that the first value is greater than the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreaterOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)",264,276,"/**
* Validates the relationship between two values.
* @param value1 first value
* @param value1Name name of the first value
* @param value2 second value
* @param value2Name name of the second value
*/","* Validates that the first value is greater than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkLessOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)",285,297,"/**
* Validates the relationship between two values.
* @param value1 first value
* @param value1Name name of first value
* @param value2 second value
* @param value2Name name of second value
*/","* Validates that the first value is less than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)",306,318,"/**
* Validates numeric value against specified inclusive range.
* @param value value to check
* @param valueName descriptive name of value
* @param minValueInclusive minimum allowed value (inclusive)
* @param maxValueInclusive maximum allowed value (inclusive)
*/","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)",327,339,"/**
* Validates a double value against an inclusive minimum and maximum range.
* @param value value to validate
* @param valueName name of the value being validated
* @param minValueInclusive minimum allowed value (inclusive)
* @param maxValueInclusive maximum allowed value (inclusive) 
*/","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)",393,398,"/**
* Validates that an array has at least one element.
* @param arraySize size of the array
* @param argName name of the argument being validated
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validateBulkDeletePaths,"org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)",39,48,"/**
* Validates and filters a collection of paths based on page size and base path.
* @param paths Collection of paths to validate
* @param pageSize Maximum number of paths per page
* @param basePath Base directory for valid paths
*/","* Preconditions for bulk delete paths.
   * @param paths paths to delete.
   * @param pageSize maximum number of paths to delete in a single operation.
   * @param basePath base path for the delete operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File),177,182,"/**
* Initializes block upload data from a given file.
* @param file the file containing the data to be uploaded
*/","* File constructor; input stream and byteArray will be null.
     *
     * @param file file to upload",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,requireIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable),352,356,"/**
* Casts snapshot to IOStatisticsSnapshot.
* @param snapshot object to cast
*/","* Require the parameter to be an instance of {@link IOStatisticsSnapshot}.
   * @param snapshot object to validate
   * @return cast value
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,<init>,"org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)",43,52,"/**
* Initializes the input wrapper with a socket and stream.
* @param s socket connection
* @param is input stream to wrap
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,mapEnumNamesToValues,"org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)",109,124,"/**
* Creates a map of enum constants by prefix and lowercase value.
* @param prefix unique identifier for the enum values
* @param enumClass Class of the Enum to fetch constants from
* @return Map of prefixes to Enum values or null if duplicate found
*/","* Given an enum class, build a map of lower case names to values.
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param enumClass class of enum
   * @param <E> enum type
   * @return a mutable map of lower case names to enum values
   * @throws IllegalArgumentException if there are two entries which differ only by case.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,sortRanges,org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List),358,361,"/**
* Processes file ranges and returns a list of filtered results.
* @param input list of file ranges to process
*/","* Sort the input ranges by offset; no validation is done.
   * <p>
   * This method is used externally and must be retained with
   * the signature unchanged.
   * @param input input ranges.
   * @return a new list of the ranges, sorted by offset.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,perms,org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission),65,67,"/**
 * Creates a Perms object from an FsPermission.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seekInternal,org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal(),79,96,"/**
* Advances the stream position to match or exceed nextPos.
* @throws IOException if SFTP error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,checkNotClosed,org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed(),135,141,"/**
* Validates if the stream is still open before executing.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isParentOf,"org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",648,657,"/**
* Checks if a path is a direct child of another path.
* @param parent the parent directory
* @param child the potential child directory
* @return true if child is direct child of parent, false otherwise
*/","* Probe for a path being a parent of another
   * @param parent parent path
   * @param child possible child path
   * @return true if the parent's path matches the start of the child's",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,isSameFS,"org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2306,2312,"/**
* Checks if two qualified paths have the same namespace and different file names.
* @param qualPath1 first path to compare
* @param qualPath2 second path to compare
* @return true if paths differ only in file name, false otherwise
*/","* Are qualSrc and qualDst of the same file system?
   * @param qualPath1 - fully qualified path
   * @param qualPath2 - fully qualified path
   * @return is same fs true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,deleteOnExit,org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path),1804,1812,"/**
* Checks and deletes file with specified path.
* @param f Path to the file
* @return true if successful, false otherwise
*/","* Mark a path to be deleted when its FileSystem is closed.
   * When the JVM shuts down cleanly, all cached FileSystem objects will be
   * closed automatically. These the marked paths will be deleted as a result.
   *
   * If a FileSystem instance is not cached, i.e. has been created with
   * {@link #createFileSystem(URI, Configuration)}, then the paths will
   * be deleted in when {@link #close()} is called on that instance.
   *
   * The path must exist in the filesystem at the time of the method call;
   * it does not have to exist at the time of JVM shutdown.
   *
   * Notes
   * <ol>
   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>
   *   <li>The time to shut down a FileSystem will depends on the number of
   *   files to delete. For filesystems where the cost of checking
   *   for the existence of a file/directory and the actual delete operation
   *   (for example: object stores) is high, the time to shutdown the JVM can be
   *   significantly extended by over-use of this feature.</li>
   *   <li>Connectivity problems with a remote filesystem may delay shutdown
   *   further, and may cause the files to not be deleted.</li>
   * </ol>
   * @param f the path to delete.
   * @return  true if deleteOnExit is successful, otherwise false.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,processDeleteOnExit,org.apache.hadoop.fs.FileSystem:processDeleteOnExit(),1833,1848,"/**
* Deletes OnExit paths and masks their associated functions.
*/","* Delete all paths that were marked as delete-on-exit. This recursively
   * deletes all files and directories in the specified paths.
   *
   * The time to process this operation is {@code O(paths)}, with the actual
   * time dependent on the time for existence and deletion operations to
   * complete, successfully or not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,keystoreExists,org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists(),58,61,"/**
* Calculates and returns a function mask value.
* @throws IOException if an I/O error occurs during calculation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,compareTo,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode),140,151,"/**
* Computes a mask value based on the MRNflyNode's status.
* @param other another MRNflyNode for comparison
* @return integer mask value or -1/-0 if invalid statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getModificationTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime(),504,507,"/**
* Calls realStatus's m1() to retrieve its value.
* @return long result from realStatus.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getModificationTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime(),80,83,"/**
* Calls underlying file system's m1() method.
* @return result of underlying file system's m1() call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeFileName,org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String),261,268,"/**
* Returns masked filename based on system version.
* @param fname original filename
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,compareTo,org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData),593,596,"/**
* Calls m1 on underlying path data.
* @param o PathData object containing path information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path),98,101,"/**
* Checks if a file has a valid CRC mask.
* @param file Path object representing the file to check
*/","* Return true iff file is a checksum file name.
   *
   * @param file the file path.
   * @return if is checksum file true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path),130,133,"/**
* Checks if a file path matches a specific mask (file.ext.crc).
* @param file Path object to check
* @return true if the path matches the mask, false otherwise
*/","* Return true if file is a checksum file name.
   *
   * @param file the file path.
   * @return if file is a checksum file true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fixBlockLocations,"org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)",423,457,"/**
* Updates BlockLocation array based on given range and file offset.
* @param locations input BlockLocation array
* @param start start of range
* @param len length of range
* @param fileOffsetInHar file offset in HAR format
* @return updated BlockLocation array
*/","* Fix offset and length of block locations.
   * Note that this method modifies the original array.
   * @param locations block locations of har part file
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @param fileOffsetInHar the offset of the desired file in the har part file
   * @return block locations with fixed offset and length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus),411,413,"/**
* Recursively fetches status metadata from nested file.
* @param o FileStatus object to retrieve metadata from
*/","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[]),114,122,"/**
* Converts an array of FileStatus to an array of Paths.
* @param stats array of file statuses
* @return array of Paths or null if input is null
*/","* convert an array of FileStatus to an array of Path
   *
   * @param stats
   *          an array of FileStatus objects
   * @return an array of paths corresponding to the input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,addPartFileStatuses,org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path),1148,1152,"/**
* Processes files in the specified directory using a mask.
* @param path directory to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,merge,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1228,1242,"/**
* Merges two arrays of file statuses, excluding paths already present in the 'to' array.
* @param toStatuses source file statuses
* @param fromStatuses target file statuses
* @return merged array of file statuses or empty array if all paths are excluded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath(),529,532,"/**
* Calls realStatus.m1() to perform operation.
* @return Result of realStatus.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,merge,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1622,1636,"/**
* Computes file statuses by applying a mask to the provided statuses.
* @param toStatuses source file statuses
* @param fromStatuses target file statuses
* @return filtered array of file statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveIntermediate,org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path),2353,2361,"/**
* Resolves file system link for the given path.
* @param f input path to resolve
*/","* Resolves all symbolic links in the specified path leading up 
   * to, but not including the final path component.
   * @param f path to resolve
   * @return the new path object.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReplication,org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path),1603,1606,"/**
* Calls m1() and retrieves its m2 result.
* @param src source file path
*/","* Get the replication factor.
   *
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @param src file name
   * @return file replication
   * @throws FileNotFoundException if the path does not resolve.
   * @throws IOException an IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getReplication,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication(),499,502,"/**
* Returns the current status from the real status object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getReplication,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication(),75,78,"/**
* Calls underlying file system operation.
* @return status code from underlying file system
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBlockSize,org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path),2742,2745,"/**
* Calls m1() and retrieves its m2() result.
*/","* Get the block size for a particular file.
   * @param f the filename
   * @return the number of bytes in a block
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @throws FileNotFoundException if the path is not present
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getBlockSize,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize(),494,497,"/**
* Calls the m1() method on the RealStatus instance.
* @return result of the RealStatus.m1() method call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getBlockSize,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize(),70,73,"/**
* Calls underlying file system's m1() method to retrieve data.
* @return result of myFs.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getAccessTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime(),509,512,"/**
* Calls realStatus's m1() to fetch its status value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getAccessTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime(),85,88,"/**
* Calls underlying file system to perform operation 1. 
* @return result of operation 1 as a long value",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPermission,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission(),514,517,"/**
* Returns file system permission based on real status.
* @return FsPermission value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getPermission,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission(),90,93,"/**
* Returns file system permissions based on underlying FS implementation. 
* @return FsPermission object representing current permissions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions(),73,79,"/**
* Retrieves file status and permissions.
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,isPermissionLoaded,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded(),927,929,"/**
* Checks for mask condition based on parent's internal state.
* @return true if condition is met, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getOwner,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner(),519,522,"/**
* Delegates to realStatus's m1() method.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getOwner,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner(),95,98,"/**
* Calls underlying file system's m1() method.
* @return result from myFs.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getGroup,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup(),524,527,"/**
* Calls realStatus's m1() to retrieve its status.
* @return Status string from realStatus object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getGroup,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup(),100,103,"/**
* Calls underlying file system's m1() method. 
* @return result of file system's m1() operation 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,msync,org.apache.hadoop.fs.HarFileSystem:msync(),661,664,"/**
* Calls file system operation m1.
* @throws IOException if I/O error occurs
* @throws UnsupportedOperationException if unsupported operation is encountered
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,msync,org.apache.hadoop.fs.FilterFileSystem:msync(),465,468,"/**
* Calls underlying file system operation m1.
* @throws IOException if I/O error occurs
* @throws UnsupportedOperationException if not supported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(),1294,1298,"/**
 * Calls the underlying implementation of method m1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),2785,2787,"/**
* Calls default implementation of m1().","* Get the default replication for a path.
   * The given path will be used to locate the actual FileSystem to query.
   * The full path does not have to exist.
   * @param path of the file
   * @return default replication for the path's filesystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(),431,434,"/**
* Calls method m1 on instance fs and returns its result.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,cleanUp,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp(),4148,4159,"/**
* Updates internal data structures with masked values.
* @param data input data to process
*/","* Performs clean-up action when the associated thread is garbage
       * collected.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,<init>,"org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)",48,53,"/**
* Initializes an FsUrlConnection instance with a configuration and URL.
* @param conf Hadoop Configuration object
* @param url FTP or SFTP URL to connect to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,validatePositionedReadArgs,"org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)",102,116,"/**
* Validates and positions the byte array for masking operation.
* @param position index to start masking from
* @param buffer input byte array
* @param offset starting offset in buffer
* @param length number of bytes to mask
*/","* Validation code, available for use in subclasses.
   * @param position position: if negative an EOF exception is raised
   * @param buffer destination buffer
   * @param offset offset within the buffer
   * @param length length of bytes to read
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the amount of
   * data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkUploadId,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[]),80,85,"/**
* Validates and parses the provided upload ID.
* @param uploadId byte array containing the upload identifier
*/","* Utility method to validate uploadIDs.
   * @param uploadId Upload ID
   * @throws IllegalArgumentException invalid ID",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPartHandles,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map),92,100,"/**
* Validates and processes part handles in the provided map.
* @param partHandles map of PartHandle objects keyed by integer IDs
*/","* Utility method to validate partHandles.
   * @param partHandles handles
   * @throws IllegalArgumentException if the parts are invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/PathCapabilitiesSupport.java,validatePathCapabilityArgs,"org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)",42,49,"/**
* Computes a mask for the given file system capability.
* @param path file system path
* @return capability-specific mask as a string, or throws on invalid input
*/","* Validate the arguments to
   * {@link PathCapabilities#hasPathCapability(Path, String)}.
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return the string to use in a switch statement.
   * @throws IllegalArgumentException if a an argument is invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)",74,78,"/**
* Initializes an interrupt escalation service with a specified shutdown time.
* @param owner Service launcher instance
* @param shutdownTimeMillis Time in milliseconds until service is shut down
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,<init>,"org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)",78,83,"/**
* Constructs an IRQ handler with a given name and interrupt handler.
* @param name unique identifier for the handler
* @param handler Interrupted object to handle interrupts
*/","* Create an IRQ handler bound to the specific interrupt.
   * @param name signal name
   * @param handler handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,partition,"org.apache.hadoop.util.Lists:partition(java.util.List,int)",269,284,"/**
* Partitions the input list into pages of specified size.
* @param originalList list to be partitioned
* @param pageSize size of each page
* @return List of sublists, each representing a page
*/","* Returns consecutive sub-lists of a list, each of the same size
   * (the final list may be smaller).
   * @param originalList original big list.
   * @param pageSize desired size of each sublist ( last one
   *                 may be smaller)
   * @param <T> Generics Type.
   * @return a list of sub lists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,<init>,"org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)",105,113,"/**
* Initializes JSON serialization for a given class type.
* @param classType the class to serialize
* @param failOnUnknownProperties whether to fail on unknown properties during deserialization
* @param pretty whether to format output with indentation
*/","* Create an instance bound to a specific type.
   * @param classType class to marshall
   * @param failOnUnknownProperties fail if an unknown property is encountered.
   * @param pretty generate pretty (indented) output?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,fetch,"org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)",86,116,"/**
* Returns a Long value based on the given StatisticsData and key.
* @param data StatisticsData object
* @param key Statistic key (e.g. bytesRead, writeOps)
* @return Long value or null for unknown keys
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,<init>,"org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",47,54,"/**
* Initializes storage statistics from given I/O statistics.
* @param name unique identifier
* @param scheme storage scheme (e.g. file system)
* @param ioStatistics input/output statistics to base on
*/","* Instantiate.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param ioStatistics IOStatistics source.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/EmptyStorageStatistics.java,<init>,org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String),28,30,"/**
* Initializes empty storage statistics with given name.
* @param name identifier of the storage instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,<init>,"org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])",79,92,"/**
* Initializes UnionStorageStatistics with name and array of StorageStatistics.
* @param name unique identifier for union storage
* @param stats array of individual storage statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getScheme,org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme(),127,130,"/**
 * Delegates execution to underlying 'm1' method of Stats object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,org.apache.hadoop.fs.FileSystem:getStatistics(),4560,4567,"/**
* Retrieves a map of function masks keyed by unique identifiers.
*/","* Get the Map of Statistics object indexed by URI Scheme.
   * @return a Map having a key as URI scheme and value as Statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path),846,851,"/**
* Resolves file system hierarchy using inode tree.
* @param path input path to resolve
* @return list of strings representing resolved path components
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listXAttrs,org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path),387,390,"/**
* Executes file system operation on specified Path.
*@param path File system path to operate on
*@return list of strings or throws IOException if an error occurs 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",1275,1284,"/**
* Creates a new FSDataOutputStream with specified permissions and configuration.
* @param f the file to be created
* @param permission file system permissions
* @param flags creation flags (e.g. overwrite)
* @param bufferSize buffer size for I/O operations
* @param replication data block replication factor
* @param blockSize block size for storage
* @param progress progress monitor for write operation
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param permission file permission
   * @param flags {@link CreateFlag}s to use for this stream.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,create,"org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",201,212,"/**
* Wraps the core FSDataOutputStream creation in a method for consistency.
* @param f file path
* @return FSDataOutputStream object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1456,1463,"/**
* Creates a new FSDataOutputStream with specified permissions and settings.
* @param f the file path
* @param permission file system permissions
* @param overwrite whether to overwrite existing files
*/","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param permission file permission
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",222,229,"/**
* Delegates FSDataOutputStream creation to underlying file system.
* @param f Path to create output stream for
* @param permission File permissions
* @param flags Create flags
* @param bufferSize Buffer size
* @param replication Replication factor
* @param blockSize Block size
* @param progress Progress monitor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String),24,26,"/**
* Constructs an instance of PathAccessDeniedException with the specified path and error message.
* @param path the path that caused the access denial
*/",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String),26,28,"/**
* Constructs a PathPermissionException with the specified file system path.
* @param path file system path where the operation was denied
*/",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)",34,36,"/**
* Constructs a custom exception with error details for a specific file system path.
* @param path affected file system path
* @param error brief description of the issue
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String),26,28,"/**
* Constructs a PathNotFoundException with the specified path.
* @param path the non-existent file path
*/",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)",34,36,"/**
* Constructs a custom PathNotFoundException with specified path and error message.
* @param path affected file or directory path
* @param error detailed description of the exception cause
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathExistsException with the specified file path.
 * @param path unique file identifier
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,"org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)",30,32,"/**
* Constructs a custom exception for an invalid file system path.
* @param path the non-existent or inaccessible file system path
* @param error additional error information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,org.apache.hadoop.fs.PathIOException:<init>(java.lang.String),43,45,"/**
* Constructs an IOException with the specified file path.
* @param path file path associated with the exception
*/","* Constructor a generic I/O error exception
   *  @param path for the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ClosedIOException.java,<init>,"org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)",36,38,"/**
 * Constructs a new instance of ClosedIOException with a specified file path and error message.
 */","* Appends the custom error-message to the default error message.
   * @param path path that encountered the closed resource.
   * @param message custom error message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getThisBuilder,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder(),102,102,"/**
* Returns a mask defining the function's behavior. 
* Must be implemented by subclasses.",* Return the concrete implementation of the builder instance.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission),43,47,"/**
* Converts FsPermission to FsPermissionProto.
* @param p FsPermission object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkReturnValue,"org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1510,1518,"/**
* Sets file system permissions for a given path.
* @param rv indicates success or failure
* @param p the file path
* @param permission the desired FsPermission value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,write,org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput),179,183,"/**
* Writes legacy function mask data to output stream.
* @param out DataOutput stream to write to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toExtendedShort,org.apache.hadoop.fs.permission.FsPermission:toExtendedShort(),240,243,"/**
 * Returns the function mask value (m1).
 */","* Encodes the object to a short.  Unlike {@link #toShort()}, this method may
   * return values outside the fixed range 00000 - 01777 if extended features
   * are encoded into this permission, such as the ACL bit.
   *
   * @return short extended short representation of this permission",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toOctal,org.apache.hadoop.fs.permission.FsPermission:toOctal(),251,255,"/**
* Calculates a FUNC_MASK value from internal data.
* @return short integer representing the mask
*/","* Returns the FsPermission in an octal format.
   *
   * @return short Unlike {@link #toShort()} which provides a binary
   * representation, this method returns the standard octal style permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,hashCode,org.apache.hadoop.fs.permission.FsPermission:hashCode(),269,270,"/**
* Returns a functional mask value by calling the underlying function 'm1'. 
* @return Functional mask integer value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringInterner.java,internStringsInArray,org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[]),81,86,"/**
* Masks sensitive information in an array of strings using the m1 function.
* @param strings input string array
* @return modified string array with sensitive info masked
*/","* Interns all the strings in the given array in place,
   * returning the same array.
   *
   * @param strings strings.
   * @return internStringsInArray.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartProperty,org.apache.hadoop.conf.Configuration$Parser:handleStartProperty(),3267,3294,"/**
* Initializes configuration properties from reader.
* @param reader input data source
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isDir,org.apache.hadoop.fs.FileStatus:isDir(),240,243,"/**
* Returns a deprecated flag value.","* Old interface, instead use the explicit {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * @return true if this is a directory.
   * @deprecated Use {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isDirectory,org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path),446,453,"/**
* Tries to resolve file path and returns true if successful.
* @param f Path object to resolve
* @return true if resolution is successful, false otherwise
*/","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus 
   * returned by getFileStatus() or listStatus() methods.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processPath,org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData),133,143,"/**
* Applies mask operation to PathData based on its status.
* @param src input data with file attributes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isPathRecursable,org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData),418,420,"/**
* Checks if PathData item has valid file mask.
* @param item PathData object to check
* @return true if valid, false otherwise
*/","* Determines whether a {@link PathData} item is recursable. Default
   * implementation is to recurse directories but can be overridden to recurse
   * through symbolic links.
   *
   * @param item
   *          a {@link PathData} object
   * @return true if the item is recursable, false otherwise
   * @throws IOException
   *           if anything goes wrong in the user-implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,getAclEntries,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData),283,289,"/**
* Returns ACL entries based on the given PathData item.
* @param item PathData object to determine ACL entries from
*/","* Returns the ACL entries to use in the API call for the given path.  For a
     * recursive operation, returns all specified ACL entries if the item is a
     * directory or just the access ACL entries if the item is a file.  For a
     * non-recursive operation, returns all specified ACL entries.
     *
     * @param item PathData path to check
     * @return List<AclEntry> ACL entries to use in the API call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPathArgument,org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData),209,217,"/**
* Calls child-specific logic or delegates to parent based on summary flag.
* @param item PathData object containing file information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isDirectory,org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path),1877,1884,"/**
* Legacy method to check if file exists and has a valid metadata.
* @param f Path object representing the file to check
*/","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by getFileStatus() or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is directory true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory(),484,487,"/**
* Calls real status check.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isDirectory,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory(),60,63,"/**
* Delegates call to underlying file system implementation.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,applyNewPermission,org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus),48,54,"/**
* Calculates the function mask for a given file.
* @param file FileStatus object
* @return short representing permissions and flags
*/","* Apply permission against specified file and determine what the
   * new mode would be
   * @param file File against which to apply mode
   * @return File's new mode if applied.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isFile,org.apache.hadoop.fs.FileStatus:isFile(),220,222,"/**
* Evaluates a mask condition based on m1() and m2().
* @return true if both conditions are false, false otherwise
*/","* Is this a file?
   * @return true if this is a file",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,getSymlink,org.apache.hadoop.fs.FileStatus:getSymlink(),393,398,"/**
* Returns the path of the symbolic link.
* @throws IOException if the current path is not a symbolic link
*/","* @return The contents of the symbolic link.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink(),489,492,"/**
* Calls underlying status check and returns result.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink(),65,68,"/**
* Calls equivalent method on underlying file system.
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getFileLength,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength(),270,275,"/**
* Retrieves file length using M1/M2 function.
* @return File length in bytes or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength(),330,335,"/**
* Returns the length of the target file in bytes.
* @return file length as a 64-bit integer, or -1L if unknown
*/","* Calculate length of file if not already cached.
     * @return file length.
     * @throws IOException any IOE.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,totalPartsLen,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List),168,174,"/**
* Calculates total length of file system paths.
* @param partHandles list of file system path handles
* @return total length in bytes or -1 on IO error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLength,org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path),1912,1915,"/**
* Calculates a function mask using the provided file path.
* @param f Path to calculate the mask for
*/","* The number of bytes in a file.
   * @param f the path.
   * @return the number of bytes; 0 for a directory
   * @deprecated Use {@link #getFileStatus(Path)} instead.
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getLen,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen(),474,477,"/**
* Calls underlying status's m1() method.
* @return result of underling status's m1() method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getLen,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen(),50,53,"/**
* Calls the equivalent method on the underlying file system.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)",82,91,"/**
* Initializes FsServerDefaults object with specified parameters.
* @param blockSize block size
* @param bytesPerChecksum checksum size
* @param writePacketSize write packet size
* @param replication replication factor
* @param fileBufferSize file buffer size
* @param encryptDataTransfer encryption flag
* @param trashInterval trash interval
* @param checksumType checksum type
* @param keyProviderUri key provider URI
* @param storagepolicy storage policy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path),418,422,"/**
* Delegates execution of BlockStoragePolicySpi to underlying file system.
* @param src source path
* @return BlockStoragePolicySpi instance or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),914,919,"/**
* Resolves block storage policy for a given source path.
* @param src source file system path
* @return BlockStoragePolicySpi object or null if not found
*/","* Retrieve the storage policy for a given file or directory.
   *
   * @param src file or directory path.
   * @return storage policy for give file.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStoragePolicy,org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path),432,436,"/**
* Calls underlying file system's m1() to create block storage policy.
* @param src source path
* @return BlockStoragePolicySpi instance or null if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",816,822,"/**
* Resolves and delegates path-based operation to target file system.
* @param path  input path
* @param name  attribute name
* @param value attribute value
* @param flag  flags for attribute operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,setXAttr,"org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",1358,1362,"/**
* Creates or replaces extended attributes on a file.
* @param path file system path
* @param name attribute name
* @param value attribute value
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",365,369,"/**
* Calls underlying file system to perform operation 'm1'.
* @param path File system path
* @param name Attribute name
* @param value Attribute value (byte array)
* @param flag Operation flags
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,blockSize,org.apache.hadoop.fs.Options$CreateOpts:blockSize(long),46,48,"/**
* Creates a BlockSize instance from the given value.
* @param bs input block size value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bufferSize,org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int),49,51,"/**
* Creates a BufferSize instance from an integer value.
* @param bs buffer size in bytes
* @return BufferSize object representing the given size
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,repFac,org.apache.hadoop.fs.Options$CreateOpts:repFac(short),52,54,"/**
* Creates a ReplicationFactor instance from a short value.
* @param rf replication factor as short integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bytesPerChecksum,org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short),55,57,"/**
* Creates BytesPerChecksum object from given CRC value.
* @param crc short checksum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,checksumParam,org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt),58,61,"/**
* Creates a ChecksumParam object from a ChecksumOpt instance.
* @param csumOpt Checksum option data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,progress,org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable),62,64,"/**
* Creates a progress instance with mask functionality.
* @param prog Progressable object to be masked
* @return Masked Progress instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createParent,org.apache.hadoop.fs.Options$CreateOpts:createParent(),68,70,"/**
 * Returns a CreateParent instance with mask enabled.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,donotCreateParent,org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent(),71,73,"/**
 * Creates a CreateParent instance with mask set to false.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",32,36,"/**
* Constructs a PathAccessDeniedException with specified path, error message and optional cause.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a custom exception for file system permission issues.
 * @param path affected file or directory path
 * @param error detailed error message
 * @param cause underlying exception (optional)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a new instance of PathNotFoundException with the specified details.
 * @param path affected file system path
 * @param error descriptive error message
 * @param cause underlying exception that caused this exception (optional)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,"org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)",52,54,"/**
* Creates an instance of IOException with I/O-related error code.
* @param path affected file or directory path
* @param cause underlying cause of the exception
*/","* Appends the text of a Throwable to the default error message
   * @param path for the exception
   * @param cause a throwable to extract the error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path),794,800,"/**
* Resolves inode tree and delegates remaining path processing to target file system.
* @param path input path to resolve
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAcl,org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path),344,347,"/**
* Calls M1 on the underlying file system.
* @param path Path object representing the location to operate on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,seek,org.apache.hadoop.fs.AvroFSInput:seek(long),75,78,"/**
 * Invokes M1 operation on the underlying stream.
 * @param p long parameter value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",924,937,"/**
* Creates a HarFsInputStream instance for reading from a file.
* @param fs FileSystem instance
* @param path Path to the file
* @param start Start offset in bytes
* @param length Length of the data to read (must be non-negative)
* @param bufferSize Buffer size for underlying stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,skip,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long),1006,1022,"/**
* Calculates the maximum data chunk size for a given number.
* @param n input value
* @return maximum chunk size or 0 if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seek,org.apache.hadoop.io.SequenceFile$Reader:seek(long),2818,2824,"/**
* Decompresses data at specified position and updates internal state.
* @param position file offset to decompress
*/","* Set the current byte position in the input file.
     *
     * <p>The position passed must be a position returned by {@link
     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary
     * position, use {@link SequenceFile.Reader#sync(long)}. </p>
     *
     * @param position input position.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,"org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)",87,106,"/**
* Reads data from stream into buffer.
* @param b buffer to fill
* @param off offset in buffer
* @param len number of bytes to read
* @return actual number of bytes read or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,tell,org.apache.hadoop.fs.AvroFSInput:tell(),80,83,"/**
* Retrieves the function mask from the input stream.
* @throws IOException if an I/O error occurs while reading from the stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,available,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available(),939,946,"/**
* Calculates a mask value based on the remaining stream length.
* @return int representing the mask value, capped at Integer.MAX_VALUE
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readRecordLength,org.apache.hadoop.io.SequenceFile$Reader:readRecordLength(),2538,2558,"/**
* Reads and returns the function mask length from input stream.
* @throws IOException if file is corrupt or invalid
*/","* Read and return the next record length, potentially skipping over 
     * a sync block.
     * @return the length of the next record or -1 if there is no next record
     * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getPosition,org.apache.hadoop.io.SequenceFile$Reader:getPosition(),2873,2875,"/**
* Retrieves and returns a mask value from input stream.
* @throws IOException if an I/O error occurs.","* @return Return the current byte position in the input file.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setOwner,"org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",214,219,"/**
* Calls M1 and delegates to file system implementation for further processing.
* @param f Path object representing the file
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData),173,190,"/**
* Updates file system permissions for a given PathData item.
* @param item PathData object to modify
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setOwner,"org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",533,537,"/**
* Calls file system operation with provided path and credentials.
* @param p Path to operate on
* @param username User ID for access control
* @param groupname Group ID for access control
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,registerExpression,org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class),59,69,"/**
* Registers an expression factory with the given class.
* @param expressionClass Class to register
*/","* Invokes ""static void registerExpression(FindExpressionFactory)"" on the
   * given class. This method abstracts the contract between the factory and the
   * expression class. Do not assume that directly invoking registerExpression
   * on the given class will have the same effect.
   *
   * @param expressionClass
   *          class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,registerCommands,org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class),64,72,"/**
* Registers commands for the specified registrar class.
* @param registrarClass Class to register commands for
*/","* Invokes ""static void registerCommands(CommandFactory)"" on the given class.
   * This method abstracts the contract between the factory and the command
   * class.  Do not assume that directly invoking registerCommands on the
   * given class will have the same effect.
   * @param registrarClass class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,"org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",1308,1328,"/**
* Handles function mask error by creating a new RpcCall and invoking m1.
* @param t the Throwable object
* @param status the RpcStatusProto object (may be null)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),436,443,"/**
* Resolves file status for a given path in the underlying file system.
* @param f Path to resolve
* @return FileStatus object or null if not found
* @throws various exceptions on access or file system errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUri,org.apache.hadoop.fs.FilterFs:getUri(),179,182,"/**
* Resolves a URI based on file system operations.
* @return resolved URI object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path),544,547,"/**
* Calls realStatus.m1() with Path parameter.
* @param p input path to pass to realStatus.m1()",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(),968,972,"/**
* Retrieves a single byte from the buffer and returns its value.
* @return The retrieved byte value or -1 on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[]),978,982,"/**
* Calls m1 with full array and returns result.
* @param b input byte array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,seek,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long),1029,1034,"/**
* Advances underlying stream to specified position and updates local position.
* @param pos target position in the stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)",1058,1071,"/**
* Clips and fetches data from underlying stream.
* @param pos position in the stream
* @param b buffer to read into
* @param offset offset within the buffer
* @param length requested data length
* @return actual data length or -1 if invalid request
*/",* implementing position readable.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,readFully,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)",1076,1087,"/**
* Reads data from underlying stream.
* @param pos current position
* @param b buffer for data
* @param offset starting index in buffer
* @param length number of bytes to read
*/",* position readable again.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setReadahead,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long),1089,1092,"/**
* Delegates read-ahead operation to underlying stream.
* @param readahead amount of data to read ahead
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setDropBehind,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean),1094,1097,"/**
 * Calls underlying stream's 'm1' method with dropBehind parameter.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,getCurrentTrashDir,org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path),198,200,"/**
* Applies trash policy to the given file path.
* @param path the file path to apply policy to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,completed,"org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)",365,383,"/**
* Processes result and read data for given file range.
* @param result -1 indicates EOFException
* @param r file index
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expandLeftmost,org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset),86,146,"/**
* Parses glob pattern and generates list of alternative strings.
* @param filePatternWithOffset Glob pattern with offset
* @return List of alternative strings or null if invalid pattern
*/","* Expand the leftmost outer curly bracket pair containing a
   * slash character (""/"") in <code>filePattern</code>.
   * @param filePatternWithOffset
   * @return expanded file patterns
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatusBatch,"org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])",2060,2068,"/**
* Returns directory entries for a given file path.
* @param f the file path to retrieve directory entries for
* @return DirectoryEntries object containing file status array and other metadata
*/","* Given an opaque iteration token, return the next batch of entries in a
   * directory. This is a private API not meant for use by end users.
   * <p>
   * This method should be overridden by FileSystem subclasses that want to
   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.
   * @param f Path to list
   * @param token opaque iteration token returned by previous call, or null
   *              if this is the first call.
   * @return directory entries.
   * @throws FileNotFoundException when the path does not exist.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrCodec.java,encodeValue,"org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)",109,119,"/**
* Encodes a byte array using the specified codec.
* @param value input byte array
* @param encoding encoding scheme (HEX, BASE64, or UTF-8)
* @return encoded string or null if invalid encoding
*/","* Encode byte[] value to string representation with encoding. 
   * Values encoded as text strings are enclosed in double quotes (\""), 
   * while strings encoded as hexadecimal and base64 are prefixed with 
   * 0x and 0s, respectively.
   * @param value byte[] value
   * @param encoding encoding.
   * @return String string representation of value
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2076,2085,"/**
* Filters file statuses based on a provided filter, adding matching statuses to the result list.
* @param results list of filtered FileStatus objects
* @param f directory path to scan
* @param filter PathFilter instance for filtering files
*/","* Filter files/directories in the given path using the user-supplied path
   * filter. Results are added to the given array <code>results</code>.
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,<init>,"org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",43,46,"/**
* Constructs a new MetricsTag instance with provided info and value.
* @param info MetricsInfo object containing tag details
* @param value Tag value string
*/","* Construct the tag with name, description and value
   * @param info  of the tag
   * @param value of the tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounter.java,<init>,org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
* Initializes a new MutableCounter instance with given metrics information.
* @param info MetricsInfo object containing counter details.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGauge.java,<init>,org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
* Initializes MutableGauge with provided MetricsInfo.
* @param info metrics information (cannot be null)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,<init>,org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry),48,50,"/**
 * Initializes mutable rates instance with provided metrics registry.
 * @param registry metrics registry to be used
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)",34,37,"/**
* Initializes MetricsInfoImpl with name and description.
* @param name unique metric identifier
* @param description human-readable metric description
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,<init>,org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo),41,43,"/**
 * Initializes an AbstractMetric instance with given metrics information.
 * @param info MetricsInfo object containing metric details
 */","* Construct the metric
   * @param info  about the metric",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)",389,403,"/**
* Retrieves a new delegated token using Kerberos authentication.
* @param url Hadoop service URL
* @param token existing token with authentication information
* @param renewer user or service to delegate as
* @param doAsUser user to impersonate
* @return new delegationToken or null on failure
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",433,446,"/**
* Authenticates and fetches data from the given URL using Kerberos delegation.
* @param url URL to access
* @param token authentication token with delegating capabilities
* @param doAsUser user ID for which to perform operation
* @return operation result (long) or throws exception on failure","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",472,484,"/**
* Authenticates with Kerberos using a delegation token.
* @param url URL to authenticate against
* @param token Token containing the delegation token
* @param doAsUser User to impersonate during authentication
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",222,260,"/**
* Initializes ValueQueue with specified parameters and settings.
* @param numValues total number of values
* @param lowWatermark watermark threshold as a fraction (0 < x <= 1)
* @param expiry time-to-live for cache entries in milliseconds
* @param numFillerThreads number of threads to refill queues
* @param policy synchronization generation policy
* @param refiller queue refiller instance
*/","* Constructor takes the following tunable configuration parameters
   * @param numValues The number of values cached in the Queue for a
   *    particular key.
   * @param lowWatermark The ratio of (number of current entries/numValues)
   *    below which the <code>fillQueueForKey()</code> funciton will be
   *    invoked to fill the Queue.
   * @param expiry Expiry time after which the Key and associated Queue are
   *    evicted from the cache.
   * @param numFillerThreads Number of threads to use for the filler thread
   * @param policy The SyncGenerationPolicy to use when client
   *    calls ""getAtMost""
   * @param refiller implementation of the QueueRefiller",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Preconditions.java,checkNotNull,org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object),68,70,"/**
* Validates an object instance against a null check message.
* @param obj object to validate
*/","* <p>Preconditions that the specified argument is not {@code null},
   * throwing a NPE exception otherwise.
   *
   * <p>The message of the exception is
   * &quot;The validated object is null&quot;.</p>
   *
   * @param <T> the object type
   * @param obj  the object to check
   * @return the validated object
   * @throws NullPointerException if the object is {@code null}
   * @see #checkNotNull(Object, Object)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path),809,814,"/**
* Resolves ACL status for the given file system path.
* @param path file system path to resolve
* @return AclStatus object or throws IOException if an error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAclStatus,org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path),354,357,"/**
* Retrieves ACL status from file system.
* @param path file system path to query
* @return AclStatus object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,put,"org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)",73,93,"/**
* Retrieves or computes storage statistics by name.
* @param name unique identifier
* @param provider optional provider to compute stats
* @return StorageStatistics object or null if not found
*/","* Create or return the StorageStatistics object with the given name.
   *
   * @param name        The storage statistics object name.
   * @param provider    An object which can create a new StorageStatistics
   *                      object if needed.
   * @return            The StorageStatistics object with the given name.
   * @throws RuntimeException  If the StorageStatisticsProvider provides a null
   *                           object or a new StorageStatistics object with the
   *                           wrong name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,next,org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next(),127,139,"/**
* Fetches the current storage statistics with a specific mask.
* @return StorageStatistics object or throws NoSuchElementException if list is empty
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,clearStatistics,org.apache.hadoop.fs.FileSystem:clearStatistics(),4610,4612,"/**
* Updates global storage statistics using m1() function.
*/",* Reset all statistics for all file systems.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(),410,410,"/**
* Constructs an UnknownCommandException with no command.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close(),258,266,"/**
* Closes the block upload data and deletes associated files.
* @throws IOException on I/O errors
*/","* Close: closes any upload stream and byteArray provided in the
     * constructor.
     *
     * @throws IOException inherited exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,copyFileUnbuffered,"org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)",1138,1161,"/**
* Copies a file using platform-specific native method or standard Java I/O.
* @param src source file to copy
* @param dst destination file path
*/","* Unbuffered file copy from src to dst without tainting OS buffer cache
   *
   * In POSIX platform:
   * It uses FileChannel#transferTo() which internally attempts
   * unbuffered IO on OS with native sendfile64() support and falls back to
   * buffered IO otherwise.
   *
   * It minimizes the number of FileChannel#transferTo call by passing the the
   * src file size directly instead of a smaller size as the 3rd parameter.
   * This saves the number of sendfile64() system call when native sendfile64()
   * is supported. In the two fall back cases where sendfile is not supported,
   * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,
   * respectively.
   *
   * In Windows Platform:
   * It uses its own native wrapper of CopyFileEx with COPY_FILE_NO_BUFFERING
   * flag, which is supported on Windows Server 2008 and above.
   *
   * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows
   * platform. Unfortunately, the wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0)
   * used by FileChannel#transferTo for unbuffered IO is not implemented on Windows.
   * Based on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0
   * on Windows simply returns IOS_UNSUPPORTED.
   *
   * Note: This simple native wrapper does minimal parameter checking before copy and
   * consistency check (e.g., size) after copy.
   * It is recommended to use wrapper function like
   * the Storage#nativeCopyFileUnbuffered() function in hadoop-hdfs with pre/post copy
   * checks.
   *
   * @param src                  The source path
   * @param dst                  The destination path
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStream,org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable),276,280,"/**
* Closes and masks the provided input stream. 
* @param stream input stream to close and mask
*/","* Closes the stream ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param stream the Stream to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStreams,org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[]),288,292,"/**
* Closes all provided Closeable streams and/or resources.
* @param streams zero or more Closeable objects to be closed
*/","* Closes the streams ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param streams the Streams to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop(),207,218,"/**
* Stops data processing and releases resources.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,close,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close(),111,117,"/**
* Flushes random generator output to underlying stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,close,org.apache.hadoop.crypto.random.OsSecureRandom:close(),120,126,"/**
* Closes and logs the current output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,diskIoCheckWithoutNativeIo,org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File),283,302,"/**
* Deletes a file and updates its metadata.
* @param file File object to be deleted
*/","* Try to perform some disk IO by writing to the given file
   * without using Native IO.
   *
   * @param file
   * @throws IOException if there was a non-retriable error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hflush,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush(),501,504,"/**
 * Applies mask to data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hsync,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync(),510,514,"/**
* Performs mask-related operations.
* Invokes methods m1 and m3 through fos, an object of unknown type.","* HSync calls sync on fhe file descriptor after a local flush() call.
     * @throws IOException failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,close,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close(),95,105,"/**
* Updates function mask with statistics and logs.
*@param failed boolean indicating failed state
*/","* Set the finished time and then update the statistics.
   * If the operation failed then the key + .failures counter will be
   * incremented by one.
   * The operation min/mean/max values will be updated with the duration;
   * on a failure these will all be the .failures metrics.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,skip,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long),274,283,"/**
* Reads and skips specified number of bytes in the stream.
* @param n number of bytes to skip
* @return number of skipped bytes or 0 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)",479,488,"/**
* Writes data to output stream and updates write statistics.
* @throws IOException if write operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int),490,499,"/**
* Writes data to file stream and updates IO statistics.
* @param b data to write
* @throws IOException if write operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasCapability,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String),516,527,"/**
* Checks if a given capability is a mask for functional capabilities.
* @param capability the capability to check
* @return true if it's an IO statistics or context capability, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)",52,58,"/**
* Initializes a PartialListing object with the given path and data or an exception.
* @param listedPath path of the listing
* @param partialListing list of results (can be null)
* @param exception remote exception (can be null)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,setCount,org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int),78,83,"/**
* Updates and returns the previous buffer size.
* @param newCount new buffer size
*/","* Set the count for the current buf.
     * @param newCount the new count to set
     * @return the original count",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,"org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)",60,65,"/**
* Initializes a CallReturn object with return value and/or exception.
* @param r the return value (or null)
* @param t the thrown exception (or null)
* @param s the current program state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getConnectorAddress,org.apache.hadoop.http.HttpServer2:getConnectorAddress(int),1330,1342,"/**
* Retrieves an InetSocketAddress from the web server's connectors list.
* @param index position of the connector in the list
* @return InetSocketAddress object or null if invalid or not found
*/","* Get the address that corresponds to a particular connector.
   *
   * @param index index.
   * @return the corresponding address for the connector, or null if there's no
   *         such connector or the connector is not bounded or was closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",63,80,"/**
* Initializes a block of IV bytes with a counters and mask.
* @param initIV initial IV bytes
* @param counter counter value to incorporate into IV blocks
* @param iv resulting IV byte array
* @param blockSize size of each IV block
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",55,72,"/**
* Updates IV with masked block data.
* @param initIV initial IV bytes
* @param counter counter value to mask
* @param iv updated IV array
* @param blockSize size of each block
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,<init>,"org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)",125,151,"/**
* Initializes GcTimeMonitor with specified parameters.
* @param observationWindowMs time window for monitoring
* @param sleepIntervalMs interval between checks
* @param maxGcTimePercentage maximum GC time percentage threshold
* @param alertHandler handler for sending alerts
*/","* Create an instance of GCTimeMonitor. Once it's started, it will stay alive
   * and monitor GC time percentage until shutdown() is called. If you don't
   * put a limit on the number of GCTimeMonitor instances that you create, and
   * alertHandler != null, you should necessarily call shutdown() once the given
   * instance is not needed. Otherwise, you may create a memory leak, because
   * each running GCTimeMonitor will keep its alertHandler object in memory,
   * which in turn may reference and keep in memory many more other objects.
   *
   * @param observationWindowMs the interval over which the percentage
   *   of GC time should be calculated. A practical value would be somewhere
   *   between 30 sec and several minutes.
   * @param sleepIntervalMs how frequently this thread should wake up to check
   *   GC timings. This is also a frequency with which alertHandler will be
   *   invoked if GC time percentage exceeds the specified limit. A practical
   *   value would likely be 500..1000 ms.
   * @param maxGcTimePercentage A GC time percentage limit (0..100) within
   *   observationWindowMs. Once this is exceeded, alertHandler will be
   *   invoked every sleepIntervalMs milliseconds until GC time percentage
   *   falls below this limit.
   * @param alertHandler a single method in this interface is invoked when GC
   *   time percentage exceeds the specified limit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ServletUtil.java,getRawPath,"org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)",107,110,"/**
* Generates function mask by combining servlet name and path.
* @param request HTTP request object
* @param servletName unique identifier for the servlet
*/","* Parse the path component from the given request and return w/o decoding.
   * @param request Http request to parse
   * @param servletName the name of servlet that precedes the path
   * @return path component, null if the default charset is not supported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getMovableTypes,org.apache.hadoop.fs.StorageType:getMovableTypes(),78,80,"/**
* Returns a list of supported storage types.
* @return List of StorageType objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getTypesSupportingQuota,org.apache.hadoop.fs.StorageType:getTypesSupportingQuota(),82,84,"/**
* Returns list of storage types based on masking function. 
* @return List of StorageType objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,parseStorageType,org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String),90,92,"/**
* Converts string to m2 value using StringUtils.m1 and returns corresponding StorageType. 
* @param s input string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initMode,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode(),631,638,"/**
* Determines the system initialization mode from property or environment variable.
* @return InitMode object representing the initialization mode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",838,844,"/**
* Resolves file system hierarchy by path and fetches metadata.
* @param path filesystem path
* @param names list of attribute names to retrieve
* @return Map of attribute names to their corresponding byte values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,"org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",381,385,"/**
* Delegates file system operation to underlying storage.
* @param path file system path
* @param names list of file names
* @return map of file metadata or throws IOException if fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,unbuffer,org.apache.hadoop.fs.FSDataInputStream:unbuffer(),237,240,"/**
* Invokes M1 operation on Stream Capabilities Policy.
* @param in input stream (assumed parameter)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,equals,org.apache.hadoop.fs.FileStatus:equals(java.lang.Object),435,445,"/**
* Compares two FileStatus objects using deep equality.
* @param o FileStatus object to compare with
* @return true if equal, false otherwise
*/","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,equals,org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object),598,603,"/**
* Verifies and processes a PathData object.
* @param o PathData object to process
* @return true if valid and processed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,hashCode,org.apache.hadoop.fs.FileStatus:hashCode(),453,456,"/**
* Calls m1() to fetch intermediate result and delegates execution of m2().
*/","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,hashCode,org.apache.hadoop.fs.shell.PathData:hashCode(),605,608,"/**
* Calls and returns result of path.m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path),534,537,"/**
* Calls realStatus's m1() method with provided Path parameter.
* @param p input path to be processed by realStatus
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,calculateFolderSize,org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String),38,43,"/**
* Calculates file system size mask based on specified folder.
* @param folder directory path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,check,org.apache.hadoop.fs.DUHelper:check(java.lang.String),45,53,"/**
* Calculates and formats disk usage statistics for a given folder.
* @param folder path to the folder
* @return formatted string with used files, total disk space, and usage percentage
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",853,858,"/**
* Resolves and updates the target file system with a new name.
* @param path Path to resolve
* @param name New name for the resolved target file system
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeXAttr,"org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",392,395,"/**
* Calls underlying file system's m1 operation.
* @param path directory path to operate on
* @param name related data item name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,setSamplesAndSum,"org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)",157,161,"/**
* Updates mask with new sum and increments counter.
* @param sampleCount current count of samples
* @param newSum new sum value for updating mask
*/","* Set the sum and samples.
   * Synchronized.
   * @param sampleCount new sample count.
   * @param newSum new sum",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,add,org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic),212,230,"/**
* Merges two MeanStatistic objects into one.
* @param other MeanStatistic object to merge with
* @return merged MeanStatistic object
*/","* Add another MeanStatistic.
   * @param other other value
   * @return mean statistic.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,equals,org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object),254,269,"/**
* Compares object for functional equality using m1(), m2(), m3(), and m4().
* @param o Object to compare
* @return true if functionally equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,toString,org.apache.hadoop.fs.statistics.MeanStatistic:toString(),285,289,"/**
* Formats a string representation of masked function data.
* @return formatted string with sample count, total, and mean value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)",133,149,"/**
* Formats a StringBuilder with key-value pairs from a map, separated by a separator.
* @param sb StringBuilder to append the formatted string
* @param type Type identifier (e.g. ""User"")
* @param map Map of key-value pairs
* @param separator Separator between key-value pairs
*/","* Given a map, add its entryset to the string.
   * The entries are only sorted if the source entryset
   * iterator is sorted, such as from a TreeMap.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param separator separator
   * @param <E> type of values of the map",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,entryToString,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry),136,139,"/**
* Invokes recursive M3 function with key-value pair's components.
* @param entry a map entry containing key and value
*/","* Convert an entry to the string format used in logging.
   *
   * @param entry entry to evaluate
   * @param <E> entry type
   * @return formatted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)",48,50,"/**
* Increments duration tracker with given key and count. 
* @param key unique tracking identifier
* @param count duration value to add (in milliseconds)
*/","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   *
   * The statistics counter with the key name will be incremented
   * by the given count.
   *
   * The expected use is within a try-with-resources clause.
   *
   * The default implementation returns a stub duration tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return an object to close after an operation completes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/EmptyPrefetchingStatistics.java,prefetchOperationStarted,org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted(),45,48,"/**
 * Returns the duration tracker with the specified mask value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLong,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String),97,104,"/**
* Retrieves a unique identifier mask for the given key.
* @param key unique string key
* @return Long value representing the mask or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,isTracked,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String),106,110,"/**
* Checks if the given key matches any mask in user preferences.
* @param key unique identifier to check against masks
* @return true if key matches a mask, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,toLongStatistic,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry),85,87,"/**
* Creates a statistic from map entry key-value pairs.
* @param e map entry containing metric values
*/","* Convert a counter/gauge entry to a long statistics.
   * @param e entry
   * @return statistic",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,next,org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next(),70,78,"/**
* Calculates a statistic using the current key and data.
* @return statistic value as a Long object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,<init>,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(),51,53,"/**
* Initializes map of statistics with passthrough function.
*/",* Construct with the copy function being simple passthrough.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addCounterFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)",91,93,"/**
* Evaluates function with specified mask and key.
* @param key identifier for evaluation
* @param eval function to be evaluated
*/","* add a mapping of a key to a counter function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addGaugeFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)",100,102,"/**
* Evaluates a gauge with a custom function.
* @param key unique gauge identifier
* @param eval custom function to apply to key
*/","* add a mapping of a key to a gauge function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMinimumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)",109,111,"/**
* Evaluates the specified function on the given key and updates the minimums. 
* @param key unique identifier
* @param eval function to be evaluated (returns a Long value)
*/","* add a mapping of a key to a minimum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMaximumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)",118,120,"/**
* Evaluates and stores function result in maximums.m1.
* @param key unique identifier
* @param eval function to evaluate
*/","* add a mapping of a key to a maximum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)",127,130,"/**
* Evaluates mean statistic using provided function.
* @param key unique identifier
* @param eval statistical evaluation function
*/","* add a mapping of a key to a meanStatistic function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,wrap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics),116,118,"/**
* Returns an IOStatisticsSource instance wrapping the provided statistics.
* @param statistics IOStatistics object to be wrapped
*/","* Take an IOStatistics instance and wrap it in a source.
   * @param statistics statistics.
   * @return a source which will return the values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getAggregator,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator(),49,52,"/**
* Returns an empty IO statistics aggregator instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatisticsStore,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore(),107,109,"/**
* Returns an empty IO statistics store with default mask.
* @return EmptyIOStatisticsStore instance with default mask
*/","* Get the shared instance of the immutable empty statistics
   * store.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getIOStatistics,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics(),54,57,"/**
* Returns an empty IO statistics instance. 
* @return EmptyIOStatistics object",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics(),98,100,"/**
* Returns an empty IO statistics instance.
*/","* Get the shared instance of the immutable empty statistics
   * object.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)",172,176,"/**
* Sets a counter value with the given key and value.
* @param key unique identifier for the counter
* @param value new value for the counter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)",199,202,"/**
* Sets a mask value in the underlying map.
* @param key unique identifier for the mask
* @param value new value to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)",209,212,"/**
* Updates minimum map value for given key.
* @param key unique identifier (key in minumum map)
* @param value new value to store in minimum map
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)",235,238,"/**
* Updates gauge map with new value using provided key.
* @param key unique identifier for gauge map entry
* @param value new value to be stored in map
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)",178,197,"/**
* Increments or retrieves a counter value based on the provided key and value.
* @param key unique identifier for the counter
* @param value increment value (must be non-negative)
* @return current counter value after incrementing or null if unknown key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)",204,207,"/**
* Calculates and returns the FUNC_MASK value based on the given key and value.
* @param key unique identifier or filter criteria
* @param value associated numeric value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)",214,217,"/**
* Computes function mask using minimum map and provided value.
* @param key unique key identifier
* @param value input value to compute with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)",240,243,"/**
* Calculates functional mask based on key and value. 
* @param key unique identifier
* @param value associated value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)",219,225,"/**
* Updates or initializes minimum value in map.
* @param key unique identifier for map entry
* @param value new minimum value to store
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)",227,233,"/**
* Updates atomic mask value in maximum map.
* @param key unique identifier for the mask
* @param value new mask value to update with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)",253,259,"/**
* Updates mask statistic in MeanStatistic map.
* @param key unique identifier
* @param value new mask value to update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String),372,375,"/**
* Retrieves and returns a mask value from the counter map using the provided key.
* @param key unique identifier or key to look up in the counter map
*/","* Get a reference to the atomic instance providing the
   * value for a specific counter. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String),385,388,"/**
* Retrieves atomic long value associated with given key from maximum map.
* @param key unique identifier of the value to fetch
*/","* Get a reference to the atomic instance providing the
   * value for a specific maximum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String),398,401,"/**
* Retrieves the minimum value mask from the minimum map.
* @param key unique key identifier
*/","* Get a reference to the atomic instance providing the
   * value for a specific minimum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String),411,414,"/**
* Retrieves the atomic long value associated with the given key from gaugeMap.
* @param key unique identifier for the atomic long value
*/","* Get a reference to the atomic instance providing the
   * value for a specific gauge. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String),422,425,"/**
* Retrieves the mean statistic for the given key.
* @param key unique identifier for the statistic
*/","* Get a mean statistic.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,asDuration,org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration(),87,90,"/**
* Delegates to firstDuration's m1() method.
* @return Duration object from firstDuration's calculation.",* @return the global duration,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,counters,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters(),55,58,"/**
* Calls m1() to retrieve intermediate results and then invokes m2() on them.
* @return A map of string keys to long values, as produced by m1().m2()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,gauges,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges(),79,82,"/**
* Calls m1() to retrieve intermediate data and then calls m2() on it.
* @return Map of String to Long values from m1()'s result.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,minimums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums(),84,87,"/**
* Calls m1() to delegate execution and returns the result's m2() output.
* @return Map of String to Long values from m1()'s m2() result",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,maximums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums(),89,92,"/**
* Calls m1(), then delegates to its m2() result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics(),94,97,"/**
* Calls m1() to retrieve intermediate results and then calls its m2() method. 
* @return Map of statistics from m1()'s m2() method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,setWrapped,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics),73,77,"/**
* Sets the wrapped statistics object.
* @param wrapped new statistics object to be wrapped
*/","* Set the wrapped statistics.
   * Will fail if the field is already set.
   * @param wrapped new value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,activeInstance,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance(),63,66,"/**
* Returns the dynamically generated I/O statistics instance.
* @return DynamicIOStatistics object or null if not built
*/","* Get the statistics instance.
   * @return the instance to build/return
   * @throws IllegalStateException if the builder has already been built.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,checkMutable,org.apache.hadoop.fs.impl.FlagSet:checkMutable(),125,128,"/**
 * Validates that FlagSet is mutable before performing an operation. 
 * @throws IllegalArgumentException if FlagSet is immutable.","* Check for mutability before any mutating operation.
   * @throws IllegalStateException if the set is still mutable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toByteArray,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray(),235,250,"/**
* Loads block data from file or stream.
* @throws IOException on I/O errors
*/","* Convert to a byte array.
     * If the data is stored in a file, it will be read and returned.
     * If the data was passed in via an input stream (which happens if the
     * data is stored in a bytebuffer) then it will be converted to a byte
     * array -which will then be cached for any subsequent use.
     *
     * @return byte[] after converting the uploadBlock.
     * @throws IOException throw if an exception is caught while reading
     *                     File/InputStream or closing InputStream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,bind,org.apache.hadoop.service.launcher.IrqHandler:bind(),89,100,"/**
* Initializes a signal handler with the given name.
* @throws IllegalArgumentException if initialization fails
*/","* Bind to the interrupt handler.
   * @throws IllegalArgumentException if the exception could not be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CloseableReferenceCount.java,unreference,org.apache.hadoop.util.CloseableReferenceCount:unreference(),65,70,"/**
* Checks if the object is closed based on its reference count.
* @return true if the object is closed, false otherwise
*/","* Decrement the reference count.
   *
   * @return          True if the object is closed and has no outstanding
   *                  references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,run,org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run(),285,296,"/**
* Continuously executes a masked operation until interrupted.
* @throws InterruptedException if interrupted while running
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setZooKeeperRef,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper),1226,1231,"/**
* Sets ZooKeeper instance and notifies listeners.
* @param zk ZooKeeper connection to use
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)",214,221,"/**
* Creates a deep copy of the provided map's values using the specified function.
* @param source original map
* @param copyFn function to apply to each value during copying
* @return copied map with same keys and updated values
*/","* Take a snapshot of a supplied map, using the copy function
   * to replicate the source values.
   * @param source source map
   * @param copyFn function to copy the value
   * @param <E> type of values.
   * @return a concurrent hash map referencing the same values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",445,450,"/**
* Computes and returns a masked result using the provided factory, statistic, and input.
* @param factory DurationTrackerFactory instance
* @param statistic statistic to be processed
* @param input CallableRaisingIOE containing computation logic
* @return B result object or throws IOException if an error occurs","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the result of the operation.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,pairedTrackerFactory,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",687,691,"/**
* Combines two duration tracker factories into a single factory.
* @param first primary factory
* @param second secondary factory
*/","* Create a DurationTrackerFactory which aggregates the tracking
   * of two other factories.
   * @param first first tracker factory
   * @param second second tracker factory
   * @return a factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",60,66,"/**
* Applies mask-related metrics to the MetricsRecordBuilder.
* @param builder MetricsRecordBuilder instance
* @param all whether to apply mask for all or current state only
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit(),82,84,"/**
* Returns a cached function mask value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared(),86,88,"/**
* Returns the function mask value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated(),90,92,"/**
 * Retrieves the cached mask value. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected(),358,360,"/**
* Returns functional mask value from RPC client.","* Returns the number of disconnected backoffs.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcSlowCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls(),420,422,"/**
* Returns the mask value from RPC slow calls.
* @return The masked value as a long integer
*/","* Returns the number of slow calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls(),428,431,"/**
* Returns the function mask value from RPC requeue calls.
* @return The function mask value as a long integer.","* Returns the number of requeue calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,counters,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters(),48,51,"/**
* Calls m1() to fetch intermediate data and then calls m2() on it.
* @return Map of String to Long values from m1().m2()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,gauges,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges(),53,56,"/**
 * Delegates to m1() to fetch nested data, returning it as a map of string-long pairs. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,minimums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums(),58,61,"/**
* Calls and delegates to m1(), then calls its m2() method.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,maximums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums(),63,66,"/**
* Calls m1() and delegates to its m2() method.
* @return Map of String keys to Long values from m1()'s m2()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics(),68,71,"/**
 * Calls and returns the result of m2() from the map returned by m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,aggregate,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),73,76,"/**
* Calls m1() to execute m2 with provided statistics.
* @param statistics optional IOStatistics object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)",78,81,"/**
 * Invokes delegate's m2() method with provided key and value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)",83,86,"/**
* Calls nested operation on internal state with given key and value.
* @param key unique identifier
* @param value associated value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)",88,91,"/**
* Calls m1() and delegates execution of m2 with provided key and value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)",93,96,"/**
* Delegates m2 call to underlying object.
* @param key string key
* @param value long value",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)",98,101,"/**
* Calls M1's m2 method with provided key and value.
* @param key unique identifier
* @param value associated data (long)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)",103,106,"/**
* Delegates to underlying storage implementation to update metric value.
* @param key unique metric identifier
* @param value new value for the metric
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)",108,112,"/**
* Invokes nested method m2 with provided key and value.
* @param key unique identifier (passed to nested method)
* @param value associated value (passed to nested method)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)",114,118,"/**
* Delegates execution of m2 to the wrapped object. 
* @param key data key
* @param value data value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)",120,124,"/**
* Calls the nested method m2 on the object returned by m1.
* @param key unique identifier for the operation
* @param value associated data value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)",126,129,"/**
* Calls m1() to delegate execution of m2 operation.
* @param key key string
* @param value associated value (long)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",131,135,"/**
* Calls M1 to process key and value.
* @param key unique identifier
* @param value MeanStatistic object to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",137,141,"/**
* Calls delegate's second method with provided key and value.
* @param key arbitrary key string
* @param value arbitrary long value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,reset,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset(),143,146,"/**
* Calls m2() on the result of m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String),148,151,"/**
* Calls underlying cache to fetch value associated with given key.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String),153,156,"/**
* Delegate to m1() to perform M2 operation.
* @param key input parameter for M2 operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String),158,161,"/**
* Calls underlying cache with given key to fetch atomic value.
* @param key unique identifier of cached value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String),163,166,"/**
* Delegate to M1's m2 method with given key.
* @param key unique key string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String),168,171,"/**
 * Calls m1() to fetch intermediate statistics and then invokes m2() on the result with the given key. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)",173,178,"/**
* Calls m2 on the result of m1 with provided prefix and duration.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",180,184,"/**
 * Calls m1() and delegates to its m2() with provided prefix and duration.
 * @param prefix prefix string
 * @param duration time interval
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withDurationTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[]),77,93,"/**
* Configures IO statistics store builder with custom prefixes.
* @param prefixes zero or more unique prefix strings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withSampleTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[]),95,105,"/**
* Applies a set of prefix-based configurations to the builder.
* @param prefixes variable-length array of configuration prefixes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,reset,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset(),101,105,"/**
* Clears IO statistics context and logs the action.
* @param id unique identifier for logging purposes
*/",* Reset the thread +.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(),113,115,"/**
* Initializes an IO statistics snapshot object.
* Creates internal data structures.",* Construct.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setCounter,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)",226,229,"/**
* Stores a key-value pair in the underlying data structure.
* @param key unique identifier
* @param value associated value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setGauge,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)",231,235,"/**
* Sets a mask value in the underlying data structure.
* @param key unique identifier of the mask to set
* @param value new value for the specified mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMaximum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)",237,241,"/**
 * Updates a function mask with the given key and value.
 * @param key unique identifier for the function mask
 * @param value new value for the function mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMinimum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)",243,246,"/**
* Updates function mask with given key and value.
* @param key unique identifier for function mask
* @param value associated value for the key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",248,251,"/**
 * Updates mean statistic using provided key and value. 
 * @param key unique identifier for statistics update
 * @param value MeanStatistic object to be updated
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,enabled,org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled(),95,97,"/**
* Returns true if m1() returns true.
* @return result of IOStatisticsContextIntegration.m1()
*/","* Static probe to check if the thread-level IO statistics enabled.
   *
   * @return if the thread-level IO statistics enabled.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,retrieveIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object),78,88,"/**
* Retrieves IO statistics from the given source object.
* @param source Object containing IO statistics
* @return IOStatistics object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>(),123,133,"/**
* Initializes BuiltInGzipDecompressor with default state.
*/",* Creates a new (pure Java) gzip decompressor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,available,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available(),189,192,"/**
* Combines child and parent class results of m1().
* @return combined result of m1() from both classes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,available,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available(),225,228,"/**
* Calculates combined result by calling child and parent methods. 
* @throws IOException if an I/O error occurs during computation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long),221,227,"/**
* Evaluates whether data exists at the given position.
* @param targetPos position to check
* @return true if data is present or has been updated, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,readChunk,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",229,267,"/**
* Reads data and checksum from file, updating internal state.
* @param pos current file position
* @param buf buffer to read into
* @param offset buffer offset
* @param len number of bytes to read
* @param checksum checksum array (may be null)
* @return number of bytes read or -1 on EOF
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,verifySums,"org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)",336,359,"/**
* Verifies the checksum of a byte array chunk.
* @param b the byte array to verify
* @param off offset into the byte array
* @throws ChecksumException if verification fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,throwChecksumException,"org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)",514,521,"/**
* Throws ChecksumException if checksum mismatch occurs.
* @param type checksum type
* @param algorithm checksum algorithm
* @param filename file name where mismatch occurred
* @param errPos error position in file
* @param expected expected checksum value
* @param computed actual checksum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getCounter,org.apache.hadoop.crypto.CryptoInputStream:getCounter(long),288,290,"/**
* Calculates a function mask value based on input position.
* @param position input position to calculate mask from
* @return calculated function mask value as a long integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPadding,org.apache.hadoop.crypto.CryptoInputStream:getPadding(long),292,294,"/**
* Calculates function mask from given position.
* @param position position value to calculate mask from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,updateEncryptor,org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor(),220,228,"/**
* Encrypts and initializes encryption context with a given key.
* @throws IOException if an I/O error occurs
*/",Update the {@link #encryptor}: calculate counter and {@link #padding}.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkBufferSize,"org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)",90,95,"/**
* Calculates the optimal buffer size for crypto operations.
* @param codec CryptoCodec object
* @param bufferSize initial buffer size
* @return adjusted buffer size, ensuring it's a multiple of codec's minimum block size
*/","* Check and floor buffer size.
   *
   * @param codec crypto codec.
   * @param bufferSize the size of the buffer to be used.
   * @return calc buffer size.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,link,"org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)",1080,1087,"/**
* Copies source file to destination using native method or fallback implementation.
* @param src source file
* @param dst destination file
*/","* Creates a hardlink ""dst"" that points to ""src"".
   *
   * This is deprecated since JDK7 NIO can create hardlinks via the
   * {@link java.nio.file.Files} API.
   *
   * @param src source file
   * @param dst hardlink location
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getInstance,org.apache.hadoop.fs.DelegationTokenRenewer:getInstance(),200,205,"/**
* Returns singleton instance of DelegationTokenRenewer.
* @return DelegationTokenRenewer instance or null if not initialized
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,makeRequestIfNeeded,org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded(),84,96,"/**
* Handles mask-related logic based on current index state.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
* Initializes checksum calculator with CRC and MD5 parameters.
* @param bytesPerCRC number of bytes per CRC calculation
* @param crcPerBlock initial CRC value for each block
* @param md5 MD5 hash object
*/","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(),43,45,"/**
 * Constructs an MD5MD5CRC32FileChecksum object with default values.
 */","Same as this(0, 0, null)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
* Initializes an instance of the checksum class with specified parameters.
* @param bytesPerCRC number of bytes per CRC value
* @param crcPerBlock initial CRC value for each block
* @param md5 MD5 hash object
*/","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,accept,org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path),79,82,"/**
* Combines file pattern matching and user filter checks.
* @param path file system path to evaluate
* @return true if both conditions pass, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,set,org.apache.hadoop.fs.GlobPattern:set(java.lang.String),74,157,"/**
* Compiles a glob string into a regular expression pattern.
* @param glob the input glob string
*/","* Set and compile a glob pattern
   * @param glob  the glob pattern string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFsStatus,org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path),146,150,"/**
* Delegates file status retrieval to underlying FS implementation.
* @param f file path to query
* @return FsStatus object or throws an exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listStatusIterator,org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path),291,295,"/**
* Delegates file status iteration to underlying File System.
* @param f Path to iterate over
* @return iterator of FileStatus objects or null if not found
*/",Return a remote iterator for listing in a directory,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,isRegularFile,org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File),632,634,"/**
 * Calls m1 with default verbosity (true).
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,"org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)",696,703,"/**
* Resolves file path to canonical representation.
* @param file the file object
* @param makeCanonicalPath whether to resolve to absolute path or not
*/","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @param makeCanonicalPath
   *          Whether to make canonical path for the file passed
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,permissionsFromMode,org.apache.hadoop.fs.FileUtil:permissionsFromMode(int),783,793,"/**
* Extracts POSIX file permissions from a given mode.
* @param mode numeric file mode
*/","* The permission operation of this method only involves users, user groups, and others.
   * If SUID is set, only executable permissions are reserved.
   * @param mode Permissions are represented by numerical values
   * @return The original permissions for files are stored in collections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unpackEntries,"org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)",1130,1186,"/**
* Expands a Tar archive entry to the specified output directory.
* @param tis TarArchiveInputStream object
* @param entry TarArchiveEntry object being expanded
* @param outputDir target directory for expansion
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])",1084,1086,"/**
* Concatenates array of strings with specified separator.
* @param separator character to join strings
* @param strings array of string values
* @return concatenated string or empty string if null input
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execute,org.apache.hadoop.util.Shell$ShellCommandExecutor:execute(),1275,1283,"/**
* Validates and executes function mask operation.
* @throws IOException if invalid command string is encountered
*/","* Execute the shell command.
     * @throws IOException if the command fails, or if the command is
     * not well constructed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkWindowsCommandLineLength,org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[]),127,140,"/**
* Validates the combined length of shell commands.
* @param commands variable number of shell command strings
*/","* Checks if a given command (String[]) fits in the Windows maximum command
   * line length Note that the input is expected to already include space
   * delimiters, no extra count will be added for delimiters.
   *
   * @param commands command parts, including any space delimiters
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,buildPSScript,"org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)",115,158,"/**
* Builds and executes a PowerShell script to terminate a process at a specific host.
* @param processName name of the process to terminate
* @param host hostname where the process is running
* @return generated PowerShell script as a string or null on failure
*/","* Build a PowerShell script to kill a java.exe process in a remote machine.
   *
   * @param processName Name of the process to kill. This is an attribute in
   *                    CommandLine.
   * @param host Host where the process is.
   * @return Path of the PowerShell script.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toString,org.apache.hadoop.fs.permission.FsPermission:toString(),272,283,"/**
* Concatenates user, group, and other action symbols with optional sticky bit formatting.
*@return Formatted string representation of actions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)",1058,1060,"/**
* Joins iterable collection of objects into a string with specified separator.
* @param separator character to separate elements
* @param strings iterable collection of objects to be joined
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close(),710,722,"/**
* Calls superclass method m2 and performs cleanup.
* @throws IOException on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,close,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close(),644,653,"/**
* Invokes sub-methods and marks self as closed. 
* Calls m1(), then updates sums and datas, ensuring closure on completion.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,close,org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close(),391,400,"/**
* Invokes child methods and marks this instance as closed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,"org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)",36,41,"/**
* Initializes a DU instance for testing purposes.
* @param path directory to monitor
* @param interval monitoring interval in milliseconds
* @param jitter randomization factor for monitoring intervals
* @param initialUsed initial used disk space value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,refresh,org.apache.hadoop.fs.DU:refresh(),50,58,"/**
* Retrieves and logs disk usage information.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,connect,"org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)",123,183,"/**
* Establishes an SFTP connection to the specified host using the provided credentials and key file.
* @param host remote server hostname
* @param port remote server port (0 for default)
* @param user username or null to use system username
* @param password password or empty string if not used
* @param keyFile path to private key file or null if not used
* @return ChannelSftp object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,disconnect,org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp),185,211,"/**
* Manages SFTP connection by ID; closes and reinitializes if max connections exceeded.
* @param channel active ChannelSftp object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",86,90,"/**
* Constructs an FSDataOutputStream with the specified output stream and 
* position cache.
* @param out OutputStream to write data to
* @param stats FileSystem statistics
* @param startPosition starting position in the file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,getChecksumSize,org.apache.hadoop.fs.FSOutputSummer:getChecksumSize(),197,199,"/**
* Calls the underlying sum calculation method.
* @return result of sum calculation
*/",@return the size for a checksum.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getChecksumSize,org.apache.hadoop.util.DataChecksum:getChecksumSize(int),345,347,"/**
* Calculates optimal buffer size based on data size and m1 result.
* @param dataSize total data size
* @return optimal buffer size
*/","* the required checksum size given the data length.
   * @param dataSize data size.
   * @return the required checksum size given the data length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,convertToByteStream,"org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)",237,239,"/**
* Computes mask bytes based on given Checksum and size.
* @param sum Checksum object
* @param checksumSize size of checksum
* @return array of mask bytes or null if invalid input
*/","* Converts a checksum integer value to a byte stream
   *
   * @param sum check sum.
   * @param checksumSize check sum size.
   * @return byte stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long),4206,4208,"/**
* Updates bytes read by incrementing the internal counter.
* @param newBytes number of additional bytes read
*/","* Increment the bytes read in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long),4214,4216,"/**
 * Updates bytes written counter with new data.
 * @param newBytes additional bytes to add to the total
 */","* Increment the bytes written in the statistics.
     * @param newBytes the additional bytes written",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int),4222,4224,"/**
* Increments the read operations counter by specified amount.
* @param count number of operations to increment by
*/","* Increment the number of read operations.
     * @param count number of read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int),4230,4232,"/**
 * Increments large read operations counter by specified count.
 * @param count number of large read operations to increment
 */","* Increment the number of large read operations.
     * @param count number of large read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int),4238,4240,"/**
* Increments write operations by specified count.
* @param count number of operations to increment by
*/","* Increment the number of write operations.
     * @param count number of write operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long),4246,4248,"/**
* Updates bytes read erasure coded value.
* @param newBytes number of new bytes to add
*/","* Increment the bytes read on erasure-coded files in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadByDistance,"org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)",4258,4275,"/**
* Updates bytesRead counters based on distance.
* @param distance network distance (0-4)
* @param newBytes number of bytes to increment
*/","* Increment the bytes read by the network distance in the statistics
     * In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.
     * @param distance the network distance
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,increaseRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long),4281,4283,"/**
* Updates remote read time in milliseconds.
* @param durationMS time to add (in ms)
*/","* Increment the time taken to read bytes from remote in the statistics.
     * @param durationMS time taken in ms to read bytes from remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,visitAll,org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator),4295,4302,"/**
* Aggregates statistics from root data and referenced sub-data.
* @param visitor StatisticsAggregator object to process data
* @return aggregated result of type T or null if failed
*/","* Apply the given aggregator to all StatisticsData objects associated with
     * this Statistics object.
     *
     * For each StatisticsData object, we will call accept on the visitor.
     * Finally, at the end, we will call aggregate to get the final total.
     *
     * @param         visitor to use.
     * @return        The total.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),603,606,"/**
* Invokes file system operation on specified path.
* @param path Path to operate on
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary$Builder:<init>(),47,48,"/**
* Initializes a new instance of the Builder class.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[]),108,112,"/**
* Calls superclass's m1 with consumed types and returns current builder instance.
* @param typeConsumed array of consumed data types
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,"org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",114,118,"/**
* Configures builder with storage type and quota.
* @param type Storage type (e.g. HDD, SSD)
* @param quota Quota size in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,"org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",120,124,"/**
* Configures storage parameters and delegates to parent builder.
* @param type Storage type (e.g. disk, cloud)
* @param consumed Consumed storage capacity in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[]),126,130,"/**
 * Overloads the builder with custom type and quota settings.
 * @param typeQuota array of type and quota values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,build,org.apache.hadoop.fs.QuotaUsage$Builder:build(),93,95,"/**
* Creates and returns a new QuotaUsage instance with this object's details.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder),194,204,"/**
* Initializes ContentSummary object from Builder.
* @param builder ContentSummary builder instance
*/","* Constructor for ContentSummary.Builder.
   *
   * @param builder builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getAlgorithmName,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName(),60,64,"/**
* Generates a function mask based on CRC and byte count.
* @return A string representing the MD5 hash of CRC and byte count
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt(),94,97,"/**
 * Creates and returns a new ChecksumOpt instance with specified parameters.
 * @return ChecksumOpt object initialized with m1() value and bytesPerCRC parameter.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$ChecksumOpt:<init>(),255,257,"/**
* Initializes checksum calculator with default settings.
*/",* Create a uninitialized one,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createDisabled,org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled(),287,289,"/**
* Returns a ChecksumOpt instance with NULL data type and invalid mask value.","* Create a ChecksumOpts that disables checksum.
     *
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt(),69,72,"/**
* Creates a ChecksumOpt instance with CRC type and byte count. 
* @return ChecksumOpt object with specified properties */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,write,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput),106,111,"/**
* Writes CRC and MD5 data to output stream.
* @param out DataOutput stream for writing data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,hasPattern,org.apache.hadoop.fs.GlobFilter:hasPattern(),75,77,"/**
* Checks function mask bit using pattern.m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet),149,162,"/**
* Validates and extracts create flags from the provided EnumSet.
* @param flag EnumSet containing CreateFlag options
*/","* Validate the CreateFlag and throw exception if it is invalid
   * @param flag set of CreateFlag
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrSetFlag.java,validate,"org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)",53,70,"/**
* Validates and enforces XAttr operation flags.
* @param xAttrName name of the XAttribute
* @param xAttrExists existence status of the XAttribute
* @param flag one of XAttrSetFlag values (CREATE, REPLACE)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkScheme,"org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)",291,300,"/**
* Validates URI scheme against a supported scheme.
* @param uri URI object to validate
* @param supportedScheme expected scheme (e.g. http, https)
*/","* Check that the Uri's scheme matches.
   *
   * @param uri name URI of the FS.
   * @param supportedScheme supported scheme.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String),38,40,"/**
* Constructs an InvalidPathException with the specified invalid path name.
* @param path the invalid path name
*/","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,"org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)",48,51,"/**
* Constructs an InvalidPathException with the specified path and optional reason.
* @param path invalid file system path
* @param reason optional reason for the invalid path (may be null) 
*/","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.
   * @param reason Reason <code>path</code> is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[]),210,219,"/**
* Returns the first non-null element from an array of objects.
* @param inputs array of objects to search
* @return first non-null object or throws exception if all null
*/","* Find the valid input from all the inputs.
   *
   * @param <T> Generics Type T.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[]),163,172,"/**
* Returns the first non-null element from an array of inputs.
* @param inputs array of values to search
* @throws HadoopIllegalArgumentException if all elements are null
*/","* Find the valid input from all the inputs.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][]),91,103,"/**
* Validates and ensures all input buffers are non-null and match the expected length.
* @param buffers array of byte arrays to be validated
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[]),129,145,"/**
* Validates and checks consistency of ByteBuffer array.
* @param buffers array of ByteBuffers to check
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][]),121,133,"/**
* Validates and sanitizes input buffers.
* @param buffers array of byte arrays to be validated
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[]),91,107,"/**
* Validates and checks consistency across an array of ByteBuffer objects.
* @param buffers Array of ByteBuffers to validate
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkPrimitive,org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class),71,79,"/**
* Validates and extracts the primitive type from a given class.
* @param componentType Class<?> to validate and extract type from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkDeclaredComponentType,org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class),81,88,"/**
* Validates the component type of an input array against its declared type.
* @param componentType actual component type of the array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkArray,org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object),90,98,"/**
* Validates and extracts array mask from provided object.
* @param value Object containing the array mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseGetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)",172,188,"/**
* Parses -getlevel command arguments and initializes operation, host name, and class name.
* @return the index of the next expected argument
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseSetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)",190,207,"/**
* Parses -setlevel command and sets operation to SETLEVEL.
* @param args array of command-line arguments
* @return next argument index (or -1 if error)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,stopProxy,org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object),792,821,"/**
* Closes a proxy object, invoking its close method if available.
* @param proxy Object to be closed
*/","* Stop the proxy. Proxy must either implement {@link Closeable} or must have
   * associated {@link RpcInvocationHandler}.
   * 
   * @param proxy
   *          the RPC proxy object to be stopped
   * @throws HadoopIllegalArgumentException
   *           if the proxy does not implement {@link Closeable} interface or
   *           does not have closeable {@link InvocationHandler}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String),206,208,"/**
* Constructs BadAclFormatException with specified error message.
* @param message user-specified error description.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String),216,218,"/**
* Constructs a BadAuthFormatException with a custom error message.
* @param message detailed description of authentication failure",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)",302,328,"/**
* Computes checksum options by merging default and user settings.
* @param defaultOpt default checksum options
* @param userOpt user-provided checksum options
* @param userBytesPerChecksum number of bytes per checksum for user option (0 to use default)
* @return merged ChecksumOpt object or null if invalid input
*/","* A helper method for processing user input and default value to 
     * create a combined checksum option. This is a bit complicated because
     * bytesPerChecksum is kept for backward compatibility.
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option. Ignored if null.
     * @param userBytesPerChecksum User-specified bytesPerChecksum
     *                Ignored if {@literal <} 0.
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setPermission,"org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",221,226,"/**
* Calls m1 and then performs additional file system operations.
* @param f Path to operate on
* @param permission File permissions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setPermission,"org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",545,549,"/**
* Delegates file system operation to underlying FS.
* @param p Path of file/directory
* @param permission File permissions to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",877,882,"/**
 * Resolves and applies a snapshot to the target file system.
 * @param path input path
 * @param snapshotName name of the snapshot to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",409,413,"/**
* Calls underlying file system to perform operation 1.
* @param path file system path
* @param snapshotName name of the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getDefaultPortIfDefined,org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem),69,72,"/**
* Returns the default port value from the file system implementation.
* @param theFsImpl File system instance
* @return Default port number or DELEGATE_TO_FS_DEFAULT_PORT if not set
*/","* Returns the default port if the file system defines one.
   * {@link FileSystem#getDefaultPort()} returns 0 to indicate the default port
   * is undefined.  However, the logic that consumes this value expects to
   * receive -1 to indicate the port is undefined, which agrees with the
   * contract of {@link URI#getPort()}.
   *
   * @param theFsImpl file system to check for default port
   * @return default port, or -1 if default port is undefined",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI),402,417,"/**
* Reconstructs a URI with corrected fragment and scheme.
* @param uri input URI to modify
* @return modified URI or original if unchanged
*/","* Canonicalize the given URI.
   *
   * This is implementation-dependent, and may for example consist of
   * canonicalizing the hostname using DNS and adding the default
   * port if not specified.
   *
   * The default implementation simply fills in the default port if
   * not specified and if {@link #getDefaultPort()} returns a
   * default port.
   *
   * @param uri url.
   * @return URI
   * @see NetUtils#getCanonicalUri(URI, int)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory(),74,77,"/**
* Calls underlying implementation of method m1.
* @return result from underlying implementation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory(),324,327,"/**
* Calls M1 method on underlying file system.
* @return result of M1 operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),484,488,"/**
* Retrieves file status from the underlying file system.
* @param f Path to the file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),162,165,"/**
* Delegates file status retrieval to Hadoop FileSystem.
* @param f path to file or directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),136,146,"/**
* Retrieves file status and potentially updates metadata.
* @param f path to the file
* @return FileStatus object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getLinkTarget,org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),257,260,"/**
 * Delegates file system operation to implementation.
 * @param f input file path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getLinkTarget,org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),494,496,"/**
* Wraps file system operation in an FS interface.
* @param f input path to process
* @return result of underlying operation (Path)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),167,170,"/**
* Calls underlying file system's m1() operation on the provided path.
* @param f the input path to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,truncate,"org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)",200,204,"/**
* Calls m1 and delegates to fsImpl's m2.
* @param f file path
* @param newLength new file length
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,truncate,"org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)",260,263,"/**
* Calls underlying file system to update file length.
* @param f Path to modify
* @param newLength desired file size in bytes
* @return true if operation is successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setReplication,"org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",228,233,"/**
* Calls m1 and delegates to fsImpl's m2.
* @param f file path
* @param replication replication factor
* @return true if successful
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setReplication,"org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",240,243,"/**
* Calls file system's m1 method to manage replica count.
* @param src Path object representing source location
* @param replication short value specifying number of replicas
* @return true if successful, false otherwise
*/","* Set replication for an existing file.
   * 
   * @param src file name
   * @param replication new replication
   * @throws IOException raised on errors performing I/O.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setTimes,"org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",235,239,"/**
* Calls low-level file system implementation to update timestamps.
* @param f Path to the file
* @param mtime last modified time in milliseconds
* @param atime last accessed time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,updateTime,org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData),177,195,"/**
* Updates PathData item with timestamp and modification/access times.
* @param item PathData object to update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setTimes,"org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",539,543,"/**
* Invokes native file system operation on specified path.
* @param p Path to operate on
* @param mtime last modified time in milliseconds
* @param atime last accessed time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean),241,244,"/**
* Calls underlying file system implementation to perform operation 1.
* @param verifyChecksum true to validate checksum, false otherwise. 
* @throws IOException if an I/O error occurs during execution.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean),512,515,"/**
 * Calls file system operation m1 with checksum verification flag. 
 * @param verifyChecksum whether to verify checksum during operation",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks(),246,249,"/**
* Delegates to underlying file system implementation.
* @return true if operation succeeds, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks(),490,492,"/**
* Calls underlying file system service to perform operation 1.
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createSymlink,"org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",251,255,"/**
* Delegates file system operation to underlying implementation.
* @param target target path
* @param link symbolic link path
* @param createParent whether to create parent directory if needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSymlink,"org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",476,482,"/**
* Calls underlying file system's m1 method to perform file operation.
* @param target the target path
* @param link the link path
* @param createParent whether to create parent directory if needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",156,160,"/**
* Delegates file system operation to underlying FS implementation.
* @param target destination path
* @param link symbolic link path
* @param createParent whether to create parent directories if necessary
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)",559,566,"/**
* Resizes a file to the specified length.
* @param f Path of the file
* @param newLength New size in bytes
* @return true if resize successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Opens an OutputStream with specified permissions and progress tracking.
* @param path file system path
* @param fsPermission file system permissions
* @param b unknown parameter (deprecated?)
* @param i unknown parameter (deprecated?)
* @param i1 unknown parameter (deprecated?)
* @param l unknown parameter (deprecated?)
* @param progressable progress tracker
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Creates an FSDataOutputStream with specified parameters.
* @param path file system path
* @param fsPermission permissions for the output stream
* @param b unknown boolean parameter (purpose unclear)
* @param i int value (unknown purpose)
* @param i1 short value (unknown purpose)
* @param l long value (unknown purpose)
* @param progressable progress tracking object (not used)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
* Creates an FSDataOutputStream with a specific function mask.
* @param path file system path
* @param i function mask value
* @param progressable progress tracking object (not used in this implementation)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
 * Creates an output stream to write to a file at the specified path.
 * @param path file location
 * @param i reserved for future use (currently ignored)
 * @param progressable callback for monitoring progress (not implemented)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Compares two paths to determine if they match the function mask.
* @param path original file path
* @param path1 target file path
* @return true if paths match, false otherwise (currently always throws)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Throws an exception when attempting to perform a specific file system operation.
* @param path1 first file path (not used)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
* Throws an exception when attempting to perform a specific operation on the given file system path.
* @param path the file system path to operate on
* @param b unknown parameter, not used in this implementation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
* Throws unsupported operation exception; override to implement functionality.
* @param path file or directory path
* @param b additional flag value (purpose depends on implementation)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
* Returns an array of file statuses for the specified path.
* @param path HDFS path to fetch statuses from
* @return Array of FileStatus objects or null if not supported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
* Retrieves file status array for the given path.
* @param path directory or file path to query
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
* Checks if file/directory has specific function mask.
* @param path file system path to check
* @param fsPermission permissions to verify against
* @return true if matches, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
* Checks if the given file system permission matches the function mask.
* @param path file system path to check
* @param fsPermission file system permissions to validate against
* @return true if permissions match, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory(),102,105,"/**
* Returns the working directory path.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory(),102,105,"/**
* Returns the function mask as the working directory.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Applies mask to provided file path.
 * @param path file path to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
* Applies mask to file or directory at specified Path location.
* @param path target file system location
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpsFileSystem:getUri(),56,59,"/**
* Returns URI representing function mask configuration.
* @return URI object containing function mask settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpFileSystem:getUri(),56,59,"/**
* Returns the function mask URI. 
* @return URI of the function mask resource 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,skip,org.apache.hadoop.fs.BufferedFSInputStream:skip(long),70,78,"/**
* Calculates and stores functional mask value.
* @param n non-negative input value to increment the mask
* @return original input value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads(),169,172,"/**
* Delegates call to PositionedReadable interface implementation.
* @return result of m1() on underlying reader
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads(),294,297,"/**
* Delegates call to PositionedReadable's m1() implementation.
* @return result of m1() operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads(),174,177,"/**
* Delegates call to PositionedReadable's m1() method.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads(),299,302,"/**
* Delegates call to PositionedReadable's m1() method.
* @return result of m1() operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,from,org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer),40,42,"/**
* Creates a PartHandle instance from a ByteBuffer.
* @param byteBuffer input buffer containing part handle data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,equals,org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object),54,62,"/**
* Recursively checks equality by comparing this PartHandle with the given one.
* @param other PartHandle to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",891,897,"/**
* Resolves file system target for given path and applies policy.
* @param path Path object to resolve
* @param policyName Name of the policy to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",420,424,"/**
* Invokes MyFS operation m1 on provided Path and policyName.
* @param path file system path
* @param policyName name of access control policy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,disconnect,org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient),248,260,"/**
* Performs FTP client logout and disconnection.
* @param client active FTPClient instance
*/","* Logout and disconnect the given FTPClient. *
   * 
   * @param client
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,close,org.apache.hadoop.fs.ftp.FTPFileSystem$1:close(),104,107,"/**
* Calls child node's m1() method.
* Throws IOException if any I/O error occurs. 
*/",* Close the underlying output stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,close,org.apache.hadoop.fs.ftp.FTPInputStream:close(),103,121,"/**
* Completes a file transfer operation.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFsAction,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)",440,453,"/**
* Calculates FsAction based on file permissions in the specified access group.
* @param accessGroup identifier of access group
* @param ftpFile FTPFile object to check permissions for
* @return FsAction enum value or NONE if no permissions match
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasNext,org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext(),2320,2324,"/**
* Checks if there are more entries to process.
* @return true if there are more entries, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(),149,150,"/**
 * Legacy constructor for deprecated ContentSummary objects. 
 */",Constructor deprecated by ContentSummary.Builder,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)",177,187,"/**
* Constructs a ContentSummary object with specified metrics.
* @param length total content length
* @param fileCount number of files
* @param directoryCount number of directories
* @param quota overall storage quota
* @param spaceConsumed consumed space in bytes
* @param spaceQuota allocated space quota in bytes
*/","* Constructor, deprecated by ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   * @param quota quota.
   * @param spaceConsumed space consumed.
   * @param spaceQuota space quota.
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,equals,org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object),257,275,"/**
* Performs object equality check with the provided object.
* @param to Object to compare for equality
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,hashCode,org.apache.hadoop.fs.ContentSummary:hashCode(),277,284,"/**
* Combines results from multiple methods using bitwise XOR, 
* then applies parent class method and casts to int.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",824,829,"/**
* Resolves file system and fetches metadata by path and name.
* @param path file system path
* @param name file or directory name
* @return file contents as byte array or throws IOException
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttr,"org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",371,374,"/**
 * Delegates file system operation to underlying storage.
 * @param path filesystem path
 * @param name associated file name
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getDelay,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit),82,86,"/**
* Calculates time units from milliseconds until next renewal.
* @param unit requested time unit (e.g. TimeUnit.MINUTES)
* @return equivalent value in the specified unit
*/",Get the delay until this event should happen.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,updateRenewalTime,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long),116,118,"/**
* Updates the renewal time based on the provided delay. 
* @param delay time to be added to the current time, with a 10% adjustment
*/","* Set a new time for the renewal.
     * It can only be called when the action is not in the queue or any
     * collection because the hashCode may change
     * @param delay the renewal time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,touch,org.apache.hadoop.ipc.Client$Connection:touch(),465,467,"/**
* Updates last activity timestamp using current system time. 
* @param none 
* @return none 
*/",Update lastActivity with the current time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,sleepAtLeastIgnoreInterrupts,org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long),39,50,"/**
* Waits for a specified duration by sleeping and checking time periodically.
* @param millis milliseconds to wait
*/","* Cause the current thread to sleep as close as possible to the provided
   * number of milliseconds. This method will log and ignore any
   * {@link InterruptedException} encountered.
   * 
   * @param millis the number of milliseconds for the current thread to sleep",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,now,org.apache.hadoop.util.Timer:now(),39,41,"/**
* Returns current time in milliseconds.","* Current system time.  Do not use this to calculate a duration or interval
   * to sleep, because it will be broken by settimeofday.  Instead, use
   * monotonicNow.
   * @return current time in msec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AsyncDiskService.java,awaitTermination,org.apache.hadoop.util.AsyncDiskService:awaitTermination(long),131,147,"/**
* Awaits termination of all async disk services within a specified time limit.
* @param milliseconds maximum wait duration in milliseconds
* @return true if all services terminated, false on timeout or exception
*/","* Wait for the termination of the thread pools.
   * 
   * @param milliseconds  The number of milliseconds to wait
   * @return   true if all thread pools are terminated without time limit
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,ceiling,"org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)",322,324,"/**
* Calculates time mask value based on given timestamp and interval.
* @param time current timestamp
* @param interval time interval to consider
* @return calculated mask value as a long integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readChunk,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",266,307,"/**
* Reads data and checksum from file, updating sums as needed.
*@param pos current position in file
*@param buf buffer to store read data
*@param offset starting offset in buffer
*@param len number of bytes to read
*@param checksum checksum array (may be null)
*@return actual number of bytes read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long),258,264,"/**
* Checks if data is available at specified position.
* @param targetPos target file position
* @return true if data found or source updated, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,checkBytes,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)",376,426,"/**
* Validates CRC32 checksums in a file.
* @param sumsBytes ByteBuffer containing CRC32 values
* @param sumsOffset offset of CRC32 values in sumsBytes
* @param data ByteBuffer containing file data
* @param dataOffset offset of file data in data ByteBuffer
* @return validated file data ByteBuffer
*/","* Check the data against the checksums.
     * @param sumsBytes the checksum data
     * @param sumsOffset where from the checksum file this buffer started
     * @param data the file data
     * @param dataOffset where the file data started (must be a multiple of
     *                  bytesPerSum)
     * @param bytesPerSum how many bytes per a checksum
     * @param file the path of the filename
     * @return the data buffer
     * @throws CompletionException if the checksums don't match",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)",124,131,"/**
* Calculates optimal buffer size for summing.
* @param bytesPerSum data chunk size
* @param bufferSize total buffer capacity
* @param file input file path
* @return calculated buffer size
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,open,org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path),721,724,"/**
* Opens an FSDataInputStream to the specified file.
* @param f the Path object pointing to the file
*/","* The specification of this method matches that of
   * {@link FileContext#open(Path)} except that Path f must be for this
   * file system.
   *
   * @param f the path.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return input stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getServerDefaults,org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path),163,166,"/**
* Delegates to MyFS server defaults retrieval.
* @param f path to fetch defaults for
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",111,113,"/**
* Calculates functional mask based on file size and internal calculations.
* @param file Path to file (not used in calculation)
* @param fileSize size of the file
* @return calculated function mask value
*/","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return check sum file length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path),582,614,"/**
* Wraps an iterator to filter results based on a predicate.
* @param f input path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,useStatIfAvailable,org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable(),96,99,"/**
* Enables or disables legacy file status functionality.
* @see Stat#m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createOutputStream,"org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)",570,573,"/**
* Creates an OutputStream instance with specified file and append mode.
* @param f Path to the file
* @param append true to append, false to overwrite
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path),3041,3043,"/**
* Returns a default FsStatus object with maximum values.
*/","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * @param p Path for which status should be obtained. null means
   * the default partition.
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus(),1130,1133,"/**
* Returns default file system status with mask set to zero.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus(),445,449,"/**
* Returns a default FsStatus object with all values set to zero.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,registerCommands,org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,54,"/**
* Registers file mode commands with the CommandFactory.
* @param factory CommandFactory instance to register commands with
*/","* Register the permission related commands with the factory
   * @param factory the command factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,registerCommands,org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
* Configures a CommandFactory instance with a test command mask.
* @param factory CommandFactory to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,registerCommands,org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),41,45,"/**
* Configures command masks for snapshot operations.
* @param factory CommandFactory instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerCommands,org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),49,51,"/**
* Registers find command with given CommandFactory.
* @param factory instance of CommandFactory to register command
*/","* Register the names for the count command
   * 
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,registerCommands,org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),40,42,"/**
* Configures command factory with mask functionality.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,registerCommands,org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),45,48,"/**
* Configures command masks for Ls and Lsr commands.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,registerCommands,org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,44,"/**
* Registers Tail command with specified parameters.
* @param factory CommandFactory instance used to register commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,registerCommands,org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,50,"/**
* Configures command factory with masking functionality.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,registerCommands,org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,45,"/**
* Configures command masks for given factory.
* @param factory CommandFactory instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,registerCommands,org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,55,"/**
* Configures command factory with mask commands.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,registerCommands,org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,48,"/**
* Configures command factory with count mask.
* @param factory CommandFactory instance to configure
*/","* Register the names for the count command
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,registerCommands,org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),43,46,"/**
* Configures command factory with touch-related commands.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,registerCommands,org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),39,41,"/**
 * Configures command factory with mkdir functionality.
 * @param factory CommandFactory instance to configure
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,registerCommands,org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),38,40,"/**
* Registers command with specified class and alias in CommandFactory.
* @param factory CommandFactory instance to register command with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,registerCommands,org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),44,52,"/**
* Configures CommandFactory with supported commands and their aliases.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,registerCommands,org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,39,"/**
* Configures command factory with move-related commands.
* @param factory CommandFactory instance to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,registerCommands,org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),53,55,"/**
* Registers Stat class with CommandFactory with mask ""-stat"". 
* @param factory instance of CommandFactory to register class with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,registerCommands,org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),62,66,"/**
* Configures command masks with specified factories.
* @param factory CommandFactory instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,registerCommands,org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),47,50,"/**
* Registers ACL commands with the given CommandFactory.
* @param factory instance of CommandFactory to register commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,registerCommands,org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,37,"/**
 * Configures CommandFactory with Truncate command.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,registerCommands,org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
* Configures command factory with replication settings.
* @param factory CommandFactory instance to modify
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])",45,48,"/**
* Legacy constructor to initialize CommandFormat with name and bounds.
* @param name command identifier
* @param min minimum value
* @param max maximum value
* @param possibleOpt optional parameters (varargs)
*/","* @deprecated use replacement since name is an unused parameter
   * @param name of command, but never used
   * @param min see replacement
   * @param max see replacement
   * @param possibleOpt see replacement
   * @see #CommandFormat(int, int, String...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)",361,403,"/**
* Deletes or schedules deletion of trash checkpoints older than the specified interval.
* @param trashRoot root directory for trash policy
* @param deleteImmediately whether to delete immediately, or schedule for later
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystemPathHandle.java,verify,org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus),54,61,"/**
* Validates file status and content timestamp.
* @param stat FileStatus object to validate
* @throws InvalidPathHandleException if validation fails or handle is invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",802,807,"/**
* Resolves the file system state for a given path and applies ACL specification.
* @param path file system location
* @param aclSpec list of access control entries to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setAcl,"org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",349,352,"/**
* Delegates file system operation to underlying file system.
* @param path file system path
* @param aclSpec access control list specification
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,startUpload,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path),98,110,"/**
* Uploads file to storage service asynchronously.
* @param filePath path to local file
* @return CompletableFuture containing upload handle or null if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,putPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)",112,122,"/**
* Fetches PartHandle by uploading file chunk.
* @param uploadId unique upload identifier
* @param partNumber part number of the uploaded chunk
* @return PartHandle object or null if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,complete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",176,185,"/**
* Performs file upload and returns a PathHandle.
* @param uploadId unique upload identifier
* @param filePath path to the uploaded file
* @return PathHandle object or null if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,eval,org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE),179,182,"/**
* Wraps a Callable raising IO exception in a CompletableFuture.
* @param callable function to be executed with potential IO exception
*/","* Evaluate a CallableRaisingIOE in the current thread,
   * converting IOEs to RTEs and propagating.
   * See {@link FutureIO#eval(CallableRaisingIOE)}.
   *
   * @param callable callable to invoke
   * @param <T> Return type.
   * @return the evaluated result.
   * @throws UnsupportedOperationException fail fast if unsupported
   * @throws IllegalArgumentException invalid argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,concat,"org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",188,191,"/**
* Calls file system method to process source files.
* @param f target file
* @param psrcs array of source files
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,rejectUnknownMandatoryKeys,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)",358,362,"/**
* Wraps mandatory key validation with additional error text.
* @param knownKeys existing keys to validate against
* @param extraErrorText custom error message for missing keys
*/","* Reject a configuration if one or more mandatory keys are
   * not in the set of mandatory keys.
   * The first invalid key raises the exception; the order of the
   * scan and hence the specific key raising the exception is undefined.
   * @param knownKeys a possibly empty collection of known keys
   * @param extraErrorText extra error text to include.
   * @throws IllegalArgumentException if any key is unknown.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileRangeImpl.java,toString,org.apache.hadoop.fs.impl.FileRangeImpl:toString(),54,58,"/**
* Generates a formatted string representing a mask range.
* @return Mask range string with specified values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,pathCapabilities,org.apache.hadoop.fs.impl.FlagSet:pathCapabilities(),209,213,"/**
* Retrieves capability values from namesToValues pipeline.
*/","* Generate the list of capabilities.
   * @return a possibly empty list.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,buildHttpReferrer,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer(),190,223,"/**
* Builds a URI header for auditor.
* @return Header string or empty if construction failed
*/","* Build the referrer string.
   * This includes dynamically evaluating all of the evaluated
   * attributes.
   * If there is an error creating the string it will be logged once
   * per entry, and """" returned.
   * @return a referrer string or """"",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,<init>,"org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)",100,106,"/**
* Initializes a WeakReferenceMap with a provided key-value factory and an optional callback for lost references.
* @param factory function to create values when keys are not yet stored
* @param referenceLost callback to invoke when a weakly referenced key is garbage collected (optional)","* instantiate.
   * @param factory supplier of new instances
   * @param referenceLost optional callback on lost references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)",80,82,"/**
* Writes function mask to output stream.
* @param out OutputStream to write to
* @param capability Function capability string
*/","* Probe for an output stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param out output stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)",92,94,"/**
* Checks if input stream matches specified capability mask.
* @param in InputStream to check
* @param capability Capability mask string
*/","* Probe for an input stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param in input stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/ExecutorServiceFuturePool.java,shutdown,"org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Executes Hadoop job with specified executor and timeout settings.
* @param logger logging instance
* @param timeout execution timeout in specified time unit
* @param unit timeout time unit (e.g. SECONDS, MINUTES)
*/","* Utility to shutdown the {@link ExecutorService} used by this class. Will wait up to a
   * certain timeout for the ExecutorService to gracefully shutdown.
   *
   * @param logger Logger
   * @param timeout the maximum time to wait
   * @param unit the time unit of the timeout argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),128,131,"/**
* Initializes an End operation with given parameters.
* @param op Operation object containing kind and block number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder),133,137,"/**
* Calls superclass's m2 with StringBuilder passed to this instance's m1.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDebugInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo(),139,142,"/**
* Calls superclass's overridden method and appends custom output.
* @return concatenated string with custom prefix and superclass result",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,add,org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),160,166,"/**
* Executes an operation and logs it in debug mode.
* @param op the operation to execute
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,canRelease,org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData),318,322,"/**
* Checks if the buffer data indicates completion.
* @param data BufferData object to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get(),411,420,"/**
* Prefetches block using M5 algorithm.
* @throws Exception if error occurs during prefetched
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,distance,"org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)",226,228,"/**
* Calculates the function mask value based on buffer data and block number.
* @param data BufferData object containing relevant information
* @param blockNumber specific block identifier
* @return calculated mask value as an integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,find,org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int),305,316,"/**
* Retrieves BufferData object associated with the given block number.
* @param blockNumber unique identifier for a block
* @return BufferData object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,close,org.apache.hadoop.fs.impl.prefetch.BufferPool:close(),257,275,"/**
* Resets and reallocates buffer pool resources.
* @see m3() for data iteration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire(),70,73,"/**
* Returns a mask value using m1 with enable parameter set to true.
* @return Masked value of type T
*/",* Acquires a resource blocking if necessary until one becomes available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire(),78,81,"/**
* Returns a function mask value using m1 with false flag.
* @return Function mask value of type T
*/",* Acquires a resource blocking if one is immediately available. Otherwise returns null.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,close,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(),113,124,"/**
* Recursively processes and clears created items, invoking secondary method on each. 
* @param item T: type of item in collection
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable(),148,150,"/**
* Calculates the function mask value based on size and item counts.
* @return calculated function mask integer value
*/","* Number of items available to be acquired. Mostly for testing purposes.
   * @return the number available.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,duration,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration(),144,146,"/**
* Calculates and returns the difference between m1() and op.m1(), scaled by 1 billion.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,analyze,org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder),297,367,"/**
* Processes operations and writes block-level statistics to the StringBuilder.
* @param sb StringBuilder for outputting statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,<init>,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",223,234,"/**
* Initializes SingleFilePerBlockCache with prefetching statistics, max blocks count,
* and duration tracker factory.
* @param prefetchingStatistics statistics for prefetching
* @param maxBlocksCount maximum number of blocks to cache
* @param trackerFactory factory for tracking durations
*/","* Constructs an instance of a {@code SingleFilePerBlockCache}.
   *
   * @param prefetchingStatistics statistics for this stream.
   * @param maxBlocksCount max blocks count to be kept in cache at any time.
   * @param trackerFactory tracker with statistics to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",71,82,"/**
* Constructs a SemaphoredDelegatingExecutor instance.
* @param executorDelegatee underlying executor service
* @param permitCount initial semaphore permits count
* @param fair whether semaphore is fair or not
* @param trackerFactory factory for duration tracking
*/","* Instantiate.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""
   * @param trackerFactory duration tracker factory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListHead,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),314,321,"/**
* Executes masked operation on Entry object.
*@param entry Entry object to process
*/","* Helper method to add the given entry to the head of the linked list.
   *
   * @param entry Block entry to add.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,validateEntry,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)",551,566,"/**
* Validates the consistency of an Entry object with a ByteBuffer.
* @param entry Entry object to validate
* @param buffer ByteBuffer containing data to compare
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setDone,org.apache.hadoop.fs.impl.prefetch.BufferData:setDone(),223,231,"/**
* Completes the function by setting state to DONE and checking checksum.
* @throws IllegalStateException if checksum mismatch occurs
*/",* Indicates that this block is no longer of use and can be reclaimed.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,close,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close(),502,508,"/**
* Handles mask functionality; logs and performs necessary operations.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,toString,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString(),540,549,"/**
* Formats and returns user statistics as a string.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferData:toString(),289,299,"/**
* Formats a string with block number and other object properties.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,throwIfInvalidBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer(),298,300,"/**
* Validates buffer parameter is not null.
* @param buffer input data (must not be null)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,bufferSize,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int),133,136,"/**
* Initializes DataInputStream builder with specified buffer size.
* @param bufSize maximum buffer size
*/","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,builder,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder(),146,148,"/**
* Returns a DataInputStreamBuilder with the specified mask.
* @return DataInputStreamBuilder instance
*/","* Get the builder.
   * This must be used after the constructor has been invoked to create
   * the actual builder: it allows for subclasses to do things after
   * construction.
   *
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,getForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread(),45,47,"/**
* Composes and returns a mask value using nested functions.
*/","* Get the value for the current thread, creating if needed.
   * @return an instance.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,removeForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread(),53,55,"/**
* Computes and returns a functional mask value through nested function calls.
* @return The computed functional mask value of type V.","* Remove the reference for the current thread.
   * @return any reference value which existed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,setForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object),70,90,"/**
* Returns a unique value for the given new value, or retrieves and updates an existing one.
* @param newVal new value to be processed
*/","* Set the new value for the current thread.
   * @param newVal new reference to set for the active thread.
   * @return the previously set value, possibly null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,<init>,"org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)",44,47,"/**
* Combines two file ranges into one.
* @param offset starting position
* @param end ending position
* @param original original file range to append
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,merge,"org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)",79,89,"/**
* Validates overlap between two file ranges.
* @param otherOffset offset of other file range
* @param otherEnd end of other file range
* @param other other FileRange object
* @param minSeek minimum seek distance required
* @param maxSize maximum size difference allowed
* @return true if overlapping, false otherwise
*/","* Merge this input range into the current one, if it is compatible.
   * It is assumed that otherOffset is greater or equal the current offset,
   * which typically happens by sorting the input ranges on offset.
   * @param otherOffset the offset to consider merging
   * @param otherEnd the end to consider merging
   * @param other the underlying FileRange to add if we merge
   * @param minSeek the minimum distance that we'll seek without merging the
   *                ranges together
   * @param maxSize the maximum size that we'll merge into a single range
   * @return true if we have merged the range into this one",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createBulkDelete,org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path),5002,5006,"/**
 * Creates a bulk delete operation on the specified Path.
 * @param path Hadoop file system path to delete files from
 */","* Create a bulk delete operation.
   * The default implementation returns an instance of {@link DefaultBulkDeleteOperation}.
   * @param path base path for the operation.
   * @return an instance of the bulk delete.
   * @throws IllegalArgumentException any argument is invalid.
   * @throws IOException if there is an IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBufferSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize(),64,67,"/**
* Calls superclass implementation of m1.
* @return result from superclass m1 method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getReplication,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication(),69,72,"/**
* Calls superclass implementation of m1().",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFlags,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags(),74,77,"/**
* Calls superclass implementation to retrieve create flags.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getChecksumOpt,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt(),79,82,"/**
* Calls superclass implementation of checksum option. 
* @return ChecksumOpt enum value or null if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBlockSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize(),84,87,"/**
* Calls superclass implementation of M1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList),153,177,"/**
* Validates and extracts command line options for FUNC_MASK operation.
* @param args list of command line arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeXAttr,"org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",656,659,"/**
* Delegates file system operation to underlying FS implementation.
* @param path file system path
* @param name file name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList),53,59,"/**
* Handles command with unknown option ""-t"".
* @throws UnknownOptionException for -t option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,registerExpression,org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),31,35,"/**
* Initializes And operator mask in ExpressionFactory.
* @param factory ExpressionFactory instance to configure
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,registerExpression,org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),30,34,"/**
* Configures expression factories with print and print0 functions.
* @param factory ExpressionFactory instance to be configured
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,registerExpression,org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),33,37,"/**
* Configures ExpressionFactory with mask-related expressions.
* @param factory ExpressionFactory instance to configure
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print:<init>(),45,47,"/**
* Initializes printer with newline character.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(boolean),57,62,"/**
* Initializes a new Name command with optional case sensitivity.
* @param caseSensitive true to consider input case, false otherwise
*/","* Construct a Name {@link Expression} with a specified case sensitivity.
   *
   * @param caseSensitive if true the comparisons are case sensitive.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,apply,"org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)",82,93,"/**
* Evaluates path data against glob pattern.
* @param item PathData object to evaluate
* @param depth Unused parameter, currently ignored
* @return PASS or FAIL result based on match
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,addCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec),64,75,"/**
* Updates codecs cache with compressed data from the given CompressionCodec.
* @param codec CompressionCodec object containing compressed data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path),199,217,"/**
* Retrieves CompressionCodec for the given file path.
* @param file Path object representing the file
*/","* Find the relevant compression codec for the given file based on its
   * filename suffix.
   * @param file the filename to check
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)",94,96,"/**
* Initializes metrics configuration with given prefix.
* @param c Configuration object
* @param prefix unique identifier prefix
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,isLocalhost,org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String),491,501,"/**
* Checks if the provided host matches a local IP address.
* @param host hostname or IP address to check
* @return true if local IP, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,resolvePropertyName,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)",216,221,"/**
* Formats SSL factory mode into a string using a given template.
* @param mode SSL factory mode (e.g. PROD, DEV)
* @param template format string with placeholders for mode
* @return formatted string or null if template is invalid
*/","* Resolves a property name to its client/server version if applicable.
   * <p>
   * NOTE: This method is public for testing purposes.
   *
   * @param mode client/server mode.
   * @param template property name template.
   * @return the resolved property name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String),860,880,"/**
* Checks if the specified capability is supported by the input stream.
* @param capability capability to check
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,getConfigSuffix,org.apache.hadoop.crypto.CipherSuite:getConfigSuffix(),98,106,"/**
* Generates a functional mask from the module name.
*/","* Returns suffix of cipher suite configuration.
   * @return String configuration suffix",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,combine,org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result),59,62,"/**
* Performs bitwise AND operation on two Result objects.
* @param other Result object to perform operation with
* @return Result object with ANDed values
*/","* Returns the combination of this and another result.
   * @param other other.
   * @return result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,negate,org.apache.hadoop.fs.shell.find.Result:negate(),68,70,"/**
* Creates a Result instance with masked values.
* @return Result object containing m1() inverted and m2()
*/","* Negate this result.
   * @return Result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,toString,org.apache.hadoop.fs.shell.find.Result:toString(),72,75,"/**
 * Returns a formatted string containing the result of m1 and m2 functions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name$Iname:<init>(),97,99,"/**
* Initializes this instance with a new name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print$Print0:<init>(),72,74,"/**
 * Initializes a new instance of the class with a custom print stream. 
 * @param printStream The print stream to use for output. In this case, prints a null character ('\0'). 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,createOptions,org.apache.hadoop.fs.shell.find.Find:createOptions(),244,252,"/**
* Creates and configures a FindOptions object for database queries.
*/",Create a new set of find options.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isExpression,org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String),445,448,"/**
* Checks if an expression exists by name.
* @param expressionName name of the expression to check
*/",Asks the factory whether an expression is recognized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,setOptions,org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions),67,73,"/**
* Recursively processes child expressions based on provided find options.
* @param options FindOptions object to guide the search
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,prepare,org.apache.hadoop.fs.shell.find.BaseExpression:prepare(),75,80,"/**
* Recursively traverses and processes all Expression children.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,finish,org.apache.hadoop.fs.shell.find.BaseExpression:finish(),82,87,"/**
* Recursively invokes m1() on all children of current expression. 
* @throws IOException if an I/O error occurs during recursive traversal
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,isAction,org.apache.hadoop.fs.shell.find.BaseExpression:isAction(),147,155,"/**
* Recursively checks if any child expression evaluates to true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,toString,org.apache.hadoop.fs.shell.find.BaseExpression:toString(),119,145,"/**
* Constructs a SQL query string with parameters and child expressions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addChildren,"org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)",219,223,"/**
* Applies mask function to each expression in deque and counts results.
* @param exprs deque of expressions
* @param count number of iterations
*/","* Add a specific number of children to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param exprs
   *          deque of expressions from which to take the children
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addArguments,"org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)",250,254,"/**
* Applies mask operation to deque elements.
* @param args deque of strings
* @param count number of iterations
*/","* Add a specific number of arguments to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param args
   *          deque of arguments from which to take the argument
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate(),244,274,"/**
* Deletes credential by alias, prompting for confirmation if interactive mode is enabled.
* @return true if deletion was successful or cancelled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate(),354,381,"/**
* Confirms deletion of all versions for a specified key.
* @return true if deletion confirmed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,confirmForceManual,org.apache.hadoop.ha.HAAdmin:confirmForceManual(),466,479,"/**
* Confirms user intention to proceed with --FORCEMANUAL flag, 
* warning of potential data corruption and recommending alternative shutdown.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,relativize,"org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)",410,435,"/**
* Computes a relative path from the current directory to the source URI.
* @param cwdUri current working directory URI
* @param srcUri source URI
* @param isDir whether the operation is directory-based
* @return relative path string or CUR_DIR if in same directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,stringToUri,org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String),550,591,"/**
* Parses a string into a URI with scheme and authority.
* @param pathString input string to parse
* @return URI object or throws exception if invalid
*/","Construct a URI from a String with unescaped special characters
   *  that have non-standard semantics. e.g. /, ?, #. A custom parsing
   *  is needed to prevent misbehavior.
   *  @param pathString The input path in string form
   *  @return URI",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean),69,71,"/**
 * Sets whether to display mask values in human-readable format.
 * @param humanReadable true for human-readable, false for raw mask value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean),69,71,"/**
* Sets whether to display error messages in human-readable format.
* @param humanReadable true to enable human-readable output, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the usage table to be used by the builder.
 * @param usagesTable TableBuilder object containing usage data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the usage table builder instance.
 * @param usagesTable TableBuilder object to be assigned
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable(),61,63,"/**
* Returns an instance of TableBuilder with a mask.
* @return TableBuilder instance with mask configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable(),61,63,"/**
* Returns the table builder with function mask configuration.
* @return TableBuilder instance for further customization
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,isSorted,org.apache.hadoop.fs.shell.Ls:isSorted(),248,254,"/**
* Evaluates mask condition based on multiple factors.
* @return true if any factor is false; otherwise, false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,initialiseOrderComparator,org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator(),374,402,"/**
* Configures the order comparator based on conditionals m1(), m2().
* @param none
*/","* Initialise the comparator to be used for sorting files. If multiple options
   * are selected then the order is chosen in the following precedence: -
   * Modification time (or access time if requested) - File size - File name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getAdditionalTokenIssuers,org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers(),718,723,"/**
* Returns an array of delegationToken issuers.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)",218,220,"/**
* Constructs NotEnoughArgumentsException with provided argument counts.
* @param expected number of arguments expected
* @param actual number of arguments passed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)",202,204,"/**
* Constructs TooManyArgumentsException with expected and actual argument counts.
* @param expected expected number of arguments
* @param actual actual number of arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage(),222,225,"/**
* Returns an error message when called without overloading, 
* delegating to superclass implementation. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage(),206,209,"/**
* Returns error message when method is overridden with incorrect argument count.
* @return Error string indicating too many arguments.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList),70,100,"/**
* Parses command-line options and validates input parameters.
* @param args list of user-provided arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path),640,643,"/**
* Delegates file system operation to underlying FS implementation.
* @param path file system path to operate on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttr,"org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",635,638,"/**
* Wraps file system call to retrieve data from specified path and name.
* @param path filesystem path
* @param name filename
* @return byte array containing data or null on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,popPreserveOption,org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List),191,210,"/**
* Processes command-line arguments and applies corresponding actions.
* @param args list of command-line arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList),159,171,"/**
* Renames a snapshot under the specified path.
* @param items list of PathData objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",579,583,"/**
* Invokes file system operation m1.
* @param path filesystem path to operate on
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isDeprecated,org.apache.hadoop.fs.shell.Command:isDeprecated(),557,559,"/**
* Checks if mask is present.
* @return true if mask is available, false otherwise
*/","* Is the command deprecated?
   * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getName,org.apache.hadoop.fs.shell.Command:getName(),519,523,"/**
* Returns a masked version of the command name.
* @return Masked string representation or original name if invalid
*/","* The name of the command.  Will first try to use the assigned name
   * else fallback to the command's preferred name
   * @return name of the command",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAclStatus,org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path),618,621,"/**
* Retrieves ACL status for the specified file/directory.
* @param path file system path to query
* @return AclStatus object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,setPreserve,org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean),125,133,"/**
* Updates or clears file attributes based on the preservation flag.
* @param preserve true to update, false to clear attributes
*/","* If true, the last modified time, last access time,
   * owner, group and permission information of the source
   * file will be preserved as far as target {@link FileSystem}
   * implementation allows.
   *
   * @param preserve preserve.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setAcl,"org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",613,616,"/**
* Delegate file system operation to underlying implementation. 
* @param path filesystem path
* @param aclSpec ACL entries for the specified path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,<init>,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)",41,45,"/**
* Initializes MBeanInfo builder with name and description.
* @param name unique MBean identifier
* @param desc human-readable MBean description
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)",58,68,"/**
* Constructs a MetricsRecordBuilder instance.
* @param parent the parent MetricsCollector
* @param info MetricsInfo object
* @param rf metrics filter (record)
* @param mf metrics filter (metric)
* @param acceptable flag indicating record acceptability
*/","* @param parent {@link MetricsCollector} using this record builder
   * @param info metrics information
   * @param rf
   * @param mf
   * @param acceptable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,"org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)",103,107,"/**
* Initializes a new ChunkedArrayList instance with specified chunk capacity and maximum size constraints.
* @param initialChunkCapacity the starting capacity for each chunk
* @param maxChunkSize the maximum allowed size for any chunk
*/","* @param initialChunkCapacity the capacity of the first chunk to be
   * allocated
   * @param maxChunkSize the maximum size of any chunk allocated",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,calculatePivotOnDefaultEntries,org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List),87,94,"/**
* Finds the index of the first default ACL entry in the builder list.
* @return the index or PIVOT_NOT_FOUND if not found
*/","* Returns the pivot point in the list between the access entries and the
   * default entries.  This is the index of the first element in the list that
   * is a default entry.
   *
   * @param aclBuilder ArrayList<AclEntry> containing entries to build
   * @return int pivot point, or -1 if list contains no default entries",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAcl,org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path),608,611,"/**
* Calls underlying file system's m1 operation on specified Path.
* @param path file system path to operate on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",591,595,"/**
* Calls file system's M1 operation with provided parameters.
* @param path directory or file path to operate on
* @param aclSpec access control list specifications
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",597,601,"/**
* Calls underlying file system's m1 method to perform operation on specified Path.
* @param path the path to operate on
* @param aclSpec list of ACL entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList),77,88,"/**
* Creates a snapshot of the provided data.
* @param items list of PathData objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createSnapshot,org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path),3089,3091,"/**
 * Wraps the main file system access call with IOException handling.
 * @param path file system path
 */","* Create a snapshot with a default name.
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSnapshot,"org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",573,577,"/**
* Calls underlying file system's m1 method.
* @param path file system path
* @param snapshotName name of a snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,addOptionWithValue,org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String),73,78,"/**
* Validates and sets an option mask.
* @param option unique option identifier
*/","* add option with value
   *
   * @param option option name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList),117,128,"/**
* Deletes a snapshot for the given path.
* @param items list of PathData objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",585,589,"/**
* Invokes M1 operation on file system.
* @param path directory path to operate on
* @param snapshotName name of the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,<init>,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[]),274,278,"/**
* Constructs a new TableBuilder instance with the given column headers.
* @param headers variable number of column headers as Objects
*/","* Create a table with headers
     * @param headers list of headers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,isEmpty,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty(),348,350,"/**
* Checks if mask flag is set to zero.
* @return true if mask flag is zero, false otherwise
*/","* Does table have any rows 
     * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path),831,836,"/**
* Resolves file system tree for the given path.
* @param path input file system path
* @return map of file system metadata or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path),376,379,"/**
 * Delegates file system operation to underlying storage engine.
 * @param path file or directory path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean),496,498,"/**
* Initializes a new Location instance with specified permission to change. 
* @param allowChanged true if location can be modified, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean),471,473,"/**
* Initializes a new Data instance with specified change tracking behavior.
* @param allowChanged whether to track changes in data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,toString,org.apache.hadoop.fs.CompositeCrcFileChecksum:toString(),84,87,"/**
* Generates a function mask string combining CRC and hexadecimal values.
* @return formatted string containing CRC value and hexadecimal representation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(long),910,912,"/**
 * Initializes a new Shell instance with the specified interval.
 * @param interval time interval in milliseconds
 */","* Create an instance with a minimum interval between executions; stderr is
   * not merged with stdout.
   * @param interval interval in milliseconds between command executions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,equals,org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object),72,79,"/**
* Recursively checks equivalence with another PathHandle object.
* @param other the PathHandle to compare
* @return true if equivalent, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,hashCode,org.apache.hadoop.fs.RawPathHandle:hashCode(),81,84,"/**
* Calls and returns the result of m2() from the returned value of m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,toString,org.apache.hadoop.fs.RawPathHandle:toString(),86,89,"/**
* Calls m1() and delegates to its m2() method.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,run,org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run(),209,236,"/**
* Continuously refreshes disk usage metrics until the threshold is met.
*@throws InterruptedException if thread is interrupted while waiting
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text),93,99,"/**
* Initializes or updates the text mask with the provided owner.
* @param owner Text object to be assigned to this instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRealUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text),122,128,"/**
* Sets or resets the user's text profile.
* @param realUser user's text profile or null to clear
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",86,91,"/**
* Initializes a Token object with provided credentials and metadata.
* @param identifier unique token identifier
* @param password token password bytes
* @param kind token type, represented as text
* @param service token service, represented as text
*/","* Construct a token from the components.
   * @param identifier the token identifier
   * @param password the token's password
   * @param kind the kind of token
   * @param service the service for this token",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(),96,101,"/**
* Initializes a new Token instance with default values.
*/",* Default constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",65,72,"/**
* Initializes Globber instance with provided context, pattern, and filter.
* @param fc FileContext object for tracing and configuration
* @param pathPattern globbing pattern as Path object
* @param filter optional PathFilter to apply during globbing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",100,110,"/**
* Creates a new Globber instance to search for files matching the given pattern.
* @param fc FileContext object
* @param pathPattern file path pattern to match
* @param filter optional PathFilter for filtering results
* @param resolveSymlinks whether to follow symbolic links
*/","* File Context constructor for use by {@link GlobBuilder}.
   * @param fc file context
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getMessage,org.apache.hadoop.fs.PathIOException:getMessage(),86,104,"/**
* Constructs a detailed error message by combining operation, path, target path, and fully qualified path details.
*@return formatted error message as a string
*/","Format:
   * cmd: {operation} `path' {to `target'}: error string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,hasNext,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext(),49,52,"/**
* Checks if mask function is available.
* @return true if m1() returns non-null value, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,next,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next(),64,71,"/**
* Recursively fetches and returns statistic M2.
* @return Long value of statistic M2 or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAndIncrDirNumLastAccessed,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(),282,284,"/**
* Calls itself recursively with an initial value of 1.
* @return Result of recursive call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",860,866,"/**
* Resolves inode tree for the given file system and returns a new path.
* @param path original file system path
* @param snapshotName name of the snapshot to resolve against
* @return Path object representing the resolved inode tree
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSnapshot,"org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",397,401,"/**
* Wraps file system operation by delegating to myFs.
* @param path filesystem path
* @param snapshotName name of the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAbstractFileSystem,"org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)",339,363,"/**
* Retrieves an AbstractFileSystem instance using user credentials.
* @param user UserGroupInformation object
* @param uri URI of the file system
* @param conf Configuration object
* @return AbstractFileSystem instance or throws exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsUser,"org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)",552,559,"/**
* Executes privileged action on user's behalf and returns result.
* @param ugi UserGroupInformation instance
* @param action PrivilegedExceptionAction to execute
* @return Result of the action or throws IOException if interrupted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleSaslConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)",705,757,"/**
* Handles connection setup exceptions with retry logic.
* @param currRetries current number of retries
* @param maxRetries maximum allowed retries
* @param ex exception encountered
* @param rand random number generator
* @param ugi user group information
* @throws IOException if setup fails after all retries
*/","* If multiple clients with the same principal try to connect to the same
     * server at the same time, the server assumes a replay attack is in
     * progress. This is a feature of kerberos. In order to work around this,
     * what is done is that the client backs off randomly and tries to initiate
     * the connection again. The other problem is to do with ticket expiry. To
     * handle that, a relogin is attempted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,isViewFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem),50,52,"/**
* Checks if the given file system has a specific mask.
* @param fileSystem file system to check
*/","* Check if the FileSystem is a ViewFileSystem.
   *
   * @param fileSystem file system.
   * @return true if the fileSystem is ViewFileSystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,open,"org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",171,175,"/**
* Wraps fs.m1() call to fetch data from file handle.
* @param fd PathHandle representing the file descriptor
* @param bufferSize buffer size for data retrieval
* @return FSDataInputStream object or null on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",561,566,"/**
* Calls underlying file system to check permission on given path.
* @param f Path object to check
* @param absolutePermission Permission to verify (absolute)
* @return true if permitted, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuota,"org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)",1965,1968,"/**
* Applies file system quotas to the specified path.
* @param src Path to apply quotas to
* @param namespaceQuota quota for namespace usage
* @param storagespaceQuota quota for storage space usage
*/","* Set quota for the given {@link Path}.
   *
   * @param src the target path to set quota for
   * @param namespaceQuota the namespace quota (i.e., # of files/directories)
   *                       to set
   * @param storagespaceQuota the storage space quota to set
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuotaByStorageType,"org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)",1978,1981,"/**
 * Applies mask to source file based on storage type and quota.
 * @param src Path to source file
 * @param type Type of storage (e.g. disk, cloud)
 * @param quota Maximum allowed size in bytes
 */","* Set per storage type quota for the given {@link Path}.
   *
   * @param src the target path to set storage type quota for
   * @param type the storage type to set
   * @param quota the quota to set for the given storage type
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createMultipartUploader,org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),4987,4992,"/**
* Configures MultipartUploader with base path.
* @param basePath directory path where upload will take place
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path),277,281,"/**
* Wraps file system iterator to remote iterator.
* @param path directory path to iterate over
* @return Path iterator or null if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),2261,2264,"/**
* Retrieves an iterator of located file statuses for the given path.
* @param f the directory or file to list
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * Return the file's status and block locations If the path is a file.
   *
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories
   *         in the given path
   *
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),980,984,"/**
* Returns an iterator over located file statuses in the given directory.
* @param f directory path to iterate over
* @return iterator over LocatedFileStatus objects or null if not found
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given patch
   * @throws IOException if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",214,219,"/**
* Delegates file status iteration to underlying filesystem.
* @param f file path
* @param filter optional filter for iteration results
* @return iterator over located file statuses or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)",97,102,"/**
* Creates an AccessControlException for a forbidden file system operation.
* @param operation the attempted operation (e.g., read, write)
* @param p the affected file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)",175,180,"/**
* Creates an AccessControlException for a read-only directory.
* @param operation the attempted file system operation
* @param p the affected file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String),41,43,"/**
* Constructs an AuthorizationException with a custom error message.
* @param message detailed explanation of the authorization issue
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolveLink,org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path),498,500,"/**
 * Calls underlying file system's m1 method to process a path.
 * @param f the input path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileChecksum,org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path),2980,2982,"/**
* Calculates file checksum up to max bytes.
* @param f Path to the file
*/","* Get the checksum of a file, if the FS supports checksums.
   *
   * @param f The file path
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",507,510,"/**
* Delegates file checksum calculation to underlying filesystem service.
* @param f path of file to calculate checksum for
* @param length total length of file in bytes
* @return FileChecksum object if successful, otherwise throws IOException
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setXAttr,"org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",3244,3248,"/**
* Creates or replaces extended attributes on a file.
* @param path file path
* @param name attribute name
* @param value attribute value
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default outcome).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",629,633,"/**
* Invokes underlying file system operation with provided parameters.
* @param path file system path
* @param name attribute name
* @param value attribute value
* @param flag set of flags for XAttrSetFlag enum
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,"org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",645,649,"/**
* Delegates file system operation to underlying storage.
* @param path file system path
* @param names list of string identifiers
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listXAttrs,org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path),651,654,"/**
 * Delegates file system operation to underlying FS implementation.
 * @param path file or directory path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),661,664,"/**
* Invokes file system operation on given path.
* @param src Path to operate on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",666,670,"/**
* Calls file system's m1 method with given Path and policy name.
* @param src source path to process
* @param policyName access control policy name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),672,675,"/**
 * Calls the file system's m1 operation on the specified source path.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),677,681,"/**
 * Delegate to underlying file system implementation to process block storage policy.
 * @param src input path to be processed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies(),1925,1939,"/**
* Retrieves all available block storage policies.
* @return Collection of BlockStoragePolicySpi objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies(),683,687,"/**
* Delegates call to underlying file system to fetch block storage policies.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",4806,4816,"/**
* Opens a file with specified options and returns an input stream.
* @param path file location
* @param parameters open file parameters
*/","* Execute the actual open file operation.
   *
   * This is invoked from {@code FSDataInputStreamBuilder.build()}
   * and from {@link DelegateToFileSystem} and is where
   * the action of opening the file should begin.
   *
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1090,1101,"/**
* Opens file for reading with standard options.
* @param path file system path
* @param parameters open file parameters
* @return FSDataInputStream object or null on error
*/","* Open the file as a blocking call to {@link #open(Path, int)}.
   *
   * {@inheritDoc}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1603,1612,"/**
* Retrieves data from a file at the specified path using the provided open file parameters.
* @param path file location
* @param parameters configuration for opening the file
* @return FSDataInputStream containing the file's contents or null if an error occurs
*/","* Open a file with the given set of options.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)}in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",4834,4852,"/**
* Opens a file asynchronously using the provided path handle and open file parameters.
* @param pathHandle unique file identifier
* @param parameters OpenFileParameters object containing file options
* @return CompletableFuture<FSDataInputStream> representing the opened file stream or an error if unsupported.","* Execute the actual open file operation.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param pathHandle path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key
   * @throws UnsupportedOperationException PathHandles are not supported.
   * This may be deferred until the future is evaluated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,isValidName,org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String),322,325,"/**
* Calls file system's m1 method with given source string.
* @param src input string to be processed by file system
* @return result of file system's m1 method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",770,776,"/**
* Resolves file system path and applies ACL specification.
* @param path Path to resolve
* @param aclSpec List of AclEntry objects to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",327,331,"/**
* Delegates file system operation to underlying storage.
* @param path file system path
* @param aclSpec access control list entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",778,784,"/**
* Resolves ACL specification for a given file system path.
* @param path the file system path to resolve
* @param aclSpec list of access control entries to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAclEntries,"org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",333,337,"/**
* Delegate file system operation to underlying storage.
* @param path File path
* @param aclSpec Access control list specification
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),786,792,"/**
* Resolves an inode tree and delegates remaining operations to the target file system.
* @param path input file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path),339,342,"/**
 * Delegates file system operation to underlying storage engine.
 * @param path file location in the file system
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",868,875,"/**
* Resolves file system path and recursively updates snapshots.
* @param path Path to resolve
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameSnapshot,"org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",403,407,"/**
* Calls MyFS implementation of method m1.
* @param path file system path
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),401,404,"/**
* Calls underlying file system to perform operation 'm1' on the specified path.
* @param path file system location
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),884,889,"/**
* Resolves file system hierarchy by traversing inode tree.
*@param path input file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),415,418,"/**
* Calls underlying file system's implementation of m1.
* @param path directory path to operate on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),899,905,"/**
* Resolves and processes a path in the file system.
* @param src input path to resolve
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),426,430,"/**
* Calls MyFS implementation of m1 on the provided path.
* @param src file system path to operate on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies(),424,428,"/**
* Returns collection of BlockStoragePolicySpi instances from file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStoragePolicies,org.apache.hadoop.fs.FileContext:getAllStoragePolicies(),2916,2919,"/**
* Retrieves block storage policies using the default file system.
* @return Collection of BlockStoragePolicySpi objects
*/","* Retrieve all the storage policies supported by this file system.
   *
   * @return all storage policies supported by this filesystem.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFs:getAllStoragePolicies(),438,442,"/**
* Retrieves block storage policies from the file system.
* @return collection of BlockStoragePolicySpi instances
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,supportsSymlinks,org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks(),436,439,"/**
* Calls and returns result of myFs.m1().",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,supportsSymlinks,org.apache.hadoop.fs.FilterFs:supportsSymlinks(),296,299,"/**
* Delegates call to file system service.
* @return true if operation is successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSymlink,"org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",301,305,"/**
* Delegates file system operation to underlying storage.
* @param target target file path
* @param link symbolic link to create
* @param createParent flag to create parent directories if needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path),669,674,"/**
* Resolves path and delegates to target file system's m3 method.
* @param f input path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getLinkTarget,org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path),307,310,"/**
* Delegate method to perform file operation on the given path.
* @param f input file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String),459,462,"/**
* Executes file system operation 'm1' with specified renewer.
* @param renewer parameter passed to underlying FS operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getDelegationTokens,org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String),317,320,"/**
* Retrieves token list from file system.
* @param renewer unique identifier of token renewer
* @return List of Token objects or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1073,1078,"/**
* Returns checksum of a file at the specified path.
* @param f file path (must be a file for checksum computation)
* @throws FileNotFoundException if path does not point to a file
* @throws IOException on I/O error during checksum calculation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,open,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1301,1306,"/**
* Throws exception if given path is a directory instead of a file.
* @param f Path to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initializeMountedFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List),943,959,"/**
* Initializes a map of FileSystems from the provided list of mount points.
* @param mountPoints list of InodeTree.MountPoint<FileSystem> objects
* @return Map<String, FileSystem> containing initialized file systems
*/","* Initialize the target filesystem for all mount points.
   * @param mountPoints The mount points
   * @return Mapping of mount point and the initialized target filesystems
   * @throws RuntimeException when the target file system cannot be initialized",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String),732,761,"/**
* Fetches a list of tokens from all mount points and the root fallback filesystem.
* @param renewer string used for token generation
* @return List of Token<?> objects or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1422,1425,"/**
* Throws NotInMountpointException for given file or directory.
* @param path Path to file or directory
* @param name Name of attribute (not used in this method)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1427,1430,"/**
* Throws NotInMountpointException when trying to access getXAttrs at the given path.
* @param path file system path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1432,1436,"/**
* Throws exception when trying to fetch extended attributes from an unmounted volume.
* @param path file system path
* @throws NotInMountpointException if the specified path is not mounted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1438,1441,"/**
* Throws an exception when attempting to list extended attributes on a non-mountpoint directory.
* @param path directory path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1785,1788,"/**
* Throws an exception when attempting to access server defaults for an unmounted file.
* @param f Path to the unmounted file
* @throws NotInMountpointException if server defaults cannot be retrieved
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path),1790,1793,"/**
* Returns the function mask for the given file path.
* @param f Path to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path),1795,1798,"/**
* Returns mask value indicating disk is not in mount point.
* @param f Path object representing disk to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1848,1851,"/**
* Throws exception when trying to access an attribute outside mount point.
* @param path file system path
* @throws IOException if operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1853,1856,"/**
* Throws NotInMountpointException if the specified file is not in mount point.
* @param path file path to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1858,1862,"/**
* Throws exception if file system attributes cannot be retrieved.
* @param path file system path to access
* @param names attribute names to retrieve
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1864,1867,"/**
* Throws exception when attempting to list extended attributes outside mount point.
* @param path file system path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path),1896,1899,"/**
* Throws an exception when quota usage is not available in the given mount point.
* @param f Path to the mount point
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),1920,1923,"/**
* Throws exception when attempting to fetch storage policy for an unmounted path.
* @param src path to check for mount status
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,serializeToString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString(),111,116,"/**
* Constructs function mask by concatenating resolved destination path, 
* internal separator, source regex string and replacement string.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(),961,964,"/**
 * Returns the function mask value.
 * @throws NotInMountpointException if operation is not supported
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(),966,969,"/**
* Returns the function mask (short) associated with this operation.
* @throws NotInMountpointException if getDefaultReplication is not supported.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(),971,974,"/**
* Returns server defaults for file system with given function mask.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)",366,378,"/**
* Deletes a file or directory.
* @param f Path to the file/directory
* @param recursive whether to recursively delete subdirectories
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path),451,469,"/**
* Resolves file system path and returns an iterator of file statuses.
* @param f the input file path
* @return RemoteIterator of FileStatus objects or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path),471,490,"/**
* Resolves file path and returns iterator for located file status.
* @param f input file path
* @return RemoteIterator<LocatedFileStatus> or WrappingRemoteIterator if resolved
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildResolveResultForRegexMountPoint,"org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",1053,1081,"/**
* Resolves file system path and returns a ResolveResult object.
* @param resultKind type of resolution (e.g. ResultKind)
* @param resolvedPathStr resolved path string
* @param targetOfResolvedPathStr target of resolved path string
* @param remainingPath remaining path after resolution
* @return ResolveResult object or null on error","* Build resolve result.
   * Here's an example
   * Mountpoint: fs.viewfs.mounttable.mt
   *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(??&lt;username&gt;\w+)
   * Value: /targetTestRoot/$username
   * Dir path to test:
   * viewfs://mt/user/hadoop_user1/hadoop_dir1
   * Expect path: /targetTestRoot/hadoop-user1/hadoop_dir1
   * resolvedPathStr: /user/hadoop_user1
   * targetOfResolvedPathStr: /targetTestRoot/hadoop-user1
   * remainingPath: /hadoop_dir1
   *
   * @param resultKind resultKind.
   * @param resolvedPathStr resolvedPathStr.
   * @param targetOfResolvedPathStr targetOfResolvedPathStr.
   * @param remainingPath remainingPath.
   * @return targetFileSystem or null on exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,fsGetter,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter(),216,219,"/**
* Returns FsGetter instance with custom mask.
* @return Custom FsGetter object
*/","* This method is overridden because in ViewFileSystemOverloadScheme if
   * overloaded scheme matches with mounted target fs scheme, file system
   * should be created without going into {@literal fs.<scheme>.impl} based
   * resolution. Otherwise it will end up in an infinite loop as the target
   * will be resolved again to ViewFileSystemOverloadScheme as
   * {@literal fs.<scheme>.impl} points to ViewFileSystemOverloadScheme.
   * So, below method will initialize the
   * {@literal fs.viewfs.overload.scheme.target.<scheme>.impl}.
   * Other schemes can follow fs.newInstance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,getBlockLocations,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations(),112,115,"/**
* Returns an array of block locations.
* @return array of BlockLocation objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",167,169,"/**
* Initializes an INodeDir instance with specified path and user identity.
* @param pathToNode directory path
* @param aUgi user's identity information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])",344,349,"/**
* Initializes an INodeLink instance with the provided parameters.
* @param pathToNode node path
* @param aUgi user group information
* @param targetMergeFs target file system
* @param aTargetDirLinkList list of target directory links
*/",* Construct a mergeLink or nfly.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)",354,362,"/**
* Initializes the node link with specified path, user credentials, and file system creation method.
* @param pathToNode path to the node
* @param aUgi user credentials
* @param createFileSystemMethod function to create file system
* @param aTargetDirLink target directory link
*/",* Construct a simple link (i.e. not a mergeLink).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addLink,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",222,228,"/**
* Throws exception if file already exists or creates new link.
* @param pathComponent component of the file path
* @param link node link to create (or null for existing check) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildLinkRegexEntry,"org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",818,845,"/**
* Creates a LinkEntry object from configuration and mount entry data.
* @param config Configuration object
* @param ugi UserGroupInformation object
* @param mntEntryStrippedKey Mount entry key (stripped)
* @param mntEntryValue Mount entry value
* @return LinkEntry object or null if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,processThrowable,"org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])",917,933,"/**
* Logs and propagates failure to perform operation on NflyNode.
* @param nflyNode targeted node
* @param op operation being performed
* @param t underlying exception
* @param ioExceptions list of IOExceptions to add to, or null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory(),861,864,"/**
* Invokes M1 operation on initial node's file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,replaceRegexCaptureGroupInPath,"org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)",246,261,"/**
* Replaces placeholders in the destination path with actual values.
* @param parsedDestPath initial destination path
* @param srcMatcher source matcher object
* @param regexGroupNameOrIndexStr group name or index to replace
* @param groupRepresentationStrSetInDest set of variable names in dest path
* @return updated destination path with replaced placeholders
*/","* Use capture group named regexGroupNameOrIndexStr in mather to replace
   * parsedDestPath.
   * E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   * srcMatcher is from /user/hadoop.
   * Then the params will be like following.
   * parsedDestPath: s3://$user.apache.com/_${user},
   * regexGroupNameOrIndexStr: user
   * groupRepresentationStrSetInDest: {user:$user; user:${user}}
   * return value will be s3://hadoop.apache.com/_hadoop
   * @param parsedDestPath
   * @param srcMatcher
   * @param regexGroupNameOrIndexStr
   * @param groupRepresentationStrSetInDest
   * @return return parsedDestPath while ${var},$var replaced or
   * parsedDestPath nothing found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootDir,org.apache.hadoop.fs.viewfs.InodeTree:getRootDir(),520,523,"/**
* Returns the root directory as an INodeDir.
* @return root node cast to INodeDir
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootLink(),525,528,"/**
* Returns the root node link with type T.
* @return INodeLink object representing the root node
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootFallbackLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink(),543,546,"/**
* Returns the fallback link of the root node.
* @return Fallback link to the root node
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStart,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart(),156,184,"/**
* Starts or resumes the async call processor for the current thread.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,kill,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon),192,198,"/**
* Kills a daemon and updates the running state.
* @param d Daemon object to be killed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,offer,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object),101,104,"/**
* Adds item to queue and asserts its successful addition.
*@param c item to be added
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)",246,265,"/**
* Decrypts data using provided Decryptor and writes result to output buffer.
* @param decryptor encryption handler
* @param inBuffer input data to decrypt
* @param outBuffer output buffer for decrypted data
* @param padding block size for padding (if any)
*/","* Do the decryption using inBuffer as input and outBuffer as output.
   * Upon return, inBuffer is cleared; the decrypted data starts at 
   * outBuffer.position() and ends at outBuffer.limit();",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,checkState,org.apache.hadoop.crypto.OpensslCipher:checkState(),289,291,"/**
* Validates the application context is non-null.
* @param context the application context (must not be null)
*/",Check whether context is initialized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,parentZNodeExists,org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists(),341,350,"/**
* Checks if the specified ZNode exists in ZooKeeper.
* @return true if ZNode exists, false otherwise
*/","* @return true if the configured parent znode exists
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getConfigViewFsPrefix,org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(),43,46,"/**
* Returns the default mount table prefix.
* @return string representing the default mount table prefix
*/","* Get the config variable prefix for the default mount table
   * @return the config variable prefix for the default mount table",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1537,1542,"/**
* Verifies if a given path is a file and returns its checksum.
* @param f path to the file to check
* @throws FileNotFoundException if the path points to a directory, not a file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1732,1737,"/**
* Throws exception when path points to directory instead of file.
* @throws FileNotFoundException if the path does not point to a regular file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,deserializeFromString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String),125,136,"/**
* Constructs RegexMountPointResolvedDstPathReplaceInterceptor from serialized string.
* @param serializedString serialized string containing interceptor data
* @return RegexMountPointResolvedDstPathReplaceInterceptor object or null on serialization error
*/","* Create interceptor from config string. The string should be in
   * replaceresolvedpath:wordToReplace:replaceString
   * Note that we'll assume there's no ':' in the regex for the moment.
   *
   * @return Interceptor instance or null on bad config.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,getReturnValue,org.apache.hadoop.io.retry.CallReturn:getReturnValue(),71,77,"/**
* Returns the function mask object after checking for exceptions and validating return state.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,getReadableByteChannel,org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel(),81,86,"/**
* Returns a readable byte channel from the socket's input stream.
* @return SocketInputStream object
*/","* @return an underlying ReadableByteChannel implementation.
   * @throws IllegalStateException if this socket does not have a channel",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",143,145,"/**
* Calculates a mask value based on file size.
* @param file Path object representing the file
* @param fileSize total size of the file in bytes
* @return calculated mask value as a long integer
*/","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return checksum length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getFilesystem,org.apache.hadoop.fs.DF:getFilesystem(),72,82,"/**
* Retrieves filesystem mask value based on platform.
* @throws IOException if an I/O error occurs
*/","* @return a string indicating which filesystem volume we're checking.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getMount,org.apache.hadoop.fs.DF:getMount(),110,127,"/**
* Retrieves the mount path for file system operations.
* @throws IOException if directory does not exist
*/","* @return the filesystem mount point for the indicated volume.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,refresh,org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh(),44,47,"/**
 * Calculates mask value from primary data.
 * @param df primary data source
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getPercentUsed,org.apache.hadoop.fs.DF:getPercentUsed(),100,104,"/**
* Calculates utilization percentage of a resource.
* @return Utilization percentage as an integer value
*/","@return the amount of the volume full, as a percent.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,startPositionWithoutWindowsDrive,org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String),324,330,"/**
* Calculates a mask value based on the input file path.
* @param path file path to analyze
* @return 0, 2 or 3 depending on path structure and separator usage
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,toString,org.apache.hadoop.fs.Path:toString(),476,503,"/**
* Constructs a URL string from the provided URI components.
*@return fully constructed URL as a string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,<init>,"org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)",87,91,"/**
* Initializes the input checker with file and retry settings.
* @param file Path to the file being checked
* @param numOfRetries Number of retries for failed checks
*/","Constructor
   * 
   * @param file The name of the file to be read
   * @param numOfRetries Number of read retries when ChecksumError occurs
   * @param sum the type of Checksum engine
   * @param chunkSize maximun chunk size
   * @param checksumSize the number byte of each checksum
   * @param verifyChecksum verify check sum.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,seek,org.apache.hadoop.fs.FSInputChecker:seek(long),428,451,"/**
* Advances to the specified position and loads any necessary data.
* @param pos target file position
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,streamHasByteBufferRead,org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream),37,46,"/**
* Checks if the InputStream is compatible with the function mask.
* @param stream input stream to check
* @return true if compatible, false otherwise
*/",* Determine if a stream can do a byte buffer read via read(ByteBuffer buf),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,init,org.apache.hadoop.fs.audit.CommonAuditContext:init(),193,197,"/**
 * Applies mask to thread-specific data.
 * @param paramThread1 thread ID or related parameter
 */",* Initialize.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,noteEntryPoint,org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object),278,288,"/**
* Masks tool command by extracting and processing class name.
* @param tool Object containing tool data
*/","* Add the entry point as a context entry with the key
   * {@link AuditConstants#PARAM_COMMAND}
   * if it has not  already been recorded.
   * This is called via ToolRunner but may be used at any
   * other entry point.
   * @param tool object loaded/being launched.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>(),404,405,"/**
 * Private constructor to prevent direct instantiation of this class. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)",93,95,"/**
* Sets a function mask with the given key and value.
* @param key unique identifier
* @param value integer value to set
*/","* Set optional int parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)",108,111,"/**
* Converts floating-point value to FUNC_MASK using m1 function.
* @param key unique identifier
* @param value numeric value to convert
*/","* This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)",121,123,"/**
* Converts a long value to a bitmask using the specified key.
* @param key unique identifier
* @param value long value to convert
*/","* Set optional long parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use  {@link #optLong(String, long)} where possible.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)",136,139,"/**
* Calculates and returns the FUNC_MASK value based on input key and value.
* @param key unique identifier string
* @param value numeric value used for calculation
*/","* Pass an optional double parameter for the Builder.
   * This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)",207,209,"/**
* Sets FUNC_MASK bit in the configuration using the given key and value.
* @param key unique configuration identifier
* @param value integer value to be set for the key
*/","* Set mandatory int option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)",221,224,"/**
* Returns a FUNC_MASK value based on the given key and floating-point value.
* @param key unique identifier
* @param value numerical value to be converted
*/","* This parameter is converted to a long and passed
   * to {@link #mustLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use {@link #mustDouble(String, double)} to set floating point.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)",234,237,"/**
 * Returns a function mask with the specified key and value.
 * @param key unique identifier for the mask
 * @param value function mask value
 */","* Set mandatory long option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)",248,251,"/**
* Converts a floating-point value to an integer mask using FUNC1 function. 
* @param key unique identifier
* @param value numeric value to be converted
*/","* Set mandatory long option, despite passing in a floating
   * point value.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,getRow,org.apache.hadoop.tools.TableListing$Column:getRow(int),100,116,"/**
* Formats a single row into an array of strings with optional wrapping and justification.
* @param idx index of the row to format
*/","* Return the ith row of the column as a set of wrapped strings, each at
     * most wrapWidth in length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,from,org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer),40,42,"/**
* Creates an UploadHandle instance from the provided ByteBuffer.
* @param byteBuffer input data buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,equals,org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object),54,61,"/**
* Recursively compares this upload handle with another.
* @param other the object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,startLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",396,400,"/**
* Calls underlying file system's m1 method to process files.
* @param fsOutputFile path to output file in file system
* @param tmpLocalFile path to temporary local file
* @return Path object representing result of processing
*/","* Returns a local File that the user can write output to.  The caller
   * provides both the eventual FS target name and the local working
   * file.  If the FS is local, we write directly into the target.  If
   * the FS is remote, we write into the tmp local area.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setWriteChecksum,org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean),517,520,"/**
 * Calls file system's m1 method with checksum flag.
 * @param writeChecksum whether to write checksum
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",86,88,"/**
* Constructs an FsPermission object with specified permissions.
* @param u user read/write/execute permission
* @param g group read/write/execute permission
* @param o other (owner) read/write/execute permission
* @param sb sticky bit flag
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,fromShort,org.apache.hadoop.fs.permission.FsPermission:fromShort(short),174,177,"/**
* Applies a mask function to FsAction values based on the input n value.
* @param n input value used to select and combine FsAction values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,"org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)",247,277,"/**
* Calculates effective file system action for ACL entry.
* @param entry AclEntry object
* @param permArg additional permission bits to consider
* @return FsAction value or throws IllegalArgumentException if invalid input
*/","* Get the effective permission for the AclEntry. <br>
   * Recommended to use this API ONLY if client communicates with the old
   * NameNode, needs to pass the Permission for the path to get effective
   * permission, else use {@link AclStatus#getEffectivePermission(AclEntry)}.
   * @param entry AclEntry to get the effective action
   * @param permArg Permission for the path. However if the client is NOT
   *          communicating with old namenode, then this argument will not have
   *          any preference.
   * @return Returns the effective permission for the entry.
   * @throws IllegalArgumentException If the client communicating with old
   *           namenode and permission is not passed as an argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,<init>,"org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",72,76,"/**
* Initializes a new PermissionStatus object with specified details.
* @param user the username
* @param group the group name
* @param permission the file system permissions","* Constructor.
   *
   * @param user user.
   * @param group group.
   * @param permission permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclEntry,"org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)",263,323,"/**
* Parses ACL string into AclEntry object.
* @param aclStr ACL specification string
* @param includePermission whether to include permission in the entry
* @return AclEntry object or null if invalid
*/","* Parses a string representation of an ACL into a AclEntry object.<br>
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclStr
   *          String representation of an ACL.<br>
   *          Example: ""user:foo:rw-""
   * @param includePermission
   *          for setAcl operations this will be true. i.e. Acl should include
   *          permissions.<br>
   *          But for removeAcl operation it will be false. i.e. Acl should not
   *          contain permissions.<br>
   *          Example: ""user:foo,group:bar,mask::""
   * @return Returns an {@link AclEntry} object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,<init>,"org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",65,70,"/**
* Initializes a new FsCreateMode instance with the given permission masks.
* @param masked permissions to apply when creating a file system object
* @param unmasked initial permissions for the created file system object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,equals,org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object),88,101,"/**
* Compares this FsCreateModes object with another.
* @param o the Object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toStringStable,org.apache.hadoop.fs.permission.AclEntry:toStringStable(),119,136,"/**
* Builds and returns a string representation of an ACL entry.
* @return formatted string or empty string if no components are present
*/","* Returns a string representation guaranteed to be stable across versions to
   * satisfy backward compatibility requirements, such as for shell command
   * output or serialization.  The format of this string representation matches
   * what is expected by the {@link #parseAclSpec(String, boolean)} and
   * {@link #parseAclEntry(String, boolean)} methods.
   *
   * @return stable, backward compatible string representation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntryType.java,toString,org.apache.hadoop.fs.permission.AclEntryType:toString(),59,65,"/**
 * Returns the function mask value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,"org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)",418,424,"/**
* Splits input string into an array of substrings based on the specified delimiter.
* @param str input string to split
* @param delim delimiter character or string
* @return Array of substrings or null if empty collection
*/","* Returns an arraylist of strings.
   * @param str the string values
   * @param delim delimiter to separate the values
   * @return the arraylist of the separated string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStringCollection,org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String),431,434,"/**
* Splits input string into collection of substrings using comma as delimiter.
* @param str input string to split
*/","* Returns a collection of strings.
   * @param str comma separated string values
   * @return an <code>ArrayList</code> of string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,<init>,"org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)",51,62,"/**
* Parses permission string into symbolic or octal format.
* @param modeStr string to parse
* @param symbolic Pattern for symbolic permissions (e.g. 'rwx')
* @param octal Pattern for octal permissions (e.g. '755')
*/","* Begin parsing permission stored in modeStr
   * 
   * @param modeStr Permission mode, either octal or symbolic
   * @param symbolic Use-case specific symbolic pattern to match against
   * @throws IllegalArgumentException if unable to parse modeStr",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,combineModes,"org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)",175,183,"/**
* Combines existing flags with new settings using bitwise operations.
* @param existing existing flag value
* @param exeOk whether to execute the new settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",649,651,"/**
 * Initializes the factory with directory mapping and configuration.
 * @param keyToBufferDir directory mapping
 * @param conf Hadoop configuration object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",526,528,"/**
* Initializes ArrayBlockFactory with key to buffer directory and configuration.
* @param keyToBufferDir path to buffer directory
* @param conf Hadoop configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,requestBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int),659,663,"/**
* Allocates a ByteBuffer from the pool with specified size limit.
* @param limit maximum size of the buffer to allocate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,releaseBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer),665,669,"/**
* Releases and returns a ByteBuffer to the pool.
* @param buffer ByteBuffer object to be released
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)",853,863,"/**
* Initializes a DiskBlock object with the specified file and upload parameters.
* @param bufferFile file to store disk blocks
* @param limit maximum block size
* @param index current block index
* @param statistics BlockUploadStatistics object for tracking progress
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",574,581,"/**
* Initializes a new byte array block with the given index and limit.
* @param index starting position
* @param limit maximum size of the block
* @param statistics upload statistics to be tracked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long),869,872,"/**
* Checks if the given byte count is within the allowed limit.
* @param bytes the number of bytes to check
* @return true if within limit, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString(),945,955,"/**
* Returns a string representation of the FileBlock object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,innerClose,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose(),905,933,"/**
* Handles destination state and performs necessary actions.
* @throws IOException if an I/O error occurs
*/","* The close operation will delete the destination file if it still
     * exists.
     *
     * @throws IOException IO problems",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,checkOpenState,org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState(),89,92,"/**
* Validates function mask state.
*@throws FSException if stream is closed.","* Check the open state.
   * @throws IllegalStateException if the stream is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterState,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",349,355,"/**
* Updates state to the next destination state.
* @param current current destination state
* @param next new destination state to transition to
*/","* Atomically enter a state, verifying current state.
     *
     * @param current current state. null means ""no check""
     * @param next    next state
     * @throws IllegalStateException if the current state is not as expected",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)",424,433,"/**
* Validates buffer parameters and returns a mask value (always 0).
* @param buffer byte array for validation
* @param offset starting position in the buffer
* @param length number of bytes to validate
*/","* Write a series of bytes from the buffer, from the offset.
     * Returns the number of bytes written.
     * Only valid in the state {@code Writing}.
     * Base class verifies the state but does no writing.
     *
     * @param buffer buffer.
     * @param offset offset.
     * @param length length of write.
     * @return number of bytes written.
     * @throws IOException trouble",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush(),442,444,"/**
* Writes mask to output.
* @throws IOException on write failure
*/","* Flush the output.
     * Only valid in the state {@code Writing}.
     * In the base class, this is a no-op
     *
     * @throws IOException any IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,set,"org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)",246,248,"/**
 * Updates a mask with a new value and key.
 * @param key unique identifier
 * @param value new value to be masked
 */","* Set an attribute. If the value is non-null/empty,
   * it will be used as a query parameter.
   *
   * @param key key to set
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,extractQueryParameters,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String),331,347,"/**
* Extracts and parses URI query parameters into a Map.
* @param header input string used to construct the URI
* @return map of parameter names to values or an empty map if none found
*/","* Split up the string. Uses httpClient: make sure it is on the classpath.
   * Any query param with a name but no value, e.g ?something is
   * returned in the map with an empty string as the value.
   * @param header URI to parse
   * @return a map of parameters.
   * @throws URISyntaxException failure to build URI from header.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long),602,605,"/**
* Checks if the provided byte count is within the allowed limit.
* @param bytes number of bytes to check
* @return true if within limit, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,remainingCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity(),607,610,"/**
* Calculates function mask value based on limit and m1() result.
* @return function mask integer value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,dataSize,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize(),718,720,"/**
* Returns a mask value based on optional data size.
* @return non-zero integer if dataSize is set; otherwise result of m1()
*/","* Get the amount of data; if there is no buffer then the size is 0.
       *
       * @return the amount of data available to upload.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long),733,736,"/**
* Checks if input byte count is within allowed mask limit.
* @param bytes total number of bytes to check
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hashCode,org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode(),3891,3894,"/**
* Calculates a composite value by combining scheme, authority, and user group identifier.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hashCode,org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode(),492,495,"/**
 * Delegates execution of m1() to the underlying RealUser instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,hashCode,org.apache.hadoop.ipc.Client$ConnectionId:hashCode(),1843,1859,"/**
* Computes a weighted sum of various factors.
* @return the computed value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,equals,org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object),3900,3913,"/**
* Checks if the given object matches this instance's key.
* @param obj Object to compare with
* @return true if matching, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,equals,org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object),481,490,"/**
* Performs deep equality check between this object and another.
* @param o the object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory(),78,81,"/**
* Calls underlying file system's implementation of m1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolvePath,org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path),616,619,"/**
* Applies a mask to the given file path.
* @param f original file path
*/","* Resolve the path following any symlinks or mount points
   * @param f to be resolved
   * @return fully qualified resolved path
   * 
   * @throws FileNotFoundException  If <code>f</code> does not exist
   * @throws AccessControlException if access denied
   * @throws IOException If an IO Error occurred
   * @throws UnresolvedLinkException If unresolved link occurred.
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   *
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,msync,org.apache.hadoop.fs.FileContext:msync(),1267,1269,"/**
 * Calls the default file system's m1() method.
 */","* Synchronize client metadata state.
   *
   * @throws IOException If an I/O error occurred.
   * @throws UnsupportedOperationException If file system for <code>f</code> is
   *                                       not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,msync,org.apache.hadoop.fs.FilterFs:msync(),127,130,"/**
* Invokes file system operation 'm1' to perform custom task.
* @throws IOException if I/O error occurs during execution
* @throws UnsupportedOperationException if unsupported operation is detected
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,printStatistics,org.apache.hadoop.fs.FileContext:printStatistics(),2412,2414,"/**
* Calls M1 on the default abstract file system.","* Prints the statistics to standard output. File System is identified by the
   * scheme and authority.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getStatistics,org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI),191,204,"/**
* Retrieves and caches statistics for a given URI.
* @param uri the input URI
* @return cached or newly computed Statistics object
*/","* Get the statistics for a particular file system.
   * 
   * @param uri
   *          used as key to lookup STATISTICS_TABLE. Only scheme and authority
   *          part of the uri are used.
   * @return a statistics object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path),209,213,"/**
* Wraps the file system iterator with a remote iterator.
* @param path file path to iterate over
* @return RemoteIterator of Path objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createMultipartUploader,org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),1634,1639,"/**
* Initializes MultipartUploaderBuilder with base path.
* @param basePath directory path for uploading files 
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,obtainContext,org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String),107,117,"/**
* Retrieves or initializes an AllocatorPerContext instance per context configuration item name.
* @param contextCfgItemName unique identifier for the context configuration item
*/","This method must be used to obtain the dir allocation context for a 
   * particular value of the context name. The context name must be an item
   * defined in the Configuration object for which we want to control the 
   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will
   * create a context for that name if it doesn't already exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStatistics,org.apache.hadoop.fs.FilterFs:getStatistics(),68,71,"/**
* Calls underlying file system to retrieve statistics.
* @return Statistics object containing file system metrics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getPos,org.apache.hadoop.fs.FSDataOutputStream:getPos(),97,99,"/**
* Delegates position cache operation to underlying PositionCache instance.
* @return result of PositionCache.m1()
*/","* Get the current position in the output stream.
   *
   * @return the current position in the output stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncFs,org.apache.hadoop.io.SequenceFile$Writer:syncFs(),1382,1387,"/**
* Writes mask value to output stream.
* @throws IOException if write operation fails
*/","* flush all currently written data to the file system.
     * @deprecated Use {@link #hsync()} or {@link #hflush()} instead
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hflush,org.apache.hadoop.io.SequenceFile$Writer:hflush(),1396,1401,"/**
* Recursively calls out's m1 method if out is not null.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hsync,org.apache.hadoop.io.SequenceFile$Writer:hsync(),1389,1394,"/**
 * Calls the 'm1' method on the output stream instance, if available.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,quota,org.apache.hadoop.fs.ContentSummary$Builder:quota(long),90,94,"/**
* Initializes builder with given quota and returns self-reference.
* @param quota maximum allowed value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceConsumed,org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long),96,100,"/**
* Initializes builder with consumed memory.
* @param spaceConsumed total memory used by object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceQuota,org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long),102,106,"/**
* Initializes builder with specified space quota.
* @param spaceQuota allocated storage capacity
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,build,org.apache.hadoop.fs.ContentSummary$Builder:build(),132,136,"/**
* Creates and returns ContentSummary object with masked file counts.
* @return ContentSummary object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)",87,100,"/**
* Initializes the application class loader with a specified URL path and parent.
* @param urls URL array for loading classes
* @param parent parent class loader (cannot be null)
* @param systemClasses list of system classes to load; null or empty uses default system classes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollection,org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String),490,495,"/**
* Generates a collection of unique strings from input string, 
* applying specified rules (m1 and m2).
* @param str input string to process
*/","* Splits a comma separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value. Duplicate and empty values are removed.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Collection</code> of <code>String</code> values, empty
   *         Collection if null String input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/LoggingStateChangeListener.java,<init>,org.apache.hadoop.service.LoggingStateChangeListener:<init>(),51,53,"/**
 * Initializes the listener with default logging settings.",* Log events to the static log for this class,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String),48,50,"/**
* Constructs ServiceStateException with specified error message.
* @param message detailed description of the service state exception","* Instantiate
   * @param message error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,"org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)",78,83,"/**
* Constructs a ServiceStateException with specified exit code and error details.
* @param exitCode service-specific exit code
* @param message human-readable error description
* @param cause underlying exception (if any)
*/","* Instantiate, using the specified exit code as the exit code
   * of the exception, irrespetive of any exit code supplied by any inner
   * cause.
   *
   * @param exitCode exit code to declare
   * @param message exception message
   * @param cause inner cause",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,"org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)",120,126,"/**
* Converts exception to RuntimeException or ServiceStateException.
* @param text error message
* @param fault underlying exception
* @return RuntimeException or ServiceStateException instance
*/","* Convert any exception into a {@link RuntimeException}.
   * If the caught exception is already of that type, it is typecast to a
   * {@link RuntimeException} and returned.
   *
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param text text to use if a new exception is created
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable),101,107,"/**
* Wraps Throwables into RuntimeException or ServiceStateException.
* @param fault Throwable to be wrapped
*/","* Convert any exception into a {@link RuntimeException}.
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,<init>,org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String),60,62,"/**
* Initializes service state with specified name and default status.
* @param name unique service identifier
*/","* Create the service state model in the {@link Service.STATE#NOTINITED}
   * state.
   *
   * @param name input name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,isInState,org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE),451,454,"/**
* Compares current service state to the specified expected state.
* @param expected target state to compare against
* @return true if states match, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,isValidStateTransition,"org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",149,153,"/**
* Evaluates state transition validity based on current and proposed states.
* @param current Current system state
* @param proposed Proposed new system state
* @return true if the state transition is valid, false otherwise
*/","* Is a state transition valid?
   * There are no checks for current==proposed
   * as that is considered a non-transition.
   *
   * using an array kills off all branch misprediction costs, at the expense
   * of cache line misses.
   *
   * @param current current state
   * @param proposed proposed new state
   * @return true if the transition to a new state is valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,toString,org.apache.hadoop.service.ServiceStateModel:toString(),159,163,"/**
* Concatenates user name and state with a separator.
* @return formatted string or empty if no name
*/","* return the state text as the toString() value
   * @return the current state's description",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,<init>,org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(),71,73,"/**
 * Initializes the Hadoop Uncaught Exception Handler with default settings.
 */","* Basic exception handler -logs simple exceptions, then continues.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,handle,org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal),125,131,"/**
* Handles signal interrupt by updating counters and notifying the handler.
* @param s Signal object containing relevant interrupt data
*/","* Handler for the JVM API for signal handling.
   * @param s signal raised",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,getService,org.apache.hadoop.service.launcher.InterruptEscalator:getService(),84,87,"/**
* Recursively fetches service instance.
* @return Service object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)",188,191,"/**
* Initializes forced shutdown of a service with specified time.
* @param service service to be shut down
* @param shutdownTimeMillis milliseconds until shutdown occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,lookup,org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String),153,160,"/**
* Retrieves the IRQ handler associated with a given signal name.
* @param signalName signal identifier
* @return IrqHandler instance or null if not found
*/","* Look up the handler for a signal.
   * @param signalName signal name
   * @return a handler if found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)",49,51,"/**
* Constructs ServiceLaunchException with specified exit code and underlying cause.
* @param exitCode service launch exit status
* @param cause root exception causing service launch failure
*/","* Create an exception with the specific exit code.
   * @param exitCode exit code
   * @param cause cause of the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)",58,60,"/**
* Constructs a custom service launch exception with specified exit code and error message.
* @param exitCode system exit code
* @param message detailed error description
*/","* Create an exception with the specific exit code and text.
   * @param exitCode exit code
   * @param message message to use in exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])",74,79,"/**
* Constructs a ServiceLaunchException with a formatted message and optional cause.
* @param exitCode service launch exit code
* @param format error message format string
* @param args variable arguments to fill the format string, last arg can be a Throwable for the cause
*/","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * <p>
   * If the last argument is a throwable, it becomes the cause of the exception.
   * It will also be used as a parameter for the format.
   * @param exitCode exit code
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)",1083,1086,"/**
* Constructs a KerberosDiagsFailure object with specified category and failure message.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])",91,94,"/**
* Constructs a ServiceLaunchException with a specific error code and message.
* @param exitCode service launch exit code
* @param cause underlying exception (if any)
* @param format error message format string
* @param args variable arguments for error message formatting
*/","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * @param exitCode exit code
   * @param cause inner cause
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,run,org.apache.hadoop.service.launcher.ServiceShutdownHook:run(),82,85,"/**
* Calls function m1(). 
*/","* Shutdown handler.
   * Query the service hook reference -if it is still valid the 
   * {@link Service#stop()} operation is invoked.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String),184,186,"/**
* Constructs a new ServiceLauncher instance with the given service class name.
* @param serviceClassName name of the service class to launch
*/","* Create an instance of the launcher.
   * @param serviceClassName classname of the service",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,toString,org.apache.hadoop.service.launcher.ServiceLauncher:toString(),253,264,"/**
* Constructs and returns the ServiceLauncher string.
* @return formatted String representation of ServiceLauncher
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,noteException,org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException),331,345,"/**
* Handles ExitUtil.ExitException by logging and storing the exception details.
* @param exitException ExitException instance with failure information
*/","* Record that an Exit Exception has been raised.
   * Save it to {@link #serviceException}, with its exit code in
   * {@link #serviceExitCode}
   * @param exitException exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,bindCommandOptions,org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions(),321,323,"/**
 * Initializes command options using function m1(). 
 */","* Set the {@link #commandOptions} field to the result of
   * {@link #createOptions()}; protected for subclasses and test access.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,loadConfigurationClasses,org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses(),422,449,"/**
* Creates and checks a specified number of Configuration instances.
* @return count of successful instances created
*/","* @return This creates all the configurations defined by
   * {@link #getConfigurationsToCreate()} , ensuring that
   * the resources have been pushed in.
   * If one cannot be loaded it is logged and the operation continues
   * except in the case that the class does load but it isn't actually
   * a subclass of {@link Configuration}.
   * @throws ExitUtil.ExitException if a loaded class is of the wrong type",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerServiceListener,org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),354,357,"/**
 * Registers a service state change listener with the specified callback.
 * @param l ServiceStateChangeListener instance to be notified on changes
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerGlobalListener,org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),369,371,"/**
* Registers service state change listener.
* @param l ServiceStateChangeListener instance to register
*/","* Register a global listener, which receives notifications
   * from the state change events of all services in the JVM
   * @param l listener",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterServiceListener,org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),359,362,"/**
* Registers a ServiceStateChangeListener with the specified listener. 
* @param l listener to be registered
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterGlobalListener,org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),378,380,"/**
* Checks if listener is already registered with global listeners.
* @param l ServiceStateChangeListener to check
*/","* unregister a global listener.
   * @param l listener to unregister
   * @return true if the listener was found (and then deleted)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,resetGlobalListeners,org.apache.hadoop.service.AbstractService:resetGlobalListeners(),385,388,"/**
* Initializes mask functionality.
* @ VisibleForTesting indicates this is for testing purposes only. 
* This function calls m1() on globalListeners.",* Package-scoped method for testing -resets the global listener list,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,notifyListeners,org.apache.hadoop.service.AbstractService:notifyListeners(),409,416,"/**
* Notifies registered listeners and global listeners of an event.
* @throws RuntimeException if listeners throw an exception
*/","* Notify local and global listeners of state changes.
   * Exceptions raised by listeners are NOT passed up.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,getServiceState,org.apache.hadoop.service.AbstractService:getServiceState(),118,121,"/**
* Returns the functional mask of the current state.
* @return Functional mask value as defined in the state model.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,serviceInit,org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration),312,317,"/**
* Updates configuration mask when provided Configuration object differs from expected.
* @param conf new Configuration object
*/","* All initialization code needed by a service.
   *
   * This method will only ever be called once during the lifecycle of
   * a specific service instance.
   *
   * Implementations do not need to be synchronized as the logic
   * in {@link #init(Configuration)} prevents re-entrancy.
   *
   * The base implementation checks to see if the subclass has created
   * a new configuration instance, and if so, updates the base class value
   * @param conf configuration
   * @throws Exception on a failure -these will be caught,
   * possibly wrapped, and will trigger a service stop",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStop,org.apache.hadoop.util.JvmPauseMonitor:serviceStop(),88,100,"/**
* Stops the monitor thread and calls superclass method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStart,org.apache.hadoop.service.CompositeService:serviceStart(),115,126,"/**
* Starts a list of services and calls superclass method.
* @throws Exception if an error occurs during service startup
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,addIfService,org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object),88,95,"/**
* Checks if the given object is of type Service and calls m1() if so.
* @param object Object to check
* @return True if object is a Service, False otherwise
*/","* If the passed object is an instance of {@link Service},
   * add it to the list of services managed by this {@link CompositeService}
   * @param object object.
   * @return true if a service is added, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)",79,88,"/**
* Logs and returns an exception when stopping a service.
* @param log logging instance
* @param service Service object to stop
* @return Exception or null if successful
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @deprecated to be removed with 3.4.0. Use {@link #stopQuietly(Logger, Service)} instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)",100,108,"/**
* Catches exceptions during service termination.
* @param log logging utility
* @param service Service instance to be stopped
* @return Exception object if termination fails, or null on success
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @see ServiceOperations#stopQuietly(Service)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable),969,971,"/**
* Constructs a ProgressableOption instance from a given Progressable value.
* @param value the Progressable object to be wrapped
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable:<init>(short),37,39,"/**
* Initializes Writable with given short value.
* @param value short integer to be written.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long),1874,1876,"/**
 * Constructs a new LengthOption instance with the specified numeric value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long),1867,1869,"/**
* Initializes StartOption with a specified value.
* @param value unique option identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long),925,927,"/**
* Constructs BlockSizeOption with specified value.
* @param value long integer representing block size option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,read,org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(),31,37,"/**
* Retrieves byte value from scratch array using m1() function.
* @return byte value or -1 on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,reset,org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[]),83,85,"/**
* Processes multiple byte buffers using buffers.m1.
* @param input one or more ByteBuffer objects to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),233,246,"/**
* Determines whether input stream supports positioned reading.
* @param in InputStream to check
* @return true if positioned readable, false otherwise
*/","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the stream implements the interface (including a wrapped stream)
   * and that it declares the stream capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,isAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable(),433,435,"/**
* Evaluates a function mask based on internal calculations. 
* @return true if masked, false otherwise
*/","* Is the wrapped IO class loaded?
   * @return true if the instance is loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,toString,org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object),352,359,"/**
* Converts an object of type T to a string representation.
* @param instance the object to be converted
*/","* Convert an instance to a string form for output. This is a robust
   * operation which will convert any JSON-generating exceptions into
   * error text.
   * @param instance non-null instance
   * @return a JSON string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedFunction,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE),84,86,"/**
* Returns a function that wraps the input function with unchecked exception handling.
* @param fun the original function to be masked
*/","* Convert a {@link FunctionRaisingIOE} as a {@link Supplier}.
   * @param fun function to wrap
   * @param <T> type of input
   * @param <R> type of return value.
   * @return a new function which invokes the inner function and wraps
   * exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromInstance,org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object),236,238,"/**
* Applies two intermediate transformations (m1 and m2) to the given object. 
* @param instance object to transform
* @return transformed object of type T or null if an error occurs
*/","* clone by converting to JSON and back again.
   * This is much less efficient than any Java clone process.
   * @param instance instance to duplicate
   * @return a new instance
   * @throws IOException IO problems.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromBytes,org.apache.hadoop.util.JsonSerialization:fromBytes(byte[]),331,333,"/**
* Parses byte array into object using UTF-8 encoding.
* @param bytes input byte array
* @return parsed object of type T or null if failed
*/","* Deserialize from a byte array.
   * @param bytes byte array
   * @throws IOException IO problems
   * @throws EOFException not enough data
   * @return byte array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,<init>,org.apache.hadoop.io.VIntWritable:<init>(int),38,38,"/**
* Initializes VIntWritable with integer value.
* @param value integer value to be written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,equals,org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object),57,68,"/**
* Checks if the given object is a valid key.
* @param rhs Object to check
* @return true if object is key, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int),1009,1013,"/**
* Constructs SyncIntervalOption with specified value, 
* falling back to default if negative. 
* @param val sync interval value (negative values default to SYNC_INTERVAL)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int),932,934,"/**
* Initializes a new ReplicationOption instance with the specified value.
* @param value integer representation of replication option",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int),1881,1883,"/**
* Constructs a BufferSizeOption instance with the specified value.
* @param value buffer size option value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int),919,921,"/**
* Constructs a new BufferSizeOption with specified value.
* @param value size of buffer as an integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8),78,80,"/**
 * Copies contents from another UTF8 instance.
 * @param utf8 source UTF8 object to copy from
 */","* Construct from a given string.
   * @param utf8 input utf8.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,writeString,"org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)",351,365,"/**
* Encodes string to output stream with length masking.
* @param out DataOutput stream
* @param s String to encode
* @return encoded string length or -1 on error
*/","* @return Write a UTF-8 encoded string.
   *
   * @see DataOutput#writeUTF(String)
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,skip,org.apache.hadoop.io.UTF8:skip(java.io.DataInput),144,147,"/**
* Writes length and mask data to output stream.
* @param in input DataInput stream
*/","* Skips over one UTF8 in the input.
   * @param in datainput.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,skipCompressedByteArray,org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput),54,59,"/**
* Processes input data to extract and apply mask.
* @param in DataInput stream containing mask information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compare,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)",3258,3263,"/**
* Computes comparison score between two integers using a custom comparator.
* @param I first integer
* @param J second integer
* @return comparison score
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,org.apache.hadoop.io.SetFile:<init>(),35,35,"/**
* Initializes (constructs) this SetFile instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,org.apache.hadoop.io.ArrayFile:<init>(),35,35,"/**
* Prevents direct instantiation of ArrayFile class.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable:<init>(long),37,37,"/**
 * Constructs a new LongWritable instance with the specified value.
 * @param value long integer value to be wrapped in a LongWritable object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,seek,org.apache.hadoop.io.ArrayFile$Reader:seek(long),112,115,"/**
* Performs operation m1 on key with input n and recursively calls itself.
* @param n input parameter
*/","* Positions the reader before its <code>n</code>th value.
     *
     * @param n n key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,get,"org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)",147,151,"/**
* Updates key with value and returns updated key.
* @param n unused parameter
* @param value writable value to update key with
* @return updated key object
*/","* Return the <code>n</code>th value in the file.
     * @param n n key.
     * @param value value.
     * @throws IOException raised on errors performing I/O.
     * @return writable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class),952,954,"/**
 * Constructs a new ValueClassOption instance with the specified value class.
 * @param value the Class of the value to be used in this option",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class),270,272,"/**
* Constructs a new KeyClassOption instance with the specified class.
* @param value the class to be used as an option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class),945,947,"/**
 * Initializes a new instance of KeyClassOption with the specified class type.
 * @param value Class type to be used in this option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable:<init>(byte),34,34,"/**
* Initializes a new instance with the specified byte value.
* @param value the byte value to initialize this object with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,<init>,org.apache.hadoop.io.OutputBuffer:<init>(),71,73,"/**
 * Initializes an empty output buffer with default settings.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getData,org.apache.hadoop.io.OutputBuffer:getData(),86,86,"/**
* Delegates to buffer's m1() method.
*/","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return the current contents of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getLength,org.apache.hadoop.io.OutputBuffer:getLength(),93,93,"/**
* Calls m1() on the underlying buffer.","* Returns the length of the valid data currently in the buffer.
   * @return the length of the valid data
   *          currently in the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,reset,org.apache.hadoop.io.OutputBuffer:reset(),96,99,"/**
* Calls internal method m1 on buffer and returns this instance.",@return Resets the buffer to empty.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)",215,218,"/**
* Calls m1 with WritableComparable parameters.
* @param a and @param b will be cast to WritableComparable
*/","* Compare two Object.
   *
   * @param a the first object to be compared.
   * @param b the second object to be compared.
   * @return compare result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,binarySearch,org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable),782,799,"/**
* Performs binary search to find the index of a given key.
* @param key WritableComparable object to search for
* @return index of key or negative offset if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compareBytes,"org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)",230,233,"/**
* Performs fast byte comparison using m1 function.
* @param b1 first byte array
* @param s1 start index of first array
* @param l1 length of first array
* @param b2 second byte array
* @param s2 start index of second array
* @param l2 length of second array
*/","* Lexicographic order of binary data.
   * @param b1 b1.
   * @param s1 s1.
   * @param l1 l1.
   * @param b2 b2.
   * @param s2 s2.
   * @param l2 l2.
   * @return compare bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,hashBytes,"org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)",255,257,"/**
* Extracts specified byte range from input array.
* @param bytes input byte array
* @param length length of byte range to extract
*/","* Compute hash for binary data.
   * @param bytes bytes.
   * @param length length.
   * @return hash for binary data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readFloat,"org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)",290,292,"/**
* Computes a float value from byte array using m1 and m2 functions.
* @param bytes input byte array
* @param start starting index in the array
*/","* Parse a float from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return float from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readLong,"org.apache.hadoop.io.WritableComparator:readLong(byte[],int)",300,303,"/**
* Combines two 32-bit values from the input byte array into a single 64-bit long value.
* @param bytes input byte array
* @param start starting index for the combined value
* @return a 64-bit long value representing the combined 32-bit values
*/","* Parse a long from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return long from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readVInt,"org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)",347,349,"/**
* Extracts and returns an integer mask from the given byte array starting at the specified index.
* @param bytes byte array containing data
* @param start starting index in the byte array
*/","* Reads a zero-compressed encoded integer from a byte array and returns it.
   * @param bytes byte array with the encoded integer
   * @param start start index
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable:<init>(byte[]),61,63,"/**
 * Constructs a new BytesWritable instance from an existing byte array.
 * @param bytes the byte array to initialize with
 */","* Create a BytesWritable using the byte array as the initial value.
   * @param bytes This array becomes the backing storage for the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,get,org.apache.hadoop.io.BytesWritable:get(),102,105,"/**
 * Returns function mask bytes.
 */","* Get the data from the BytesWritable.
   * @deprecated Use {@link #getBytes()} instead.
   * @return data from the BytesWritable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,getSize,org.apache.hadoop.io.BytesWritable:getSize(),120,123,"/**
* Returns an integer mask value (deprecated).
*/","* Get the current size of the buffer.
   * @deprecated Use {@link #getLength()} instead.
   * @return current size of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setCapacity,org.apache.hadoop.io.BytesWritable:setCapacity(int),155,160,"/**
* Updates buffer size and bytes array to match specified capacity.
* @param capacity new buffer capacity
*/","* Change the capacity of the backing storage. The data is preserved.
   *
   * @param capacity The new capacity in bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable:<init>(int),37,37,"/**
 * Initializes an IntWritable instance with the specified integer value.
 * @param value The integer value to be stored in this object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)",89,101,"/**
* Creates a ByteBuffer with optional direct access and specified length.
* @param direct whether to use direct buffer access
* @param length length of the buffer
* @return ByteBuffer object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),103,119,"/**
* Updates the FUNC_MASK data structure with buffered values.
* @param buffer ByteBuffer containing updated data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,size,org.apache.hadoop.io.ElasticByteBufferPool:size(boolean),127,131,"/**
* Calls m1() and delegates to its m2() method.
* @param direct flag indicating whether to use direct access or not
*/","* Get the size of the buffer pool, for the specified buffer type.
   *
   * @param direct Whether the size is returned for direct buffers
   * @return The size",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(),159,161,"/**
* Initializes internal state.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,updateProgress,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long),3611,3616,"/**
* Updates progress and merges with overall processed bytes.
* @param bytesProcessed number of processed bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,readaheadStream,"org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)",102,149,"/**
* Fetches read-ahead request based on current position and previous read-ahead data.
* @param identifier unique identifier
* @param fd file descriptor
* @param curPos current position in the file
* @param readaheadLength requested length of read-ahead data
* @param maxOffsetToRead maximum offset to read from
* @param lastReadahead cached read-ahead request (if any)
* @return ReadaheadRequest object or null if not found","* Issue a request to readahead on the given file descriptor.
   * 
   * @param identifier a textual identifier that will be used in error
   * messages (e.g. the file name)
   * @param fd the file descriptor to read ahead
   * @param curPos the current offset at which reads are being issued
   * @param readaheadLength the configured length to read ahead
   * @param maxOffsetToRead the maximum offset that will be readahead
   *        (useful if, for example, only some segment of the file is
   *        requested by the user). Pass {@link Long#MAX_VALUE} to allow
   *        readahead to the end of the file.
   * @param lastReadahead the result returned by the previous invocation
   *        of this function on this file descriptor, or null if this is
   *        the first call
   * @return an object representing this outstanding request, or null
   *        if no readahead was performed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,append,org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable),97,99,"/**
 * Writes the given key to output with default value.
 * @param key WritableComparable object to be written
 */","* Append a key to a set.  The key must be strictly greater than the
     * previous key added to the set.
     * @param key input key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,next,org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable),144,147,"/**
* Compares a writable key with null.
* @param key WritableComparable object to compare
*/","* Read the next key in a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns true if such a key exists
     *    and false when at the end of the set.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,compare,"org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)",433,439,"/**
* Computes the mask for two byte arrays using bitwise operations.
* @param b1 first byte array
* @param s1 start index in b1
* @param l1 length of b1
* @param b2 second byte array
* @param s2 start index in b2
* @param l2 length of b2
* @return result mask value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,key,org.apache.hadoop.io.ArrayFile$Reader:key(),136,138,"/**
* Returns the function mask value from the key.
* @throws IOException if an I/O error occurs
*/","* Returns the key associated with the most recent call to {@link
     * #seek(long)}, {@link #next(Writable)}, or {@link
     * #get(long,Writable)}.
     *
     * @return key key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream),1860,1862,"/**
 * Constructs an InputStreamOption from a FSDataInputStream.
 * @param value underlying input stream data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator),289,291,"/**
 * Creates a ComparatorOption instance for the given WritableComparator value.
 * @param value the WritableComparator object to wrap
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable:<init>(double),41,43,"/**
* Constructs a new DoubleWritable instance with the given double value.
* @param value the initial numeric value to store in this object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VersionedWritable.java,readFields,org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput),49,54,"/**
* Validates function mask data by checking version compatibility.
* @param in DataInput stream containing the function mask data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator),476,478,"/**
 * Creates an option with a comparator that masks values.
 * @param value WritableComparator instance to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,access,"org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)",815,818,"/**
* Checks if a file has a specific access mask.
* @param path file system path
* @param desiredAccess required access right
* @return true if the file has the desired access, false otherwise
*/","* Checks whether the current process has desired access rights on
     * the given path.
     * 
     * Longer term this native function can be substituted with JDK7
     * function Files#isReadable, isWritable, isExecutable.
     *
     * @param path input path
     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE
     * @return true if access is allowed
     * @throws IOException I/O exception on error",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable(),360,362,"/**
* Checks if native code is loaded and enabled.
* @return true if native code is loaded, false otherwise
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO:isAvailable(),869,871,"/**
* Checks if native code is loaded and enabled.
* @return true if native code is loaded and enabled, false otherwise
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>(),38,48,"/**
* Initializes Unix groups mapping implementation.
* @param none
* @return none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>(),37,47,"/**
* Initializes Unix groups netgroup mapping implementation.
* Falls back to shell-based implementation if native code is not loaded.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeCrc32.java,isAvailable,org.apache.hadoop.util.NativeCrc32:isAvailable(),35,41,"/**
* Determines whether to use native code based on platform.
* @return true if native code should be used, false otherwise
*/",* Return true if the JNI-based native CRC extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,setPmdkSupportState,org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int),173,181,"/**
* Finds and sets the support state based on the given state code.
* @param stateCode unique identifier for a support state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getPmdkSupportStateMessage,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage(),183,189,"/**
* Returns the PDK library support state with optional lib path.
* @return concatenated string or null if lib path is not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isPmdkAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable(),191,194,"/**
* Checks PMDK support state.
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,chmod,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)",387,404,"/**
* Sets file mode and permissions on a given path.
* @param path file system path
* @param mode file mode (integer value)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,posixFadviseIfPossible,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",293,298,"/**
* Calls POSIX I/O function to perform an operation on a file descriptor.
* @param identifier native identifier for the file
* @param fd FileDescriptor object
* @param offset starting position in the file
* @param len length of operation
* @param flags bitwise flags controlling the operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,munmap,org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer),491,501,"/**
* Unmaps the specified MappedByteBuffer, if supported.
* @param buffer MappedByteBuffer object to unmapping
*/","* Unmaps the block from memory. See munmap(2).
     *
     * There isn't any portable way to unmap a memory region in Java.
     * So we use the sun.nio method here.
     * Note that unmapping a memory region could cause crashes if code
     * continues to reference the unmapped code.  However, if we don't
     * manually unmap the memory, we are dependent on the finalizer to
     * do it, and we have no idea when the finalizer will run.
     *
     * @param buffer    The buffer to unmap.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,freeDB,org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer),47,57,"/**
* Frees the given ByteBuffer if unmapping is supported.
* @param buffer ByteBuffer to be freed
*/","* Forcibly free the direct buffer.
   *
   * @param buffer buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getName,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)",629,648,"/**
* Fetches a cached or freshly retrieved user/group name by ID.
* @param id unique identifier (user or group)
* @return name associated with the ID
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOperatingSystemPageSize,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize(),289,291,"/**
*Makes native call to m1 function.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,memSync,org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion),252,258,"/**
* Applies a function mask to the given PmemMappedRegion, invoking POSIX I/O routines accordingly.
* @param region PmemMappedRegion object with associated metadata
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,"org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])",57,60,"/**
* Creates an ArrayWritable instance with specified class and array of writable elements.
* @param valueClass Class of the writable elements in the array
* @param values Array of writable elements to be stored in this ArrayWritable instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)",59,61,"/**
* Initializes buffer with specified range of bytes.
* @param buf byte array
* @param offset starting offset within the array
* @param limit ending offset within the array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,write,"org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)",132,134,"/**
 * Delegates to buffer's m1() method with provided input and length.
 * @param in DataInput stream
 * @param length data length
 */","* Writes bytes from a DataInput directly into the buffer.
   * @param in data input.
   * @param length length.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/TwoDArrayWritable.java,<init>,"org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])",38,41,"/**
* Initializes TwoDArrayWritable object with provided array data and type.
* @param valueClass Type of elements in the two-dimensional array
* @param values The actual array data to be wrapped
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,"org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)",70,72,"/**
* Creates an EnumSetWritable instance from the given EnumSet.
* @param value the EnumSet to wrap
* @param elementType the type of enum elements in the set
*/","* Construct a new EnumSetWritable. If the <tt>value</tt> argument is null or
   * its size is zero, the <tt>elementType</tt> argument must not be null. If
   * the argument <tt>value</tt>'s size is bigger than zero, the argument
   * <tt>elementType</tt> is not be used.
   * 
   * @param value enumSet value.
   * @param elementType elementType.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringArray,org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput),165,173,"/**
* Reads and returns an array of strings from the input stream.
* @param in DataInput stream
* @return Array of strings or null if length is -1
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeStringArray,"org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])",136,141,"/**
* Writes an array of strings to the output stream.
* @param out DataOutput object
* @param s array of strings to be written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,equals,org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object),200,216,"/**
* Recursively compares this object with the given object for equality.
* @param obj Object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)",56,65,"/**
* Dynamically resizes input buffer and reads from InputStream.
* @param in InputStream to read from
* @param len number of bytes to read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,fillReservoir,org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int),61,73,"/**
* Fills the reservoir with a random stream if it's near empty.
* @param min minimum size threshold for filling
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,<init>,org.apache.hadoop.io.DataInputBuffer:<init>(),134,136,"/**
* Creates a new instance of DataInputBuffer using an internal buffer.
*/",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)",149,151,"/**
* Invokes buffer-specific operation with input data.
* @param input byte array to process
* @param length number of bytes in input array
*/","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)",160,162,"/**
 * Calls m1 on the internal buffer with provided parameters.
 * @param input data to process
 * @param start starting index in input array
 * @param length number of bytes to process
 */","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getData,org.apache.hadoop.io.DataInputBuffer:getData(),164,166,"/**
* Delegates to buffer's m1() method to retrieve data. 
* @return Byte array containing retrieved data.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getPosition,org.apache.hadoop.io.DataInputBuffer:getPosition(),173,173,"/**
* Calls nested method on Buffer object.
* @return result of Buffer.m1()
*/","* Returns the current position in the input.
   *
   * @return position.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getLength,org.apache.hadoop.io.DataInputBuffer:getLength(),181,181,"/**
* Calls m1 on wrapped buffer object. 
* @return result of m1 on wrapped buffer object 
*/","* Returns the index one greater than the last valid character in the input
   * stream buffer.
   *
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map),69,93,"/**
* Initializes ECSchema with options, validating required parameters.
* @param allOptions map of schema options, must contain codec and data/parity units
*/","* Constructor with schema name and provided all options. Note the options may
   * contain additional information for the erasure codec to interpret further.
   * @param allOptions all schema options",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,"org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)",101,103,"/**
* Initializes an ECSchema object with provided parameters.
* @param codecName name of the encoding scheme
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
*/","* Constructor with key parameters provided.
   * @param codecName codec name
   * @param numDataUnits number of data units used in the schema
   * @param numParityUnits number os parity units used in the schema",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,setCodecOptions,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions),65,68,"/**
* Updates codec options and schema from ErasureCodecOptions.
* @param options ErasureCodecOptions instance to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumDataBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks(),54,56,"/**
* Returns a predefined mask value using the schema's m1 function.
* @return integer mask value
*/","* Get required data blocks count in a BlockGroup.
   * @return count of required data blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumParityBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks(),62,64,"/**
* Returns a mask value from database schema.
* @return integer mask value
*/","* Get required parity blocks count in a BlockGroup.
   * @return count of required parity blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",40,47,"/**
* Initializes the ErasureCodec instance with configuration and options.
* @param conf Hadoop Configuration object
* @param options ErasureCodecOptions object containing codec settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ErasureCoderOptions.java,<init>,"org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)",34,36,"/**
* Creates ErasureCoderOptions object with specified data and parity units.
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,getName,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName(),49,51,"/**
* Returns the function mask value from the database.
* @return The function mask string as retrieved from the schema.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,createBlockGrouper,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper(),86,91,"/**
* Creates a BlockGrouper instance with filtered data.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,<init>,org.apache.hadoop.io.erasurecode.CodecRegistry:<init>(),62,69,"/**
* Initializes the CodecRegistry with default factory instances.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,getCoderByName,"org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)",161,172,"/**
* Retrieves a RawErasureCoderFactory instance by codec and coder name.
* @param codecName name of the erasure codec
* @param coderName name of the erasure coder
* @return matching RawErasureCoderFactory or null if not found
*/","* Get a specific coder factory defined by codec name and coder name.
   * @param codecName name of the codec
   * @param coderName name of the coder
   * @return the specific coder, null if not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,makeBlockGroup,"org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",72,77,"/**
* Combines data and parity blocks into an ECBlockGroup instance.
* @param dataBlocks array of data blocks
* @param parityBlocks array of corresponding parity blocks
*/","* Calculating and organizing BlockGroup, to be called by ECManager
   * @param dataBlocks Data blocks to compute parity blocks against
   * @param parityBlocks To be computed parity blocks
   * @return ECBlockGroup.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlockGroup.java,getErasedCount,org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount(),61,73,"/**
* Counts the total number of blocks with m1() flags set.","* Get erased blocks count
   * @return erased count of blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[]),143,152,"/**
* Counts the number of blocks with a specific condition.
* @param inputBlocks array of ECBlock objects to scan
* @return count of matching blocks
*/","* Find out how many blocks are erased.
   * @param inputBlocks all the input blocks
   * @return number of erased blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,hasCodec,org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String),163,165,"/**
* Checks if codec is registered in registry.
* @param codecName name of codec to check
* @return true if codec is registered, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECChunk.java,toBuffers,org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),83,97,"/**
* Creates a byte buffer array from ECChunk objects.
* @param chunks Array of ECChunk objects
* @return Array of ByteBuffer objects or nulls if corresponding ECChunk is null
*/","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convert into buffers
   * @return an array of ByteBuffers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release(),61,66,"/**
 * Calls the corresponding method on the raw encoder, if not null. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release(),78,86,"/**
* Invokes m1 on raw encoders, if present.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),70,72,"/**
* Extracts and returns an array of blocks from the given ECBlockGroup.
* @param blockGroup group of blocks to extract
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),59,84,"/**
* Filters ECBlock instances in the given group based on specific criteria.
* @param blockGroup input ECBlockGroup to process
* @return array of filtered ECBlock instances or empty array if none found
*/","* Which blocks were erased ? For XOR it's simple we only allow and return one
   * erased block, either data or parity.
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),66,68,"/**
* Returns an array of EC blocks based on the provided group.
* @param blockGroup Group containing EC block data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits(),175,177,"/**
* Calls m1() on coder options.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits(),152,154,"/**
* Calls m1() on coderOptions.
* @return result of m1() call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits(),179,181,"/**
* Calls m1() on coderOptions and returns result.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits(),156,158,"/**
* Calls m1() method from coder options.
* @return result of coderOptions.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),71,82,"/**
* Extracts and combines ECBlock elements from the given group.
* @param blockGroup Group of ECBlock elements
* @return Array of extracted ECBlock elements
*/","* We have all the data blocks and parity blocks as input blocks for
   * recovering by default. It's codec specific
   * @param blockGroup blockGroup.
   * @return input blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release(),60,65,"/**
 * Calls {@code m1()} on associated RS raw decoder, if available. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release(),88,96,"/**
* Calls m1() on raw decoder and encoder, if not null.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Initializes an instance of XORRawDecoder with specified ErasureCoderOptions.
 * @param coderOptions options for erasure coding
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Initializes a new instance of DummyRawDecoder with provided ErasureCoderOptions.
 * @param coderOptions configuration settings for erasure coding.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Initializes an instance of AbstractNativeRawDecoder with given ErasureCoderOptions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer(),101,104,"/**
 * Returns whether to include function masks in filtering.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer(),98,101,"/**
* Returns whether the function mask is enabled.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)",150,153,"/**
* Calculates bitwise XOR of two integers.
* @param x first integer
* @param y second integer
* @return result of Xor operation
*/","* Compute the sum of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of addition",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)",162,165,"/**
* Retrieves value from multiplication table at specified coordinates.
* @param x row index (0 <= x < m1())
* @param y column index (0 <= y < m1())
* @return value at position (x, y) or throws AssertionError if indices are out of bounds
*/","* Compute the multiplication of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of multiplication",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,divide,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)",174,177,"/**
* Retrieves value from division table at specified coordinates.
* @param x row index
* @param y column index
* @return integer value or throws exception if invalid indices
*/","* Compute the division of two fields
   *
   * @param x input field
   * @param y input field
   * @return x/y",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,power,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)",186,200,"/**
* Computes a masked value based on integer logarithm and modular exponentiation.
* @param x input value
* @param n exponent
* @return the computed mask value
*/","* Compute power n of a field
   *
   * @param x input field
   * @param n power
   * @return x^n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunk,org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk),93,102,"/**
* Logs ECChunk data as a hexadecimal dump.
* @param chunk ECChunk object to process
*/","* Print data in hex format in a chunk.
   * @param chunk chunk.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Initializes an AbstractNativeRawEncoder instance with the given ErasureCoderOptions.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Initializes an instance of XORRawEncoder with given ErasureCoderOptions.
 * @param coderOptions configuration options for the encoder
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
* Constructs a DummyRawEncoder instance with specified ErasureCoderOptions.
* @param coderOptions options for erasure coding
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits(),183,185,"/**
 * Calls the underlying implementation to fetch a specific value.
 * @return The fetched value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits(),160,162,"/**
* Calls m1() on CoderOptions instance.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs(),202,204,"/**
* Calls m1() on coder options.","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs(),179,181,"/**
 * Calls the corresponding method on CoderOptions instance.
 */","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump(),210,212,"/**
 * Calls coderOptions.m1() and returns its result.
 */","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump(),187,189,"/**
* Calls coderOptions.m1() to determine result.
* @return true/false based on coderOptions.m1() outcome
*/","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])",98,109,"/**
* Computes and updates masked error signature for given inputs.
* @param inputs input array
* @param inputOffsets input offset array
* @param dataLen total length of data
* @param erasedIndexes indexes to be updated
* @param outputs output array
* @param outputOffsets output offset array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)",63,69,"/**
* Masks specified length in buffer.
* @param buffer input buffer
* @param len length to mask
* @return modified ByteBuffer object
*/","* Ensure a buffer filled with ZERO bytes from current readable/writable
   * position.
   * @param buffer a buffer ready to read / write certain size bytes
   * @return the buffer itself, with ZERO bytes written, the position and limit
   *         are not changed after the call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)",77,82,"/**
* Copies function mask into specified buffer region.
* @param buffer destination buffer
* @param offset starting position in buffer
* @param len length of data to copy
*/","* Ensure the buffer (either input or output) ready to read or write with ZERO
   * bytes fully in specified length of len.
   * @param buffer bytes array buffer
   * @return the buffer itself",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState(),62,84,"/**
* Initializes ByteArrayEncodingState with encoded inputs and outputs.
* @param encoder encoding object
* @param encodeLength encoding length
* @return ByteArrayEncodingState instance
*/",* Convert to a ByteArrayEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState(),69,85,"/**
* Creates a new ByteBufferEncodingState instance with encoded inputs and outputs.
* @return ByteBufferEncodingState object
*/",* Convert to a ByteBufferEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState(),73,89,"/**
* Initializes a new decoding state with masked input and output buffers.
* @param inputs input buffers to be masked
* @param outputs output buffers for masking result
*/",* Convert to a ByteBufferDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState(),66,91,"/**
* Initializes ByteArrayDecodingState with decoded inputs and outputs.
* @param decoder decoding context
* @param decodeLength total length to decode
* @param erasedIndexes array of indexes to erase
* @return initialized ByteArrayDecodingState object
*/",* Convert to a ByteArrayDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,initTables,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])",47,58,"/**
* Initializes and applies mask to coding matrix.
* @param k number of columns in the matrix
* @param rows number of rows in the matrix
* @param codingMatrix byte array representing the matrix
* @param matrixOffset starting index for modification
* @param gfTables array of GF256 tables used for computation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,genCauchyMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)",67,80,"/**
* Initializes byte array 'a' with specific mask values.
* @param a the byte array to initialize
* @param m the total size of the byte array
* @param k the size of the square sub-mask
*/","* Ported from Intel ISA-L library.
   *
   * @param k k.
   * @param a a.
   * @param m m.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GF256.java,gfInvertMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)",203,262,"/**
* Applies matrix inversion algorithm to create a mask.
* @param inMatrix input byte array
* @param outMatrix output byte array
* @param n square size of the matrices
*/","* Invert a matrix assuming it's invertible.
   *
   * Ported from Intel ISA-L library.
   *
   * @param inMatrix inMatrix.
   * @param outMatrix outMatrix.
   * @param n n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])",97,143,"/**
* Performs GF256 multiplication on multiple inputs and outputs.
* @param gfTables precomputed tables for GF256 operations
* @param dataLen length of the input data in bytes
* @param inputs array of input byte arrays
* @param inputOffsets array of input offsets
* @param outputs array of output byte arrays
* @param outputOffsets array of output offsets
*/","* Encode a group of inputs data and generate the outputs. It's also used for
   * decoding because, in this implementation, encoding and decoding are
   * unified.
   *
   * The algorithm is ported from Intel ISA-L library for compatible. It
   * leverages Java auto-vectorization support for performance.
   *
   * @param gfTables gfTables.
   * @param dataLen dataLen.
   * @param inputs inputs.
   * @param inputOffsets inputOffsets.
   * @param outputs outputs.
   * @param outputOffsets outputOffsets.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",152,200,"/**
* Performs GF256 multiplication on multiple inputs and stores results in outputs.
* @param gfTables precomputed tables for GF256 multiplication
* @param inputs array of ByteBuffer inputs to multiply
* @param outputs array of ByteBuffer outputs to store results
*/","* See above. Try to use the byte[] version when possible.
   *
   * @param gfTables gfTables.
   * @param inputs inputs.
   * @param outputs outputs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)",102,115,"/**
* Creates or retrieves a Galois field instance with the specified size and primitive polynomial.
* @param fieldSize size of the Galois field
* @param primitivePolynomial defining polynomial of the field
* @return GaloisField object for the given parameters, or null if not found (should be created)
*/","* Get the object performs Galois field arithmetics.
   *
   * @param fieldSize           size of the field
   * @param primitivePolynomial a primitive polynomial corresponds to the size
   * @return GaloisField.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,solveVandermondeSystem,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])",210,212,"/**
* Copies elements from array y to array x up to length specified by n.
* @param x destination array
* @param y source array
* @param n number of elements to copy
*/","* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such
   * that Vz=y. The output z will be placed in y.
   *
   * @param x the vector which describe the Vandermonde matrix
   * @param y right-hand side of the Vandermonde system equation. will be
   *          replaced the output in this vector",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlock.java,<init>,org.apache.hadoop.io.erasurecode.ECBlock:<init>(),37,39,"/**
* Initializes an empty ECBlock with default settings.
* @param isInitialized whether block has been initialized (false by default)
* @param isVerified whether block has been verified (false by default)",* A default constructor. isParity and isErased are false by default.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,compare,"org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)",110,113,"/**
* Calls superclass method with swapped parameters.
* @param b1 input array 1
* @param s1 start index 1
* @param l1 length 1
* @param b2 input array 2
* @param s2 start index 2
* @param l2 length 2
* @return result of superclass method call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close(),444,453,"/**
* Calls child and parent methods, resetting flag after successful execution.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,updatePos,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean),560,564,"/**
* Updates compressed stream position based on whether to include add-on data.
* @param shouldAddOn true to include add-on, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,updateReportedByteCount,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int),184,187,"/**
* Updates compressed stream read counter and calls m1 function.
* @param count number of bytes read from compressed stream
*/","* This method is called by the client of this
   * class in case there are any corrections in
   * the stream position.  One common example is
   * when client of this code removes starting BZ
   * characters from the compressed stream.
   *
   * @param count count bytes are added to the reported bytes
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,readAByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream),196,202,"/**
* Reads and masks input stream to retrieve a single integer value.
* @param inStream InputStream object to read from
* @return the masked integer value or -1 on error
*/","* This method reads a Byte from the compressed stream. Whenever we need to
  * read from the underlying compressed stream, this method should be called
  * instead of directly calling the read method of the underlying compressed
  * stream. This method does important record keeping to have the statistic
  * that how many bytes have been read off the compressed stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CRC.java,<init>,org.apache.hadoop.io.compress.bzip2.CRC:<init>(),86,88,"/**
 * Initializes and returns the current CRC value. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock(),572,589,"/**
* Updates CRC values based on block integrity check.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,createHuffmanDecodingTables,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)",793,819,"/**
* Computes minimum and maximum character lengths for each group.
* @param nGroups number of groups
* @param alphaSize alphabet size
*/",* Called by recvDecodingTables() exclusively.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock(),774,786,"/**
* Resets mask state and recalculates allowable block size.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutUByte,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int),940,942,"/**
* Applies mask to input character.
* @param c input character to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutInt,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int),944,949,"/**
* Extracts and processes four 8-bit masks from a given integer.
* @param u the input integer to extract masks from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues4,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4(),1199,1241,"/**
* Updates in-use flags and writes live data to output stream.
* @throws IOException if write operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues5,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)",1243,1278,"/**
* Updates live and buffer state based on group and selector counts.
* @param nGroups number of groups
* @param nSelectors number of selectors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues1,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)",1029,1145,"/**
* Computes the optimal number of selectors based on given parameters.
* @param nGroups the number of groups
* @param alphaSize the size of the alphabet
* @return the optimal number of selectors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)",1174,1197,"/**
* Computes and updates MTF values for each group.
* @param nGroups number of groups
* @param alphaSize alphabet size
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainQSort3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",1634,1736,"/**
* Reorganizes the fmap array based on a given data structure.
* @param dataShadow Data shadow object containing relevant arrays
* @param loSt Lower start value for reorganization
* @param hiSt Higher start value for reorganization
* @param dSt Depth value for reorganization
*/","* Method ""mainQSort3"", file ""blocksort.c"", BZip2 1.0.2",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(),66,68,"/**
 * Initializes the decompressor with default settings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)",70,88,"/**
* Initializes and configures direct buffer from given byte array.
* @param b the input byte array
* @param off starting offset within the array
* @param len length of data to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput(),112,130,"/**
* Determines mask status based on buffer lengths.
* @return true if no buffers are present, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten(),182,185,"/**
* Recursively computes an unknown value using stream.
* @param stream input stream
*/","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead(),192,195,"/**
* Recursively calls itself and m1(), returning a value from m2(stream).
* @param stream input stream (not used in this method)
*/","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getRemaining,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining(),204,208,"/**
* Recursively calculates total stream size by adding local buffer length.
* @return combined stream size
*/","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset(),213,223,"/**
* Resets internal state and stream, recalculating direct buffer sizes.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(),64,66,"/**
* Initializes Bzip2 compressor with default settings.
* @param blockSize compression block size
* @param workFactor compression factor
* @param directBufferSize direct buffer size
*/","* Creates a new compressor with a default values for the
   * compression block size and work factor.  Compressed data will be
   * generated in bzip2 format.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)",124,142,"/**
* Initializes and processes user buffer with given data.
* @param b byte array to process
* @param off starting offset in the array
* @param len length of data to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput(),158,181,"/**
* Determines whether to use a function mask based on buffer state.
* @return true if user buffer is empty or function mask can be used, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten(),244,248,"/**
* Recursively calls m2 with stream as argument.
* @param stream input data stream
* @return result value (type not specified)
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead(),255,259,"/**
* Recursively computes the result using stream and returns it.
* @return computed result
*/","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset(),261,274,"/**
* Resets internal state and buffers to initial values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream),251,255,"/**
* Initializes BZip2 compression output stream with specified OutputStream.
* @param out OutputStream to compress
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",36,47,"/**
* Initializes a CompressorStream with the specified output stream, 
* compressor, and buffer size.
* @param out output stream to write compressed data
* @param compressor compression algorithm instance
* @param bufferSize size of the internal buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream),58,60,"/**
 * Initializes a new compressor stream with the specified output stream.
 * @param out target output stream
 */","* Allow derived classes to directly set the underlying stream.
   * 
   * @param out Underlying output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,writeStreamHeader,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader(),257,261,"/**
 * Writes mask data to output stream. 
 * @throws IOException if I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,"org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)",62,78,"/**
* Compresses data in-place and flushes compressor state.
* @param b byte array to compress
* @param off starting offset within the array
* @param len number of bytes to compress
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,finish,org.apache.hadoop.io.compress.CompressorStream:finish(),87,95,"/**
* Continuously processes data until compression is successful.
* @throws IOException on processing error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(),65,67,"/**
* Initializes decompressor with default buffer size.
*/",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,setInput,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)",83,101,"/**
* Initializes and updates internal buffers for direct decompression.
* @param b input byte array
* @param off starting offset in the array
* @param len length of data to process
*/","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,needsInput,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput(),138,156,"/**
* Determines whether to mask function input based on buffer lengths and state.
* @return true if no buffers or user data, false otherwise
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,finished,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished(),310,313,"/**
* Checks whether end of input has been reached and calls parent implementation.
* @return true if end of input or parent's result is true, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompress,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)",192,230,"/**
* Decompresses and copies data from a buffer to another.
* @param b target byte array
* @param off starting offset in the array
* @param len number of bytes to copy
* @return actual number of bytes copied
*/","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the uncompressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompressDirect,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",275,305,"/**
* Decompresses data from source to destination buffers.
* @param src compressed data buffer
* @param dst uncompressed data buffer
* @return number of decompressed bytes or -1 on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,reset,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset(),315,319,"/**
 * Calls superclass's implementation of m1 and marks the end of input.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(),67,69,"/**
 * Initializes SnappyCompressor with default direct buffer size.
 */",* Creates a new compressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,compress,"org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)",177,225,"/**
* Reads and decompresses data from the buffer into the provided byte array.
* @param b target byte array
* @param off offset in the target array to write to
* @param len length of the data to read
* @return number of bytes written or -1 if an error occurs
*/","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,reinit,org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration),248,251,"/**
 * Initializes mask functionality based on configuration settings.
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)",51,66,"/**
* Initializes a DecompressorStream with the specified decompressor and buffers.
* @param in input stream to read from
* @param decompressor decompression strategy
* @param bufferSize size of primary buffer
* @param skipBufferSize size of secondary buffer for skipped bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream),85,87,"/**
* Initializes decompression stream with input from given InputStream.
* @param in input stream to decompress
*/","* Allow derived classes to directly set the underlying stream.
   * 
   * @param in Underlying input stream.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SplitCompressionInputStream.java,<init>,"org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)",39,44,"/**
* Initializes a SplitCompressionInputStream with a specified compression range.
* @param in input stream to compress and split
* @param start starting byte offset of the compression range
* @param end ending byte offset of the compression range
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.DecompressorStream:getCompressedData(),175,180,"/**
* Returns function mask value from input stream.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,available,org.apache.hadoop.io.compress.DecompressorStream:available(),215,219,"/**
* Returns a mask value based on file end condition.
* @return 0 if at EOF, 1 otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,resetState,org.apache.hadoop.io.compress.BlockDecompressorStream:resetState(),137,142,"/**
* Resets block size counters and calls superclass method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,compress,org.apache.hadoop.io.compress.BlockCompressorStream:compress(),147,155,"/**
* Compresses and writes data to output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)",87,104,"/**
* Initializes direct buffer with user data from specified byte array.
* @param b the byte array to initialize from
* @param off offset into array where data starts
* @param len length of data in array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput(),134,151,"/**
* Determines whether to mask data based on buffer conditions.
* @return true to mask, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finished,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished(),302,305,"/**
* Combines end-of-input condition with superclass result.
* @return true if end of input reached and/or superclass confirms it
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining(),216,221,"/**
* Calculates total mask bytes to consume.
* @return total mask bytes as an integer
*/","* <p>Returns the number of bytes remaining in the input buffers;
   * normally called when finished() is true to determine amount of post-stream
   * data.</p>
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset(),226,238,"/**
* Resets processing state and initializes buffers.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,decompress,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)",166,207,"/**
* Decompresses data from the given byte array and returns the length of decompressed data.
* @param b compressed byte array
* @param off start offset in bytes
* @param len length to decompress in bytes
* @return length of decompressed data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)",122,139,"/**
* Initializes direct buffers from input byte array.
* @param b input data
* @param off starting offset in bytes
* @param len length of data in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput(),156,181,"/**
* Determines whether to apply function mask based on buffer conditions.
* @return true if mask should be applied, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,compress,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)",195,243,"/**
* Compresses a byte array in-place using direct buffers.
* @param b input byte array
* @param off offset into the array to compress
* @param len length of the data to compress
* @return compressed size or -1 if error occurred
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten(),250,254,"/**
* Returns the function mask value.
* @return Function mask as a long integer value
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead(),261,265,"/**
* Returns the function mask based on read bytes.
* @return Function mask value
*/","* <p>Returns the total number of uncompressed bytes input so far.</p>
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset(),267,283,"/**
* Resets internal state and clears buffers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData(),114,135,"/**
* Reads and caches a block from the input stream.
* @return length of the cached block
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(),77,79,"/**
 * Initializes a new instance of the decompressor with default buffer size.
 */",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,setInput,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)",95,113,"/**
* Initializes a byte buffer with provided data.
* @param b input byte array
* @param off starting offset in the array
* @param len length of the data to process
*/","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,needsInput,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput(),150,168,"/**
* Determines whether to use a mask function based on buffer conditions.
* @return true if no buffers are present, false otherwise
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,decompress,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)",204,242,"/**
* Decompresses data from buffer.
* @param b compressed byte array
* @param off starting offset
* @param len length to decompress
* @return number of bytes decompressed or -1 on failure
*/","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of uncompressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int),95,97,"/**
 * Constructs an Lz4Compressor with the specified direct buffer size.
 * @param directBufferSize size of the direct buffer to use
 */","* Creates a new compressor.
   *
   * @param directBufferSize size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,compress,"org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)",212,260,"/**
* Reads and decompresses data from the buffer into the provided byte array.
* @param b target byte array
* @param off starting offset in the byte array
* @param len number of bytes to read
* @return number of bytes read or 0 if finished
*/","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,reinit,org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration),283,286,"/**
* Applies functional mask to configuration.
* @param conf Configuration object to modify
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String),246,257,"/**
* Retrieves a CompressionCodec instance by its name.
* @param codecName unique compression codec identifier
* @return CompressionCodec object or null if not found
*/","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,payback,"org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)",105,122,"/**
* Checks if a codec is in the pool.
* @param pool map of codecs by class
* @param codec codec to check for existence
* @return true if codec exists, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,updateLeaseCount,"org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)",131,137,"/**
 * Updates the usage count for a given codec class by adding or subtracting a delta value.
 * @param usageCounts cache of codec usage counts
 * @param codec codec instance (used to determine its class)
 * @param delta update value to add to/subtract from the usage count
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedCompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),245,248,"/**
* Calculates a mask value based on the given compression codec.
* @param codec CompressionCodec instance
* @return integer mask value or 0 if codec is null
*/","* Return the number of leased {@link Compressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedDecompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),257,260,"/**
* Computes a mask value based on the provided compression codec.
* @param codec CompressionCodec instance
* @return integer mask value or 0 if codec is null
*/","* Return the number of leased {@link Decompressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,checkNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded(),62,77,"/**
* Validates availability of native zStandard libraries.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,isNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded(),79,82,"/**
* Verifies compression and decompression integrity using ZStandard algorithms.
* @return true if both compression and decompression are successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,flush,org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush(),66,72,"/**
* Invokes methods m1(), m2(), and m3() on a CompressionOutputStream instance. 
* @throws IOException if an I/O error occurs during compression
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getCompressorType,org.apache.hadoop.io.compress.GzipCodec:getCompressorType(),69,74,"/**
* Determines compressor class based on configuration.
* @return Compressor subclass (e.g. GzipZlibCompressor or BuiltInGzipCompressor)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getDecompressorType,org.apache.hadoop.io.compress.GzipCodec:getDecompressorType(),102,107,"/**
* Returns decompressor class based on zlib factory configuration.
* @return Decompressor subclass (GzipZlibDecompressor or BuiltInGzipDecompressor)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration),97,101,"/**
* Returns compressor class based on configuration.
* @param conf Hadoop Configuration object
*/","* Return the appropriate type of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib compressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration),121,125,"/**
* Returns decompression class based on configuration.
* @param conf Configuration object
*/","* Return the appropriate type of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib decompressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,loadNativeZLib,org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib(),52,64,"/**
* Initializes and checks native zlib library loading status.
* @param none
* @return none (logs result)
*/","* Load native library and set the flag whether to use native library. The
   * method is also used for reset the flag modified by setNativeZlibLoaded",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,init,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration),156,165,"/**
* Initializes compression settings and decompression state.
* @param conf configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration),65,83,"/**
* Initializes Zlib compressor based on provided configuration.
* @param conf Configuration object
*/","* reinit the compressor with the given configuration. It will reset the
   * compressor's compression level and compression strategy. Different from
   * <tt>ZlibCompressor</tt>, <tt>BuiltInZlibDeflater</tt> only support three
   * kind of compression strategy: FILTERED, HUFFMAN_ONLY and DEFAULT_STRATEGY.
   * It will use DEFAULT_STRATEGY as default if the configured compression
   * strategy is not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",261,274,"/**
* Initializes ZlibCompressor with specified compression level, strategy,
* and header. Allocates direct buffers for decompression.
* @param level Compression level
* @param strategy Compression strategy
* @param header Compression header
* @param directBufferSize Size of direct buffers
*/","* Creates a new compressor using the specified compression level.
   * Compressed data will be generated in ZLIB format.
   * 
   * @param level Compression level #CompressionLevel
   * @param strategy Compression strategy #CompressionStrategy
   * @param header Compression header #CompressionHeader
   * @param directBufferSize Size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)",300,318,"/**
* Initializes direct buffers with user data.
* @param b byte array to load
* @param off starting offset in the array
* @param len length of data to load
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput(),340,367,"/**
* Checks if a compressed or uncompressed mask is present.
* @return true if the mask is available, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten(),432,436,"/**
* Calculates and returns a value using m1() and stream.
* @param stream input data stream (usage in m2())
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead(),443,447,"/**
* Recursively processes stream and returns its size.
* @param stream input data stream
* @return total number of elements in the stream
*/","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset(),449,461,"/**
* Performs cleanup and compression tasks.
* @see m1(), m2(stream), uncompressedDirectBuf.m3(),
*      compressedDirectBuf.m4(directBufferSize), compressedDirectBuf.m5(directBufferSize)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,compress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)",81,132,"/**
* Processes input data and compresses it according to the current state.
* @param b input byte array
* @param off offset into the input array
* @param len length of the input data
* @return number of compressed bytes written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finished,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished(),360,363,"/**
* Delegates to superclass while considering end-of-input condition. 
* @return true if superclass call returns true and end of input is reached, false otherwise.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",107,115,"/**
* Initializes ZlibDecompressor object with compression header and direct buffer size.
* @param header CompressionHeader containing decompression settings
* @param directBufferSize Size of direct buffers for compressed and uncompressed data
*/","* Creates a new decompressor.
   * @param header header.
   * @param directBufferSize directBufferSize.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)",121,139,"/**
* Initializes and configures user buffer from a byte array.
* @param b the byte array containing data
* @param off offset into the array to start processing
* @param len length of data in the array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput(),170,188,"/**
* Determines whether to apply function mask based on buffer lengths.
* @return true if no valid buffers, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten(),242,245,"/**
* Calculates result using helper function and stream.
* @param stream input data stream (assumed to be provided elsewhere)
*/","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead(),252,255,"/**
* Recursively computes result using stream.
* @param stream input data stream
* @return computed result
*/","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining(),264,268,"/**
* Recursively calculates total buffer length by adding local and stream lengths.
* @return Total buffer length or -1 if error occurs
*/","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset(),273,283,"/**
* Resets internal state and initializes direct buffers.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finalize,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize(),293,296,"/**
* Executes function mask operation. 
* Calls m1() to perform underlying logic.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeTrailerState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState(),367,416,"/**
* Processes trailer data and verifies CRC and size consistency.
* @throws IOException if stream or stored size mismatches
*/","* Parse the gzip trailer (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy trailer bytes (all 8 of 'em) to a local buffer.</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,processBasicHeader,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader(),509,524,"/**
* Validates gzip header and extracts flags.
* @throws IOException if file is not a valid gzip archive
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedString,org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput),87,91,"/**
* Converts input stream to a UTF-8 encoded string.
* @param in DataInput stream to read from
* @return resulting string or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeVInt,"org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)",257,259,"/**
 * Writes mask value to output stream.
 * @param stream DataOutput stream to write to
 * @param i integer value to be written as mask
 */","* Serializes an integer to a binary stream with zero-compressed encoding.
   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the
   * actual value.
   * For other values of i, the first byte value indicates whether the
   * integer is positive or negative, and the number of bytes that follow.
   * If the first byte value v is between -113 and -116, the following integer
   * is positive, with number of bytes that follow are -(v+112).
   * If the first byte value v is between -121 and -124, the following integer
   * is negative, with number of bytes that follow are -(v+120). Bytes are
   * stored in the high-non-zero-byte-first order.
   *
   * @param stream Binary output stream
   * @param i Integer to be serialized
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,write,org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput),54,57,"/**
* Writes a FUNC_MASK value to the output stream.
* @param out DataOutput stream to write to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVLong,org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput),313,326,"/**
* Extracts and interprets a FUNC_MASK from the input stream.
* @param stream input data stream
* @return long integer value representing the mask or -1L if invalid
*/","* Reads a zero-compressed encoded long from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized long from stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)",82,107,"/**
* Retrieves a pre-allocated ByteBuffer of specified length.
* @param direct whether to use direct or non-direct access
* @param length the desired size of the ByteBuffer
* @return a ByteBuffer object or null if not available
*/","* {@inheritDoc}
   *
   * @param direct whether we want a direct byte buffer or a heap one.
   * @param length length of requested buffer.
   * @return returns equal or next greater than capacity buffer from
   * pool if already available and not garbage collected else creates
   * a new buffer and return it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),113,130,"/**
* Inserts or updates ByteBuffer in cache with unique Key.
* @param buffer ByteBuffer to cache
*/","* Return buffer to the pool.
   * @param buffer buffer to be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,charAt,org.apache.hadoop.io.Text:charAt(int),163,169,"/**
* Retrieves a mask value based on the given position.
* @param position index within the byte array
* @return non-negative mask value or -1 if out of bounds
*/","* Returns the Unicode Scalar Value (32-bit integer value)
   * for the character at <code>position</code>. Note that this
   * method avoids using the converter or doing String instantiation.
   *
   * @param position input position.
   * @return the Unicode scalar value at position or -1
   *          if the position is invalid or points to a
   *          trailing byte",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(java.lang.String),228,237,"/**
* Processes input string and initializes associated variables.
* @param string the input string to process
*/","* Set to contain the contents of a string.
   *
   * @param string input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,encode,org.apache.hadoop.io.Text:encode(java.lang.String),514,517,"/**
* Encodes input string into ByteBuffer using default encoding.
* @param string input string to encode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,buildCacheKey,org.apache.hadoop.security.token.Token:buildCacheKey(),449,452,"/**
* Generates a unique mask using UUID and user credentials.
* @return Masked string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,"org.apache.hadoop.io.Text:set(byte[],int,int)",272,277,"/**
* Masks the specified UTF-8 encoded byte array in-place.
* @param utf8 the input byte array to mask
* @param start starting index of the data to process
* @param len length of the data to process
*/","* Set the Text to range of bytes.
   *
   * @param utf8 the data to copy from
   * @param start the first position of the new string
   * @param len the number of bytes of the new string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,append,"org.apache.hadoop.io.Text:append(byte[],int,int)",286,294,"/**
* Copies and inserts UTF-8 encoded data into the internal byte buffer.
* @param utf8 input data to be inserted
* @param start starting position in the buffer
* @param len length of the input data
*/","* Append a range of bytes to the end of the given text.
   *
   * @param utf8 the data to copy from
   * @param start the first position to append from utf8
   * @param len the number of bytes to append",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readWithKnownLength,"org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)",383,388,"/**
* Reads and sets mask data from input stream.
* @param in DataInput stream to read from
* @param len Number of bytes to read
*/","* Read a Text object whose length is already known.
   * This allows creating Text from a stream which uses a different serialization
   * format.
   *
   * @param in input in.
   * @param len input len.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,org.apache.hadoop.io.Text:decode(byte[]),457,459,"/**
* Decodes UTF-8 byte array to string.
* @param utf8 input byte array
* @throws CharacterCodingException if decoding fails
*/","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If the input is malformed,
   * replace by a default value.
   *
   * @param utf8 input utf8.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int)",461,464,"/**
* Decodes UTF-8 bytes to a string.
* @param utf8 byte array containing encoded characters
* @param start starting index of the bytes to decode
* @param length number of bytes to decode
* @return decoded string or throws CharacterCodingException if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)",480,483,"/**
* Converts UTF-8 encoded byte array to string with optional replacement.
* @param utf8 UTF-8 encoded byte array
* @param start starting index in the byte array
* @param length number of bytes to process
* @param replace whether to replace invalid characters
* @return resulting string or null if conversion fails
*/","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If <code>replace</code> is true, then
   * malformed input is replaced with the
   * substitution character, which is U+FFFD. Otherwise the
   * method throws a MalformedInputException.
   *
   * @param utf8 input utf8.
   * @param start input start.
   * @param length input length.
   * @param replace input replace.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,validateUTF8,org.apache.hadoop.io.Text:validateUTF8(byte[]),626,628,"/**
* Processes UTF-8 encoded byte array.
* @param utf8 input byte array
*/","* Check if a byte array contains valid UTF-8.
   *
   * @param utf8 byte array
   * @throws MalformedInputException if the byte array contains invalid UTF-8",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,addToMap,org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class),91,101,"/**
* Recursively assigns a unique ID to a given class.
* @param clazz Class object to assign ID to
*/","* Add a Class to the maps if it is not already present.
   * @param clazz clazz.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,<init>,org.apache.hadoop.io.AbstractMapWritable:<init>(),145,166,"/**
* Initializes a new AbstractMapWritable instance with default configuration.
* Adds primitive writable types to the map. 
*/",constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,readFields,org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput),194,216,"/**
* Loads and initializes classes based on input data.
* @param in DataInput object containing class metadata
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,write,org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput),180,192,"/**
* Writes function mask data to output stream.
* @param newClasses number of classes to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Metadata:<init>(),735,737,"/**
* Initializes metadata with an empty map. 
* @param none (default) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,<init>,org.apache.hadoop.io.VLongWritable:<init>(long),38,38,"/**
* Initializes a new VLongWritable instance with the specified value.
* @param value long integer value to be wrapped.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getSerializer,org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class),81,87,"/**
* Recursively attempts to resolve a custom serializer for the given class.
* @param c class type to resolve serializer for
* @return Serializer object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getDeserializer,org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class),89,95,"/**
* Retrieves a deserializer instance for the given class type, 
* attempting to delegate to an existing serializer if present.
* @param c Class type of the deserialization target
* @return Deserializer instance or null on failure.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,compare,"org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)",47,51,"/**
* Calculates mask value based on two object comparisons.
* @param o1 first object to compare
* @param o2 second object to compare
* @return calculated mask value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerialization.java,deserialize,org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object),54,63,"/**
* Deserializes object of type T from input stream.
* @throws IOException if deserialization fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,filesystem,org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem),1002,1005,"/**
* Returns functional mask option for given file system.
* @param fs the file system instance
*/","* @deprecated only used for backwards-compatibility in the createWriter methods
     * that take FileSystem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,<init>,org.apache.hadoop.io.DataInputByteBuffer:<init>(),74,76,"/**
* Constructs DataInputByteBuffer with default buffer.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getData,org.apache.hadoop.io.DataInputByteBuffer:getData(),87,89,"/**
* Retrieves an array of ByteBuffer objects from the buffers instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getPosition,org.apache.hadoop.io.DataInputByteBuffer:getPosition(),91,93,"/**
* Calls m1() on underlying buffer.
* @return result of buffer's m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getLength,org.apache.hadoop.io.DataInputByteBuffer:getLength(),95,97,"/**
* Calls underlying buffer's m1 method to perform some operation. 
* @return result of the operation (type and meaning unknown)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,"org.apache.hadoop.util.bloom.Key:<init>(byte[],double)",98,100,"/**
* Constructs a new Key with specified value and weight.
* @param value byte array representing key data
* @param weight double value representing key importance
*/","* Constructor.
   * <p>
   * Builds a key with a specified weight.
   * @param value The value of <i>this</i> key.
   * @param weight The weight associated to <i>this</i> key.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup(),3916,3921,"/**
* Calls superclass and parent methods to perform some operation.
* @throws IOException if an I/O error occurs in the process
*/","The default cleanup. Subclasses can override this with a custom 
       * cleanup",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,grow,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(),3193,3200,"/**
* Resizes internal arrays to maintain 3/2 factor.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType),977,979,"/**
* Constructs CompressionOption with specified type and no custom options.
* @param value CompressionType enum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,"org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",1056,1059,"/**
* Creates a compression option with specified type and codec.
* @param value Compression type
* @param codec Compression codec
* @return CompressionOption object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,toArray,org.apache.hadoop.util.GenericsUtil:toArray(java.util.List),86,88,"/**
* Recursively maps and merges two lists into one.
* @param list input list
*/","* Converts the given <code>List&lt;T&gt;</code> to a an array of 
   * <code>T[]</code>. 
   * @param list the list to convert
   * @param <T> Generics Type T.
   * @throws ArrayIndexOutOfBoundsException if the list is empty. 
   * Use {@link #toArray(Class, List)} if the list may be empty.
   * @return T Array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,<init>,org.apache.hadoop.io.InputBuffer:<init>(),69,71,"/**
* Initializes an InputBuffer with a new underlying buffer.
*/",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int)",83,85,"/**
 * Calls the low-level buffer operation with the provided input data. 
 * @param input byte array containing data to process
 * @param length number of bytes in the input array to consider
 */","* Resets the data that the buffer reads.
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)",93,95,"/**
 * Calls buffer's m1 method with provided parameters.
 * @param input byte array to process
 * @param start starting index in the array
 * @param length number of bytes to process
 */","* Resets the data that the buffer reads.
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getPosition,org.apache.hadoop.io.InputBuffer:getPosition(),101,101,"/**
* Calls m1 on underlying buffer object.
* @return result of buffer's m1 operation
*/","* Returns the current position in the input.
   * @return the current position in the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getLength,org.apache.hadoop.io.InputBuffer:getLength(),107,107,"/**
* Calls underlying method m1 on 'buffer' object and returns result.","* Returns the length of the input.
   * @return length of the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,read,org.apache.hadoop.io.MD5Hash:read(java.io.DataInput),87,91,"/**
* Computes and returns an MD5 hash based on input data.
* @param in Data to be hashed, provided as a DataInput stream
* @return MD5Hash object representing the computed hash value
*/","* Constructs, reads and returns an instance.
   * @param in in.
   * @throws IOException raised on errors performing I/O.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream),138,147,"/**
* Computes the MD5 hash of input stream data.
* @param in input stream to be hashed
* @return MD5Hash object containing the calculated hash value
*/","* Construct a hash value for the content from the InputStream.
   * @param in input stream.
   * @return MD5Hash.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)",156,162,"/**
* Computes the MD5 hash of a byte array.
* @param data input data
* @param start starting index in data
* @param len length of data to process
* @return MD5Hash object containing the digest
*/","* Construct a hash value for a byte array.
   * @param data data.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)",171,179,"/**
* Computes the combined MD5 hash of multiple byte arrays.
* @param dataArr array of byte arrays to combine
* @param start offset in each array where hashing starts
* @param len length of bytes in each array to include in hash
* @return MD5Hash object representing the combined hash
*/","* Construct a hash value for an array of byte array.
   * @param dataArr dataArr.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,hashCode,org.apache.hadoop.io.MD5Hash:hashCode(),234,237,"/**
* Returns the function mask value using the m1() method.
* @return Function mask integer value
*/","Returns a hash code value for this object.
   * Only uses the first 4 bytes, since md5s are evenly distributed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,setDigest,org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String),283,293,"/**
* Computes and stores a byte array representing the MD5 hash of the given hex string.
* @param hex input hexadecimal string
*/","* Sets the digest value from a hex string.
   * @param hex hex.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,metadata,org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata),1048,1050,"/**
* Creates a metadata option with the given mask.
* @param value metadata to be wrapped in an option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream),912,914,"/**
 * Creates a new StreamOption instance from an FSDataOutputStream.
 * @param stream output stream to manage with this option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object),48,50,"/**
* Converts an arbitrary object to a Writable representation.
* @param instance the object to convert 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,tryInstantiateProtobuf,"org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)",351,389,"/**
* Parses Message object from input stream or data buffer.
* @param protoClass protocol class
* @param dataIn input stream or data buffer
* @return parsed Message object or throws IOException if failed
*/","* Try to instantiate a protocol buffer of the given message class
   * from the given input stream.
   * 
   * @param protoClass the class of the generated protocol buffer
   * @param dataIn the input stream to read from
   * @return the instantiated Message instance
   * @throws IOException if an IO problem occurs",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,fsync,org.apache.hadoop.io.IOUtils:fsync(java.io.File),392,413,"/**
* Synchronizes a file or directory with its external representation.
* @param fileToSync the file/directory to sync
*/","* Ensure that any writes to the given file is written to the storage device
   * that contains it. This method opens channel on given File and closes it
   * once the sync is done.<br>
   * Borrowed from Uwe Schindler in LUCENE-5588
   * @param fileToSync the file to fsync
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,equals,org.apache.hadoop.io.MapWritable:equals(java.lang.Object),78,94,"/**
* Compares the object with a MapWritable instance for equality.
* @param obj Object to compare
* @return True if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,putAll,org.apache.hadoop.io.MapWritable:putAll(java.util.Map),123,128,"/**
* Applies a mask operation to the given map of writable values.
* @param t Map containing key-value pairs to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean),939,941,"/**
 * Sets whether to append existing files in the option.
 * @param value true to append, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>(),1889,1891,"/**
* Initializes this OnlyHeaderOption instance with default settings.
* Calls superclass constructor to set flag.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path),1852,1854,"/**
 * Initializes a new instance of FileOption with the specified file path.
 * @param value Path to the file option.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path),890,892,"/**
* Constructs FileOption with specified file path.
* @param path file system path to the option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable:<init>(boolean),41,43,"/**
* Constructs a BooleanWritable instance with the given boolean value.
* @param value true or false value to initialize this object",* @param value value.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,toString,org.apache.hadoop.io.BooleanWritable:toString(),102,105,"/**
* Evaluates boolean expression using M2 algorithm from Boolean class.
* @return string representation of result (true or false) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,waitAsyncValue,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)",205,217,"/**
* Returns FUNC_MASK value or throws TimeoutException if not available within the specified time.
* @param timeout duration to wait for value
* @param unit time unit of timeout (e.g., seconds, milliseconds)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)",225,230,"/**
* Determines retry action based on exception and retry/failover counts.
* @param e the caught exception
* @param retries remaining retry attempts
* @param failovers remaining failover attempts
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at-most-once
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision),49,51,"/**
* Creates a new RetryAction with default values.
* @param action initial retry decision",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,"org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)",53,55,"/**
* Constructs a new RetryAction instance with the given decision and delay time.
* @param action the initial retry decision
* @param delayTime the initial delay time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)",331,333,"/**
* Initializes retry mechanism with configurable maximum retries and fixed sleep duration.
* @param maxRetries maximum number of attempts before failure
* @param sleepTime time to wait between retries in specified time unit
* @param timeUnit unit of time for the sleep duration (e.g. milliseconds, seconds)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)",627,638,"/**
* Initializes ExponentialBackoffRetry with maximum retries and sleep time.
* @param maxRetries maximum number of retry attempts
* @param sleepTime initial sleep duration in specified time unit
* @param timeUnit time unit for sleep duration (e.g. milliseconds, seconds)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)",367,369,"/**
* Initializes retry behavior with proportional backoff.
* @param maxRetries maximum number of retries
* @param sleepTime initial sleep duration in specified unit
* @param timeUnit time unit for sleep duration (e.g. seconds, milliseconds)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)",669,672,"/**
* Initializes FailoverOnNetworkExceptionRetry with fallback policy and maximum failovers.
* @param fallbackPolicy Retry policy to use when failing over
* @param maxFailovers Maximum number of network exceptions allowed before retrying
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",674,677,"/**
* Constructs a FailoverOnNetworkExceptionRetry instance with default initial retry count.
* @param fallbackPolicy Retry policy to apply when all failovers are exhausted
* @param maxFailovers Maximum number of network retries allowed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",217,222,"/**
* Creates a custom retry policy with failover on network exception.
* @param fallbackPolicy default retry policy to fall back to
* @param maxFailovers maximum number of failovers allowed
* @param maxRetries maximum number of retries before failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByRemoteException,"org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",177,181,"/**
* Creates a custom retry policy based on exception types.
* @param defaultPolicy the base retry policy
* @param exceptionToPolicyMap map of exceptions to override policies
*/","* <p>
   * A retry policy for RemoteException
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",582,594,"/**
* Resolves Retry Policy for given exception and parameters.
* @param e the caught exception
* @param retries maximum number of retries
* @param failovers maximum number of failovers
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at-most-once
* @return configured RetryPolicy instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,<init>,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)",237,243,"/**
* Constructs an asynchronous call with specified handler.
* @param asyncCallHandler handler for asynchronous calls
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo(),268,277,"/**
* Determines the retry strategy based on calculated wait time.
* @return CallReturn value indicating whether to wait or retry
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)",250,257,"/**
* Creates a RetryInfo object with specified delay, retry action, and failure details.
* @param delay time to wait before retrying
* @param action action to perform on retry
* @param expectedFailoverCount expected number of failovers
* @param failException exception thrown during last attempt
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long),96,99,"/**
* Checks if task can be processed within given time frame.
* @param time time threshold
* @return true if task can be processed, false otherwise
*/",Is the queue empty for more than the given time in millisecond?,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty(),106,110,"/**
* Resets timer when queue is updated.
* @param queue updated queue state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,clearNameMaps,org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps(),154,159,"/**
* Resets internal data structures and updates timestamp.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,isExpired,org.apache.hadoop.security.ShellBasedIdMapping:isExpired(),161,163,"/**
* Checks if mask update is overdue.
* @return true if update is due, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run(),4207,4223,"/**
* Updates total requests per second metric.
* @param none
* @return none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,now,org.apache.hadoop.util.SysInfoWindows:now(),61,64,"/**
* Returns 32-bit time mask based on m1(). 
* @return 32-bit time value as long
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNow,org.apache.hadoop.util.Timer:monotonicNow(),50,50,"/**
* Returns time in milliseconds since epoch (January 1, 1970).","* Current time from some arbitrary time base in the past, counting in
   * milliseconds, and not affected by settimeofday or similar system clock
   * changes.  This is appropriate to use when computing how much longer to
   * wait for an interval to expire.
   * @return a monotonic clock that counts in milliseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryOtherThanRemoteAndSaslException,"org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",194,199,"/**
* Creates a retry policy that depends on exceptions other than Remote and SASL.
* @param defaultPolicy the default retry policy
* @param exceptionToPolicyMap map of custom exception to retry policies
*/","* <p>
   * A retry policy where RemoteException and SaslException are not retried, other individual
   * exception types can have RetryPolicy overrides, and any other exception type without an
   * override is not retried.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,getProxy,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy(),45,48,"/**
* Creates proxy info with specified proxy and no filter.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",332,338,"/**
* Initializes the RetryInvocationHandler with proxy provider, default retry policy,
* and custom policies for specific method calls.
* @param proxyProvider FailoverProxyProvider instance
* @param defaultPolicy Default retry policy to apply
* @param methodNameToPolicyMap Map of method names to custom retry policies
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,close,org.apache.hadoop.io.retry.RetryInvocationHandler:close(),457,460,"/**
 * Invokes client-side operation ""m1"" via the proxy descriptor.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,getString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String),50,52,"/**
* Generates a function mask string with method name and proxy info.
* @param methodName name of the method to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,toString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString(),54,57,"/**
* Returns a string representing the function mask.
* @return concatenated string of m1 and proxyInfo strings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getFailoverCount,org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount(),345,347,"/**
* Calls m1() on the associated proxy descriptor.
* @return result of m1() call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",432,443,"/**
* Invokes a Method with provided arguments, handling exceptions and caching results.
* @param method the Method to invoke
* @param args array of arguments for the invocation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getCallId,org.apache.hadoop.ipc.Client:getCallId(),137,139,"/**
* Returns a mask value based on callId's m1 result or fallback to m2. 
* @return mask integer value",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",287,307,"/**
* Initializes a new Call object with specified RPC kind, parameter, and optional ID and retry count.
* @param rpcKind the type of RPC request
* @param param the request parameter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getConnectionId,org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId(),462,465,"/**
* Returns connection ID based on proxy descriptor.
* @return unique connection identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,getConnectionId,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId(),122,125,"/**
* Returns connection ID based on first proxy.
* @return unique identifier or null if failed
*/","* Since this is incapable of returning multiple connection IDs, simply
     * return the first one. In most cases, the connection ID should be the same
     * for all proxies.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone(),226,228,"/**
* Checks if a non-null value is present.
* @return true if a valid value exists, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode(),451,454,"/**
* Calls m1() to fetch nested result and returns its value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object),456,464,"/**
* Recursively compares two objects for equality.
* @param that object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,parseCommaSeparatedString,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String),483,514,"/**
* Parses string into MultipleLinearRandomRetry object.
* @param s comma-separated values
*/","* Parse the given string as a MultipleLinearRandomRetry object.
     * The format of the string is ""t_1, n_1, t_2, n_2, ..."",
     * where t_i and n_i are the i-th pair of sleep time and number of retries.
     * Note that the white spaces in the string are ignored.
     *
     * @param s input string.
     * @return the parsed object, or null if the parsing fails.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByException,"org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",162,165,"/**
* Creates an ExceptionDependentRetry policy by wrapping the given default policy.
* @param defaultPolicy default retry policy
* @param exceptionToPolicyMap map of exceptions to custom RetryPolicies
* @return ExceptionDependentRetry policy instance
*/","* <p>
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @param defaultPolicy defaultPolicy.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,calculateExponentialTime,"org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)",769,771,"/**
* Calculates a value using the M1 algorithm with default max iterations.
* @param time current time
* @param retries initial number of retries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason(),353,356,"/**
* Generates function mask string based on maximum time and unit.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason(),293,295,"/**
* Generates a function mask string based on maximum retries value.
* @param maxRetries maximum number of retries allowed
* @return generated function mask string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode(),305,308,"/**
* Calls and returns result of m2() on UserProfile returned by m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object),310,318,"/**
* Recursively compares objects using a custom comparison logic.
* @param that object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,createIOException,org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List),50,58,"/**
* Combines a list of IOExceptions into a single MultipleIOException.
* @param exceptions List of IOExceptions to combine
*/","* A convenient method to create an {@link IOException}.
   * @param exceptions IOException List.
   * @return IOException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressionAlgorithmByName,org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String),359,370,"/**
* Retrieves the Algorithm instance based on the provided compression name.
* @param compressName unique identifier for the compression algorithm
* @return Algorithm object or throws exception if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getSupportedAlgorithms,org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms(),372,382,"/**
* Retrieves and returns an array of algorithm names that meet the specified condition.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName(),524,526,"/**
 * Returns compressed mask value using m1 compression algorithm.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockCount,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount(),687,689,"/**
* Returns a mask value based on dataIndex's m1 and m2 methods.
*/","* Get the number of data blocks.
     * 
     * @return the number of data blocks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,readAndVerify,org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput),920,929,"/**
* Validates input stream as a BCFile by reading magic bytes.
* @param in input stream to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,format,"org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",73,78,"/**
* Formats a long integer using specified alignment and width.
* @param l number to format
* @param width minimum field width
* @param align formatting alignment (ZERO_PADDED or other)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,addEntry,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry),782,784,"/**
* Masks meta index entry using provided parameters.
* @param indexEntry MetaIndexEntry object to mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionAlgorithm,org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm(),341,343,"/**
* Delegates call to dataIndex's m1() method. 
* @return result of m1() or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName(),652,654,"/**
* Returns the function mask value by accessing nested properties of dataIndex. 
* @return string representation of the function mask
*/","* Get the name of the default compression algorithm.
     * 
     * @return the name of the default compression algorithm.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,hashCode,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode(),1973,1976,"/**
* Calculates the mask value using M1 and key buffer.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",45,49,"/**
* Delegates to a6 method with extracted values from comparable objects.
* @param o1 first RawComparable object
* @param o2 second RawComparable object
* @return result of a6
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput),948,952,"/**
* Initializes BlockRegion object from binary data.
* @param in DataInput stream containing block region data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readVInt,org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput),177,184,"/**
* Converts input from DataInput into an integer, masking overflow.
* @param in input data stream
* @return integer value or throws exception if out of range
*/","* Decoding the variable-length integer. Synonymous to
   * <code>(int)Utils#readVLong(in)</code>.
   * 
   * @param in
   *          input stream
   * @return the decoded integer
   * @throws IOException raised on errors performing I/O.
   * 
   * @see Utils#readVLong(DataInput)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String),2070,2096,"/**
* Creates a BytesComparator instance based on the provided comparator.
* @param comparator input comparator object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput),960,964,"/**
* Writes mask data to output stream.
* @param out DataOutput stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeVInt,"org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)",55,57,"/**
* Writes mask data to output stream.
* @param out DataOutput stream
* @param n integer parameter
*/","* Encoding an integer into a variable-length encoding format. Synonymous to
   * <code>Utils#writeVLong(out, n)</code>.
   * 
   * @param out
   *          output stream
   * @param n
   *          The integer to be encoded
   * @throws IOException raised on errors performing I/O.
   * @see Utils#writeVLong(DataOutput, long)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,isSorted,org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted(),864,866,"/**
* Calls m1() on underlying metadata object.
* @return true if successful, false otherwise
*/","* Is the TFile sorted?
     * 
     * @return true if TFile is sorted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec(),273,273,"/**
* Returns compression codec flags mask. 
* @throws IOException on encoding error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,equals,org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object),395,400,"/**
* Compares object equality using a custom mask operation.
* @param other object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount(),873,875,"/**
* Retrieves the function mask from file metadata.
* @return Function mask value as a long integer.","* Get the number of key-value pair entries in TFile.
     * 
     * @return the number of key-value pairs in TFile",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader:close(),824,827,"/**
* Calls method m1 on BCF (Binary Configuration File) reader.
*/","* Close the reader. The state of the Reader object is undefined after
     * close. Calling close() for multiple times has no effect.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getComparatorName,org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName(),855,857,"/**
* Returns a string representation of the mask from file metadata.
* @return Mask value as a string
*/","* Get the string representation of the comparator.
     * 
     * @return If the TFile is not sorted by keys, an empty string will be
     *         returned. Otherwise, the actual comparator string that is
     *         provided during the TFile creation time will be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable),40,42,"/**
 * Copies data from another BytesWritable instance.
 * @param other source BytesWritable object
 */","* Constructing a ByteArray from a {@link BytesWritable}.
   * 
   * @param other other.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[]),50,52,"/**
* Initializes ByteArray with given byte array.
* @param buffer byte array to be wrapped
*/","* Wrap a whole byte array as a RawComparable.
   * 
   * @param buffer
   *          the byte array buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int),2031,2033,"/**
 * Calculates a function mask value based on the given current bid.
 * @param curBid the current bid integer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,addEntry,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry),2262,2266,"/**
* Updates index and record count based on given file index entry.
* @param keyEntry FileIndexEntry object containing relevant data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)",468,471,"/**
* Masks data in a specified range with provided raw value.
* @param raw raw value to mask with
* @param begin start index of the region
* @param end end index of the region (inclusive)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockIndexNear,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long),745,756,"/**
* Finds the index of a block region in the data index by offset.
* @param offset unique identifier
* @return index or -1 if not found
*/","* Find the smallest Block index whose starting offset is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          User-specific offset.
     * @return the index to the data Block if such block exists; or -1
     *         otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable),2187,2201,"/**
* Searches for a key in the sorted TFile.
* @param key RawComparable object to search for
* @return Index position of the key or -1 if not found
*/","* @param key
     *          input key.
     * @return the ID of the first block that contains key >= input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int),45,51,"/**
* Writes an integer to buffer, handling buffer overflow.
* @param b integer value to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,"org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)",53,65,"/**
* Writes data to buffer or delegates to underlying stream.
* @param b byte array to write
* @param off offset into the array
* @param len number of bytes to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,flush,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush(),67,71,"/**
 * Invokes M1 and Out's M2 methods in synchronized block.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable),2209,2223,"/**
* Searches for a key in the sorted TFile and returns its index.
* @param key RawComparable object to search for
*/","* @param key
     *          input key.
     * @return the ID of the first block that contains key > input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),2244,2248,"/**
* Calculates the function mask for a given reader location.
* @param location Reader.Location object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),741,744,"/**
 * Wraps Location-based call to m1 with indices.
 * @param other location object containing indices
 */",* @see java.lang.Comparable#compareTo(java.lang.Object),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)",705,707,"/**
 * Initializes location with specified block and record indices.
 * @param blockIndex index of the data block
 * @param recordIndex index within the block",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,set,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),734,736,"/**
* Calls m1 with block and record indices from Location object.
* @param other Location object containing block and record indices
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[]),1775,1777,"/**
* Reads an integer from the given buffer starting at offset 0.
* @param buf input byte array
*/","* Copy the key into user supplied buffer.
         * 
         * @param buf
         *          The buffer supplied by user. The length of the buffer must
         *          not be shorter than the key length.
         * @return The length of the key.
         * 
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)",1859,1890,"/**
* Reads and returns the length of data from the buffer, 
* or throws exception if buffer is too small.
* @param buf input byte array
* @param offset starting position in the buffer
*/","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value (starting from the offset). The
         * value part of the key-value pair pointed by the current cursor is not
         * cached and can only be examined once. Calling any of the following
         * functions more than once without moving the cursor will result in
         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @param offset offset.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState),549,552,"/**
 * Initializes a BlockReader with a RBlockState object.
 * @param rbs RBlockState containing block data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getRawSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize(),585,587,"/**
* Calls m2() on result of m1() in rBlkState. 
* @return result of m2() operation 
*/","* Get the uncompressed size of the block.
       * 
       * @return uncompressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize(),594,596,"/**
* Retrieves value from nested state using recursive approach. 
* @return result of m2() on nested state object
*/","* Get the compressed size of the block.
       * 
       * @return compressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getStartPos,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos(),603,605,"/**
* Retrieves the function mask value from the block state.","* Get the starting position of the block in the file.
       * 
       * @return the starting position of the block in the file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[]),394,397,"/**
* Invokes m1 with default offset 0 and length equal to input array.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputOutputStream.java,constructOutputStream,org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput),43,49,"/**
* Wraps a DataOutput object as an OutputStream, or returns it as-is.
* @param out the DataOutput to wrap
* @return wrapped OutputStream instance
*/","* Construct an OutputStream from the given DataOutput. If 'out'
   * is already an OutputStream, simply returns it. Otherwise, wraps
   * it in an OutputStream.
   * @param out the DataOutput to wrap
   * @return an OutputStream instance that outputs to 'out'",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable:<init>(float),34,34,"/**
* Constructs a new FloatWritable instance with the specified float value. 
* @param value the float value to be wrapped in this writable object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FastByteComparisons.java,compareTo,"org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)",188,242,"/**
* Compares two byte arrays with potential stride differences.
* @param buffer1 first byte array
* @param offset1 offset into first byte array
* @param length1 length of first byte array
* @param buffer2 second byte array
* @param offset2 offset into second byte array
* @param length2 length of second byte array
* @return comparison result or difference in lengths","* Lexicographically compare two arrays.
       *
       * @param buffer1 left operand
       * @param buffer2 right operand
       * @param offset1 Where to start comparing in the left buffer
       * @param offset2 Where to start comparing in the right buffer
       * @param length1 How much to compare from the left buffer
       * @param length2 How much to compare from the right buffer
       * @return 0 if equal, < 0 if left is less than right, etc.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(),89,91,"/**
* Initializes a new instance of the DataOutputBuffer class.
* @param buffer underlying buffer object (default is a new Buffer instance)
*/",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(int),93,95,"/**
* Initializes DataOutputBuffer with specified buffer size.
* @param size allocated buffer size in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getData,org.apache.hadoop.io.DataOutputBuffer:getData(),108,108,"/**
* Calls M1 function on underlying buffer. 
* @return result of M1 operation as a byte array
*/","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return data byte.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getLength,org.apache.hadoop.io.DataOutputBuffer:getLength(),114,114,"/**
* Calls m1() on associated buffer object.
*/","* Returns the length of the valid data currently in the buffer.
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,writeInt,"org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)",154,164,"/**
* Writes a 32-bit integer value to the buffer at specified offset.
* @param v 32-bit integer value
* @param offset buffer offset in bytes
*/","* Overwrite an integer into the internal buffer. Note that this call can only
   * be used to overwrite existing data in the buffer, i.e., buffer#count cannot
   * be increased, and DataOutputStream#written cannot be increased.
   *
   * @param v v.
   * @param offset offset.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getHostnameByIP,org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress),44,62,"/**
* Resolves IP address to domain name or returns the original host string.
* @param address InetAddress object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistance,"org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",324,365,"/**
* Calculates the minimum distance between two nodes in a cluster.
* @param node1 first node
* @param node2 second node
* @return minimum distance or MAX_VALUE if either node is not found
*/","Return the distance between two nodes
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2 which is zero if they are the same
   *  or {@link Integer#MAX_VALUE} if node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isNodeInScope,"org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)",1023,1029,"/**
* Checks if a node is within the specified scope.
* @param node Node object to check
* @param scope Scope string with path separator (e.g., ""/"")
* @return true if node is within scope, false otherwise
*/","* Checks whether a node belongs to the scope.
   * @param node  the node to check.
   * @param scope scope to check.
   * @return true if node lies within the scope",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,getPathComponents,org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node),124,126,"/**
* Generates an array of masked strings from a given Node.
* @param node input Node object
* @return array of masked string values or null if unsuccessful
*/","* Get the path components of a node.
   * @param node a non-null node
   * @return the path of a node",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,equals,org.apache.hadoop.net.NodeBase:equals(java.lang.Object),128,137,"/**
* Recursively checks equality between this node and the given object.
* @param to the object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,hashCode,org.apache.hadoop.net.NodeBase:hashCode(),139,142,"/**
* Recursively calls m1 and returns its result.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,toString,org.apache.hadoop.net.NodeBase:toString(),145,148,"/**
 * Returns a string representation of the function mask.
 */",@return this node's path as its string representation,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,remove,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node),228,254,"/**
* Removes a node from the network topology, updating cluster and rack counts.
* @param node Node object to remove
*/","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDatanodesInRack,org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String),202,215,"/**
* Retrieves a list of nodes matching the specified location mask.
* @param loc location mask
* @return List of Node objects or empty list if no matches
*/","* Given a string representation of a rack, return its children
   * @param loc a path-like string representation of a rack
   * @return a newly allocated list with all the node's children",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNode,org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String),271,281,"/**
* Retrieves a node from the cluster map based on location.
* @param loc location string to resolve
* @return Node object or null if not found
*/","Given a string representation of a node, return its reference
   * 
   * @param loc
   *          a path-like string representation of a node
   * @return a reference to the node; null if the node is not in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,locationToDepth,org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String),209,219,"/**
* Calculates the depth of a directory path.
* @param location directory path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,toString,org.apache.hadoop.net.NetworkTopology:toString(),714,732,"/**
* Generates a formatted string with user profile details.
* @return A StringBuilder object containing the profile information
*/",convert a network tree to a string.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isOnSameRack,"org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",409,415,"/**
* Checks if two nodes are functionally equivalent.
* @param node1 first node to compare
* @param node2 second node to compare
* @return true if nodes have the same functionality, false otherwise
*/","Check if two nodes are on the same rack
   * @param node1 one node (can be null)
   * @param node2 another node (can be null)
   * @return true if node1 and node2 are on the same rack; false otherwise
   * @exception IllegalArgumentException when either node1 or node2 is null, or
   * node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)",569,637,"/**
* Returns a Node from the given InnerNode and scope constraints.
* @param parentNode InnerNode to search
* @param excludedScopeNode node to exclude from selection
* @param excludedNodes collection of nodes to exclude from selection
* @return selected Node or null if not found
*/","* Randomly choose one node under <i>parentNode</i>, considering the exclude
   * nodes and scope. Should be called with {@link #netlock}'s readlock held.
   *
   * @param parentNode        the parent node
   * @param excludedScopeNode the node corresponding to the exclude scope.
   * @param excludedNodes     a collection of nodes to be excluded from
   * @param totalInScopeNodes total number of nodes under parentNode, excluding
   *                          the excludedScopeNode
   * @param availableNodes    number of available nodes under parentNode that
   *                          could be chosen, excluding excludedNodes
   * @return the chosen node, or null if none can be chosen",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getWeightUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",807,845,"/**
* Calculates a weighted similarity between two nodes based on their paths.
* @param reader Node with path to compare
* @param node Node to compare against
* @return Weighted similarity value (0-10)
*/","* Returns an integer weight which specifies how far away <i>node</i> is
   * from <i>reader</i>. A lower value signifies that a node is closer.
   * It uses network location to calculate the weight
   *
   * @param reader Node where data will be read
   * @param node Replica of data
   * @return weight",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interAddNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node),1083,1097,"/**
* Updates rack node map with decommissioned status.
* @param node Node object to process
*/","* Internal function for update empty rack number
   * for add or recommission a node.
   * @param node node to be added; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,<init>,"org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)",58,66,"/**
* Initializes a SocketIO object with specified SelectableChannel and timeout.
* @param channel the underlying SelectableChannel
* @param timeout connection timeout in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,"org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)",111,129,"/**
* Processes input data in chunks, handling IO exceptions and flushing the writer as needed.
* @throws IOException on stream closure or write failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)",202,251,"/**
* Transfers data from a FileChannel, tracking write and transfer times.
* @param fileCh target channel
* @param position starting position in the channel
* @param count total bytes to be transferred
* @param waitForWritableTime optional time tracker for writes
* @param transferToTime optional time tracker for transfers
*/","* Transfers data from FileChannel using 
   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}.
   * Updates <code>waitForWritableTime</code> and <code>transferToTime</code>
   * with the time spent blocked on the network and the time spent transferring
   * data from disk to network respectively.
   * 
   * Similar to readFully(), this waits till requested amount of 
   * data is transfered.
   * 
   * @param fileCh FileChannel to transfer data from.
   * @param position position within the channel where the transfer begins
   * @param count number of bytes to transfer.
   * @param waitForWritableTime nanoseconds spent waiting for the socket 
   *        to become writable
   * @param transferToTime nanoseconds spent transferring data
   * 
   * @throws EOFException 
   *         If end of input file is reached before requested number of 
   *         bytes are transfered.
   *
   * @throws SocketTimeoutException 
   *         If this channel blocks transfer longer than timeout for 
   *         this stream.
   *          
   * @throws IOException Includes any exception thrown by 
   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeHostNames,org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection),662,668,"/**
* Processes a collection of names and returns a list with transformed hostnames.
* @param names Collection of input names
*/","* Given a collection of string representation of hosts, return a list of
   * corresponding IP addresses in the textual representation.
   * 
   * @param names a collection of string representations of hosts
   * @return a list of corresponding IP addresses in the string format
   * @see #normalizeHostName(String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getHostDetailsAsString,"org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)",977,988,"/**
* Generates a formatted string containing local and destination host details.
* @param destHost destination hostname
* @param destPort destination port number
* @param localHost local hostname
* @return formatted string or null if any parameter is invalid
*/","* Get the host details as a string
   * @param destHost destinatioon host (nullable)
   * @param destPort destination port
   * @param localHost local host (nullable)
   * @return a string describing the destination host:port and the local host",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getIPs,"org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)",1041,1068,"/**
* Retrieves all IP addresses within a given subnet.
* @param subnet string representation of the subnet
* @param returnSubinterfaces whether to include subinterface IPs (true/false)
* @return List of InetAddress objects representing matching IP addresses
*/","* Return an InetAddress for each interface that matches the
   * given subnet specified using CIDR notation.
   *
   * @param subnet subnet specified using CIDR notation
   * @param returnSubinterfaces
   *            whether to return IPs associated with subinterfaces
   * @throws IllegalArgumentException if subnet is invalid
   * @return ips.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getFreeSocketPorts,org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int),1098,1113,"/**
* Generates a set of unique integers representing free port IDs.
* @param numOfPorts maximum number of ports to reserve (0-25)
* @return Set of available port IDs or throws an exception if acquisition fails
*/","* Return free ports. There is no guarantee they will remain free, so
   * ports should be used immediately. The number of free ports returned by
   * this method should match argument {@code numOfPorts}. Num of ports
   * provided in the argument should not exceed 25.
   *
   * @param numOfPorts Number of free ports to acquire.
   * @return Free ports for binding a local socket.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,getConf,org.apache.hadoop.net.TableMapping:getConf(),71,74,"/**
* Delegates to m1() and returns its m2() result.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,setConf,org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration),76,80,"/**
 * Calls super and child methods to perform initialization.
 * @param conf Hadoop Configuration object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>(),172,172,"/**
* Initializes an empty RawScriptBasedMapping instance.","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,<init>,org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),50,52,"/**
 * Initializes a new instance of CachedDNSToSwitchMapping from a raw mapping.
 * @param rawMapping the underlying DNSToSwitchMapping object
 */","* cache a raw DNS mapping
   * @param rawMapping the raw mapping to cache",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,org.apache.hadoop.net.NodeBase:<init>(java.lang.String),53,61,"/**
* Initializes a NodeBase instance from a file system path.
* @param path the file system path to initialize with
*/","Construct a node from its path
   * @param path 
   *   a concatenation of this node's location, the path separator, and its name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)",67,69,"/**
* Initializes a new NodeBase instance with given name and normalized location.
* @param name unique identifier of the node
* @param location geographical location of the node
*/","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)",77,81,"/**
* Initializes a new NodeBase with specified details.
* @param name node name
* @param location node location (normalized)
* @param parent parent node reference
* @param level node hierarchy level
*/","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location 
   * @param parent this node's parent node
   * @param level this node's level in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,getConf,org.apache.hadoop.net.ScriptBasedMapping:getConf(),116,119,"/**
* Calls m2() on the result of m1(), returning the returned Configuration object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,toString,org.apache.hadoop.net.ScriptBasedMapping:toString(),121,124,"/**
* Combines script-based and database-driven mappings by calling m1().",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer),602,628,"/**
* Reads data from socket into ByteBuffer.
* @param dst buffer to read into
* @return number of bytes read, or 0 if none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int),563,575,"/**
* Sends an integer value over the socket connection.
* @param val the integer value to be sent
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,"org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)",577,587,"/**
* Writes data to the underlying socket using masked bytes.
* @param b byte array containing masked data
* @param off offset into the byte array
* @param len length of the data to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(),507,519,"/**
* Retrieves a mask value via the DomainSocket interface.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,"org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)",521,532,"/**
* Reads data from a DomainSocket and returns the number of bytes read.
* @param b buffer to store read data
* @param off offset in buffer
* @param len length to read
* @return number of bytes read or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,available,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available(),534,545,"/**
* Retrieves available count from domain socket.
* @return Available count or throws IOException if an error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,<init>,"org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)",168,172,"/**
* Initializes a DomainSocket object with file descriptor and path.
* @param path socket path
* @param fd file descriptor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallback,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",386,424,"/**
* Determines whether to close a file descriptor based on handler request.
* @param caller method invoking this function
* @param entries map of file descriptors and their corresponding Entry objects
* @param fdSet set of active file descriptors
* @param fd file descriptor being evaluated
* @return true if the file descriptor should be closed, false otherwise
*/","* Send callback and return whether or not the domain socket was closed as a
   * result of processing.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   * @return true if the domain socket was closed as a result of processing",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket:isOpen(),268,270,"/**
* Calls underlying reference count method.
* @return true/false result from refCount.m1()
*/","* Return true if the file descriptor is currently open.
   *
   * @return                 True if the file descriptor is currently open.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket:close(),344,388,"/**
* Performs cleanup and shutdown operations.
* @throws IOException on I/O errors
*/",* Close the Socket.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,addNotificationSocket,"org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)",546,561,"/**
* Adds a notification socket and updates the FD set.
* @param entries map of process IDs to Entry objects
* @param fdSet set of file descriptors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,trimIdleSelectors,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long),426,444,"/**
* Removes idle selectors from the provider map.
* @param now current timestamp
*/","* Closes selectors that are idle for IDLE_TIMEOUT (10 sec). It does not
     * traverse the whole list, just over the one that have crossed 
     * the timeout.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,isLeafParent,org.apache.hadoop.net.InnerNodeImpl:isLeafParent(),302,304,"/**
* Evaluates function mask condition using m1().",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getNextAncestorName,org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node),113,127,"/**
* Extracts and sanitizes the node's path from the underlying data structure.
* @param n Node object to extract path from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)",70,76,"/**
* Establishes a new client socket connection to the specified address and port.
* @param addr target server address
* @param port target server listening port
* @return established client Socket object or throws IOException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",78,86,"/**
* Establishes a new client socket with specified remote and local addresses.
* @param addr remote server address
* @param port remote server port
* @param localHostAddr local host address
* @param localPort local port
* @return established Socket object or throws IOException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)",88,95,"/**
* Creates a new socket connection to the specified host and port.
* @param host hostname or IP address of the remote server
* @param port TCP port number to connect to
* @return established Socket object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",97,106,"/**
* Creates a new Socket instance and establishes connections to the specified host and local addresses.
* @param host remote server hostname
* @param port remote server port number
* @param localHostAddr local server IP address
* @param localPort local server port number
* @return established Socket object or throws exception if creation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)",65,71,"/**
* Establishes a new socket connection to the specified address and port.
* @param addr remote server address
* @param port remote server port number
* @return established Socket object or throws IOException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",73,81,"/**
* Creates a socket and establishes connections to remote and local addresses.
* @param addr remote address
* @param port remote port
* @param localHostAddr local host address
* @param localPort local port
* @return established Socket object or null if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)",83,90,"/**
* Establishes a client socket connection to the specified host and port.
* @param host hostname or IP address
* @param port TCP port number
* @return established Socket object
* @throws IOException network communication error
* @throws UnknownHostException host resolution failure",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",92,101,"/**
* Creates a socket connection to the specified host and port.
* @param host remote host name or IP address
* @param port remote port number
* @param localHostAddr local host IP address
* @param localPort local port number
* @return established Socket object, or throws exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,toString,org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString(),71,74,"/**
* Combines script and database mappings by calling the first with the result of the second.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,getDependency,org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String),96,115,"/**
* Retrieves a list of dependencies for the given name.
* @param name input string to resolve dependencies for
* @return List of String dependencies or empty list if not found
*/","* Get dependencies in the topology for a given host
   * @param name - host name for which we are getting dependency
   * @return a list of hosts dependent on the provided host name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,setTimeout,org.apache.hadoop.net.SocketInputWrapper:setTimeout(long),69,75,"/**
* Sets a read timeout on the underlying channel or socket.
* @param timeoutMs timeout duration in milliseconds
*/","* Set the timeout for reads from this stream.
   * 
   * Note: the behavior here can differ subtly depending on whether the
   * underlying socket has an associated Channel. In particular, if there is no
   * channel, then this call will affect the socket timeout for <em>all</em>
   * readers of this socket. If there is a channel, then this call will affect
   * the timeout only for <em>this</em> stream. As such, it is recommended to
   * only create one {@link SocketInputWrapper} instance per socket.
   * 
   * @param timeoutMs
   *          the new timeout, 0 for no timeout
   * @throws SocketException
   *           if the timeout cannot be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,"org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)",175,209,"/**
* Retrieves IP addresses from a network interface.
* @param strInterface name of the interface
* @param returnSubinterfaces whether to include subinterfaces in result
* @return array of IP addresses or cached host address if default interface
*/","* Returns all the IPs associated with the provided interface, if any, in
   * textual form.
   * 
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A string vector of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPsAsInetAddressList,"org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)",428,457,"/**
* Retrieves a list of IP addresses associated with the specified network interface.
* @param strInterface name of the network interface
* @param returnSubinterfaces whether to include subinterface addresses in the result
* @return List of InetAddress objects or cached host address if not found
*/","* Returns all the IPs associated with the provided interface, if any, as
   * a list of InetAddress objects.
   *
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A list of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,"org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)",129,132,"/**
* Wraps byte array in ByteBuffer and delegates to m2.
* @param b input byte array
* @param off offset into the array
* @param len length of data to process
* @return int result from m2 call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getRack,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String),57,80,"/**
* Resolves location to a masked string based on node properties.
* @param loc location string
* @return masked string or original location if no match found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeGroup,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String),90,118,"/**
* Recursively resolves location by traversing the node hierarchy.
* @param loc initial location
* @return resolved location or null if not found
*/","* Given a string representation of a node group for a specific network
   * location
   * 
   * @param loc
   *            a path-like string representation of a network location
   * @return a node group string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,dumpTopology,org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology(),112,133,"/**
* Generates a formatted string with mapping and topology details.
* @return a human-readable string or ""No topology information"" if null
*/","* Generate a string listing the switch mapping implementation,
   * the mapping for every known node and the number of nodes and
   * unique switches known about -each entry to a separate line.
   * @return a string that can be presented to the ops team or used in
   * debug messages.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isMappingSingleSwitch,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping),150,153,"/**
* Checks if the given DNS-to-switch mapping meets specific criteria.
* @param mapping DNSToSwitchMapping object to evaluate
* @return true if mapping is valid, false otherwise
*/","* Query for a {@link DNSToSwitchMapping} instance being on a single
   * switch.
   * <p>
   * This predicate simply assumes that all mappings not derived from
   * this class are multi-switch.
   * @param mapping the mapping to query
   * @return true if the base class says it is single switch, or the mapping
   * is not derived from this class.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getWeight,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",256,271,"/**
* Calculates a weighted mask value based on reader-node interaction.
* @param reader Node object with methods m1-m3
* @param node Node to interact with reader
* @return Weighted mask value (0-3)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeAttribute,"org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)",330,388,"/**
* Writes MBean attribute to JSON generator.
* @param jg the JSON generator
* @param oname the object name of the MBean
* @param attr the attribute info
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeObject,"org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)",395,436,"/**
* Serializes the given value to JSON using the provided generator.
* @param value object to serialize
* @param attName name of the attribute being serialized (if applicable)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,getCurrentStats,"org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)",290,297,"/**
* Retrieves summary statistics for the given recorder name and index.
* @param recorderName unique identifier of a recorder
* @param idx index to fetch statistics for
* @return SummaryStatistics object or null if not found
*/","* Return the summary information for given index.
   *
   * @param recorderName The name of the recorder.
   * @param idx The index value.
   * @return The summary information.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseProtocolArgs,"org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)",209,228,"/**
* Parses -protocol command, validating and setting the protocol.
* @param args array of command-line arguments
* @return index of next argument to process (or -1 if invalid)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,printUsage,org.apache.hadoop.log.LogLevel:printUsage(),87,90,"/**
* Prints usage message and parses command-line options.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,printGenericCommandUsage,org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream),105,107,"/**
* Prints help message to the specified output stream.
* @param out output stream to print help message to
*/","* Prints generic command-line argurments and usage information.
   * 
   *  @param out stream to write usage information to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,isLog4jLogger,org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class),95,100,"/**
* Recursively checks if class has a method 'm1'.
* @param clazz Class to check
* @return true if class or any superclass has method 'm1', false otherwise
*/","* Determine whether the log of <code>clazz</code> is Log4j implementation.
   * @param clazz a class to be determined
   * @return true if the log of <code>clazz</code> is Log4j implementation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,"org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)",159,161,"/**
* Initializes a Log Throttling Helper with specified minimum log period and primary recorder name.
* @param minLogPeriodMs minimum time in milliseconds between log messages
* @param primaryRecorderName name of the primary recorder to throttle logs for
*/","* Create a log helper with a specified primary recorder name; this can be
   * used in conjunction with {@link #record(String, long, double...)} to set up
   * primary and dependent recorders. See
   * {@link #record(String, long, double...)} for more details.
   *
   * @param minLogPeriodMs The minimum period with which to log; do not log
   *                       more frequently than this.
   * @param primaryRecorderName The name of the primary recorder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,"org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])",247,281,"/**
* Logs action with variable number of values.
* @param recorderName name of the recorder
* @param currentTimeMs current time in milliseconds
* @param values variable number of loggable values
* @return LoggingAction object or null if not logged
*/","* Record some set of values at the specified time into this helper. This can
   * be useful to avoid fetching the current time twice if the caller has
   * already done so for other purposes. This additionally allows the caller to
   * specify a name for this recorder. When multiple names are used, one is
   * denoted as the primary recorder. Only recorders named as the primary
   * will trigger logging; other names not matching the primary can <i>only</i>
   * be triggered by following the primary. This is used to coordinate multiple
   * logging points. A primary can be set via the
   * {@link #LogThrottlingHelper(long, String)} constructor. If no primary
   * is set in the constructor, then the first recorder name used becomes the
   * primary.
   *
   * If multiple names are used, they maintain entirely different sets of values
   * and summary information. For example:
   * <pre>{@code
   *   // Initialize ""pre"" as the primary recorder name
   *   LogThrottlingHelper helper = new LogThrottlingHelper(1000, ""pre"");
   *   LogAction preLog = helper.record(""pre"", Time.monotonicNow());
   *   if (preLog.shouldLog()) {
   *     // ...
   *   }
   *   double eventsProcessed = ... // perform some action
   *   LogAction postLog =
   *       helper.record(""post"", Time.monotonicNow(), eventsProcessed);
   *   if (postLog.shouldLog()) {
   *     // ...
   *     // Can use postLog.getStats(0) to access eventsProcessed information
   *   }
   * }</pre>
   * Since ""pre"" is the primary recorder name, logging to ""pre"" will trigger a
   * log action if enough time has elapsed. This will indicate that ""post""
   * should log as well. This ensures that ""post"" is always logged in the same
   * iteration as ""pre"", yet each one is able to maintain its own summary
   * information.
   *
   * <p>Other behavior is the same as {@link #record(double...)}.
   *
   * @param recorderName The name of the recorder. This is used to check if the
   *                     current recorder is the primary. Other names are
   *                     arbitrary and are only used to differentiate between
   *                     distinct recorders.
   * @param currentTimeMs The current time.
   * @param values The values to log.
   * @return The LogAction for the specified recorder.
   *
   * @see #record(double...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,fromInternalName,org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String),148,156,"/**
* Retrieves an Event object by matching its name.
* @param name the name to search for
* @return Event object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfilerDisabledServlet.java,doGet,"org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",34,48,"/**
* Returns an internal server error response with profiler servlet disabled message.
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,<init>,org.apache.hadoop.http.ProfileServlet:<init>(),177,181,"/**
* Initializes servlet instance with PID and async profiler home path.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,needsQuoting,org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String),68,74,"/**
* Recursively checks if the input string is empty.
* @param str input string
* @return true if empty, false otherwise
*/","* Does the given string need to be quoted?
   * @param str the string to check
   * @return does the string contain any of the active html characters?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,quoteHtmlChars,org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String),114,131,"/**
* Encrypts and decrypts user profile data using custom methods.
* @param item string to be encrypted/decrypted
* @return encrypted/decrypted string or null on failure
*/","* Quote the given item to make it html-safe.
   * @param item the string to quote
   * @return the quoted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addJerseyResourcePackage,"org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)",1018,1022,"/**
* Updates package metadata with specified file system paths.
* @param packageName unique package identifier
* @param pathSpec file system paths to update
*/","* Add a Jersey resource package.
   * @param packageName The Java package name containing the Jersey resource.
   * @param pathSpec The path spec for the servlet",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addServlet,"org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)",1050,1053,"/**
* Registers an HTTP servlet class with the specified name and path specification.
* @param name name of the servlet
* @param pathSpec path specification for the servlet
* @param clazz servlet class to register
*/","* Add a servlet in the server.
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addInternalServlet,"org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)",1065,1068,"/**
* Convenience constructor for HttpServlet instances.
* @param name servlet name
* @param pathSpec URL pattern specification
* @param clazz servlet class type
*/","* Add an internal servlet in the server.
   * Note: This method is to be used for adding servlets that facilitate
   * internal communication and not for user facing functionality. For
   * servlets added using this method, filters are not enabled.
   *
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addFilter,"org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)",1170,1193,"/**
* Adds a filter with specified class and parameters to the web application context.
* @param name filter name
* @param classname filter class
* @param parameters filter parameters
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addGlobalFilter,"org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)",1195,1206,"/**
* Adds a global filter with the specified name, class, and parameters.
* @param name filter name
* @param classname filter class name
* @param parameters filter parameter map
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,defineFilter,"org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])",1217,1222,"/**
* Configures and registers a servlet filter with the given context.
* @param ctx ServletContextHandler instance
* @param name Filter name
* @param classname Filter class name
* @param parameters Filter initialization parameters
* @param urls List of URLs to map the filter
*/","* Define a filter for a context and set up default url mappings.
   *
   * @param ctx ctx.
   * @param name name.
   * @param classname classname.
   * @param parameters parameters.
   * @param urls urls.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForSinglePort,"org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)",1478,1493,"/**
* Continuously retries connecting to server on specified port.
* @param listener ServerConnector instance
* @param port initial port number (0 for auto-discovery)
*/","* Bind using single configured port. If findPort is true, we will try to bind
   * after incrementing port till a free port is found.
   * @param listener jetty listener.
   * @param port port which is set in the listener.
   * @throws Exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,toString,org.apache.hadoop.http.HttpServer2:toString(),1631,1642,"/**
* Generates server status string with listener addresses.
* @return formatted status message or null on invalid state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getEnum,org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String),1946,1954,"/**
* Maps a string to an XFrameOption enum value.
* @param value input string
*/","* We cannot use valueOf since the AllowFrom enum differs from its value
     * Allow-From. This is a helper method that does exactly what valueof does,
     * but allows us to handle the AllowFrom issue gracefully.
     *
     * @param value - String must be DENY, SAMEORIGIN or ALLOW-FROM.
     * @return XFrameOption or throws IllegalException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,init,org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig),1860,1864,"/**
 * Initializes filter configuration and starts processing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doFilter,"org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",1870,1893,"/**
* Processes incoming request and sets response headers.
* @param request ServletRequest object
* @param response ServletResponse object
* @param chain FilterChain instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,init,org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig),114,118,"/**
* Initializes user profile from filter configuration.
* @param conf filter configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)",75,89,"/**
* Initializes a FileMonitoringTimerTask instance with file paths to monitor and callback handlers.
* @param filePaths list of file paths to track
* @param onFileChange consumer to handle file changes
* @param onChangeFailure consumer to handle change failures
*/","* Create file monitoring task to be scheduled using a standard
   * Java {@link java.util.Timer} instance.
   *
   * @param filePaths The path to the file to monitor.
   * @param onFileChange The function to call when the file has changed.
   * @param onChangeFailure The function to call when an exception is
   *                       thrown during the file change processing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNonNegative,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)",408,417,"/**
* Retrieves and validates the flush offset interval in milliseconds.
* @param key configuration key
* @param defaultValue default value if not found or invalid
* @return non-negative flush offset interval value
*/","* Return the property value if it's non-negative and throw an exception if
   * it's not.
   *
   * @param key the property key
   * @param defaultValue the default value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkIfPropertyExists,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String),424,429,"/**
* Verifies presence of specified Metrics2 configuration property.
* @param key unique property identifier
*/","* Throw a {@link MetricsException} if the given property is not set.
   *
   * @param key the key to validate",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkForErrors,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String),905,910,"/**
* Throws metrics exception on error condition.
* @param message error message
*/","* If the sink isn't set to ignore errors, throw a {@link MetricsException}
   * if the stream encountered an exception.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.
   * @throws MetricsException thrown if there was an error and the sink isn't
   * ignoring errors",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String),940,944,"/**
* Throws a MetricsException with file path context.
* @param message error message
*/","* If the sink isn't set to ignore errors, throw a new
   * {@link MetricsException}.  The message parameter will be used  as the
   * new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String),29,31,"/**
* Constructs a MetricsConfigException with the specified error message.
* @param message detailed description of configuration issue
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkMetricName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String),434,452,"/**
* Validates and registers a metric name.
* @param name unique metric identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkTagName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String),454,458,"/**
* Checks if a tag with the given name already exists.
* @param name unique tag identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class),37,46,"/**
* Retrieves a metrics factory instance of the specified type.
* @param cls Class of the desired metrics factory
* @return Instance of the requested class or throws MetricsException if unknown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,build,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build(),76,92,"/**
* Creates or returns a MetricsSource instance based on the provided source and annotation conditions.
* @throws MetricsException if hybrid metrics are used without a registry or no valid @Metric annotation is found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newTag,org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class),125,139,"/**
* Returns a MutableMetric instance for the given class, or throws an exception if unsupported.
* @param resType Class type to generate metric for
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getRollInterval,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval(),341,399,"/**
* Parses and validates the flush interval configuration.
* @return time in milliseconds or throws an exception if invalid
*/","* Extract the roll interval from the configuration and return it in
   * milliseconds.
   *
   * @return the roll interval in millis",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)",924,929,"/**
* Throws a MetricsException with error message and exception details.
* @param message human-readable error message
* @param t the underlying exception to include in the exception
*/","* If the sink isn't set to ignore errors, wrap the Throwable in a
   * {@link MetricsException} and throw it.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) and the Throwable's string representation
   * appended to it.
   *
   * @param message the exception message. The message will have a colon, the
   * current file name ({@link #currentFilePath}), and the Throwable's string
   * representation (wrapped in square brackets) appended to it.
   * @param t the Throwable to wrap",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,init,org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration),45,55,"/**
* Configures output stream for subset configuration.
* @param conf SubsetConfiguration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)",33,35,"/**
 * Constructs a MetricsConfigException with a custom error message and optional root cause. 
 * @param message error message describing the configuration issue
 * @param cause underlying exception that caused this MetricsConfigException (optional)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable),37,39,"/**
* Constructs an instance of MetricsConfigException with the given cause.
* @param cause underlying exception that caused this metrics config exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,equals,org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object),71,78,"/**
* Compares given object with internal metrics tag for equality.
* @param obj Object to compare
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,toString,org.apache.hadoop.metrics2.MetricsTag:toString(),84,89,"/**
* Concatenates user information and value as a comma-separated string.
* @return formatted string or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,appendPrefix,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)",86,106,"/**
* Formats tags from metrics record into a string, filtering based on context-specific tag usage.
* @param record MetricsRecord object containing tags and context information
* @param sb StringBuilder to accumulate formatted tag strings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag),102,119,"/**
* Checks if a metric tag matches include/exclude patterns.
* @param tag MetricsTag object to evaluate
* @return true/false indicating match status
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable),121,142,"/**
* Evaluates metrics tags against include and exclude patterns.
* @param tags iterable of MetricsTags to check
* @return true if all tags match, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,context,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context(),72,80,"/**
* Returns the context from the first matching metrics tag.
* @return Context string or default value if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,"org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)",61,63,"/**
* Computes metric string builder using FUNC_MASK with provided metrics and value.
* @param info MetricsInfo object containing relevant data
* @param value Value to be processed by FUNC_MASK function
* @return MetricStringBuilder instance resulting from the computation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag),86,89,"/**
* Returns a MetricsRecordBuilder instance with FUNC_MASK applied to the given metrics tag.
* @param tag MetricsTag object containing m1() and m2() values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,setContext,org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String),97,100,"/**
* Generates metrics record builder with context mask.
* @param value input string to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,equals,org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object),75,82,"/**
* Compares two metrics for equality based on specific properties.
* @param obj metric object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,putMetrics,org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),57,80,"/**
* Formats and writes metrics data to the output stream.
* @param record MetricsRecord object containing data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,"org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)",154,177,"/**
* Creates a Record object containing metrics and tags from the given MetricsRecord.
* @param mr MetricsRecord containing data to be aggregated
* @param includingTags whether to include metrics tags in the Record
* @return Record object or null if aggregation fails
*/","* Update the cache and return the current cached record
   * @param mr the update record
   * @param includingTags cache tag values (for later lookup by name) if true
   * @return the updated cache record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,loadGangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType),185,218,"/**
* Processes Ganglia configuration by type, populating GangliaConf objects.
* @param gtype Ganglia configuration type (units, dmax, tmax, slope)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,xdr_string,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String),245,252,"/**
* Processes input string into a binary mask.
* @param s input string to process
*/","* Puts a string into the buffer by first writing the size of the string as an
   * int, followed by the bytes of the string, padded if necessary to a multiple
   * of 4.
   * @param s the string to be written to buffer at offset location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",47,104,"/**
* Emits a metric with specified name, type, value and slope.
* @param groupName group name
* @param name metric name
* @param type metric type
* @param value metric value
* @param gConf Ganglia configuration
* @param gSlope Ganglia slope
*/","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext31 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",221,252,"/**
* Logs and emits metric data with validation.
* @param groupName 
* @param name metric name
* @param type metric type
* @param value metric value
* @param gConf Ganglia configuration
* @param gSlope Ganglia slope
*/","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext30 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,calculateSlope,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",196,207,"/**
* Returns the GangliaSlope value, defaulting to DEFAULT_SLOPE if neither m1() nor slopeFromMetric is available.
* @param gConf Ganglia configuration object
* @param slopeFromMetric optional slope from metric data
* @return GangliaSlope value or DEFAULT_SLOPE if not found",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush(),176,180,"/**
* Writes to output stream if condition in m1() is met.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,connect,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect(),143,165,"/**
* Establishes a socket connection to Graphite.
* @throws MetricsException if already connected or connection fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,close,org.apache.hadoop.metrics2.sink.GraphiteSink:close(),124,127,"/**
* Calls Graphite API's m1 method.
* @throws IOException if Graphite API call fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,init,org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration),78,95,"/**
* Initializes StatsD client with configuration settings.
* @param conf SubsetConfiguration object containing service settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,close,org.apache.hadoop.metrics2.sink.StatsDSink:close(),163,166,"/**
* Calls StatsD method 'm1' to perform statistics update.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,putMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),68,82,"/**
* Processes metrics records and updates Prometheus metrics.
* @param metricsRecord Metrics record to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,getMetricKey,"org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)",165,174,"/**
* Generates a function mask for the given Prometheus metric key.
* @param promMetricKey Prometheus metric key
* @param metric Abstract metric object
* @param extendTags list of extended tags to append
* @return modified metric key or original key if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,init,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration),54,84,"/**
* Processes subset configuration for masking.
* @param conf SubsetConfiguration object to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,add,"org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)",29,31,"/**
* Creates a metrics buffer entry from given data and delegates to m1(MetricsBuffer.Entry).
* @param name metric name
* @param records iterable of metric records
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,get,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get(),33,35,"/**
* Creates a new MetricsBuffer instance with the current buffer's state.
* @return A new MetricsBuffer object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer),240,242,"/**
* Constructs a new WaitableMetricsBuffer instance from an existing MetricsBuffer.
* @param metricsBuffer underlying buffer to wrap
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,dequeue,org.apache.hadoop.metrics2.impl.SinkQueue:dequeue(),101,108,"/**
* Retrieves and returns a masked value of type T.
* @throws InterruptedException if thread is interrupted during operation
*/","* Dequeue one element from head of the queue, will block if queue is empty
   * @return  the first element
   * @throws InterruptedException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,clear,org.apache.hadoop.metrics2.impl.SinkQueue:clear(),154,161,"/**
* Clears and resets the internal data structure.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,waitForData,org.apache.hadoop.metrics2.impl.SinkQueue:waitForData(),110,118,"/**
* Retrieves and processes a masked value using a synchronized loop.
* @throws InterruptedException if thread is interrupted during execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback),308,311,"/**
* Invokes callback with result of masking operation.
* @param callback callback to be executed after masking
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)",313,315,"/**
* Registers a callback with a specified name and executes it synchronously.
* @param name unique identifier for the callback
* @param callback callback function to be executed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(),42,45,"/**
* Calls overloaded version of m1 with default parameter value.
* @param x default parameter value (always 1 in this case)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSentBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int),256,258,"/**
 * Applies mask to sent bytes based on given count.
 * @param count number of bytes to apply mask to
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrReceivedBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int),265,267,"/**
 * Sets the mask value based on the provided byte count.
 * @param count number of bytes to set in the mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)",47,54,"/**
* Constructs a MetricsRecordImpl instance with the given metrics info,
* tags, and metrics data.
* @param info MetricsInfo object
* @param timestamp Unix timestamp in milliseconds
* @param tags List of metric tags
* @param metrics Iterable of AbstractMetric objects
*/","* Construct a metrics record
   * @param info  {@link MetricsInfo} of the record
   * @param timestamp of the record
   * @param tags  of the record
   * @param metrics of the record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,toString,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString(),46,54,"/**
* Assembles a string representation of metrics with user data.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,hashCode,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode(),42,44,"/**
* Calculates the M4 value using provided results from methods m1(), m2(), and m3().
* @return M4 value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,equals,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object),29,39,"/**
* Compares two MetricsRecord objects using bitwise masks.
* @param obj the second MetricsRecord object to compare
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)",53,56,"/**
* Creates an MBean attribute info with read-write functionality.
* @param name attribute name
* @param desc attribute description
* @param type attribute type
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheTag,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)",279,282,"/**
* Updates attribute cache with metric data.
* @param tag MetricsTag object
* @param recNo record number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheMetric,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)",296,299,"/**
* Updates attribute cache with masked metric value.
* @param metric metric data
* @param recNo record number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,refreshQueueSizeGauge,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge(),168,170,"/**
* Masks queue elements using provided function.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/UniqueNames.java,uniqueName,org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String),47,65,"/**
* Generates a unique name by masking the original name with an incremental counter.
* @param name original name to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(),41,44,"/**
 * Calls m1(int) with default parameter value (1). 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",59,65,"/**
* Applies mask metrics to MetricsRecordBuilder.
* @param builder record builder instance
* @param all true for all records, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles),236,238,"/**
* Initializes a new RolloverSample instance with the specified parent quantile.
* @param parent MutableQuantiles object that serves as the parent quantile
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",129,146,"/**
* Updates metrics with quantiles and optional total value.
* @param builder MetricsRecordBuilder instance
* @param all true to include total, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addGetGroups,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long),155,162,"/**
* Updates quantile groups with latency value.
* @param latency time measurement to update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcEnQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long),277,284,"/**
* Updates queue time and quantile values.
* @param enQTime new enqueue timestamp
*/","* Sometimes, the request time observed by the client is much longer than
   * the queue + process time on the RPC server.Perhaps the RPC request
   * 'waiting enQueue' took too long on the RPC server, so we should add
   * enQueue time to RpcMetrics. See HADOOP-18840 for details.
   * Add an RPC enqueue time sample
   * @param enQTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long),290,297,"/**
* Updates queue time and quantiles with the given timestamp.
* @param qTime updated timestamp value
*/","* Add an RPC queue time sample
   * @param qTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcLockWaitTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long),299,306,"/**
* Updates RPC lock wait time and quantiles.
* @param waitTime new wait time value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long),312,319,"/**
* Updates processing time and quantiles in RPC context.
* @param processingTime new processing time value
*/","* Add an RPC processing time sample
   * @param processingTime the processing time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcResponseTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long),321,328,"/**
* Updates RPC response time and quantiles.
* @param responseTime new response time value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addDeferredRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long),330,337,"/**
* Updates processing time and quantiles for deferred RPCs.
* @param processingTime new processing time value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addWriteFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long),110,116,"/**
* Updates write latency quantile values.
* @param writeLatency new write latency value
*/","* Add the file write latency to {@link MutableQuantiles} metrics.
   *
   * @param writeLatency file write latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addReadFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long),123,129,"/**
* Updates quantile data with given read latency value.
* @param readLatency measured read latency in units
*/","* Add the file read latency to {@link MutableQuantiles} metrics.
   *
   * @param readLatency file read latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double),41,43,"/**
* Converts inverse percentile value to two thresholds.
* @param inversePercentile input inverse percentile (e.g. 95 for 95th percentile)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,initialize,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String),57,59,"/**
 * Returns a metrics system instance with specified prefix.
 * @param prefix unique identifier for metrics system
 */","* Convenience method to initialize the metrics system
   * @param prefix  for the metrics system configuration
   * @return the metrics system instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,instance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance(),68,70,"/**
* Returns metrics system instance with mask applied.
*/",* @return the metrics system object,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,shutdown,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown(),75,77,"/**
 * Invokes instance-specific method m1(). 
 */",* Shutdown the metrics system,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,setInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem),87,90,"/**
 * Returns the metrics system with specified mask applied.
 * @param ms MetricsSystem instance to modify
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName),113,116,"/**
 * Updates the internal mask reference with the value from the given ObjectName.
 * @param name ObjectName instance containing the new mask value",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeSourceName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String),118,121,"/**
 * Sets function mask based on provided name.
 * @param name unique function identifier
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTag,org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String),449,452,"/**
* Fetches metrics tag by name from registry.
* @param tagName unique identifier of the tag to fetch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,snapshot,"org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",465,472,"/**
* Populates the MetricsRecordBuilder with tags and metrics.
* @param builder record builder to populate
* @param all whether to include all metrics or only enabled ones
*/","* Sample all the mutable metrics and put the snapshot in the builder
   * @param builder to contain the metrics snapshot
   * @param all get all the metrics even if the values are not changed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,toString,org.apache.hadoop.metrics2.lib.MetricsRegistry:toString(),474,481,"/**
* Formats key-value pairs into a comma-separated string.
* @return formatted string or null if any value is missing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,getStats,org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long),292,314,"/**
* Calculates statistical summary for named metrics with at least 'minSamples' values within 'recordValidityMs' timeframe.
* @param minSamples minimum number of samples required
*/","* Retrieve a map of metric name {@literal ->} (aggregate).
   * Filter out entries that don't have at least minSamples.
   *
   * @param minSamples input minSamples.
   * @return a map of peer DataNode Id to the average latency to that
   *         node seen over the measurement period.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount(),396,398,"/**
* Returns the function mask value.
* @return calculated function mask value as a long integer
*/","* Returns the number of samples that we have seen so far.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount(),437,439,"/**
* Returns the function mask value from deferred RPC processing time. 
* @return The function mask as a long integer value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,rollOverAvgs,org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs(),247,274,"/**
* Updates average rates in current snapshot.
* @param none
* @return none
*/","* Iterates over snapshot to capture all Avg metrics into rolling structure
   * {@link MutableRollingAverages#averages}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
* Updates metrics record with mask data.
* @param builder MetricsRecordBuilder to update
* @param all true to include all mask data, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(),46,49,"/**
* Calls overloaded version of m1 with default parameter value.
* @param i default parameter value (always 1) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(),60,63,"/**
 * Calls itself recursively with an initial argument value of 1.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,info,"org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)",117,119,"/**
* Retrieves metrics info with specified name and description.
* @param name unique metric identifier
* @param description metric description
*/","* Get a metric info object.
   * @param name Name of metric info object
   * @param description Description of metric info object
   * @return an interned metric info object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",151,153,"/**
 * Retrieves a cached metrics tag using the provided info and value.
 * @param info MetricsInfo object
 * @param value string value for the tag
 */","* Get a metrics tag.
   * @param info  of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",52,58,"/**
* Applies mask functionality to metrics record.
* @param builder MetricsRecordBuilder instance for configuration
* @param all whether to apply mask to all records or not
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float),70,79,"/**
* Iteratively updates the value based on the given delta and 
* conditions the result with m3() and triggers m4() when ready.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,"org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)",123,126,"/**
* Updates interval statistics and calls secondary processing.
* @param numSamples number of samples
* @param sum accumulated sum
*/","* Add a number of samples and their sum to the running stat
   *
   * Note that although use of this method will preserve accurate mean values,
   * large values for numSamples may result in inaccurate variance values due
   * to the use of a single step of the Welford method for variance calculation.
   * @param numSamples  number of samples
   * @param sum of the samples",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,add,org.apache.hadoop.metrics2.util.SampleStat:add(double),68,71,"/**
* Computes sample statistic using min-max algorithm and then invokes M1 function with default parameters. 
* @param x input value
*/","* Add a sample the running stat.
   * @param x the sample number
   * @return  self",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean(),404,406,"/**
* Calculates and returns the functional mask value.
* @return m1.m2() result from rpcProcessingTime object
*/","* Returns mean of RPC Processing Times.
   * @return double",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean(),441,443,"/**
* Calculates and returns a masked value based on deferred RPC processing time.
* @return a double value representing the calculated result
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,min,org.apache.hadoop.metrics2.util.SampleStat:min(),134,136,"/**
* Calls minmax's m1() to perform some calculation.
* @return result of the calculation as a double value
*/",* @return  the minimum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,max,org.apache.hadoop.metrics2.util.SampleStat:max(),141,143,"/**
* Calls minmax's m1() to perform some operation.",* @return  the maximum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax),188,191,"/**
* Copies Min and Max values from another MinMax object.
* @param other MinMax object to copy from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,resetMinMax,org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax(),177,179,"/**
* Calls minMax to execute m1 function.",* Reset the all time min max of the metric,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat:reset(),40,45,"/**
* Resets sampling statistics and calls min-max calculation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
* Calls underlying implementation to process metrics record.
* @param builder MetricsRecordBuilder instance
* @param all whether to include all data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newCounter,org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class),72,87,"/**
* Creates a MutableMetric instance for the given type, if supported.
* @param type Class of the metric
* @return MutableMetric or throws exception if unsupported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
 * Delegates execution of metrics record builder to internal implementation. 
 * @param builder MetricsRecordBuilder instance
 * @param all Flag indicating whether to include all metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newGauge,org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class),106,123,"/**
* Returns a MutableMetric instance based on the provided class type.
* @param t Class to determine metric type
* @return MutableMetric or throws exception if unsupported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
* Applies mask metrics to record builder.
* @param builder MetricsRecordBuilder instance to modify
* @param all whether to apply mask for all records or only current
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getNextTgtRenewalTime,"org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)",1110,1117,"/**
* Calculates the function mask based on target end time and retry policy.
* @param tgtEndTime target end time
* @param now current timestamp
* @param rp retry policy configuration
* @return calculated function mask value
*/","* Get time for next login retry. This will allow the thread to retry with
   * exponential back-off, until tgt endtime.
   * Last retry is {@link #kerberosMinSecondsBeforeRelogin} before endtime.
   *
   * @param tgtEndTime EndTime of the tgt.
   * @param now Current time.
   * @param rp The retry policy.
   * @return Time for next login retry.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(),46,49,"/**
* Calls m1(int) with default argument 1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(),60,63,"/**
* Calls m1 with default value (1) to initialize the functionality.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,reattach,"org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)",125,127,"/**
* Updates metrics system with JVM-specific data.
* @param ms MetricsSystem instance to update
* @param jvmMetrics JVM-related metrics object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMemoryUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),157,168,"/**
* Calculates and sets memory metrics in the MetricsRecordBuilder.
* @param rb MetricsRecordBuilder to update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,equals,org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object),56,62,"/**
* Checks if a given object matches the current function mask.
* @param other object to check, expected to be a NameValuePair
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,stddev,org.apache.hadoop.metrics2.util.SampleStat:stddev(),127,129,"/**
* Calculates the result of function m1() squared.
* @return The square of the result from m1()
*/",* @return  the standard deviation of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,compress,org.apache.hadoop.metrics2.util.SampleQuantiles:compress(),176,198,"/**
* Merges adjacent samples based on a custom condition.
* @param samples collection of sample items
*/","* Try to remove extraneous items from the set of sampled items. This checks
   * if an item is unnecessary based on the desired error bounds, and merges it
   * with the adjacent item if it is.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,query,org.apache.hadoop.metrics2.util.SampleQuantiles:query(double),206,228,"/**
* Calculates the function mask value based on a given quantile.
* @param quantile target quantile value between 0 and 1
*/","* Get the estimated value at the specified quantile.
   * 
   * @param quantile Queried quantile, e.g. 0.50 or 0.99.
   * @return Estimated value at that quantile.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insertBatch,org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch(),129,169,"/**
* Updates sample data by processing the buffered values.
* @param samples sample collection
*/","* Merges items from buffer into the samples array in one pass.
   * This is more efficient than doing an insert on every item.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,offer,org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair),84,95,"/**
* Invokes M1 with the given entry's value and checks subsequent conditions.
* @param entry NameValuePair object to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,<init>,org.apache.hadoop.metrics2.util.MetricsCache:<init>(),136,138,"/**
* Initializes metrics cache with default maximum records per name. 
* @param maxRecsPerNameDefault initial value for MAX_RECS_PER_NAME constant
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,tag,"org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",66,69,"/**
* Builds metrics record builder with FUNC_MASK property.
* @param info Metrics information object
* @param value Mask value to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag),71,74,"/**
 * Creates a new MetricsRecordBuilder instance with a FUNC_MASK metric.
 * @param tag MetricsTag object containing M1 and M2 values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),76,79,"/**
* Constructs MetricsRecordBuilder from AbstractMetric.
* @param metric AbstractMetric instance to process
* @return MetricsRecordBuilder instance or null if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,setContext,org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String),81,84,"/**
 * Constructs metrics record builder with FUNC_MASK attribute.
 * @param value function mask value to be used in metric context
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",86,89,"/**
* Creates a MetricsRecordBuilder instance with a FUNC_MASK operation.
* @param info MetricsInfo object containing metadata
* @param value integer value for the mask operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",91,94,"/**
* Creates a MetricsRecordBuilder instance with FUNC_MASK metadata.
* @param info MetricsInfo object containing metadata
* @param value long value to be associated with the builder
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",96,99,"/**
* Calculates and returns a MetricsRecordBuilder instance with mask value applied.
* @param info MetricsInfo object containing necessary data
* @param value integer value for mask calculation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",101,104,"/**
* Returns a MetricsRecordBuilder instance with FUNC_MASK applied.
* @param info MetricsInfo object containing relevant data
* @param value the value to apply the mask to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",106,109,"/**
* Computes and returns a MetricsRecordBuilder instance with FUNC_MASK applied to the given MetricsInfo and value.
* @param info MetricsInfo object containing relevant data
* @param value floating-point value used in computation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",111,114,"/**
* Computes metrics record using FUNC_MASK function and provided values.
* @param info MetricsInfo object containing function metadata
* @param value input value for computation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",68,72,"/**
* Invokes RPC method m1 with ZKFCProtocolPB data. 
* @param protocol communication protocol
* @param clientVersion client software version
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",180,184,"/**
* Invokes RPC method m1 with given protocol and client version.
* @param protocol HA service protocol class name
* @param clientVersion client software version
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,getNetgroupNames,org.apache.hadoop.security.NetgroupCache:getNetgroupNames(),63,65,"/**
 * Returns a list of mask values.
 */","* Get the list of cached netgroups
   *
   * @return list of cached groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,isCached,org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String),81,83,"/**
* Checks if a group name matches a specific criteria.
* @param group the group to check
*/","* Returns true if a given netgroup is cached
   *
   * @param group check if this group is cached
   * @return true if group is cached, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getDefaults,org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults(),2097,2103,"/**
* Initializes LoginParams with default values for Kerberos functionality.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,toLowerCase,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest),132,196,"/**
* Wraps the original HttpServletRequest, filtering out parameters based on m3().
* @param request the original HTTP request
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getServerProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)",103,106,"/**
* Returns a map of key-value pairs from InetAddress.
* @param clientAddress The InetAddress to extract data from
*/","* Identify the Sasl Properties to be used for a connection with a  client.
   * @param clientAddress  client's address
   * @param ingressPort the port that the client is connecting
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getClientProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)",123,126,"/**
* Returns a map of server properties based on the provided address.
* @param serverAddress IP address of the server
*/","* Identify the Sasl Properties to be used for a connection with a server.
   * @param serverAddress server's address
   * @param ingressPort the port that is used to connect to server
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getPassword,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier),289,292,"/**
* Generates a mask from the specified token identifier.
* @param tokenid unique token identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String),65,68,"/**
* Delegates to implementation instance for fetching set of strings.
* @param user input string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String),64,67,"/**
* Calls implementation-specific method to fetch unique strings.
* @param user input parameter (purpose unclear)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String),110,131,"/**
* Fetches user groups by ID, aggregating results from multiple providers.
* @param user unique user identifier
* @return Set of String representing the user's groups
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getEnabledConfigKey,org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey(),71,73,"/**
* Returns a string representing the function mask with cross-origin filter enabled suffix.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentialsInternal,org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal(),1759,1770,"/**
* Retrieves and initializes a Credentials object based on the subject's attributes.
* @return Credentials object or newly created instance if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,"org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)",46,57,"/**
* Constructs a User object with authentication details.
* @param name user full name
* @param authMethod authentication method
* @param login login context
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getHostFromPrincipal,org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String),353,355,"/**
* Generates a functional mask from a Kerberos principal name.
* @param principalName Kerberos principal to process
*/","* Get the host name from the principal name of format {@literal <}service
   * {@literal >}/host@realm.
   * @param principalName principal name of format as described above
   * @return host name if the the string conforms to the above format, else null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupInternal,org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String),242,264,"/**
* Retrieves user-specific masks from various data sources and caches.
* @param user unique user identifier
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * @param user User's name
   * @return the group memberships of the user as Set
   * @throws IOException if user does not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,refresh,org.apache.hadoop.security.Groups:refresh(),430,441,"/**
* Clears userToGroupsMap cache and refreshes groups.
* @throws IOException on cache refresh failure
*/",* Refresh all user-to-groups mappings.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String),79,82,"/**
* Computes and returns a list of masks for the given user.
* @param user user identifier (type not specified, assuming it's a string)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String),84,90,"/**
* Extracts and returns a set of unique functional masks from the given user.
* @param user input string from which to extract functional masks
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,close,org.apache.hadoop.security.KDiag:close(),189,195,"/**
* Calls m1(), then invokes m2() on output stream if it's not null.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,"org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])",854,863,"/**
* Prints formatted message to output stream or console.
* @param format message format string
* @param args message arguments
*/","* Print a line of output. This goes to any output file, or
   * is logged at info. The output is flushed before and after, to
   * try and stay in sync with JRE logging.
   *
   * @param format format string
   * @param args any arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,usage,org.apache.hadoop.security.KDiag:usage(),246,263,"/**
* Generates a diagnostic function mask string.
* @return String containing diagnostic function flags and descriptions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapInternal,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)",224,281,"/**
* Updates a BiMap with external data from a bash command.
* @param map the map to update
* @param mapName name of the map (for logging)
* @param command bash command to execute
* @param regex regular expression to parse output lines
* @param staticMapping mapping of id to key in the map
* @return true if any entries were updated, false otherwise
*/","* Get the list of users or groups returned by the specified command,
   * and save them in the corresponding map.
   *
   * @param map map.
   * @param mapName mapName.
   * @param command command.
   * @param staticMapping staticMapping.
   * @param regex regex.
   * @throws IOException raised on errors performing I/O.
   * @return updateMapInternal.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getRunScriptCommand,org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File),443,448,"/**
* Builds command strings for Windows and Unix-like systems.
* @param script File object containing script path
*/","* Returns a command to run the given script.  The script interpreter is
   * inferred by platform: cmd on Windows or bash otherwise.
   *
   * @param script File script to run
   * @return String[] command to run the script",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,valueOf,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1504,1512,"/**
* Retrieves and returns the authentication method with a specific mask value.
* @param authMethod AuthenticationMethod to search for
* @return matching AuthenticationMethod or throws exception if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,switchBindUser,org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException),644,651,"/**
* Switches bind user on AuthenticationException.
* @param e authentication exception
*/","* Switch to the next available user to bind to.
   * @param e AuthenticationException encountered when contacting LDAP",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,"org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)",166,193,"/**
* Processes incoming buffer using SASL authentication.
* @param inBuf input data
* @param off offset from input buffer start
* @param len length of data to process
*/","* Writes <code>len</code> bytes from the specified byte array starting at
   * offset <code>off</code> to this output stream.
   * 
   * @param inBuf
   *          the data.
   * @param off
   *          the start offset in the data.
   * @param len
   *          the number of bytes to write.
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,close,org.apache.hadoop.security.SaslOutputStream:close(),213,217,"/**
* Calls m1 and delegates to outStream's m2 method.
*/","* Closes this output stream and releases any system resources associated with
   * this stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,init,org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration),175,182,"/**
* Initializes SASL factory based on configuration.
* @param conf configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,createSaslServer,"org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",48,53,"/**
* Returns a SASL PLAIN server instance if the specified mechanism matches.
* @param mechanism protocol mechanism to check
* @return SaslPlainServer instance or null if mismatched
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getAuthorizationID,org.apache.hadoop.security.SaslPlainServer:getAuthorizationID(),127,131,"/**
* Returns authorization mask.
* @return String representing authorization rights
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getNegotiatedProperty,org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String),133,137,"/**
* Returns authentication mask (""auth"") if property name matches specific criteria.
* @param propName property name to evaluate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,wrap,"org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)",139,145,"/**
* Throws exception as PLAIN authentication does not support integrity or privacy. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,unwrap,"org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)",147,153,"/**
* Throws exception as PLAIN authentication does not support integrity or privacy.
*@throws SaslException if PLAIN authentication is attempted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyStore,"org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)",1102,1109,"/**
* Loads and initializes a KeyStore instance from the specified file location.
* @param location path to the KeyStore file
* @param password password for decrypting the KeyStore
* @return initialized KeyStore object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,init,org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig),71,93,"/**
* Configures filter with custom parameters and logs CSRF protection setup.
* @param filterConfig Filter configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,handleHttpInteraction,org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction),193,203,"/**
* Handles HTTP interaction to mask function based on user agent and other conditions.
* @param httpInteraction object containing HTTP interaction details
*/","* Handles an {@link HttpInteraction} by applying the filtering logic.
   *
   * @param httpInteraction caller's HTTP interaction
   * @throws IOException if there is an I/O error
   * @throws ServletException if the implementation relies on the servlet API
   *     and a servlet API call has failed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedMethods,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig),165,174,"/**
* Configures filter methods from configuration.
* @param filterConfig FilterConfig instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doCrossFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",107,153,"/**
* Filters cross-origin requests and populates HttpServletResponse.
*@param req HttpServletRequest object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedHeaders,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig),176,185,"/**
* Configures and logs allowed headers filter.
* @param filterConfig Filter configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,parsePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)",242,269,"/**
* Extracts and validates unique group names and IDs.
* @param groupNames comma-separated list of group names
* @param groupIDs comma-separated list of corresponding group IDs
* @return Set of valid group names or empty set if invalid
*/","* Attempt to parse group names given that some names are not resolvable.
   * Use the group id list to identify those that are not resolved.
   *
   * @param groupNames a string representing a list of group names
   * @param groupIDs a string representing a list of group ids
   * @return a linked list of group names
   * @throws PartialGroupNameException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,newLoginContext,"org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",503,518,"/**
* Creates a Hadoop Login Context instance.
* @param appName application name
* @param subject user subject
* @param loginConf login configuration
* @return HadoopLoginContext object or throws LoginException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,login,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login(),2142,2155,"/**
* Updates login metrics and notifies of successful or failed login.
* @throws LoginException if login fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logout,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout(),2157,2164,"/**
* Executes login-related operation in a thread-safe manner.
* @throws LoginException if login fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createSecretKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[]),692,694,"/**
* Generates a secret key using the M1 algorithm.
* @param key input byte array to process
*/","* Convert the byte[] to a secret key
   * @param key the byte[] to create the secret key from
   * @return the secret key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,formatTokenId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),83,90,"/**
* Formats a TokenIdent object as a string mask.
* @param id TokenIdent object to be formatted
* @return formatted string or error message on exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),204,211,"/**
* Removes a token from the SQL secret manager.
* @param ident TokenIdent object containing token details
*/","* Removes the existing TokenInformation from the SQL database to
   * invalidate it.
   * @param ident TokenInformation to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Daemon.java,newThread,org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable),42,45,"/**
* Creates a daemon thread from the given Runnable.
* @param runnable task to execute in the daemon thread
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStart,org.apache.hadoop.util.JvmPauseMonitor:serviceStart(),81,86,"/**
* Initializes and starts monitoring thread.
* Calls superclass implementation of m2(). 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,reset,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset(),182,187,"/**
* Performs synchronization and initialization tasks.
* @param none
*/",* Reset all data structures and mutable state.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),350,352,"/**
* Updates mask in allKeys with the given delegation key.
* @param key DelegationKey object to update mask
*/","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),370,377,"/**
* Removes delegation key from SQL secret manager.
* @param key DelegationKey object containing ID to be removed
*/","* Removes the existing DelegationKey from the SQL database to
   * invalidate it.
   * @param key DelegationKey to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey),213,220,"/**
* Adds a delegation key, synchronizing access and validating existing keys.
* @param key DelegationKey object to add
* @throws IOException if SecretManager is running or invalid key provided
*/","* Add a previously used master key to cache (when NN restarts), 
   * should be called before activate().
   *
   * @param key delegation key.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),338,341,"/**
* Updates the mask based on the provided delegation key.
* @param key DelegationKey object containing relevant data
*/","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])",707,709,"/**
* Constructs a DelegationTokenInformation object with the given parameters.
* @param renewDate token renewal date
* @param password token password
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredKeys,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys(),478,491,"/**
* Updates current delegation key based on expiration times.
* @param allKeys iterator over expired and current keys
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenTrackingId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),561,567,"/**
* Returns token mask string from TokenIdent.
* @param identifier unique token identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),795,797,"/**
* Applies mask to token.
* @param ident Token identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),136,141,"/**
* Updates the AbstractDelegationTokenSecretManager instance and disables token management. 
* @param secretManager new secret manager to use
* @param managedSecretManager flag indicating whether token management is enabled (false)","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy(),154,158,"/**
 * Applies mask to managed secrets using Secret Manager.
 * @param none 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,stopThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads(),437,475,"/**
* Stops and cleans up various token caches and counters.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationTokenLoadingCache.java,isEmpty,org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty(),57,60,"/**
* Checks if mask is enabled.
* @return true if disabled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,"org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)",51,53,"/**
* Creates a new DelegationKey instance with specified parameters.
* @param keyId unique identifier for the delegation key
* @param expiryDate timestamp when the key expires
* @param key encoded secret key or null if not provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getConfiguration,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)",114,120,"/**
* Initializes and populates properties based on configuration prefix.
* @param configPrefix configuration prefix to use
* @param filterConfig web application filter configuration
* @return populated Properties object or null if failed
*/","* It delegates to
   * {@link AuthenticationFilter#getConfiguration(String, FilterConfig)} and
   * then overrides the {@link AuthenticationHandler} to use if authentication
   * type is set to <code>simple</code> or <code>kerberos</code> in order to use
   * the corresponding implementation with delegation token support.
   *
   * @param configPrefix parameter not used.
   * @param filterConfig parameter not used.
   * @return hadoop-auth de-prefixed configuration for the filter and handler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,initializeAuthHandler,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)",204,215,"/**
* Initializes ZKDelegationTokenSecretManager and calls superclass method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.java,get,org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get(),36,39,"/**
* Returns the function mask from the DelegationTokenAuthenticationFilter.","* Returns the remote {@link UserGroupInformation} in context for the current
   * HTTP request, taking into account proxy user requests.
   *
   * @return the remote {@link UserGroupInformation}, <code>NULL</code> if none.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>(),89,92,"/**
* Initializes authentication handler with multi-scheme type postfix. 
* @param none */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>(),49,52,"/**
* Initializes Kerberos delegated token authentication handler.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>(),50,53,"/**
* Initializes PseudoDelegationTokenAuthenticationHandler with type postfix.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,obtainDelegationTokenAuthenticator,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",128,140,"/**
* Creates or retrieves a delegationToken authenticator.
* @param dta existing authenticator, or null to create a default one
* @param connConfigurator configuration for the authenticator
* @return DelegationTokenAuthenticator instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>(),41,52,"/**
* Initializes PseudoDelegationTokenAuthenticator with current user credentials.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>(),38,45,"/**
* Initializes a Kerberos delegationToken authenticator with a fallback to pseudo token authenticator.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,isManagementOperation,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest),211,218,"/**
* Checks if request is a function mask operation.
* @param request HttpServletRequest object
*/","* This method checks if the given HTTP request corresponds to a management
   * operation.
   *
   * @param request The HTTP request
   * @return true if the given HTTP request corresponds to a management
   *         operation false otherwise
   * @throws IOException In case of I/O error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,getDelegationToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest),412,421,"/**
* Retrieves delegation token from request.
* @param request HttpServletRequest object
* @return Delegation token as a string or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,hasDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",115,132,"/**
* Checks if the provided URL contains a delegation token.
* @param url URL object to inspect
* @param token AuthenticatedURL.Token object containing authentication details
* @return true if delegation token is present, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>(),167,169,"/**
* Initializes TokenSelector with token kind.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementDelegationTokenSeqNum,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum(),504,531,"/**
* Fetches a unique sequence number, potentially fetching a batch if the current one is exhausted.
* @return next available sequence number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementCurrentKeyId,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId(),547,559,"/**
* Increments and returns the current shared key ID sequence counter value.
* @return Current key ID sequence counter value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),727,754,"/**
* Removes ZKDTSMDelegationKey by ID.
* @param key unique DelegationKey identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,equals,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object),166,182,"/**
* Checks if an object is a delegationTokenIdentifier with matching properties.
* @param obj Object to compare
* @return true if identical, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isManaged,org.apache.hadoop.security.token.Token:isManaged(),487,489,"/**
* Calls m1() to perform some operation and then invokes m2() on its result.","* Is this token managed so that it can be renewed or cancelled?
   * @return true, if it can be renewed and cancelled.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,renew,org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration),498,501,"/**
* Invokes M1 to perform task 2 with provided configuration.
* @param conf application configuration
*/","* Renew this delegation token.
   * @param conf configuration.
   * @return the new expiration time
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,cancel,org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration),510,513,"/**
* Invokes m2 on a configured instance.
* @param conf Configuration object
*/","* Cancel this delegation token.
   *
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,validate,org.apache.hadoop.security.token.DtUtilShell$Get:validate(),225,239,"/**
* Validates the presence and correctness of the service URL.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,getCommandUsage,org.apache.hadoop.security.token.DtUtilShell:getCommandUsage(),179,187,"/**
* Returns a formatted string mask for functional operations.
* @return String containing operation names and usage information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,"org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)",580,593,"/**
* Reads data from RPC buffer into provided byte array.
* @param buf target byte array
* @param off offset within the array to start writing
* @param len number of bytes to read
* @return actual number of bytes read or 0 if none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,isValidAuthType,org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),189,199,"/**
* Checks if a SaslAuth method matches the required authentication criteria.
* @param authType SaslAuth object to evaluate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getInputStream,org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream),533,538,"/**
* Wraps input stream with masking functionality if condition m1() is true.
* @param in input stream to be potentially wrapped
* @return InputStream object, possibly masked or original
*/","* Get SASL wrapped InputStream if SASL QoP requires unwrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param in - InputStream used to make the connection
   * @return InputStream that may be using SASL unwrap
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getOutputStream,org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream),549,558,"/**
* Configures output stream with masking functionality.
* @param out OutputStream to be configured
* @return Configured output stream or original if no action required
*/","* Get SASL wrapped OutputStream if SASL QoP requires wrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param out - OutputStream used to make the connection
   * @return OutputStream that may be using wrapping
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,disposeSasl,org.apache.hadoop.ipc.Client$Connection:disposeSasl(),546,554,"/**
* Calls M1 on SaslRpcClient and then sets it to null.
* @throws IOException ignored, but may occur if an I/O error occurs during the call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordWarning,"org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)",244,247,"/**
* Generates password mask based on environment and file keys.
* @param envKey environment key
* @param fileKey file key
* @return password mask string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordError,"org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)",249,251,"/**
* Generates error message with masked password.
* @param envKey environment key
* @param fileKey file key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,readMoreData,org.apache.hadoop.security.SaslInputStream:readMoreData(),95,125,"/**
* Retrieves the SASL response length and buffer.
*@return SASL response length or -1 on EOF
*/","* Read more data and get them processed <br>
   * Entry condition: ostart = ofinish <br>
   * Exit condition: ostart <= ofinish <br>
   * 
   * return (ofinish-ostart) (we have this many bytes for you), 0 (no data now,
   * but could have more later), or -1 (absolutely no more data)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,close,org.apache.hadoop.security.SaslInputStream:close(),341,348,"/**
* Closes input stream and resets flags.
*/","* Closes this input stream and releases any system resources associated with
   * the stream.
   * <p>
   * The <code>close</code> method of <code>SASLInputStream</code> calls the
   * <code>close</code> method of its underlying input stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(),37,39,"/**
* Constructs an empty AuthorizationException instance.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable),54,56,"/**
* Constructs an AuthorizationException with the given Throwable as its cause.
* @param cause underlying exception causing authentication failure
*/","* Constructs a new exception with the specified cause and a detail
   * message of <tt>(cause==null ? null : cause.toString())</tt> (which
   * typically contains the class and detail message of <tt>cause</tt>).
   * @param  cause the cause (which is saved for later retrieval by the
   *         {@link #getCause()} method).  (A <tt>null</tt> value is
   *         permitted, and indicates that the cause is nonexistent or
   *         unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token),664,667,"/**
* Initializes SASL client callback handler with authentication token.
* @param token authentication token containing user credentials
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reset,org.apache.hadoop.security.UserGroupInformation:reset(),368,379,"/**
* Resets and initializes static authentication configuration.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLogin,org.apache.hadoop.security.UserGroupInformation:getLogin(),522,526,"/**
* Retrieves an instance of HadoopLoginContext from the user object, or returns null if not found.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginSuccess,org.apache.hadoop.security.UserGroupInformation:isLoginSuccess(),537,542,"/**
* Checks if user has Hadoop login context and calls its m2() method.
* @return true if user has valid Hadoop login, false otherwise
*/","This method checks for a successful Kerberos login
    * and returns true by default if it is not using Kerberos.
    *
    * @return true on successful login",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLogin,org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext),528,530,"/**
* Calls user-specific method m1 with provided LoginContext.
* @param login context of current login session
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLastLogin,org.apache.hadoop.security.UserGroupInformation:setLastLogin(long),548,550,"/**
 * Calls user-specific method m1 with provided login time.
 */","* Set the last login time for logged in user
   * @param loginTime the number of milliseconds since the beginning of time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject),559,568,"/**
* Initializes UserGroupInformation with a Subject, extracting the User principal.
* @param subject Subject containing user authentication information
*/","* Create a UserGroupInformation for the given subject.
   * This does not change the subject or acquire new credentials.
   *
   * The creator of subject is responsible for renewing credentials.
   * @param subject the user's subject",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUserName,org.apache.hadoop.security.UserGroupInformation:getUserName(),1667,1671,"/**
* Returns a function mask string based on user configuration.
* @return Function mask or empty string if not configured.","* Get the user's full principal name.
   * @return the user's full principal name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasKerberosCredentials,org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials(),574,576,"/**
* Checks if authentication method is KERBEROS.
* @return true if KERBEROS, false otherwise
*/","* checks if logged in using kerberos
   * @return true if the subject logged via keytab or has a Kerberos TGT",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod(),1853,1855,"/**
* Delegates authentication method call to user instance.
* @return result of user's m1() call
*/","* Get the authentication method from the subject
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,fixKerberosTicketOrder,org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder(),1204,1233,"/**
* Removes and destroys expired or invalid Kerberos tickets from the cache.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasSufficientTimeElapsed,org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long),1401,1410,"/**
* Determines if login should be attempted based on time elapsed since last re-login.
* @param now current timestamp
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUser,org.apache.hadoop.security.UserGroupInformation:getRealUser(),1543,1550,"/**
* Retrieves user information from the subject, iterating over real users.
* @return first available User object or null if none found
*/","* get RealUser (vs. EffectiveUser)
   * @return realUser running over proxy user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getShortUserName,org.apache.hadoop.security.UserGroupInformation:getShortUserName(),1651,1653,"/**
* Returns the user's mask value.","* Get the user's login name.
   * @return the user's name up to the first '/' or '@'.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),1834,1837,"/**
* Calls user-specific authentication method.
* @param authMethod authentication method to invoke
*/","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)",349,360,"/**
* Performs certificate validation with custom host and subject alternative name checks.
* @param host array of allowed hosts
* @param cert X509Certificate to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)",362,456,"/**
* Validates hostname against certificates' subjectAlt and CN fields.
* @param hosts array of hostnames to validate
* @param cns array of Common Names (CNs) from certificates
* @param subjectAlts array of Subject Alternates (subjectAlts) from certificates
* @param ie6 whether to consider IE6-specific CN handling
* @param strictWithSubDomains whether to enforce strict matching with subdomains
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)",72,78,"/**
* Initializes the ReloadingX509TrustManager instance with a custom trust manager from a file.
* @param type type of trust manager
* @param location path to the file containing the trust manager
* @param password password for decrypting the trust manager (if applicable)
*/","* Creates a reloadable trustmanager. The trustmanager reloads itself
   * if the underlying trustore file has changed.
   *
   * @param type type of truststore file, typically 'jks'.
   * @param location local path to the truststore file.
   * @param password password of the truststore file.
   * changed, in milliseconds.
   * @throws IOException thrown if the truststore could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the truststore could not be
   * initialized due to a security error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path),115,123,"/**
* Reloads X509 trust manager from file at specified Path.
* @param path location of trust manager configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",69,77,"/**
* Initializes ReloadingX509KeystoreManager with keystore details.
* @param type        Keystore type
* @param location    Keystore file path
* @param storePassword Password for keystore access
* @param keyPassword  Password for private keys access
*/","* Construct a <code>Reloading509KeystoreManager</code>
   *
   * @param type type of keystore file, typically 'jks'.
   * @param location local path to the keystore file.
   * @param storePassword password of the keystore file.
   * @param keyPassword The password of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws GeneralSecurityException thrown if create encryptor error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path),123,131,"/**
* Initializes ReloadingX509KeystoreManager with a keystore at the specified path.
* @param path file system path to the keystore
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getResource,org.apache.hadoop.util.FindClass:getResource(java.lang.String),163,165,"/**
* Calls m2() on the result of m1(), passing the given name. 
* @param name input string to be processed by m2()
*/","* Get the resource
   * @param name resource name
   * @return URL or null for not found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsInputStream,org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String),2893,2908,"/**
* Retrieves the InputStream for a resource by name.
* @param name unique identifier of the resource
* @return InputStream object or null on failure/error
*/","* Get an input stream attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return an input stream attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsReader,org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String),2917,2932,"/**
* Retrieves a Reader object for the specified resource.
* @param name resource name
* @return InputStreamReader instance or null on error/failure
*/","* Get a {@link Reader} attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return a reader attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,createSSLEngine,org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine(),256,268,"/**
* Initializes and configures an SSLEngine based on the current mode.
* @throws GeneralSecurityException if SSL engine creation fails
* @throws IOException if I/O error occurs during configuration
*/","* Returns a configured SSLEngine.
   *
   * @return the configured SSLEngine.
   * @throws GeneralSecurityException thrown if the SSL engine could not
   * be initialized.
   * @throws IOException thrown if and IO error occurred while loading
   * the server keystore.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,configure,org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection),358,372,"/**
* Configures and returns an HTTP connection with masking enabled.
* @param conn the underlying HttpURLConnection to modify
* @return the modified HttpsURLConnection instance
*/","* If the given {@link HttpURLConnection} is an {@link HttpsURLConnection}
   * configures the connection with the {@link SSLSocketFactory} and
   * {@link HostnameVerifier} of this SSLFactory, otherwise does nothing.
   *
   * @param conn the {@link HttpURLConnection} instance to configure.
   * @return the configured {@link HttpURLConnection} instance.
   *
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,configureConnection,org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection),488,500,"/**
* Configures and returns the given HTTP connection with SSL settings.
* @param conn the HTTP connection to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeSSLContext,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),153,185,"/**
* Initializes the SSL context to a specified channel mode.
* @param preferredChannelMode desired SSL channel configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(),236,239,"/**
 * Creates an SSL socket using the provided factory and returns it.
 * @throws IOException if socket creation fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)",241,248,"/**
* Creates an SSL socket from the given socket and connection parameters.
* @param s existing socket
* @param host server hostname or IP address
* @param port server port number
* @param autoClose whether to close the underlying socket on creation
* @return newly created SSLSocket object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",250,257,"/**
* Establishes an SSL socket connection using the provided parameters.
* @param address server's IP address
* @param port server's port number
* @param localAddress client's local IP address
* @param localPort client's local port number
* @return established SSLSocket object or throws IOException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",259,266,"/**
* Establishes a secure socket connection using the provided parameters.
* @param host remote host address
* @param port remote port number
* @param localHost local host address
* @param localPort local port number
* @return established SSLSocket object or throws IOException if failed",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)",268,273,"/**
* Establishes an SSL socket connection to the specified host and port.
* @param host target server address
* @param port target server port number
* @return established Socket object or throws IOException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)",275,280,"/**
* Establishes an SSL/TLS connection to the specified host and port.
* @param host hostname or IP address of the server
* @param port TCP port number to connect to
* @return established SSLSocket object or throws IOException if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration),39,41,"/**
* Initializes the object with the given Hadoop Configuration.
* @param conf Hadoop configuration to be used for this instance
*/","Construct a Configured.
   * @param conf the Configuration object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,handleExecutorTimeout,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)",174,192,"/**
* Returns true if shell group lookup command timed out for the given user.
* @param executor ShellCommandExecutor instance
* @param user unique user identifier
*/","* Check if the executor had a timeout and logs the event.
   * @param executor to check
   * @param user user to log
   * @return true if timeout has occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,toString,org.apache.hadoop.util.Shell$ShellCommandExecutor:toString(),1312,1325,"/**
* Builds a string by quoting and concatenating key-value pairs.
*@return formatted string
*/","* Returns the commands of this instance.
     * Arguments with spaces in are presented with quotes round; other
     * arguments are presented raw
     *
     * @return a string representation of the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,read,org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput),262,264,"/**
 * Retrieves and processes AuthMethod from input stream.
 * @param in DataInput containing authentication data
 */","* Read from in.
     *
     * @param in DataInput.
     * @throws IOException raised on errors performing I/O.
     * @return AuthMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByExactName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String),687,704,"/**
* Resolves host name to InetAddress with optional FQDN fallback.
* @param host hostname or IP address
* @return InetAddress object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry(),2232,2285,"/**
* Configures KRB5 login module with user-specific options.
* @param params Login parameters (principal, keytab, ccache)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)",572,576,"/**
* Initializes static mappings for user and group IDs.
* @param uidMapping mapping of user IDs to internal values
* @param gidMapping mapping of group IDs to internal values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(),545,547,"/**
 * Initializes an empty PassThroughMap instance.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addUser,org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String),152,159,"/**
* Validates and adds user to the system.
* @param user unique user identifier
*/","* Add user to the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addGroup,org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String),167,177,"/**
* Validates and adds a group to the system.
* @param group name of the group to add
*/","* Add group to the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeUser,org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String),185,192,"/**
* Restrict user from being masked by validating and updating user status.
* @param user unique user identifier
*/","* Remove user from the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeGroup,org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String),200,208,"/**
* Removes or updates group mask based on conditions.
* @param group name of the group to process
*/","* Remove group from the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getUsersString,org.apache.hadoop.security.authorize.AccessControlList:getUsersString(),337,339,"/**
* Generates mask string using users data.
* @return formatted mask string
*/","* Returns comma-separated concatenated single String of the set 'users'
   *
   * @return comma separated list of users",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getGroupsString,org.apache.hadoop.security.authorize.AccessControlList:getGroupsString(),346,348,"/**
* Generates function mask using the provided input.
* @param groups input data to process
*/","* Returns comma-separated concatenated single String of the set 'groups'
   *
   * @return comma separated list of groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,printStackTrace,org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(),65,68,"/**
* Recursively logs error messages to System.err.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyGroups,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups(),175,182,"/**
* Returns a map of proxy groups to their respective access control lists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyHosts,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts(),184,193,"/**
* Builds a map of unique proxy hosts with their respective machine lists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,innerSetCredential,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])",266,281,"/**
* Stores credential entry with encrypted material.
* @param alias unique credential identifier
* @param material encrypted credential data
* @return CredentialEntry object representing stored credential
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate(),321,346,"/**
* Validates the alias and fetches a provider if -help is not requested.
* @param strict whether to enforce strict mode or not
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate(),411,436,"/**
* Validates and configures the alias.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute(),348,386,"/**
* Verifies credential alias password match.
*@throws IOException if console unavailable or password verification fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,promptForCredential,org.apache.hadoop.security.alias.CredentialShell:promptForCredential(),473,499,"/**
* Prompts user to enter and confirm password alias.
* @return confirmed password alias or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,warnIfTransientProvider,org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider(),172,176,"/**
* Emits a warning when modifying a transient provider.
* @throws Exception if m1() returns true and m2().m3() fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String),80,90,"/**
* Validates and sets file system permissions mask.
* @param perms string representation of permission mode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,isOriginalTGT,org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket),168,170,"/**
* Checks if Kerberos ticket is valid by calling M2 function on its token.
* @param ticket KerberosTicket object to verify
*/","* Check whether the server principal is the TGS's principal
   * @param ticket the original TGT (the ticket that is obtained when a 
   * kinit is done)
   * @return true or false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)",825,850,"/**
* Configures a ZKClient with SSL/TLS encryption using the provided truststore and keystore.
* @param zkClientConfig configuration for the ZooKeeper client
* @param truststoreKeystore truststore and keystore details
* @param x509Util utility for X.509 certificate handling
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KerberosAuthException.java,<init>,"org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)",51,54,"/**
 * Constructs a KerberosAuthException with an initial error message and underlying cause.
 * @param initialMsg initial error message
 * @param cause underlying exception that caused this auth exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress),116,122,"/**
* Returns a map of function masks based on client IP address.
* @param clientAddress the client's network address
*/","* Identify the Sasl Properties to be used for a connection with a client.
   * @param clientAddress client's address
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getIdentifier,"org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)",192,204,"/**
* Fetches and deserializes a token identifier using the provided secret manager.
* @param id unique identifier for fetching the token
* @param secretManager SecretManager instance to retrieve the token identifier
* @return T TokenIdentifier object or null if not found/invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkCodec,org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec),75,81,"/**
* Validates the provided CryptoCodec instance for AES/CTR/NoPadding or SM4/CTR/NoPadding. 
* @param codec CryptoCodec instance to validate
*/","* AES/CTR/NoPadding or SM4/CTR/NoPadding is required.
   *
   * @param codec crypto codec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>(),35,40,"/**
* Initializes OpenSSL AES CTR codec, throwing an exception on load failure. 
* @throws RuntimeException if loading fails with a reason message */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPos,org.apache.hadoop.crypto.CryptoInputStream:getPos(),580,585,"/**
* Calculates function mask by offset and buffer value.
* @return calculated mask value
*/",Get underlying stream position.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,available,org.apache.hadoop.crypto.CryptoInputStream:available(),672,677,"/**
* Calculates total value by combining values from input and output buffers.
* @return Total value as integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFromUnderlyingStream,org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer),223,231,"/**
* Reads and processes input data from ByteBuffer.
* @param inBuffer source buffer
* @return number of bytes processed or -1 on error
*/",Read data from underlying stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",115,126,"/**
* Initializes JceCtrCipher with specified mode, provider and suite.
* @param mode encryption/decryption mode
* @param provider name of the cryptographic provider (can be null for default)
* @param suite security protocol to use
* @param name unique identifier for cipher instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,convert,org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String),84,92,"/**
* Retrieves a CipherSuite instance by its human-readable name.
* @param name human-readable name of the cipher suite
*/","* Convert to CipherSuite from name, {@link #algoBlockSize} is fixed for
   * certain cipher suite, just need to compare the name.
   * @param name cipher suite name
   * @return CipherSuite cipher suite",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",140,143,"/**
* Masks input data and writes result to output buffer.
* @param inBuffer input data as ByteBuffer
* @param outBuffer output buffer as ByteBuffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",145,148,"/**
 * Applies mask function to input and output buffers.
 * @param inBuffer input data buffer
 * @param outBuffer output data buffer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoProtocolVersion.java,supports,org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion),54,64,"/**
* Checks if a given crypto protocol version matches any known mask.
* @param version the crypto protocol version to check
* @return true if matching, false otherwise
*/","* Returns if a given protocol version is supported.
   *
   * @param version version number
   * @return true if the version is supported, else false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherOption.java,<init>,org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite),34,36,"/**
* Initializes CipherOption with specified cipher suite.
* @param suite required CipherSuite instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,tokenizeTransformation,org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String),155,179,"/**
* Parses and validates a transformation string into a Transform object.
* @param transformation string in the format 'algorithm/id/key' (e.g. 'SHA-256/1234567890abcdef/secretKey')
* @throws NoSuchAlgorithmException if input is invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,finalize,org.apache.hadoop.crypto.OpensslCipher:finalize(),293,296,"/**
 * Executes function mask operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OpensslSecureRandom.java,next,org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int),105,118,"/**
* Generates a mask from the specified number of bits.
* @param numBits number of bits to include in the mask
* @return integer representing the generated mask
*/","* Generates an integer containing the user-specified number of
   * random bits (right justified, with leading zeros).
   *
   * @param numBits number of random bits to be generated, where
   * 0 {@literal <=} <code>numBits</code> {@literal <=} 32.
   *
   * @return int an <code>int</code> containing the user-specified number
   * of random bits (right justified, with leading zeros).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String),327,350,"/**
* Retrieves a KeyVersion object for the specified function mask version.
* @param versionName unique version identifier
* @return KeyVersion object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,innerSetKeyVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)",495,506,"/**
* Stores a key with specified cipher and material into the key store.
* @param name unique key identifier
* @param versionName key version name
* @param material encrypted key data
* @param cipher encryption algorithm used
* @return KeyVersion object representing stored key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])",611,613,"/**
* Initializes a new KMSKeyVersion object with provided details.
* @param keyName identifier of the Key
* @param versionName identifier of the Version
* @param material encrypted data for this KeyVersion
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createKeyProviderCryptoExtension,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider),605,621,"/**
* Creates a KeyProviderCryptoExtension instance.
* @param keyProvider underlying KeyProvider object
*/","* Creates a <code>KeyProviderCryptoExtension</code> using a given
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface the <code>KeyProvider</code> itself
   * will provide the extension functionality.
   * If the given <code>KeyProvider</code> implements the
   * {@link KeyProviderExtension} interface and the KeyProvider being
   * extended by the <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface, the KeyProvider being extended will
   * provide the extension functionality. Otherwise, a default extension
   * implementation will be used.
   *
   * @param keyProvider <code>KeyProvider</code> to use to create the
   * <code>KeyProviderCryptoExtension</code> extension.
   * @return a <code>KeyProviderCryptoExtension</code> instance using the
   * given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,close,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close(),623,629,"/**
* Calls m2 method on another KeyProvider instance, excluding self. 
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,readObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream),700,705,"/**
* Reads and initializes metadata from ObjectInputStream.
* @param in ObjectInputStream containing metadata data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",647,650,"/**
* Constructs a new KMSMetadata instance with specified parameters.
* @param cipher encryption algorithm
* @param bitLength key size in bits
* @param description metadata description
* @param attributes additional metadata attributes
* @param created timestamp of creation
* @param versions number of revisions",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream),694,698,"/**
* Serializes metadata and writes it to OutputStream.
* @param out OutputStream object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,printException,org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception),533,537,"/**
* Logs and reports execution failure due to an Exception.
* @param e thrown Exception instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute(),252,271,"/**
* Lists and displays keys from the KeyProvider.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getKeysMetadata,org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[]),61,64,"/**
* Retrieves metadata by name(s), delegating to key provider.
* @param names one or more metadata identifiers
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,cleanupNewAndOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",600,605,"/**
* Updates file system metadata by masking the old path and unmasking the new one.
* @param newPath new path to be masked
* @param oldPath old path to be unmasksed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,backupToOld,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path),622,630,"/**
* Checks if file exists at specified path.
* @param oldPath Path to check
* @return true if file found, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,revertFromOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)",632,637,"/**
* Masks existing files in the specified directory.
* @param oldPath Path to mask
* @param fileExisted Flag indicating if a file already exists at the target location
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,deleteKey,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String),462,493,"/**
* Removes a key and its versions from the store.
* @param name unique key identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getAlgorithm,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm(),679,682,"/**
* Returns the function mask value from metadata.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String),496,502,"/**
* Retrieves a KeyVersion object based on the provided key name.
* @param name the key name to fetch
*/","* Get the current version of the key, which should be used for encrypting new
   * data.
   * @param name the base name of the key
   * @return the version name of the current version of the key or null if the
   *    key version doesn't exist
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,generateKey,"org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)",545,552,"/**
* Generates a secret key of specified size using the given encryption algorithm.
* @param size desired key length
* @param algorithm encryption algorithm to use (e.g. AES, DES)
* @return generated secret key as byte array
*/","* Generates a key material.
   *
   * @param size length of the key.
   * @param algorithm algorithm to use for generating the key.
   * @return the generated key.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,<init>,"org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",91,95,"/**
* Initializes caching key provider with given key provider and timeout values.
* @param keyProvider key provider instance
* @param keyTimeoutMillis cache key expiration time in milliseconds
* @param currKeyTimeoutMillis current cache key expiration time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,invalidateCache,org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String),156,164,"/**
* Invokes methods to process metadata and cache updates for a given user.
* @param name user identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,invalidateCache,org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String),120,123,"/**
* Calls key provider's m1 method with given user name.
* @param name user name to pass to key provider
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute(),511,525,"/**
* Invalidates cached key versions in the KeyProvider.
* @throws IOException if cache invalidation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion),95,108,"/**
* Creates a map representation of a KeyProvider.KeyVersion object.
* @param keyVersion KeyVersion instance to serialize
* @return Map containing serialized KeyVersion data or empty map if null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])",307,325,"/**
* Encrypts a key using an encryption key and IV.
* @param encryptor Encryption engine
* @param encryptionKey Key to use for encryption
* @param key Key to be encrypted
* @param iv Initialization vector
* @return EncryptedKeyVersion object containing the encrypted key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createForDecryption,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])",103,110,"/**
* Creates an EncryptedKeyVersion object from provided key and encryption details.
* @param keyName unique key identifier
* @param encryptionKeyVersionName name of the encryption key version
* @param encryptedKeyIv initialization vector for the encrypted key
* @param encryptedKeyMaterial actual encrypted key material
* @return EncryptedKeyVersion object representing the new key version
*/","* Factory method to create a new EncryptedKeyVersion that can then be
     * passed into {@link #decryptEncryptedKey}. Note that the fields of the
     * returned EncryptedKeyVersion will only partially be populated; it is not
     * necessarily suitable for operations besides decryption.
     *
     * @param keyName Key name of the encryption key use to encrypt the
     *                encrypted key.
     * @param encryptionKeyVersionName Version name of the encryption key used
     *                                 to encrypt the encrypted key.
     * @param encryptedKeyIv           Initialization vector of the encrypted
     *                                 key. The IV of the encryption key used to
     *                                 encrypt the encrypted key is derived from
     *                                 this IV.
     * @param encryptedKeyMaterial     Key material of the encrypted key.
     * @return EncryptedKeyVersion suitable for decryption.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",410,431,"/**
* Decrypts key version using provided decryptor and encryption key.
* @param decryptor decryption object
* @param encryptionKey encryption key to use
* @param encryptedKeyVersion encrypted key version to decrypt
* @return decrypted KeyVersion object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java,createKeyProviderDelegationTokenExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider),138,148,"/**
* Creates a delegated token extension for the given key provider.
* @param keyProvider KeyProvider instance to extend with delegation
*/","* Creates a <code>KeyProviderDelegationTokenExtension</code> using a given 
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the 
   * {@link DelegationTokenExtension} interface the <code>KeyProvider</code> 
   * itself will provide the extension functionality, otherwise a default 
   * extension implementation will be used.
   * 
   * @param keyProvider <code>KeyProvider</code> to use to create the 
   * <code>KeyProviderDelegationTokenExtension</code> extension.
   * @return a <code>KeyProviderDelegationTokenExtension</code> instance 
   * using the given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate(),431,454,"/**
* Validates provider connection and key name.
* @return true on success, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,writeJson,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)",253,257,"/**
* Serializes an Object to JSON and writes it to the specified OutputStream.
* @param obj object to serialize
* @param os output stream for serialized data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,checkNotEmpty,"org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)",133,141,"/**
* Validates and returns input string.
* @param s input string to validate
* @param name parameter name for error reporting
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),951,959,"/**
* Executes encryption key versioning process.
* @param keyNames variable-length array of key names
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.KMSClientProvider:close(),1195,1207,"/**
* Releases encryption key resources and cleans up SSL factory.
*/",* Shutdown valueQueue executor threads,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,submitRefillTask,"org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)",401,443,"/**
* Executes mask operation on key queue with specified key name.
* @param keyName unique identifier for the key
* @param keyQueue queue containing keys to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,deleteByName,org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String),187,194,"/**
* Retrieves and executes a named runnable from the in-progress map.
* @param name unique identifier for the named runnable
* @return Runnable instance or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getLock,org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String),136,138,"/**
 * Retrieves a read-write lock from the lock array based on a function mask.
 * @param keyName unique identifier for the lock
 */","* Get the stripped lock given a key name.
   *
   * @param keyName The key name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,flush,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush(),557,567,"/**
* Flushes all KMS client providers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,isTransient,org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient(),56,59,"/**
* Calls key provider's m1 method and returns result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,warnIfTransientProvider,org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider(),219,223,"/**
* Checks and logs warnings when modifying a transient provider.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProviderFromUri,"org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)",81,93,"/**
* Creates and returns a KeyProvider instance for the specified URI.
* @param conf configuration object
* @param providerUri URI of the key provider to instantiate
* @return KeyProvider instance or throws IOException if instantiation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String),207,215,"/**
* Appends a single field to the builder's string, separated by the specified separator.
* @param field field value to append
*/","* Append new field to the context.
     * @param field one of fields to append.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,"org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)",223,231,"/**
* Appends a key-value pair to the builder.
* @param key unique identifier
* @param value associated value
* @return Builder instance for chaining
*/","* Append new field which contains key and value to the context.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,appendIfAbsent,"org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)",240,251,"/**
* Adds key-value pair to the builder, using FUNC_MASK syntax.
* @param key function mask key
* @param value function mask value
*/","* Append new field which contains key and value to the context
     * if the key(""key:"") is absent.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)",148,154,"/**
* Initializes a new instance of the Builder with context and field separator.
* @param context initial text to append
* @param separator field delimiter string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshResponse.java,successResponse,org.apache.hadoop.ipc.RefreshResponse:successResponse(),37,39,"/**
* Returns a pre-configured refresh response with success status.
* @return RefreshResponse object indicating successful operation
*/","* Convenience method to create a response for successful refreshes.
   * @return void response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto),82,104,"/**
* Builds a RefreshResponse object from the given protocol buffer data.
* @param proto GenericRefreshResponseProto containing refresh details
* @return RefreshResponse object with populated fields or null if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,pack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection),66,83,"/**
* Converts a collection of refresh responses to a proto response collection.
* @param responses Collection of RefreshResponse objects
* @return GenericRefreshResponseCollectionProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ObserverRetryOnActiveException.java,<init>,org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String),32,34,"/**
* Constructs an ObserverRetryOnActiveException with specified message.
* @param msg error message describing the exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientId.java,toString,org.apache.hadoop.ipc.ClientId:toString(byte[]),52,62,"/**
* Generates a unique ID string from the given client ID array.
* @param clientId byte array containing the client identifier
*/","* @return Convert a clientId byte[] to string.
   * @param clientId input clientId.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)",72,82,"/**
* Constructs a CacheEntry object from client ID, call ID, and expiration time.
* @param clientId unique 16-octet UUID
* @param callId call identifier
* @param expirationTime cache entry expiration timestamp",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue(),242,249,"/**
* Returns a flag indicating whether the queue has a fair mask.
* @return true if the queue has a fair mask, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),722,725,"/**
* Calls M1 on the schedule queue with the given Schedulable object.
* @param e Schedulable object to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isClientBackoffEnabled,org.apache.hadoop.ipc.Server:isClientBackoffEnabled(),3872,3874,"/**
* Calls and returns result of m1() from the underlying queue.
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addInternal,"org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)",306,321,"/**
* Checks and applies func mask for given entity, with optional backoff check.
* @param e the entity to process
* @param checkBackoff whether to perform backoff check
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object),335,338,"/**
* Delegate call to nested class's m2 method.
* @param e object to be processed
*/","* Insert e into the backing queue.
   * Return true if e is queued.
   * Return false if the queue is full.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,"org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",340,344,"/**
* Calls the reference's m2 method with provided parameters.
* @param e object to be referenced
* @param timeout maximum waiting time
* @param unit time unit for timeout
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getCallQueueLen,org.apache.hadoop.ipc.Server:getCallQueueLen(),3868,3870,"/**
 * Returns a function mask value.
 */","* The number of rpc calls in the queue.
   * @return The number of rpc calls in the queue.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolInterfaces,org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class),144,147,"/**
* Retrieves an array of interface masks from the given protocol.
* @param protocol the protocol class to extract masks from
*/","* Get all interfaces that the given protocol implements or extends
   * which are assignable from VersionedProtocol.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getServerAddress,org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object),740,742,"/**
* Resolves InetSocketAddress from the given proxy object.
* @param proxy Object containing network information
* @return InetSocketAddress instance or null if failed
*/","* @return Returns the server address for a given proxy.
   * @param proxy input proxy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder),71,74,"/**
 * Initializes CallerContext from Builder instance.
 * @param builder Builder object containing context and signature values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,toString,org.apache.hadoop.ipc.CallerContext:toString(),112,123,"/**
* Returns a formatted string representing the function mask.
* @return A string containing the function context and optional signature
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendPing,org.apache.hadoop.ipc.Client$Connection:sendPing(),1071,1080,"/**
* Sends periodic ping request to remote server.
* @throws IOException on I/O error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,registerProtocolEngine,"org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",288,300,"/**
* Registers or updates an RPC kind with the given request wrapper class and invoker.
* @param rpcKind type of RPC operation
* @param rpcRequestWrapperClass class wrapping the RPC request
* @param rpcInvoker RPC invocation handler
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,get,org.apache.hadoop.ipc.ExternalCall:get(),47,53,"/**
* Returns the result of execution as type T.
* @throws ExecutionException if an error occurred
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNowNanos,org.apache.hadoop.util.Timer:monotonicNowNanos(),58,60,"/**
* Returns system time in milliseconds.","* Same as {@link #monotonicNow()} but returns its result in nanoseconds.
   * Note that this is subject to the same resolution constraints as
   * {@link System#nanoTime()}.
   * @return a monotonic clock that counts in nanoseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getUserGroupInformation,org.apache.hadoop.ipc.Server$Call:getUserGroupInformation(),1112,1115,"/**
* Returns a UserGroupInformation instance with a specific mask.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteUser,org.apache.hadoop.ipc.Server:getRemoteUser(),445,448,"/**
* Retrieves user group information from current call.
* @return UserGroupInformation object or null if not applicable
*/","Returns the RPC remote user when invoked inside an RPC.  Note this
   *  may be different than the current user if called within another doAs
   *  @return connection's UGI or null if not an RPC",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable),1105,1107,"/**
* Logs and handles fatal RPC exception.
* @param t Throwable object containing error details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)",210,213,"/**
* Initializes DecayTask with DecayRpcScheduler and Timer instances.
* @param scheduler DecayRpcScheduler instance to schedule tasks
* @param timer Timer instance to manage task execution",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,putVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)",86,89,"/**
* Adds protocol signatures to cache by address and kind.
* @param addr InetSocketAddress instance
* @param protocol communication protocol
* @param rpcKind RPC variant (e.g. ""v2"")
* @param map Map of long IDs to ProtocolSignature objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)",91,94,"/**
* Retrieves protocol signature map by address, protocol, and RPC kind.
* @param addr InetSocketAddress instance
* @param protocol target protocol name
* @param rpcKind type of RPC (e.g., ""JSON"", ""gRPC"")
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprints,org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[]),121,130,"/**
* Computes hash codes for an array of Method objects.
* @param methods array of Method objects to process
* @return array of hash codes or null if input is invalid
*/","* Convert an array of Method into an array of hash codes
   * 
   * @param methods
   * @return array of hash codes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,convertProtocolSignatureProtos,org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List),149,161,"/**
* Builds a protocol signature map from a list of protocol signature protos.
* @param protoList list of protocol signature prototypes
* @return Map of protocol signatures keyed by unique ID
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,methodExists,"org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)",163,174,"/**
* Checks if a protocol signature exists for the given version and method hash.
* @param methodHash unique method identifier
* @param version protocol version
* @param versionMap map of protocol signatures by version
* @return true if signature found, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshRegistry.java,dispatch,"org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])",94,131,"/**
* Fetches refresh responses for a given identifier and arguments.
* @param identifier unique identifier
* @param args array of arguments
* @return Collection of RefreshResponse objects
*/","* Lookup the responsible handler and return its result.
   * This should be called by the RPC server when it gets a refresh request.
   * @param identifier the resource to refresh
   * @param args the arguments to pass on, not including the program name
   * @throws IllegalArgumentException on invalid identifier
   * @return the response from the appropriate handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,<init>,"org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)",40,42,"/**
 * Constructs a RemoteException with specified class name and message.
 * @param className name of the remote object that raised the exception
 * @param msg error message
 */","* @param className wrapped exception, may be null
   * @param msg may be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[]),81,96,"/**
* Tries to fetch an IOException instance using the provided types.
* @param lookupTypes array of Class<?> instances to search
* @return IOException instance or current object if not found
*/","* If this remote exception wraps up one of the lookupTypes
   * then return this exception.
   * <p>
   * Unwraps any IOException.
   * 
   * @param lookupTypes the desired exception class. may be null.
   * @return IOException, which is either the lookupClass exception or this.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(),107,115,"/**
* Retrieves a mask IOException instance.
* @return IOException object or self if exception occurs
*/","* Instantiate and return the exception wrapped up by this remote exception.
   * 
   * <p> This unwraps any <code>Throwable</code> that has a constructor taking
   * a <code>String</code> as a parameter.
   * Otherwise it returns this.
   * 
   * @return <code>Throwable</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getNumInProcessHandler,org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler(),157,160,"/**
* Returns the number of in-process handlers.
* @return The metric value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequests,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests(),175,178,"/**
* Returns the total number of requests.
* @return Number of total requests
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequestsPerSecond,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond(),180,183,"/**
* Retrieves number of total requests received per second.
* @return Count of incoming requests as a long value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)",1000,1012,"/**
* Creates a Call object representing an RPC call.
* @param id unique call identifier
* @param retryCount number of retries allowed
* @param kind type of RPC call (e.g. request, response)
* @param clientId client ID associated with the call
* @param span tracing information for the call
* @param callerContext context data from the caller
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,get,"org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)",73,75,"/**
* Calculates a duration based on given timing and unit.
* @param type Timing to convert
* @param timeUnit Unit of measurement for the result
* @return Duration in specified unit
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,toString,org.apache.hadoop.ipc.ProcessingDetails:toString(),97,108,"/**
* Formats a string with timing types and their corresponding times.
* @return formatted string or null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,getCost,org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails),100,109,"/**
* Calculates the function mask value based on input details and weights.
* @param details ProcessingDetails object
*/","* Calculates a weighted sum of the times stored on the provided processing
   * details to be used as the cost in {@link DecayRpcScheduler}.
   *
   * @param details Processing details
   * @return The weighted sum of the times. The returned unit is the same
   *         as the default unit used by the provided processing details.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,set,"org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Converts value to specified TimeUnit and passes it to nested m2 call.
* @param type timing type
* @param value initial value
* @param timeUnit target unit of measurement
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getSchedulingDecisionSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary(),904,912,"/**
* Retrieves and processes RPC scheduler state.
* @return scheduler state string or error message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount(),924,932,"/**
* Calls m2 on the RpcScheduler instance returned by m1.
* @return result of m2 or -1 if scheduler is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTotalCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume(),934,942,"/**
* Calls m2() on the DecayRpcScheduler instance returned by delegate's m1().
* @return long value or -1 if scheduler is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getAverageResponseTime,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime(),944,952,"/**
* Retrieves M2 values from a DecayRpcScheduler instance or returns default value.
* @return array of double values or default values if scheduler is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getResponseTimeCountInLastWindow,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow(),954,961,"/**
* Retrieves decay RPC scheduler from delegate and calls its m2 method.
* @return Array of long values or default array on null scheduler
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumDroppedConnections,org.apache.hadoop.ipc.Server:getNumDroppedConnections(),3859,3862,"/**
* Retrieves the function mask from the connection manager.","* The number of RPC connections dropped due to
   * too many connections.
   * @return the number of dropped rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isFull,org.apache.hadoop.ipc.Server$ConnectionManager:isFull(),4088,4091,"/**
* Checks if mask value is enabled based on connection count.
* @return true if mask value should be applied, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnections,org.apache.hadoop.ipc.Server:getNumOpenConnections(),3837,3839,"/**
* Returns functional mask value from database.
* @return integer mask value
*/","* The number of open RPC conections
   * @return the number of open rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getConnections,org.apache.hadoop.ipc.Server:getConnections(),756,759,"/**
 * Returns an array of connections with masks applied.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,startIdleScan,org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan(),4159,4161,"/**
* Calls the 'm1()' function. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,putQueue,"org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)",229,233,"/**
* Simulates function with specified priority and parameter.
* @param priority functional priority level
* @param e parameter value for simulated function call
*/","* Put the element in a queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueue,"org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)",241,248,"/**
* Evaluates mask condition based on priority and entity.
* @param priority priority value
* @param e entity object
* @return true if condition met, false otherwise
*/","* Offer the element to queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add
   * @return boolean if added to the given queue",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",269,279,"/**
* Submits an element to the prioritized queue and waits for completion.
* @param e Element to submit
* @param timeout Timeout duration in specified TimeUnit
* @param unit Time unit for timeout duration
* @return True if submission completed successfully, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable),281,290,"/**
* Evaluates an element based on its priority level.
* @param e the element to evaluate
* @return true if evaluation is successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,drainTo,org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection),366,369,"/**
* Limits iteration count to maximum value when iterating over collection.
* @param c Collection of elements
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addTerseExceptions,org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[]),174,176,"/**
* Registers exception classes for filtering.
* @param exceptionClass one or more exception class types to filter
*/","* Add exception classes for which server won't log stack traces.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addSuppressedLoggingExceptions,org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[]),183,185,"/**
 * Dispatches exceptions handling to the handler instance.
 * @param exceptionClass one or more exception class types
 */","* Add exception classes which server won't log at all.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logException,"org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)",3244,3262,"/**
* Logs functional exception with error and call context.
* @param logger logging instance
* @param e thrown exception
* @param call related call instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getSupportedProtocolVersions,"org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1146,1164,"/**
* Fetches VerProtocolImpl array matching the given protocolName and rpcKind.
* @param rpcKind RPC kind
* @param protocolName protocol name to match
* @return Array of VerProtocolImpl objects or null if no matches found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getHighestSupportedProtocol,"org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1166,1187,"/**
* Retrieves the highest protocol version for a given RPC kind and name.
* @param rpcKind type of remote procedure call
* @param protocolName target protocol identifier
* @return VerProtocolImpl object with highest version and class, or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String),32,34,"/**
 * Constructs an instance of UnexpectedServerException with a custom error message.
 * @param message human-readable description of the exception
 */","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String),33,35,"/**
* Constructs an RpcServerException with a custom error message.
* @param message detailed description of the exception cause
*/","* Constructs exception with the specified detail message.
   * @param message detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String),31,33,"/**
* Constructs an RpcClient exception with the specified error message.
* @param message detailed description of the RPC client error.","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,"org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
* Constructs an UnexpectedServerException with a custom error message and root cause.
* @param message detailed error description
* @param cause underlying exception that caused this server exception
*/","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,"org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
 * Constructs an RpcServerException with a custom error message and optional root cause.
 * @param message the detailed error message
 * @param cause the underlying exception that triggered this server exception (optional)
 */","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,"org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)",44,46,"/**
* Constructs an RpcClient exception with a custom error message and optional root cause.
* @param message detailed client-side error description
* @param cause underlying exception (can be null)
*/","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,setCapacity,org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int),60,62,"/**
* Calls m1 on underlying FramedBuffer with specified capacity.
* @param capacity buffer capacity to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset(),98,102,"/**
* Initializes mask configuration and performs initial framing calculation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,getFramedBuffer,org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer(),42,46,"/**
* Returns a framed buffer with written mask status updated.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCost,"org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)",568,600,"/**
* Updates call costs for the specified identity, applying decay and incrementing totals.
* @param identity unique identifier (cast to String)
* @param costDelta cost delta value
*/","* Adjust the stored cost for a given identity.
   *
   * @param identity the identity of the user whose cost should be adjusted
   * @param costDelta the cost to add for the given identity",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,computePriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)",609,634,"/**
* Calculates the priority level based on cost and identity.
* @param cost cost value
* @param identity unique identifier
* @return priority level or 0 if no match found
*/","* Given the cost for an identity, compute a scheduling decision.
   *
   * @param cost the cost for an identity
   * @param identity the identity of the user
   * @return scheduling decision from 0 to numLevels - 1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,setPriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",693,699,"/**
* Sets functional priority mask for a given UserGroupInformation.
* @param ugi the UserGroupInformation to set priority for
* @param priority new functional priority level (clamped within valid range)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary(),1127,1133,"/**
* Generates and returns a function mask string.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer),145,147,"/**
*Masks ByteBuffer as a Buffer.
*@param bb input byte buffer to be masked
*@return Buffer object wrapping the input buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(),514,515,"/**
 * Creates an empty RPC request. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)",517,520,"/**
* Initializes an RpcProtobufRequest object with a request header and payload.
* @param header Request header information
* @param payload Payload message data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(),652,653,"/**
 * Constructs an RPC request using Protobuf.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)",655,658,"/**
* Initializes an RPC request with a protobuf header and message payload.
* @param header Request header data
* @param payload Message to be sent in the request
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getRemoteException,org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException),56,58,"/**
* Wraps ServiceException into an IOException.
* @param se ServiceException instance to convert.","* Return the IOException thrown by the remote server wrapped in
   * ServiceException as cause.
   * @param se ServiceException that wraps IO exception thrown by the server
   * @return Exception wrapped in ServiceException or
   *         a new IOException that wraps the unexpected ServiceException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,ipc,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall),158,164,"/**
* Wraps an IpcCall and catches ServiceException, returning the result or throwing a wrapped exception.
* @param call the IPC call to execute
* @return the result of the call, or null if an IOException occurs
*/","* Evaluate a protobuf call, converting any ServiceException to an IOException.
   * @param call invocation to make
   * @return the result of the call
   * @param <T> type of the result
   * @throws IOException any translated protobuf exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String),94,96,"/**
* Fetches and returns the ByteString associated with the provided key.
* @param key unique identifier for the desired ByteString
*/","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return ByteString for frequently used fixed and small set strings.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getByteString,org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[]),104,107,"/**
* Converts byte array to ByteString using ShadedProtobufHelper. 
* @param bytes input byte data
* @return ByteString object representing the input
*/","* Get the byte string of a non-null byte array.
   * If the array is 0 bytes long, return a singleton to reduce object allocation.
   * @param bytes bytes to convert.
   * @return a value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,skipRetryCache,"org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)",206,211,"/**
* Validates function mask criteria.
* @param clientId client identifier
* @param callId unique call identifier
* @return true if conditions met, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,setState,"org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)",382,387,"/**
* Sets mask value in cache entry based on result of operation.
* @param e CacheEntry object to update
* @param success true if operation was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString(),538,547,"/**
* Constructs function mask from request headers.
* @return formatted string representing the function mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setCallIdAndRetryCount,"org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)",122,128,"/**
* Validates and executes a function with retry count.
* @param cid call ID
* @param rc retry count
* @param externalHandler user-defined handler
*/","* Set call id and retry count for the next call.
   * @param cid input cid.
   * @param rc input rc.
   * @param externalHandler input externalHandler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client:close(),1881,1885,"/**
* Executes m1 function.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkAsyncCall,org.apache.hadoop.ipc.Client:checkAsyncCall(),1430,1442,"/**
* Checks for and handles exceeding the maximum asynchronous call limit.
* @throws AsyncCallLimitExceededException if exceeded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getListenerAddress,org.apache.hadoop.ipc.Server:getListenerAddress(),3751,3753,"/**
* Returns an InetSocketAddress instance based on listener configuration.
*/","* Return the socket (ip+port) on which the RPC server is listening to.
   * @return the socket (ip+port) on which the RPC server is listening to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryListenerAddresses,org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses(),3762,3770,"/**
* Retrieves a set of InetSocketAddress instances based on the auxiliary listener map.
* @return Set of addresses or an empty set if no listeners are present
*/","* Return the set of all the configured auxiliary socket addresses NameNode
   * RPC is listening on. If there are none, or it is not configured at all, an
   * empty set is returned.
   * @return the set of all the auxiliary addresses on which the
   *         RPC server is listening on.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doStop,org.apache.hadoop.ipc.Server$Listener:doStop(),1673,1688,"/**
* Cleans up resources and notifies listeners after a read operation.
* @param selector optional selector object
* @param acceptChannel optional channel to close
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,stopClient,org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client),99,120,"/**
* Removes and stops a client from the cache.
* @param client Client object to be removed
*/","* Stop a RPC client connection 
   * A RPC client is closed only when its reference count becomes zero.
   *
   * @param client input client.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,clearClientCache,org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache(),392,395,"/**
 * Initializes client configuration with default settings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostInetAddress,org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress(),1224,1227,"/**
* Returns the server's IP address via the underlying connection.
* @return server's InetAddress or null if unavailable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server$RpcCall:getRemotePort(),1229,1232,"/**
* Delegates call to underlying database connection.
* @return result of database operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredResponse,org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable),1347,1365,"/**
* Sends a successful response if the connection is running.
*@param response Writable object for response data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,toString,org.apache.hadoop.ipc.Server$RpcCall:toString(),1404,1407,"/**
* Concatenates parent's result with RPC request and connection info.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,waitForWork,org.apache.hadoop.ipc.Client$Connection:waitForWork(),1036,1062,"/**
* Determines whether to mask function calls based on connection state and timeouts.
* @return true if masking is enabled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionTimeout,"org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)",921,932,"/**
* Retries connection to server if maximum attempts not reached.
* @param curRetries current attempt count
* @param maxRetries maximum allowed attempts
* @param ioe IOException to be re-thrown if maxRetries exceeded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)",934,968,"/**
* Handles failed connections by attempting retries based on the configured policy.
* @param curRetries current number of retries
* @param ioe exception from failed connection attempt
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,equals,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object),168,171,"/**
* Calls superclass method to perform custom logic.
* @param obj Object to process
* @return Result of superclass method invocation
*/",Override equals to avoid findbugs warnings,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getQueueSizes,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes(),438,446,"/**
* Recursively retrieves and returns an array of integers from the scheduled object in the call queue. 
* @return empty array if no scheduled objects are found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getOverflowedCalls,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls(),448,456,"/**
* Recursively fetches schedulables from the call queue.
* @return array of schedulable IDs or empty array on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,<init>,org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object),51,56,"/**
* Wraps a given Protobuf message into a legacy wrapper object.
* @param message the Protobuf message to wrap
*/","* Construct.
   * The type of the parameter is Object so as to keep the casting internal
   * to this class.
   * @param message message to wrap.
   * @throws IllegalArgumentException if the class is not a protobuf message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,switchToSimple,org.apache.hadoop.ipc.Server$Connection:switchToSimple(),2401,2405,"/**
* Resets authentication protocol and calls initial setup.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$Connection:close(),3082,3094,"/**
* Resets and handles socket/channel state.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslToken,org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2387,2399,"/**
* Processes SASL response from client.
* @param saslMessage SASL message object
* @return RpcSaslProto object with updated state and token
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkDataLength,org.apache.hadoop.ipc.Server$Connection:checkDataLength(int),2446,2459,"/**
* Validates input data length against maximum allowed value.
* @param dataLength requested data length
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseOldVersionFatal,"org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3627,3639,"/**
* Writes fatal status and error details to response stream.
* @param response ByteArrayOutputStream to write to
* @param call RpcCall object with caller context
* @param rv Writable object to receive result (not used in this method)
* @param errorClass class name of the error
* @param error detailed error message
*/","* Setup response for the IPC Call on Fatal Error from a 
   * client that is using old version of Hadoop.
   * The response is serialized using the previous protocol's response
   * layout.
   * 
   * @param response buffer to serialize the response into
   * @param call {@link Call} to which we are setting up the response
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRpcRequestWrapper,org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto),302,308,"/**
* Returns the Writable class for a given RPC kind.
* @param rpcKind RPC kind proto object
* @return Class<? extends Writable> or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString(),676,685,"/**
* Constructs function mask string from protocol buffer header.
* @return Function mask string or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(),94,97,"/**
* Calculates a hash code using client ID and call ID.
* @return A unique hash value or a calculated hash code
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,advanceIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex(),121,132,"/**
* Triggers next state when remaining requests reach zero.
*/","* Advances the index, which will change the current index
   * if called enough times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>(),398,403,"/**
* Initializes the callback implementation with server, call, and method name details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>(),430,435,"/**
* Initializes callback with server, call, method name, and setup time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),412,418,"/**
* Handles an exception by reporting metrics and notifying the caller.
* @param t the Throwable instance to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),444,450,"/**
* Processes a Throwable event with custom metrics and notifications.
* @param t the exception to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,capacity,org.apache.hadoop.ipc.ResponseBuffer:capacity(),56,58,"/**
* Calls m1() on framed buffer output.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,ensureCapacity,org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int),64,68,"/**
* Adjusts output buffer capacity to specified value.
* @param capacity new capacity of the output buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setException,org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException),341,344,"/**
* Handles IOException by masking it and invoking next step.
* @param error IOException instance to be handled
*/","Set the exception when there is an error.
     * Notify the caller the call is done.
     * 
     * @param error exception thrown by the call; either local or remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setRpcResponse,org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable),351,354,"/**
* Sets RPC response and triggers subsequent processing.
* @param rpcResponse Writable response object
*/","Set the return value when there is no error. 
     * Notify the caller the call is done.
     * 
     * @param rpcResponse return value of the rpc call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(),513,524,"/**
* Repeatedly calls superclass method with socket timeout handling.
* @throws IOException if an I/O error occurs
*/","Read a byte from the stream.
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * @throws IOException for any IO problem other than socket timeout",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,"org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)",532,543,"/**
* Repeatedly attempts to read data from socket until timeout is reached.
* @param buf buffer to store data
* @param off offset in buffer
* @param len length of data to read
* @throws IOException if an I/O error occurs
*/","Read bytes into a buffer starting from offset <code>off</code>
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * 
       * @return the total number of bytes read; -1 if the connection is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostAddress,org.apache.hadoop.ipc.Server$Call:getHostAddress(),1063,1066,"/**
* Resolves IP address to its hostname.
* @return hostname string or null if resolution fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteIp,org.apache.hadoop.ipc.Server:getRemoteIp(),385,388,"/**
* Retrieves the current function's network address.
*/","* @return Returns the remote side ip address when invoked inside an RPC
   *  Returns null in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getServerRpcInvoker,org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind),310,312,"/**
* Returns an RpcInvoker instance based on the specified RPC kind. 
* @param rpcKind RPC invocation type (e.g., synchronous or asynchronous)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server:getRemotePort(),394,397,"/**
* Returns the result of m2 in the current call stack or 0 if empty. 
* @return Result from m2 in the current call stack, defaulting to 0 if none found.","* @return Returns the remote side port when invoked inside an RPC
   * Returns 0 in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryPortEstablishedQOP,org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP(),411,423,"/**
* Returns a string mask from the current RPC connection, or null if invalid.
*/","* Returns the SASL qop for the current call, if the current call is
   * set, and the SASL negotiation is done. Otherwise return null
   * Note this only returns established QOP for auxiliary port, and
   * returns null for primary (non-auxiliary) port.
   *
   * Also note that CurCall is thread local object. So in fact, different
   * handler threads will process different CurCall object.
   *
   * Also, only return for RPC calls, not supported for other protocols.
   * @return the QOP of the current connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocol,org.apache.hadoop.ipc.Server:getProtocol(),450,453,"/**
* Retrieves and returns user profile data from current call context.
* @return UserProfile string or null if not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(),465,468,"/**
* Recursively fetches and returns the result from the current call stack. 
* @return result value or 0 if no active call found","* @return Return the priority level assigned by call queue to an RPC
   * Returns 0 in case no priority is assigned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setLogSlowRPCThresholdTime,org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long),556,560,"/**
* Sets the threshold time for logging slow RPCs.
* @param logSlowRPCThresholdMs minimum duration in milliseconds to consider an RPC slow
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setClientBackoffEnabled,org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean),3876,3878,"/**
 * Calls the next queued task with the given boolean value.
 * @param value input flag to pass to the queued task
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addAuxiliaryListener,org.apache.hadoop.ipc.Server:addAuxiliaryListener(int),3427,3443,"/**
* Binds or updates an existing listener to the specified auxiliary port.
* @param auxiliaryPort unique port identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForProtobuf,"org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3585,3607,"/**
* Generates response buffer by serializing RpcResponseHeaderProto and optional payload.
* @param header RpcResponseHeaderProto object
* @param rv Optional Writable payload
* @return serialized byte array or null if invalid input
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnectionsPerUser,org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser(),3844,3852,"/**
* Serializes and returns a string mask using the configured ObjectMapper.
* @return serialized mask or null on failure
*/",* @return Get the NumOpenConnections/User.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabled,org.apache.hadoop.ipc.Server:isServerFailOverEnabled(),3880,3883,"/**
* Calls M1 on the underlying queue.
* @return result of M1 operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processResponse,"org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)",1844,1922,"/**
* Handles response queue, processing RPC calls and writing responses.
* @param responseQueue queue of incoming RPC calls
* @param inHandler indicates if currently within handler method
* @return true if queue is empty or all calls have been handled
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,equals,org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object),1823,1841,"/**
* Compares the current object with another object for equality.
* @param obj Object to compare
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,build,org.apache.hadoop.tracing.Tracer$Builder:build(),91,96,"/**
* Returns the global tracer instance or initializes it with the given name. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,newSpan,"org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)",55,57,"/**
* Creates a new Span with the given description and context.
* @param description human-readable description of the span
* @param spanCtx SpanContext object containing additional metadata
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/NullTraceScope.java,<init>,org.apache.hadoop.tracing.NullTraceScope:<init>(),23,25,"/**
* Initializes an empty NullTraceScope instance.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/TraceScope.java,close,org.apache.hadoop.tracing.TraceScope:close(),53,57,"/**
* Recursively calls span's m1 method if span is non-null.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)",95,136,"/**
* Initializes MachineList with host entries and address factory.
* @param hostEntries Collection of host entries (IP addresses or CIDR ranges)
* @param addressFactory InetAddressFactory instance for IP address resolution
*/","* Accepts a collection of ip/cidr/host addresses
   * 
   * @param hostEntries hostEntries.
   * @param addressFactory addressFactory to convert host to InetAddress",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,includes,org.apache.hadoop.util.MachineList:includes(java.lang.String),145,160,"/**
* Validates an IP address by recursively checking its sub-network.
* @param ipAddress the IP address to validate
* @return true if valid, false otherwise
*/","* Accepts an ip address and return true if ipAddress is in the list.
   * {@link #includes(InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param ipAddress ipAddress.
   * @return true if ipAddress is part of the list",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,checkConf,org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream),136,216,"/**
* Validates configuration file and reports any issues.
* @return List of error messages or empty list if valid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,listFiles,org.apache.hadoop.util.ConfTest:listFiles(java.io.File),218,225,"/**
* Filters files in the given directory by .xml extension.
* @param dir directory to search for xml files
* @return array of matching File objects or empty array if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,"org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)",195,210,"/**
* Initializes Linux system info with specified files and time tracking parameters.
* @param procfsMemFile path to procfs file for memory info
* @param procfsCpuFile path to procfs file for CPU info
* @param procfsStatFile path to procfs file for system statistics
* @param procfsNetFile path to procfs file for network info
* @param procfsDisksFile path to procfs file for disk info
* @param jiffyLengthInMillis length of a jiffy in milliseconds
*/","* Constructor which allows assigning the /proc/ directories. This will be
   * used only in unit tests.
   * @param procfsMemFile fake file for /proc/meminfo
   * @param procfsCpuFile fake file for /proc/cpuinfo
   * @param procfsStatFile fake file for /proc/stat
   * @param procfsNetFile fake file for /proc/net/dev
   * @param procfsDisksFile fake file for /proc/diskstats
   * @param jiffyLengthInMillis fake jiffy length value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean),238,305,"/**
* Reads memory info from the file and populates various size fields.
* @param readAgain whether to re-read the file
*/","* Read /proc/meminfo, parse and compute memory information.
   * @param readAgain if false, read only on the first time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumProcessors,org.apache.hadoop.util.SysInfoLinux:getNumProcessors(),625,629,"/**
* Returns the function mask value based on the number of processors.
* @return The bit mask representing the number of available processor units.",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumCores,org.apache.hadoop.util.SysInfoLinux:getNumCores(),632,636,"/**
* Returns the function mask value based on the number of cores.
* @return The function mask value representing the available cores.",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuFrequency,org.apache.hadoop.util.SysInfoLinux:getCpuFrequency(),639,643,"/**
* Calculates CPU frequency mask.
* @return unique system identifier as a long value",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcStatFile,org.apache.hadoop.util.SysInfoLinux:readProcStatFile(),376,421,"/**
* Reads and parses CPU time from a file.
*/","* Read /proc/stat file, parse and calculate cumulative CPU.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead(),675,679,"/**
* Returns the function mask based on network bytes read.
* @return unique identifier representing current system state
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten(),682,686,"/**
* Calculates and returns the function mask.
* @return A unique identifier representing the function's execution (in bytes)
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcDisksInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile(),483,543,"/**
* Reads disk statistics from the /proc/fs/disks file and updates 
* numDisksBytesRead and numDisksBytesWritten counters.
*/","* Read /proc/diskstats file, parse and calculate amount
   * of bytes read and written from/to disks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,realloc,org.apache.hadoop.util.IdentityHashStore:realloc(int),74,90,"/**
* Resizes the internal buffer to accommodate newCapacity elements.
* @param newCapacity new size of the buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,get,org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object),152,158,"/**
* Retrieves value from a buffer using an index derived from the input key.
* @param k input key
* @return V object or null if invalid index
*/","* Retrieve a value associated with a given key.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,remove,org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object),167,177,"/**
* Retrieves a value from the buffer using the provided key.
* @param k input key
* @return associated value, or null if not found or removed
*/","* Retrieve a value associated with a given key, and delete the
   * relevant entry.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,ensureNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext(),312,327,"/**
* Fetches and updates the next element in a sequence, handling concurrent modification exceptions.
* @throws ConcurrentModificationException if modification occurs while iterating
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,"org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)",188,219,"/**
* Retrieves a value associated with the given key, updating size and modification counters as needed.
* @param index bucket index
* @param key search key
* @return Value object or null if not found
*/","* Remove the element corresponding to the key,
   * given key.hashCode() == index.
   *
   * @param key key.
   * @param index index.
   * @return If such element exists, return it.
   *         Otherwise, return null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,mergeSort,"org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)",42,83,"/**
* Merges two sorted arrays into a single sorted array.
* @param src first input array
* @param dest output array
* @param low start index of merge
* @param high end index of merge
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,setOptionalSecureTransformerAttributes,org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory),180,186,"/**
* Configures external DTD and stylesheet access masks.
*@param transformerFactory Factory for creating transformers
*/","* These attributes are recommended for maximum security but some JAXP transformers do
   * not support them. If at any stage, we fail to set these attributes, then we won't try again
   * for subsequent transformers.
   *
   * @param transformerFactory to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,string2long,org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String),906,927,"/**
* Converts string representation of a long value with optional size prefix to its numeric equivalent.
* @param s string representation of the number, e.g. ""123k"" or ""456m""
* @return the numeric value as a long
*/","* Convert a string to long.
     * The input string is first be trimmed
     * and then it is parsed with traditional binary prefix.
     *
     * For example,
     * ""-1230k"" will be converted to -1230 * 1024 = -1259520;
     * ""891g"" will be converted to 891 * 1024^3 = 956703965184;
     *
     * @param s input string
     * @return a long value represented by the input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,long2String,"org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)",937,977,"/**
* Formats a number with a specified unit and decimal places.
* @param n the value to format
* @param unit the unit of measurement (e.g. ""bytes"", ""KB"")
* @param decimalPlaces the number of decimal places to display
*/","* Convert a long integer to a string with traditional binary prefix.
     * 
     * @param n the value to be converted
     * @param unit The unit, e.g. ""B"" for bytes.
     * @param decimalPlaces The number of decimal places.
     * @return a string with traditional binary prefix.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatPercent,"org.apache.hadoop.util.StringUtils:formatPercent(double,int)",153,155,"/**
* Formats a double value as a percentage string with specified decimal places.
* @param fraction decimal value to format
* @param decimalPlaces number of decimal places to display
* @return formatted string or null if not applicable
*/","* Format a percentage for presentation to the user.
   * @param fraction the percentage as a fraction, e.g. 0.1 = 10%
   * @param decimalPlaces the number of decimal places
   * @return a string representation of the percentage",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,"org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)",183,192,"/**
* Formats byte array as a hexadecimal string.
* @param bytes input byte array
* @param start starting index
* @param end ending index
* @return hexadecimal representation of the specified range
*/","* Given an array of bytes it will convert the bytes to a hex string
   * representation of the bytes
   * @param bytes bytes.
   * @param start start index, inclusively
   * @param end end index, exclusively
   * @return hex string representation of the byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,limitDecimalTo2,org.apache.hadoop.util.StringUtils:limitDecimalTo2(double),1033,1036,"/**
* Formats double value to string with 2 decimal places.
* @param d input double value
*/","* limitDecimalTo2.
   *
   * @param d double param.
   * @return string value (""%.2f"").
   * @deprecated use StringUtils.format(""%.2f"", d).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",56,74,"/**
* Performs masked operations on an IndexedSortable object.
* @param s IndexedSortable object to operate on
* @param p starting position
* @param r ending position
* @param rep Progressable reporter (optional)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,subtract,org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes),166,169,"/**
* Calculates the difference between two GC times.
* @param other another GcTimes object for comparison
* @return a new GcTimes object with time deltas
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException),233,272,"/**
* Handles exit exceptions with logging and system termination.
* @param ee ExitException instance
*/","* Exits the JVM if exit is enabled, rethrow provided exception or any raised error otherwise.
   * Inner termination: either exit with the exception's exit code,
   * or, if system exits are disabled, rethrow the exception.
   * @param ee exit exception
   * @throws ExitException if {@link System#exit(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link System#exit(int)} is disabled and one Error arise, suppressing
   * anything else, even <code>ee</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException),286,326,"/**
* Handles halt exception by logging and potentially throwing.
* @param he the HaltException to handle
*/","* Halts the JVM if halt is enabled, rethrow provided exception or any raised error otherwise.
   * If halt is disabled, this method throws either the exception argument if no
   * error arise, the first error if at least one arise, suppressing <code>he</code>.
   * If halt is enabled, all throwables are caught, even errors.
   *
   * @param he the exception containing the status code, message and any stack
   * trace.
   * @throws HaltException if {@link Runtime#halt(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link Runtime#halt(int)} is disabled and one Error arise, suppressing
   * anyuthing else, even <code>he</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,<init>,org.apache.hadoop.util.SysInfoWindows:<init>(),56,59,"/**
 * Initializes Windows system info with default values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",319,331,"/**
* Registers a shutdown hook with custom priority and timeout.
* @param shutdownHook Runnable to execute at shutdown
* @param priority Shutdown hook priority
* @param timeout Timeout for the hook in specified TimeUnit
* @param unit Time unit for the timeout (e.g. seconds, milliseconds)
*/","*
   * Adds a shutdownHook with a priority and timeout the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order. The shutdown hook will be terminated if it
   * has not been finished in the specified period of time.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook
   * @param timeout timeout of the shutdownHook
   * @param unit unit of the timeout <code>TimeUnit</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,removeShutdownHook,org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable),340,350,"/**
* Registers or removes a shutdown hook with the specified timeout.
* @param shutdownHook Runnable to be executed on shutdown
* @return true if added or removed successfully, false otherwise
*/","* Removes a shutdownHook.
   *
   * @param shutdownHook shutdownHook to remove.
   * @return TRUE if the shutdownHook was registered and removed,
   * FALSE otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,hasShutdownHook,org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable),358,363,"/**
* Executes a shutdown hook with default timeout.
* @param shutdownHook Runnable to execute on shutdown
*/","* Indicates if a shutdownHook is registered or not.
   *
   * @param shutdownHook shutdownHook to check if registered.
   * @return TRUE/FALSE depending if the shutdownHook is is registered.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion),460,463,"/**
* Compares version of an item.
* @param o ComparableVersion object to compare with
* @return comparison result as integer value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,getResourceAsStream,org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String),91,99,"/**
* Reads a resource file by name.
* @param resourceName identifier for the resource to load
*/","* Convenience method that returns a resource as inputstream from the
   * classpath.
   * <p>
   * Uses the Thread's context classloader to load resource.
   *
   * @param resourceName resource to retrieve.
   *
   * @throws IOException thrown if resource cannot be loaded
   * @return inputstream with the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWarning,"org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",151,161,"/**
* Logs a warning for long-held locks and suppresses multiple warnings.
* @param lockHeldTime time the lock was held in milliseconds
* @param stats suppressed snapshot object containing statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWaitWarning,"org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",163,172,"/**
* Logs a warning for exceeding the lock wait time threshold.
* @param lockWaitTime milliseconds waited to acquire lock
* @param stats suppressed snapshot statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sortInternal,"org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)",69,136,"/**
* Recursively partitions and merges the IndexedSortable using a divide-and-conquer approach.
* @param s IndexedSortable object to be partitioned
* @param p left index of current partition
* @param r right index of current partition
* @param rep Progressable object for reporting progress (optional)
* @param depth recursion depth",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,org.apache.hadoop.util.LineReader:<init>(java.io.InputStream),69,71,"/**
* Constructs a new LineReader instance from an InputStream.
* @param in input stream to read from
* @param bufferSize default buffer size (optional)
*/","* Create a line reader that reads from the given stream using the
   * default buffer-size (64k).
   * @param in The input stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$1:run(),951,960,"/**
* Checks and resets mask functionality based on time interval.
* @throws IOException if an I/O error occurs
*/","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newDaemonThreadFactory,org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String),86,102,"/**
* Creates a custom thread factory with the given prefix.
* @param prefix unique identifier for threads created by this factory
*/","* Get a named {@link ThreadFactory} that just builds daemon threads.
   *
   * @param prefix name prefix for all threads created from the factory
   * @return a thread factory that creates named, daemon threads with
   * the supplied exception handler and normal priority",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,"org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)",66,81,"/**
* Initializes LightWeightResizableGSet with specified initial capacity and load factor.
* @param initCapacity non-negative initial size
* @param loadFactor threshold load ratio (0 < loadFactor <= 1.0f)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,size,org.apache.hadoop.util.LightWeightResizableGSet:size(),108,111,"/**
* Calls superclass's implementation of m1(), synchronizing access.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,getIterator,org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer),113,115,"/**
* Applies function to masked iterator and passes result to consumer.
* @param consumer callback to process masked iterator's output
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,expandIfNecessary,org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary(),148,152,"/**
* Dynamically doubles array capacity when threshold is reached.
*/","* Checks if we need to expand, and expands if necessary.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator),109,113,"/**
* Combines elements from an iterator into an ArrayList.
* @param elements iterator yielding elements of type E
* @return populated ArrayList containing all elements
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list
   * and then calling Iterators#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,addAll,"org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)",248,258,"/**
* Recursively adds elements from the given iterable to a collection.
* @param addTo the target collection
* @param elementsToAdd the source iterable
* @return true if successful, false otherwise
*/","* Adds all elements in {@code iterable} to {@code collection}.
   *
   * @return {@code true} if {@code collection} was modified as a result of
   *     this operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithCapacity,org.apache.hadoop.util.Lists:newArrayListWithCapacity(int),128,132,"/**
* Creates an empty ArrayList with specified initial capacity.
* @param initialArraySize initial size of the list
*/","* Creates an {@code ArrayList} instance backed by an array with the
   * specified initial size;
   * simply delegates to {@link ArrayList#ArrayList(int)}.
   *
   * @param <E> Generics Type E.
   * @param initialArraySize the exact size of the initial backing array for
   *     the returned array list
   *     ({@code ArrayList} documentation calls this value the ""capacity"").
   * @return a new, empty {@code ArrayList} which is guaranteed not to
   *     resize itself unless its size reaches {@code initialArraySize + 1}.
   * @throws IllegalArgumentException if {@code initialArraySize} is negative.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,computeArrayListCapacity,org.apache.hadoop.util.Lists:computeArrayListCapacity(int),192,195,"/**
* Calculates function mask value based on input array size.
* @param arraySize size of the array to generate a mask for
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,getResource,org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String),128,153,"/**
* Retrieves a resource URL by name, recursively searching parent contexts.
* @param name unique resource identifier
* @return URL object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,"org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)",160,204,"/**
* Loads a Class by name, optionally resolving it.
* @param name class name to load
* @param resolve whether to perform class resolution
* @return loaded Class or null if not found
* @throws ClassNotFoundException if class cannot be loaded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,writeJsonAsBytes,"org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)",305,312,"/**
* Writes instance data to output stream using m3() and m2() methods.
* @param instance object of type T
* @param dataOutputStream OutputStream for writing data
*/","* Write the JSON as bytes, then close the stream.
   * @param instance instance to write
   * @param dataOutputStream an output stream that will always be closed
   * @throws IOException on any failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,<init>,org.apache.hadoop.util.OperationDuration:<init>(),48,51,"/**
* Initializes operation duration tracking with current system time.
*/","* Instantiate.
   * The start time and finished time are both set
   * to the current clock time.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,finished,org.apache.hadoop.util.OperationDuration:finished(),64,66,"/**
 * Sets finished flag based on result of m1().",* Update the finished time with the current system time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,asDuration,org.apache.hadoop.util.OperationDuration:asDuration(),114,116,"/**
* Calculates duration based on intermediate values m1 and m2.","* Get the duration of an operation as a java Duration
   * instance.
   * @return a duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,getDurationString,org.apache.hadoop.util.OperationDuration:getDurationString(),72,74,"/**
* Computes and returns a mask value using nested functions m1() and m2().","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,iterator,org.apache.hadoop.util.LightWeightCache:iterator(),234,256,"/**
* Returns an iterator that wraps the superclass iterator, 
* but throws an exception on remove operation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,iterator,org.apache.hadoop.util.LightWeightGSet$Values:iterator(),240,243,"/**
 * Returns an iterator over the elements of this set.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,getMonomial,"org.apache.hadoop.util.CrcUtil:getMonomial(long,int)",52,77,"/**
* Computes the FUNC_MASK value based on lengthBytes and modulus.
* @param lengthBytes positive number of bytes
* @param mod modulus value
* @return calculated FUNC_MASK value
*/","* Compute x^({@code lengthBytes} * 8) mod {@code mod}, where {@code mod} is
   * in ""reversed"" (little-endian) format such that {@code mod & 1} represents
   * x^31 and has an implicit term x^32.
   *
   * @param lengthBytes lengthBytes.
   * @param mod mod.
   * @return monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,composeWithMonomial,"org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)",88,91,"/**
* Computes CRC mask using bitwise XOR operation.
* @param crcA initial CRC value
* @param monomial polynomial coefficient
* @param mod modulus for calculation
* @return resulting CRC mask value
*/","* composeWithMonomial.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param monomial Precomputed x^(lengthBInBytes * 8) mod {@code mod}
   * @param mod mod.
   * @return compose with monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,intToBytes,org.apache.hadoop.util.CrcUtil:intToBytes(int),113,125,"/**
* Converts an integer to a 32-bit mask in binary format.
* @param value input integer value
* @return byte array representation of the mask
*/","* @return 4-byte array holding the big-endian representation of
   *     {@code value}.
   *
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toSingleCrcString,org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[]),182,190,"/**
* Returns a hexadecimal string representation of the first 4-byte CRC.
* @param bytes input byte array (must be exactly 4 bytes)
*/","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to represent exactly one CRC, and returns a hex
   * formatted value.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toMultiCrcString,org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[]),201,218,"/**
* Formats byte array into a hexadecimal string.
* @param bytes input byte array
* @return formatted hexadecimal string or throws IOException on invalid length
*/","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to be divisible by CRC byte size, and returns a list of
   * hex formatted values.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)",119,152,"/**
* Extracts and unpacks JAR contents to a specified directory.
* @param inputStream input stream containing the JAR file
* @param toDir target directory for unpacking
* @param unpackRegex pattern for files to be extracted
*/","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)",191,222,"/**
* Unpacks JAR contents to a specified directory, filtering by regex pattern.
* @param jarFile JAR archive to unpack
* @param toDir target directory for unpacked files
* @param unpackRegex filter pattern for files to unpack
*/","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newCachedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory),36,42,"/**
* Creates an executor service with a dynamically growing thread pool.
* @param threadFactory factory for creating threads
* @return ExecutorService instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)",44,50,"/**
* Creates an ExecutorService with a specified number of threads and custom thread factory.
* @param nThreads the maximum pool size
* @param threadFactory factory for creating threads
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int),52,56,"/**
* Creates an executor service with a specified number of threads.
* @param nThreads the total number of threads to use
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int),71,74,"/**
* Creates a Hadoop scheduled thread pool executor with specified core pool size.
* @param corePoolSize number of threads to keep in pool
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)",76,79,"/**
* Creates a scheduled executor service with custom core pool size and thread factory.
* @param corePoolSize number of threads to keep in the pool
* @param threadFactory factory for creating new threads
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopScheduledThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",66,70,"/**
* Calls superclass and helper methods to handle exceptions in Runnable execution.
* @param r Runnable task to execute
* @param t Exception that occurred during execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",87,91,"/**
* Calls superclass and helper methods to handle runnables and exceptions.
* @param r Runnable object
* @param t Exception or error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,org.apache.hadoop.util.concurrent.AsyncGetFuture:get(),56,60,"/**
* Calls m1 with negative timeout and returns result of super.m2(). 
* @throws InterruptedException if interrupted while waiting
* @throws ExecutionException if execution fails in super.m2() 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,"org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)",62,67,"/**
* Calls parent's m2 with default timeout and returns result.
* @throws various exceptions if not completed within timeout
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,isDone,org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone(),69,73,"/**
* Calls parent method after delaying invocation by 0 milliseconds.
* @return result of parent method call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,<init>,org.apache.hadoop.util.StopWatch:<init>(),33,35,"/**
 * Initializes a new Stopwatch instance with a default timer. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)",78,89,"/**
* Finds the first occurrence of 'b' in utf starting from position start,
* up to a maximum of n occurrences.
* @param utf byte array to search
* @param start initial search position
* @param length maximum number of bytes to consider
* @param b target byte value
* @param n maximum number of occurrences to find","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param start starting offset
   * @param length the length of byte array
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,<init>,"org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)",33,37,"/**
* Initializes a CacheableIPList object with a given IP list and cache timeout.
* @param ipList FileBasedIPList instance containing IP addresses
* @param cacheTimeout time in milliseconds before cache expires
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,toString,org.apache.hadoop.util.WeakReferenceMap:toString(),108,115,"/**
* Returns a human-readable string representation of the WeakReferenceMap instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,put,"org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)",246,248,"/**
* Maps a key to a weakly referenced value.
* @param key unique identifier
* @param value object to be stored
* @return associated value or null if not found
*/","* Put a value under the key.
   * A null value can be put, though on a get() call
   * a new entry is generated
   *
   * @param key key
   * @param value value
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,remove,org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object),255,257,"/**
* Retrieves value associated with given key from map.
* @param key unique identifier
* @return Value object or null if not found
*/","* Remove any value under the key.
   * @param key key
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,containsKey,org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object),266,269,"/**
* Checks if a key exists in the map by fetching its associated value.
* @param key unique identifier
* @return true if key is present, false otherwise
*/","* Does the map have a valid reference for this object?
   * no-side effects: there's no attempt to notify or cleanup
   * if the reference is null.
   * @param key key to look up
   * @return true if there is a valid reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,create,org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object),197,235,"/**
* Resolves a strong reference to an object with the given key.
* @param key unique identifier
* @return V object or null if creation failed
*/","* Create a new instance under a key.
   * <p>
   * The instance is created, added to the map and then the
   * map value retrieved.
   * This ensures that the reference returned is that in the map,
   * even if there is more than one entry being created at the same time.
   * If that race does occur, it will be logged the first time it happens
   * for this specific map instance.
   * <p>
   * HADOOP-18456 highlighted the risk of a concurrent GC resulting a null
   * value being retrieved and so returned.
   * To prevent this:
   * <ol>
   *   <li>A strong reference is retained to the newly created instance
   *       in a local variable.</li>
   *   <li>That variable is used after the resolution process, to ensure
   *       the JVM doesn't consider it ""unreachable"" and so eligible for GC.</li>
   *   <li>A check is made for the resolved reference being null, and if so,
   *       the put() is repeated</li>
   * </ol>
   * @param key key
   * @return the created value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,prune,org.apache.hadoop.util.WeakReferenceMap:prune(),288,300,"/**
* Counts and removes weakly referenced objects from the map.
* @return number of removed objects
*/","* Prune all null weak references, calling the referenceLost
   * callback for each one.
   *
   * non-atomic and non-blocking.
   * @return the number of entries pruned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,snapshot,org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot(),262,268,"/**
* Returns a snapshot of the suppressed count and wait value.
* Resets the suppressed count and maximum wait on completion.
*/","* Captures the current value of the counts into a SuppressedSnapshot object
     * and resets the values to zero.
     *
     * @return SuppressedSnapshot containing the current value of the counters",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatTimeDiff,"org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)",294,297,"/**
* Calculates function execution mask based on start and finish times.
* @param finishTime end of execution timestamp
* @param startTime beginning of execution timestamp
* @return execution mask string
*/","* 
   * Given a finish and start time in long milliseconds, returns a 
   * String in the format Xhrs, Ymins, Z sec, for the time difference between two times. 
   * If finish time comes before start time then negative valeus of X, Y and Z wil return. 
   * 
   * @param finishTime finish time
   * @param startTime start time
   * @return a String in the format Xhrs, Ymins, Z sec,
   *         for the time difference between two times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollectionSplitByEquals,org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String),505,525,"/**
* Extracts key-value pairs from input string.
* @param str input string
* @return Map of key-value pairs or empty map if invalid
*/","* Splits an ""="" separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value after splitting by comma and new line separator.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Map</code> of <code>String</code> keys and values, empty
   * Collection if null String input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,"org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)",581,601,"/**
* Splits a string into an array of substrings, handling escaped separators.
* @param str input string to split
* @param escapeChar character used for escaping in the input string
* @param separator character used as the delimiter
* @return array of substrings or null if input is null
*/","* Split a string using the given separator
   * @param str a string that may have escaped separator
   * @param escapeChar a char that be used to escape the separator
   * @param separator a separator char
   * @return an array of strings",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])",701,716,"/**
* Escapes characters in a string using a specified escape character.
* @param str input string to be processed
* @param escapeChar the character used for escaping
* @param charsToEscape array of characters to be escaped
* @return modified string with escaped characters or null if input is null
*/","* escapeString.
   *
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to be escaped
   * @return escapeString.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])",748,782,"/**
* Escapes special characters in a string using a custom mask.
* @param str input string to be masked
* @param escapeChar character used for escaping
* @param charsToEscape array of characters to be escaped
* @return masked string or null if input is null
*/","* unEscapeString.
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to unescape
   * @return escape string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getVersion,org.apache.hadoop.util.VersionInfo:getVersion(),105,107,"/**
* Returns mask string from common version info.
*/","* Get the Hadoop version.
   * @return the Hadoop version string, eg. ""0.6.3-dev""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getRevision,org.apache.hadoop.util.VersionInfo:getRevision(),113,115,"/**
* Returns function mask string from common version info.
* @return Function mask string.","* Get the Git commit hash of the repository when compiled.
   * @return the commit hash, eg. ""18f64065d5db6208daf50b02c1b5ed4ee3ce547a""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBranch,org.apache.hadoop.util.VersionInfo:getBranch(),121,123,"/**
* Returns function name mask from version info.
*/","* Get the branch on which this originated.
   * @return The branch name, e.g. ""trunk"" or ""branches/branch-0.20""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getDate,org.apache.hadoop.util.VersionInfo:getDate(),129,131,"/**
* Returns the function mask string from common version info. 
* @return Function mask string or null if not available
*/","* The date that Hadoop was compiled.
   * @return the compilation date in unix date format",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUser,org.apache.hadoop.util.VersionInfo:getUser(),137,139,"/**
* Returns the version-specific function mask.
*/","* The user that compiled Hadoop.
   * @return the username of the user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUrl,org.apache.hadoop.util.VersionInfo:getUrl(),145,147,"/**
* Returns the function mask string.
* @return Function mask value as a string
*/","* Get the URL for the Hadoop repository.
   * @return the URL of the Hadoop repository",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,_getBuildVersion,org.apache.hadoop.util.VersionInfo:_getBuildVersion(),85,90,"/**
* Generates a function mask string by concatenating results of four internal methods.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getSrcChecksum,org.apache.hadoop.util.VersionInfo:getSrcChecksum(),153,155,"/**
* Returns the version info mask as a string.
*/","* Get the checksum of the source files from which Hadoop was built.
   * @return the checksum of the source files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getProtocVersion,org.apache.hadoop.util.VersionInfo:getProtocVersion(),170,172,"/**
* Returns function mask string from COMMON_VERSION_INFO.
* @return function mask string
*/","* Returns the protoc version used for the build.
   * @return the protoc version",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getCompilePlatform,org.apache.hadoop.util.VersionInfo:getCompilePlatform(),178,180,"/**
* Returns the function mask string.
* @return Functionality-specific version info as a string
*/","* Returns the OS platform used for the build.
   * @return the OS platform",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",604,610,"/**
* Constructs a filtering iterator from an existing iterator and a predicate function.
* @param source the underlying iterator to be filtered
* @param filter a function that determines whether an element should be included in the iteration
*/","* An iterator which combines filtering with transformation.
     * All source elements for which filter = true are returned,
     * transformed via the mapper.
     * @param source source iterator.
     * @param filter filter predicate.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",521,526,"/**
* Creates an iterator that maps remote data using a given function.
* @param source the original remote iterator
* @param mapper a function to transform data from type S to T
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",677,682,"/**
* Constructs a CloseRemoteIterator from a remote iterator and a closeable resource.
* @param source the remote iterator
* @param toClose the resource to close when iterating is done
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",773,778,"/**
* Constructs a haltable remote iterator with specified source and continuation criteria.
* @param source the underlying iterable data source
* @param continueWork callback to determine whether iteration should be continued or halted","* Wrap an iterator with one which adds a continuation probe.
     * The probe will be called in the {@link #hasNext()} method, before
     * the source iterator is itself checked and in {@link #next()}
     * before retrieval.
     * That is: it may be called multiple times per iteration.
     * @param source source iterator.
     * @param continueWork predicate which will trigger a fast halt if it returns false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator),553,556,"/**
* Constructs a new TypeCastingRemoteIterator instance from a given remote iterator.
* @param source the underlying remote iterator to delegate operations from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext(),634,640,"/**
* Checks if there is a next element in the stream.
* @return true if next element exists, false otherwise
*/","* Trigger a fetch if an entry is needed.
     * @return true if there was already an entry return,
     * or there was not but one could then be retrieved.set
     * @throws IOException failure in fetch operation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object),723,725,"/**
* Initializes this object with the given value, potentially closing it.
* @param o Object to initialize with, may be closed afterwards
*/","* Construct.
     * @param o object to close.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close(),415,419,"/**
 * Calls sourceToClose's m1() method to perform some operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close(),454,457,"/**
* Calls sourceToClose's m1() and handles any resulting IOException. 
* @throws IOException if an error occurs during execution of sourceToClose.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next(),829,837,"/**
* Fetches and increments current mask value.
* @return current mask value or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,submit,"org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)",82,87,"/**
* Executes a callable on an executor and returns a future result of type T.
* @param executor the executor to use for execution
* @param call the callable to execute
* @return CompletableFuture containing result of type T, or null if not found
*/","* Submit a callable into a completable future.
   * RTEs are rethrown.
   * Non RTEs are caught and wrapped; IOExceptions to
   * {@code RuntimeIOException} instances.
   * @param executor executor.
   * @param call     call to invoke
   * @param <T>      type
   * @return the future to wait for",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,<init>,org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE),43,45,"/**
* Creates a lazy auto-closeable reference to an IO resource.
* @param constructor factory function that creates the IO resource
*/","* Constructor for this instance.
   * @param constructor method to invoke to actually construct the inner object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,lazyAtomicReferenceFromSupplier,org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier),148,151,"/**
* Creates a lazy atomic reference with a given supplier.
* @param supplier function to lazily load value of type T
*/","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,eval,org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval(),51,55,"/**
* Calls superclass method with pre-check.
* @throws IOException if reference is closed
*/","* {@inheritDoc}
   * @throws IllegalStateException if the reference is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,apply,org.apache.hadoop.util.functional.LazyAtomicReference:apply(),104,107,"/**
* Returns a mask value using the m1 function.
* @throws IOException if an I/O error occurs during execution.","* Implementation of {@code CallableRaisingIOE.apply()}.
   * Invoke {@link #eval()}.
   * @return the value
   * @throws IOException on any evaluation failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,uncheckIOExceptions,org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE),45,47,"/**
* Invokes a callable function with IOE exception handling.
* @param call Callable function to be executed
*/","* Invoke any operation, wrapping IOExceptions with
   * {@code UncheckedIOException}.
   * @param call callable
   * @param <T> type of result
   * @return result
   * @throws UncheckedIOException if an IOE was raised.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedIOExceptionSupplier,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE),55,57,"/**
* Creates a supplier from a callable that ignores IO exceptions.
* @param call function to execute, ignoring IO exceptions
*/","* Wrap a {@link CallableRaisingIOE} as a {@link Supplier}.
   * @param call call to wrap
   * @param <T> type of result
   * @return a supplier which invokes the call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next(),340,349,"/**
* Returns a value based on m1() result, returning singleton and setting processed to true if m1() returns true; otherwise, throwing a NoSuchElementException. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator),591,593,"/**
* Creates a builder instance from an iterator of remote items.
* @param items Iterator of remote items
*/","* Create a task builder for the remote iterator.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,throwOne,org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection),607,622,"/**
* Merges and prioritizes exceptions in the collection.
* @param exceptions Collection of exception objects
*/","* Throw one exception, adding the others as suppressed
   * exceptions attached to the one thrown.
   * This method never completes normally.
   * @param exceptions collection of exceptions
   * @param <E> class of exceptions
   * @throws E an extracted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,<init>,org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable),161,163,"/**
* Initializes builder with an iterable of items.
* @param items Iterable collection of items to build from
*/","* Create the builder.
     * @param items items to process",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,suppressExceptions,org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(),197,199,"/**
* Returns a new builder instance with default settings.","* Suppress exceptions from tasks.
     * RemoteIterator exceptions are not suppressable.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException),254,257,"/**
* Wraps an ExecutionException into an IOException and re-throws it.
* @param e ExecutionException to be converted
*/","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * This will always raise an exception, either the inner IOException,
   * an inner RuntimeException, or a new IOException wrapping the raised
   * exception.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException),269,272,"/**
* Throws an IOException from a CompletionException.
* @param e CompletionException to rethrow as IOException
*/","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setJobConf,"org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",89,115,"/**
* Configures an object with Hadoop Job configuration.
* @param theObject object to be configured
* @param conf Hadoop Configuration instance
*/","* This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClassByName,org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String),2638,2644,"/**
* Retrieves a class by name, throwing an exception if not found.
* @param name the name of the class to load
* @return the loaded Class object or null if not found (never returned)
*/","* Load a class by name.
   * 
   * @param name the class name.
   * @return the class object.
   * @throws ClassNotFoundException if the class is not found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,printThreadInfo,"org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)",183,221,"/**
* Prints detailed thread dump to the provided PrintStream.
* @param stream output stream
* @param title title of the thread dump section
*/","* Print all of the thread's information and stack traces.
   * 
   * @param stream the stream to
   * @param title a string title for the stack trace",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(boolean),830,836,"/**
* Initializes a new Configuration instance with given default loading behavior.
* @param loadDefaults true to load configuration defaults, false otherwise
*/","A new configuration where the behavior of reading from the default 
   * resources can be turned off.
   * 
   * If the parameter {@code loadDefaults} is false, the new instance
   * will not load resources from the default files. 
   * @param loadDefaults specifies whether to load from the default files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createServletExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)",72,86,"/**
* Sets HTTP response with error JSON data.
* @param response HttpServletResponse object
* @param status HTTP status code
* @param ex Throwable exception instance
*/","* Creates a HTTP servlet response serializing the exception in it as JSON.
   *
   * @param response the servlet response
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @throws IOException thrown if there was an error while creating the
   * response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createJerseyExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)",95,104,"/**
* Creates a standardized error response with exception details.
* @param status HTTP status code for the error
* @param ex Throwable instance containing error information
* @return Error response object in JSON format
*/","* Creates a HTTP JAX-RPC response serializing the exception in it as JSON.
   *
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @return the JAX-RPC response with the set error and JSON encoded exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,throwEx,org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable),119,121,"/**
* Wraps a Throwable into an unchecked RuntimeException.
* @param ex the Throwable to wrap
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32C.java,<init>,org.apache.hadoop.util.PureJavaCrc32C:<init>(),41,43,"/**
 * Initializes Cyclic Redundancy Check-32C (CRC-32C) checksum calculator. 
 */",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,remove,org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object),326,338,"/**
* Checks if the given object conforms to the expected mask.
* @param o the object to check
* @return true if object matches, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(),256,264,"/**
* Returns an array of objects representing function masks.
* @return Array of objects, each containing a function mask value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,retainAll,org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection),372,384,"/**
* Removes elements from the collection that do not match the provided filter.
* @param collection Collection of elements to be filtered
* @return True if any elements were removed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,clear,org.apache.hadoop.util.IntrusiveCollection:clear(),389,395,"/**
* Iterates over entities and applies mask operations.
*/",* Remove all elements.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,containsAll,org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection),340,348,"/**
* Verifies all elements in a collection pass filter m1.
* @param collection Collection of objects to check
* @return true if all elements pass, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CleanerUtil.java,unmapHackImpl,org.apache.hadoop.util.CleanerUtil:unmapHackImpl(),89,160,"/**
* Returns a MethodHandle to unmapping ByteBuffers from the JVM's internal memory.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setIncludesFile,org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String),313,319,"/**
* Updates host details with a new includes file.
* @param includesFile updated includes file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setExcludesFile,org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String),321,328,"/**
* Updates host details with a new excludes file.
* @param excludesFile path to the new excludes file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,updateFileNames,"org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)",330,337,"/**
* Updates host details with new includes and excludes files.
* @param includesFile new file containing include patterns
* @param excludesFile new file containing exclude patterns
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getExcludedHosts,org.apache.hadoop.util.HostsFileReader:getExcludedHosts(),266,269,"/**
* Retrieves all network interface names from the given host details.
* @return set of network interface names
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsForUserCommand,org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String),229,239,"/**
* Generates shell command for fetching user profile information.
* @param user username to fetch info for
*/","* A command to get a given user's groups list.
   * If the OS is not WINDOWS, the command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   *
   * @param user user.
   * @return groups for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsIDForUserCommand,org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String),251,261,"/**
* Builds command line arguments for fetching user group information.
* @param user username to fetch groups for
*/","* A command to get a given user's group id list.
   * The command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   * This command does not support Windows and will only return group names.
   *
   * @param user user.
   * @return groups id for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGetPermissionCommand,org.apache.hadoop.util.Shell:getGetPermissionCommand(),279,282,"/**
* Returns command-line arguments for ls function based on platform.
* @return Array of strings containing 'm1' and/or 'ls' commands
*/","* Return a command to get permission information.
   *
   * @return permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)",291,301,"/**
* Generates chmod command based on permission and recursion flags.
* @param perm file or directory permissions
* @param recursive true for recursive operation, false otherwise
*/","* Return a command to set permission.
   *
   * @param perm permission.
   * @param recursive recursive.
   * @return set permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetOwnerCommand,org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String),326,330,"/**
* Generates platform-specific command array to change ownership.
* @param owner user identifier for ownership change
* @return Command array or null if not found
*/","* Return a command to set owner.
   *
   * @param owner owner.
   * @return set owner command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSymlinkCommand,"org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)",339,343,"/**
* Generates command strings to create a symbolic link.
* @param target path of the file or directory being linked
* @param link path where the symbolic link will be created
*/","* Return a command to create symbolic links.
   *
   * @param target target.
   * @param link link.
   * @return symlink command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getReadlinkCommand,org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String),351,355,"/**
* Returns an array of command(s) to execute based on the operating system.
* @param link path or URL to resolve
*/","* Return a command to read the target of the a symbolic link.
   *
   * @param link link.
   * @return read link command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSignalKillCommand,"org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)",373,394,"/**
* Generates shell command to manipulate process by PID.
* @param code signal code (0 for isAlive, other for kill)
* @param pid unique process identifier
* @return array of shell command arguments or null if unavailable
*/","* Return a command to send a signal to a given pid.
   *
   * @param code code.
   * @param pid pid.
   * @return signal kill command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,appendScriptExtension,"org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)",419,421,"/**
* Recursively generates a file path by appending the result of m1(basename) to the parent directory. 
* @param parent directory where the file will be created
* @param basename base name of the file (will be appended with the result of m1)
*/","* Returns a File referencing a script with the given basename, inside the
   * given parent directory.  The file extension is inferred by platform:
   * <code>"".cmd""</code> on Windows, or <code>"".sh""</code> otherwise.
   *
   * @param parent File parent directory
   * @param basename String script file basename
   * @return File referencing the script in the directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkHadoopHome,org.apache.hadoop.util.Shell:checkHadoopHome(),483,493,"/**
* Returns the Hadoop configuration directory as a File object.
* @throws FileNotFoundException if Hadoop home directory is not found
*/","*  Centralized logic to discover and validate the sanity of the Hadoop
   *  home directory.
   *
   *  This does a lot of work so it should only be called
   *  privately for initialization once per process.
   *
   * @return A directory that exists and via was specified on the command line
   * via <code>-Dhadoop.home.dir</code> or the <code>HADOOP_HOME</code>
   * environment variable.
   * @throws FileNotFoundException if the properties are absent or the specified
   * path is not a reference to a valid directory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHomeDir,org.apache.hadoop.util.Shell:getHadoopHomeDir(),620,627,"/**
* Returns the Hadoop home directory file. If a failure cause is set, throws an exception.
*/","* Get the Hadoop home directory. If it is invalid,
   * throw an exception.
   * @return a path referring to hadoop home.
   * @throws FileNotFoundException if the directory doesn't exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinInner,"org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)",656,685,"/**
* Retrieves executable file path from Hadoop bin directory.
* @param hadoopHomeDir Hadoop installation root
* @param executable target executable name
* @return File object for the executable or throws exception if not found
*/","* Inner logic of {@link #getQualifiedBin(String)}, accessible
   * for tests.
   * @param hadoopHomeDir home directory (assumed to be valid)
   * @param executable executable
   * @return path to the binary
   * @throws FileNotFoundException if the executable was not found/valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getWinUtilsFile,org.apache.hadoop.util.Shell:getWinUtilsFile(),800,808,"/**
* Returns or throws a file based on the state of WINUTILS_FAILURE.
*/","* Get a file reference to winutils.
   * Always raises an exception if there isn't one
   * @return the file instance referring to the winutils bin.
   * @throws FileNotFoundException on any failure to locate that file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,destroyAllShellProcesses,org.apache.hadoop.util.Shell:destroyAllShellProcesses(),1428,1437,"/**
* Synchronizes and iterates over child shells, applying mask operations.
*/","* Static method to destroy all running <code>Shell</code> processes.
   * Iterates through a map of all currently running <code>Shell</code>
   * processes and destroys them one by one. This method is thread safe",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run(),1406,1420,"/**
* Handles process termination and cleanup.
* @throws Exception on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownThread,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread),43,45,"/**
* Waits on the given thread to shut down within the specified timeout.
* @param thread Thread to wait for shutdown
* @return true if thread shut down successfully, false otherwise
*/","* @param thread {@link Thread to be shutdown}
   * @return <tt>true</tt> if the thread is successfully interrupted,
   * <tt>false</tt> otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownExecutorService,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService),78,81,"/**
 * Shuts down the ExecutorService after waiting for a specified duration.
 * @param service ExecutorService to be shut down
 */","* shutdownExecutorService.
   *
   * @param service {@link ExecutorService to be shutdown}
   * @return <tt>true</tt> if the service is terminated,
   * <tt>false</tt> otherwise
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addNewPhase,org.apache.hadoop.util.Progress:addNewPhase(),80,85,"/**
* Calculates progress mask using synchronized functionality.
* @return Progress object representing calculated mask
*/",Adds a new phase. Caller needs to set progress weightage,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(float),107,123,"/**
* Calculates weighted progress based on the given weightage.
* @param weightage float value between 0 and 1 representing weightage
* @return Progress object or null if calculation fails
*/","* Adds a node with a specified progress weightage to the tree.
   *
   * @param weightage weightage.
   * @return Progress.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getInternal,org.apache.hadoop.util.Progress:getInternal(),244,269,"/**
* Calculates weighted progress from all phases.
* @return total progress value or current progress if no phases
*/",Computes progress in this node.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder),282,288,"/**
* Formats status and phase information into the provided StringBuilder.
* @param buffer StringBuilder to append formatted output
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,complete,org.apache.hadoop.util.Progress:complete(),170,184,"/**
* Updates the progress and notifies the parent with method m1.
* @param none
*/","Completes this node, moving the parent node to its next child.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String),260,266,"/**
* Converts a string path to a UTF-8 encoded string representation.
* @param path input string path
* @return encoded string or null on failure
*/","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,"org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)",275,281,"/**
* Converts file system statistics to a string representation.
* @param path file system path
* @param stat file system statistics object
* @return string representation of the statistics or null if failed
*/","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @param stat Output statistics of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setData,"org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)",301,304,"/**
* Recursively processes data with UTF-8 encoding.
* @param path current processing path
* @param data input data to process
* @param version processing version number
*/","* Set data into a ZNode.
   * @param path Path of the ZNode.
   * @param data Data to set as String.
   * @param version Version of the data to store.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,"org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)",343,353,"/**
* Creates a ZNode at the specified path with provided ACL.
* @param path unique node identifier
* @param zkAcl list of Access Control List entries
* @return true if node was successfully created, false otherwise
*/","* Create a ZNode.
   * @param path Path of the ZNode.
   * @param zkAcl ACL for the node.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,delete,org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String),392,398,"/**
* Calls M1 and executes subsequent Curator operations if successful.
* @param path input path to process
* @return true if M1 succeeds, false otherwise
*/","* Delete a ZNode.
   * @param path Path of the ZNode.
   * @return If the znode was deleted.
   * @throws Exception If it cannot contact ZooKeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeCreate,"org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)",410,419,"/**
* Performs a function-specific operation on a device at the specified path.
* @param path device path
* @param data data to be processed
* @param acl access control list
* @param mode create mode
* @param fencingACL fencing ACL
* @param fencingNodePath fencing node path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeDelete,"org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)",429,437,"/**
* Performs a masked operation on the specified path based on given ACLs and node path.
* @param path path to operate on
* @param fencingACL list of access control lists
* @param fencingNodePath path of the fencing node
*/","* Deletes the path. Checks for existence of path as well.
   *
   * @param path Path to be deleted.
   * @param fencingNodePath fencingNodePath.
   * @param fencingACL fencingACL.
   * @throws Exception if any problem occurs while performing deletion.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeSetData,"org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)",439,446,"/**
* Executes a multi-step operation on the cluster by creating a safe transaction.
* @param path path to operate on
* @param data binary data for the operation
* @param version operation version
* @param fencingACL list of ACLs for fencing
* @param fencingNodePath path to the fencing node
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,addClass,"org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)",101,104,"/**
* Registers program with specified name and metadata.
* @param name unique program identifier
* @param mainClass program's primary class type
* @param description program description string
* @throws Throwable if registration fails for any reason
*/","* This is the method that adds the classed to the repository.
   * @param name The name of the string you want the class instance to be called with
   * @param mainClass The class that you want to add to the repository
   * @param description The description of the class
   * @throws NoSuchMethodException when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,impl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])",134,149,"/**
* Resolves class by name and applies builder configuration.
* @param className class name to resolve
* @param types variable number of type arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[]),166,169,"/**
* Creates a new builder instance with specified type parameters.
* @param types variable number of classes to be used as type parameters
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",171,186,"/**
* Constructs a builder for the specified class.
* @param className name of the target class
* @param types variable number of type parameters
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])",348,362,"/**
* Builds a DynConstructors instance for the given target class and argument classes.
* @param targetClass target class
* @param argClasses variable number of argument classes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstanceChecked,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[]),56,66,"/**
* Invokes constructor method with variable arguments and handles exceptions.
* @throws Exception if instantiation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])",73,89,"/**
* Invokes method 'm5' with variable arguments.
* @param target Object to pass as first argument
* @param args Variable number of additional arguments
* @return Result of invoked method, cast to return type R
* @throws Exception if invocation fails or throws an exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])",320,333,"/**
* Builds a function mask using the specified method and argument types.
* @param targetClass class containing the method
* @param methodName name of the method to invoke
* @param argClasses classes of the method's arguments
*/","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,<init>,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)",66,71,"/**
* Initializes an unbound method with a given method and name.
* @param method the underlying method
* @param name the method name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,<init>,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)",46,50,"/**
* Initializes a new instance of a Constructor object.
* @param constructor the actual constructor to be wrapped
* @param constructed the type being constructed by this wrapper
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])",423,438,"/**
* Creates a Builder instance with a dynamically loaded method.
* @param targetClass the class containing the method
* @param methodName the name of the method to load
* @param argClasses variable arguments for the method (optional)
*/","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,noop,org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String),158,160,"/**
* Creates an unbound method with the given name.
* @param name unique method identifier
*/","* Create a no-op method.
   *
   * @param name method name
   *
   * @return a no-op method.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,implemented,org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[]),169,176,"/**
* Checks if any method in the array has a specific unbound property.
* @param methods array of DynMethods.UnboundMethod objects
* @return true if no method has the property, false otherwise
*/","* Given a sequence of methods, verify that they are all available.
   *
   * @param methods methods
   *
   * @return true if they are all implemented",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,available,org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),195,197,"/**
* Checks if a method is unbound.
* @param method UnboundMethod object to check
* @return true if the method is unbound, false otherwise
*/","* Is a method available?
   * @param method method to probe
   * @return true iff the method is found and loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,bind,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object),118,125,"/**
* Binds a method to an object instance.
* @param receiver the target object
*/","* Returns this method as a BoundMethod for the given receiver.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} for this method and the receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,asStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic(),146,149,"/**
* Creates a new instance of StaticMethod.
* @return a new StaticMethod object
*/","* Returns this method as a StaticMethod.
     * @return a {@link StaticMethod} for this method
     * @throws IllegalStateException if the method is not static",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findContainingJar,org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class),38,40,"/**
* Generates a function mask by invoking methods on the provided class.
* @param clazz target class with m1() and m2() methods
*/","* Find a jar that contains a class of the same name, if any.
   * It will return a jar file, even if that is not the first thing
   * on the class path that has a class with the same name.
   * 
   * @param clazz the class to find.
   * @return a jar file that contains the class, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findClassLocation,org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class),48,50,"/**
* Generates a function mask using class methods and parameters.
* @param clazz Class containing m1() and m2() methods
*/","* Find the absolute location of the class.
   *
   * @param clazz the class to find.
   * @return the class file with absolute location, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)",178,212,"/**
* Builds RpcRequestHeaderProto with specified parameters.
* @param rpcKind RPC kind
* @param operation Operation proto
* @param callId Call ID
* @param retryCount Retry count
* @param uuid UUID bytes
* @param alignmentContext Alignment context (optional)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getHeader,org.apache.hadoop.util.DataChecksum:getHeader(),226,235,"/**
* Generates a function mask header.
* @return byte array representing the function mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,mapByteToChecksumType,org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int),204,212,"/**
* Converts an integer type code to a Type enum value, throwing an exception if invalid.
* @param type integer type code
* @throws InvalidChecksumSizeException for unknown or invalid types
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)",246,263,"/**
* Calculates and writes function mask based on data type.
* @param out DataOutputStream for writing
* @param reset flag to trigger additional processing
* @return data type size or 0 on failure
*/","* Writes the current checksum to the stream.
   * If <i>reset</i> is true, then resets the checksum.
   *
   * @param out out.
   * @param reset reset.
   * @return number of bytes written. Will be equal to getChecksumSize();
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)",275,296,"/**
* Calculates and updates checksum mask in buffer.
* @param buf input buffer
* @param offset starting index for calculation
* @param reset whether to perform additional reset operation
* @return size of the resulting mask
*/","* Writes the current checksum to a buffer.
    * If <i>reset</i> is true, then resets the checksum.
    *
    * @param buf buf.
    * @param offset offset.
    * @param reset reset.
    * @return number of bytes written. Will be equal to getChecksumSize();
    * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RateLimitingFactory.java,create,org.apache.hadoop.util.RateLimitingFactory:create(int),95,100,"/**
* Returns rate limiting object based on provided capacity.
* @param capacity maximum allowed requests
*/","* Create an instance.
   * If the rate is 0; return the unlimited rate.
   * @param capacity capacity in permits/second.
   * @return limiter restricted to the given capacity.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SignalLogger.java,register,org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger),71,92,"/**
* Installs UNIX signal handlers for TERM, HUP and INT signals.
* @param log Logger instance for logging
*/","* Register some signal handlers.
   *
   * @param log The log4j logfile to use in the signal handlers.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseItem,"org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)",455,458,"/**
* Creates an Item instance based on input type.
* @param isDigit true for digit-based item, false otherwise
* @param buf string buffer to initialize the item with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,<init>,org.apache.hadoop.util.LightWeightGSet:<init>(int),90,97,"/**
* Initializes a LightWeightGSet with the specified recommended length.
* @param recommended_length target size of the set
*/",* @param recommended_length Recommended size of the internal array.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,get,org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object),126,142,"/**
* Retrieves the functional mask associated with a given key.
* @param key unique identifier
* @return functional mask value or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,clear,org.apache.hadoop.util.LightWeightGSet$Values:clear(),256,259,"/**
* Calls superclass's m1() method.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,toString,org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString(),200,209,"/**
* Generates string representation of SemaphoredDelegatingExecutor.
* @return Human-readable string describing executor's state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToSet,"org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)",77,82,"/**
* Applies mask operation on input data.
* @param type type of mask operation
* @param filename name of the file to process
* @param set collection of strings to apply mask to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readXmlFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",146,183,"/**
* Parses XML input stream and populates map with host-node-timeout entries.
* @param type resource type (e.g., 'server')
* @param filename XML file being parsed
* @param fileInputStream input stream containing XML data
* @param map map to populate with host-node-timeout pairs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHosts,org.apache.hadoop.util.HostsFileReader:getHosts(),261,264,"/**
* Retrieves a set of mask values from the current host details.
* @return Set of String values representing masks
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)",278,283,"/**
* Updates include and exclude masks based on host details.
* @param includes set of included strings
* @param excludes set of excluded strings
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includes set to populate with included hosts
   * @param excludes set to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)",292,298,"/**
* Updates mask for hosts, including/excluding specified IDs.
* @param includeHosts set of host IDs to include in mask
* @param excludeHosts map of host IDs to exclude from mask
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includeHosts set to populate with included hosts
   * @param excludeHosts map to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,hash,"org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)",86,245,"/**
* Computes the FUNC_MASK value based on the provided key, nbytes, and initval.
* @param key cryptographic key
* @param nbytes length of the key in bytes
* @param initval initial value for calculation
* @return computed FUNC_MASK value as an integer
*/","* taken from  hashlittle() -- hash a variable-length key into a 32-bit value
   * 
   * @param key the key (the unaligned variable-length array of bytes)
   * @param nbytes number of bytes to include in hash
   * @param initval can be any integer value
   * @return a 32-bit value.  Every bit of the key affects every bit of the
   * return value.  Two keys differing by one or two bits will have totally
   * different hash values.
   * 
   * <p>The best hash table sizes are powers of 2.  There is no need to do mod
   * a prime (mod is sooo slow!).  If you need less than 32 bits, use a bitmask.
   * For example, if you need only 10 bits, do
   * <code>h = (h &amp; hashmask(10));</code>
   * In which case, the hash table should have hashsize(10) elements.
   * 
   * <p>If you are hashing n strings byte[][] k, do it like this:
   * for (int i = 0, h = 0; i &lt; n; ++i) h = hash( k[i], h);
   * 
   * <p>By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this
   * code any way you wish, private, educational, or commercial.  It's free.
   * 
   * <p>Use for hash table lookup, or anything where one collision in 2^^32 is
   * acceptable.  Do NOT use for cryptographic purposes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(int),75,84,"/**
* Returns a specific hash implementation based on the provided type.
* @param type Hash type (JENKINS_HASH or MURMUR_HASH)
* @return Hash object for specified type, or null if invalid type
*/","* Get a singleton instance of hash function of a given type.
   * @param type predefined hash type
   * @return hash function instance, or null if type is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/MurmurHash.java,hash,"org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)",40,43,"/**
* Wraps existing implementation with alternative parameter order.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(),84,84,"/**
* Initializes a new instance of the Counting Bloom Filter. 
*/",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,org.apache.hadoop.util.bloom.BloomFilter:<init>(),99,101,"/**
 * Initializes a new instance of the BloomFilter class.
 */",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(),113,113,"/**
* Initializes an empty Dynamic Bloom Filter instance. 
*/",* Zero-args constructor for the serialization.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,and,org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter),162,176,"/**
* Updates the Bloom Filter by AND-ing with another CountingBloomFilter.
* @param filter CountingBloomFilter instance to and with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,or,org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter),246,261,"/**
* Updates the Bloom filter by OR-ing it with another.
* @param filter CountingBloomFilter instance to update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,write,org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput),292,299,"/**
* Writes bucket data to output stream.
* @param out DataOutput stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,and,org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter),155,173,"/**
* Performs AND operation on two DynamicBloomFilter objects.
* @param filter the other filter to AND with this instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,not,org.apache.hadoop.util.bloom.DynamicBloomFilter:not(),190,195,"/**
* Recursively calls m1 on each row of the matrix.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,or,org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter),197,214,"/**
* Performs bitwise OR operation on filter matrices.
* @param filter DynamicBloomFilter object to combine with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,xor,org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter),216,233,"/**
* Performs XOR operation on this filter and the provided dynamic Bloom filter.
* @param filter DynamicBloomFilter instance to perform XOR with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,write,org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput),199,216,"/**
* Serializes user data to output stream, using bitwise flags and byte packing.
* @param out DataOutput stream for serialization
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,hash,org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key),108,122,"/**
* Computes an array of hash values for a given Key object.
* @param k the input key
* @return array of int hash values or throws exception on invalid input
*/","* Hashes a specified key into several integers.
   * @param k The specified key.
   * @return The array of hashed values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,compareTo,org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key),172,183,"/**
* Calculates the functional difference between two Key objects.
* @param other the other key object to compare with
* @return a unique integer representing their difference or 0 if identical
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,getWeight,org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List),381,387,"/**
* Calculates weighted sum of individual 'm1' values.
* @param keyList list of Key objects with m1() values to be summed
* @return total weighted sum as a double value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,formatMessage,"org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)",118,143,"/**
* Analyzes JVM pauses by comparing GC activity before and after a specified sleep time.
* @param extraSleepTime duration of the sleep period
* @param gcTimesAfterSleep GC times after the sleep period
* @param gcTimesBeforeSleep GC times before the sleep period
* @return detailed report of pause detection and GC differences",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,<init>,org.apache.hadoop.util.AutoCloseableLock:<init>(),38,40,"/**
 * Initializes a new instance of AutoCloseableLock using a ReentrantLock.","* Creates an instance of {@code AutoCloseableLock}, initializes
   * the underlying lock instance with a new {@code ReentrantLock}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,close,org.apache.hadoop.util.AutoCloseableLock:close(),94,97,"/**
* Calls the m1 method to perform some unknown operation.
*/","* Attempts to release the lock by making a call to {@code release()}.
   *
   * This is to implement {@code close()} method from {@code AutoCloseable}
   * interface. This allows users to user a try-with-resource syntax, where
   * the lock can be automatically released.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,isNull,org.apache.hadoop.util.ComparableVersion$StringItem:isNull(),208,211,"/**
 * Checks if the mask value is valid based on the release version index.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item),232,253,"/**
* Resolves Item value based on type, handling null items recursively.
* @param item the Item to resolve (null triggers recursive resolution)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,printStack,"org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])",234,237,"/**
* Logs an error message with exception details.
* @param e Throwable instance to log
* @param text error message template
* @param args variable arguments for the message
*/","* print a stack trace with text
   * @param e the exception to print
   * @param text text to print",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,explainResult,"org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)",368,370,"/**
* Logs an error message with a formatted string.
* @param errorcode unique error code
* @param text descriptive error message
*/","* Explain an error code as part of the usage
   * @param errorcode error code returned
   * @param text error text",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadedClass,"org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)",266,271,"/**
* Loads a class by name and logs its source URL.
* @param name class name
* @param clazz Class object
*/","* Log that a class has been loaded, and where from.
   * @param name classname
   * @param clazz class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,calculateGCTimePercentageWithinObservedInterval,org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval(),186,224,"/**
* Calculates and updates garbage collection metrics.
* @param gcBeans list of GarbageCollectorMXBean objects
* @param endIdx current buffer index
* @param bufSize buffer size
* @param startTime start time in milliseconds
* @param observationWindowMs observation window duration in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,getLatestGcData,org.apache.hadoop.util.GcTimeMonitor:getLatestGcData(),182,184,"/**
* Returns a GcData instance with mask applied.
* @return GcData object with mask applied to current data
*/","* Returns a copy of the most recent data measured by this monitor.
   * @return a copy of the most recent data measured by this monitor",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32.java,<init>,org.apache.hadoop.util.PureJavaCrc32:<init>(),45,47,"/**
* Initializes CRC-32 calculator with default settings.
*/",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,executeShutdown,org.apache.hadoop.util.ShutdownHookManager:executeShutdown(),117,136,"/**
* Iterates over hooks and checks for timeouts.
* @return number of timed out hooks
*/","* Execute the shutdown.
   * This is exposed purely for testing: do not invoke it.
   * @return the number of shutdown hooks which timed out.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,put,org.apache.hadoop.util.PriorityQueue:put(java.lang.Object),61,65,"/**
* Adds an element to the heap and updates its mask.
* @param element T object to add
*/","* Adds an Object to a PriorityQueue in log(size) time.
   * If one tries to add more objects than maxSize from initialize
   * a RuntimeException (ArrayIndexOutOfBound) is thrown.
   * @param element element.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,pop,org.apache.hadoop.util.PriorityQueue:pop(),104,114,"/**
* Retrieves the top element from a min-heap, updating its contents.
* @return the removed top element or null if empty
*/","* Removes and returns the least element of the PriorityQueue in log(size)
      time.
   * @return T Generics Type T.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,adjustTop,org.apache.hadoop.util.PriorityQueue:adjustTop(),123,125,"/**
* Executes function m1 to apply mask operation.","Should be called when the Object at top changes values.  Still log(n)
   * worst case, but it's at least twice as fast to <pre>
   *  { pq.top().change(); pq.adjustTop(); }
   * </pre> instead of <pre>
   *  { o = pq.pop(); o.change(); pq.push(o); }
   * </pre>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,addAll,"org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)",161,171,"/**
* Recursively adds a collection of comparable elements to a TreeSet.
* @param addTo the TreeSet to add elements to
* @param elementsToAdd iterable collection of elements to add
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator),191,195,"/**
* Creates and populates a hash set with iterator's elements.
* @param elements iterator over elements to add
* @return populated hash set
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set and then
   * calling Iterators#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterator) instead.</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, you should create
   * an {@link EnumSet} instead.</p>
   *
   * <p>Overall, this method is not very useful and will likely be deprecated
   * in the future.</p>
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return a new, empty thread-safe {@code Set}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSetWithExpectedSize,org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int),213,215,"/**
* Creates an initial set with estimated size based on provided capacity.
* @param expectedSize anticipated number of elements
*/","* Returns a new hash set using the smallest initial table size that can hold
   * {@code expectedSize} elements without resizing. Note that this is not what
   * {@link HashSet#HashSet(int)} does, but it is what most users want and
   * expect it to do.
   *
   * <p>This behavior can't be broadly guaranteed, but has been tested with
   * OpenJDK 1.7 and 1.8.</p>
   *
   * @param expectedSize the number of elements you expect to add to the
   *     returned set
   * @param <E> Generics Type E.
   * @return a new, empty hash set with enough capacity to hold
   *     {@code expectedSize} elements without resizing
   * @throws IllegalArgumentException if {@code expectedSize} is negative",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SequentialNumber.java,skipTo,org.apache.hadoop.util.SequentialNumber:skipTo(long),78,91,"/**
* Updates the current value to a specified mask, ensuring forward progress.
* @param newValue new mask value
*/","* Skip to the new value.
   * @param newValue newValue.
   * @throws IllegalStateException
   *         Cannot skip to less than the current value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,printUsage,org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map),85,91,"/**
* Prints valid program names and descriptions.
* @param programs map of program names to their descriptions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/CommandShell.java,run,org.apache.hadoop.tools.CommandShell:run(java.lang.String[]),63,84,"/**
* Executes a function with custom logic, handling subcommand-specific behavior and errors.
* @param args array of command-line arguments
* @return non-zero exit code on failure or error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,<init>,"org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",52,58,"/**
* Initializes a column with the specified title and settings.
* @param title The column's title
* @param justification The text justification (e.g. left, center, right)
* @param wrap Whether to wrap long titles within the column
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,build,org.apache.hadoop.tools.TableListing$Builder:build(),194,197,"/**
* Creates a table listing with default columns and settings. 
* @return TableListing object","* Create a new TableListing.
     *
     * @return TableListing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$7:getDefault(double),522,522,"/**
* Applies a function to mask values outside a specified range.
* @param value input value
* @return masked value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$3:getDefault(double),522,522,"/**
* Applies a mask function to a given double value. 
* The concrete implementation of this function is left up to subclasses.
* @param value input value subject to transformation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,isDeprecated,org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String),670,672,"/**
* Checks if a given key is deprecated.
* @param key unique identifier to check
*/","* checks whether the given <code>key</code> is deprecated.
   * 
   * @param key the parameter which is to be checked for deprecation
   * @return <code>true</code> if the key is deprecated and 
   *         <code>false</code> otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKeyInfo,org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String),678,680,"/**
* Retrieves deprecated key info using FUNC_MASK logic.
* @param key input string to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpDeprecatedKeys,org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys(),4009,4019,"/**
* Prints deprecated keys with updated masks.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,hasWarnedDeprecation,org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String),4027,4035,"/**
* Checks if a name is masked due to deprecation.
* @param name the name to check
*/","* Returns whether or not a deprecated name has been warned. If the name is not
   * deprecated then always return false
   * @param name proprties.
   * @return true if name is a warned deprecation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKey,org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String),674,676,"/**
* Generates function key mask using deprecation context.
* @param key input string to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,reloadExistingConfigurations,org.apache.hadoop.conf.Configuration:reloadExistingConfigurations(),880,888,"/**
* Reloads all existing configurations in the registry.
*/",* Reload existing configuration instances.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDefaultResource,org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String),895,904,"/**
* Applies mask to configuration based on given name.
* @param name unique identifier for masking operation
*/","* Add a default resource. Resources are loaded in the order of the resources 
   * added.
   * @param name file name. File should be present in the classpath.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)",257,259,"/**
* Constructs a new Resource instance with optional restricted parser usage.
* @param resource object to be wrapped as Resource
* @param useRestrictedParser whether to use restricted parser or not
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1951,1969,"/**
* Converts time duration string to specified unit and returns value.
* @param name identifier for logging purposes
* @param vStr time duration string (e.g., ""1h"", ""2d"")
* @param defaultUnit default time unit if string is invalid
* @param returnUnit target time unit for result
* @return converted long value or throws error if precision lost","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param defaultUnit Unit to convert the stored property, if it exists.
   * @param returnUnit Unit for the returned value.
   * @return time duration in given time unit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parse,"org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)",3045,3063,"/**
* Parses XML from a given URL.
* @param url the URL to parse
* @param restricted whether parsing is restricted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleInclude,org.apache.hadoop.conf.Configuration$Parser:handleInclude(),3296,3372,"/**
* Parses XInclude for resources, fetching and processing included content.
* @throws XMLStreamException if parsing fails
* @throws IOException on resource access errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProperty,"org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",3546,3568,"/**
* Updates a property with a mask or default value.
* @param properties the properties to update
* @param name property name
* @param attr attribute name
* @param value new value (or null for default)
* @param finalParameter whether this is a final parameter update
* @param source optional array of sources for the change
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,toString,org.apache.hadoop.conf.Configuration:toString(),3901,3913,"/**
* Builds configuration string by merging default and user settings.
* @return formatted string or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAllPropertiesByTags,org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List),4056,4062,"/**
* Creates a properties object from a list of tags.
* @param tagList collection of tag names
*/","* Get all properties belonging to list of input tags. Calls
   * getAllPropertiesByTag internally.
   * @param tagList list of input tags
   * @return Properties with matching tags",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$6:getDefault(double),522,522,"/**
* Applies a mathematical function to mask values within a specific range.
* @param value input value subject to masking
* @return transformed value or original if outside masked range
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$2:getDefault(double),522,522,"/**
* Applies a mathematical function to mask the input value.
* @param value input value to be transformed
* @return result of applying the function (implementation-specific)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redact,"org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)",65,70,"/**
* Masks sensitive text in key-value pairs.
* @param key unique identifier
* @param value textual data to be processed
* @return original text or redacted text if key matches filter criteria
*/","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redactXml,"org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)",97,102,"/**
* Masks sensitive data by returning a redacted string when key matches a specific condition.
* @param key the input key to check
* @param value the input value to mask or return as is
*/","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getReconfigurationTaskStatus,org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus(),189,196,"/**
* Returns current reconfiguration task status with mask applied.
* @return ReconfigurationTaskStatus object indicating task state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$4:getDefault(double),522,522,"/**
* Applies a function to a double value and returns the result.
* @param value input value to be processed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,startReconfigurationTask,org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask(),169,187,"/**
* Initiates server reconfiguration task.
* @throws IOException if server is stopped or another task is running
*/","* Start a reconfiguration task to reload configuration in background.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])",487,517,"/**
* Constructs a DeprecationContext from another context and deltas.
* @param other existing DeprecationContext to copy from
* @param deltas array of DeprecationDelta updates to apply
*/","* Create a new DeprecationContext by copying a previous DeprecationContext
     * and adding some deltas.
     *
     * @param other   The previous deprecation context to copy, or null to start
     *                from nothing.
     * @param deltas  The deltas to apply.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)",67,74,"/**
* Constructs a ReconfigurationException with detailed property configuration values.
* @param property name of the reconfigured property
* @param newVal new value of the reconfigured property
* @param oldVal original value of the reconfigured property
* @param cause underlying exception causing the reconfiguration issue
*/","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.
   * @param cause original exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)",82,88,"/**
* Constructs a ReconfigurationException with detailed error message.
* @param property affected system property
* @param newVal new value of the property
* @param oldVal original value of the property
*/","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndProperty,org.apache.hadoop.conf.Configuration$Parser:handleEndProperty(),3415,3446,"/**
* Processes configuration mask for a given name.
* @param confName configuration name
* @param fallbackEntered whether to use fallback values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getWarningMessage,org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String),382,384,"/**
* Returns cached result of m1(key, value) with value set to null.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration$IntegerRanges:iterator(),2283,2286,"/**
* Returns an iterator over a range of integer values.
*@return Iterator of integers within specified ranges.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String),2195,2217,"/**
* Parses comma-separated integer range strings into Range objects.
* @param newValue string of comma-delimited range values (e.g., 1-5,10)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageSize.java,parse,org.apache.hadoop.conf.StorageSize:parse(java.lang.String),50,96,"/**
* Parses string representation of storage size into StorageSize object.
* @param value string representation of storage size (e.g. ""1000MB"")
* @return parsed StorageSize object or throws IllegalArgumentException if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$5:getDefault(double),522,522,"/**
* Applies a function to mask values within a specified range.
* @param value input value to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$1:getDefault(double),522,522,"/**
* Applies a mask function to a given numeric value.
* @param value input number
* @return transformed value (type depends on implementing subclass)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reset,org.apache.hadoop.ha.ActiveStandbyElector:reset(),931,934,"/**
* Initializes system state to INIT and calls m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),144,162,"/**
* Maps StateChangeRequestInfo to HAStateChangeRequestInfoProto.
* @param reqInfo StateChangeRequestInfo object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto),87,105,"/**
* Converts HAStateChangeRequestInfoProto to StateChangeRequestInfo with determined RequestSource.
* @param proto HAStateChangeRequestInfoProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,createReqInfo,org.apache.hadoop.ha.HAAdmin:createReqInfo(),264,266,"/**
* Creates a new StateChangeRequestInfo instance with given request source.
* @return new StateChangeRequestInfo object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,createReqInfo,org.apache.hadoop.ha.ZKFailoverController:createReqInfo(),510,512,"/**
* Creates a StateChangeRequestInfo instance with RequestSource.REQUEST_BY_ZKFC.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,createReqInfo,org.apache.hadoop.ha.FailoverController:createReqInfo(),158,160,"/**
* Creates a new StateChangeRequestInfo with request source.
* @return StateChangeRequestInfo object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getServiceStatus,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)",143,178,"/**
* Retrieves HA service status and creates a response proto.
* @param request GetServiceStatusRequestProto object
* @return GetServiceStatusResponseProto object or throws ServiceException if error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,startRPC,org.apache.hadoop.ha.ZKFailoverController:startRPC(),336,338,"/**
* Sends RPC request to server using m1 method.
* @throws IOException on communication error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,parseConfiggedPort,org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String),258,266,"/**
* Converts port string to integer value.
* @param portStr string representation of port number
* @return port number as int or throws exception if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,checkArgs,org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String),72,79,"/**
* Validates and processes a mask argument for the shell fencing method.
* @param args input string containing the mask value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)",138,149,"/**
* Prints function mask with optional arguments.
* @param pStr output stream to print to
* @param cmd command name
* @param helpEntries map of usage info for commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkManualStateManagementOK,org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget),245,262,"/**
* Enables or disables manual HA state management based on automatic failover settings.
* @param target HAServiceTarget to evaluate
* @return true if manual management allowed, false otherwise
*/","* Ensure that we are allowed to manually manage the HA state of the target
   * service. If automatic failover is configured, then the automatic
   * failover controllers should be doing state management, and it is generally
   * an error to use the HAAdmin command line to do so.
   * 
   * @param target the target to check
   * @return true if manual state management is allowed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,execCommand,"org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)",177,203,"/**
* Executes a command via SSH, capturing output and error streams.
* @param session SSH connection object
* @param cmd command to execute
* @return exit status of the executed command
*/","* Execute a command through the ssh session, pumping its
   * stderr and stdout to our own logs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,enteredState,org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State),988,992,"/**
 * Updates health monitor state by invoking specific logic based on new state. 
 * @param newState updated HealthMonitor state",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,badArg,org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String),272,276,"/**
* Throws an exception with a custom error message and logs a warning.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,checkEligibleForFailover,org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover(),763,776,"/**
* Verifies service health and state for failover eligibility.
* @throws ServiceFailedException if unhealthy or not eligible
*/","* If the local node is an observer or is unhealthy it
   * is not eligible for graceful failover.
   * @throws ServiceFailedException if the node is an observer or unhealthy",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,fence,org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget),92,94,"/**
 * Calls m1 with null as the second parameter.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getFencingParameters,org.apache.hadoop.ha.HAServiceTarget:getFencingParameters(),175,179,"/**
* Initializes and configures map with default settings.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,monitorActiveStatus,org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus(),774,781,"/**
* Resets retry count and monitors active leader.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)",1145,1159,"/**
* Executes a ZooKeeper action with retries on KeeperExceptions.
* @param action ZKAction to execute
* @param retryCode Code for retrying operations
* @return result of action execution or throws exception if max retries exceeded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readInDirectBuffer,"org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)",189,216,"/**
* Reads data from a range into a ByteBuffer using the provided function.
* @param range FileRange object specifying the data to be read
* @param buffer ByteBuffer to store the read data
* @param operation Function to process the read data
*/","* Read bytes from stream into a byte buffer using an
   * intermediate byte array.
   *   <pre>
   *     (position, buffer, buffer-offset, length): Void
   *     position:= the position within the file to read data.
   *     buffer := a buffer to read fully `length` bytes into.
   *     buffer-offset := the offset within the buffer to write data
   *     length := the number of bytes to read.
   *   </pre>
   * The passed in function MUST block until the required length of
   * data is read, or an exception is thrown.
   * @param range range to read
   * @param buffer buffer to fill.
   * @param operation operation to use for reading data.
   * @throws IOException any IOE.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateAndSortRanges,"org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)",292,337,"/**
* Validates and sorts input file ranges, checking for overlaps and ensuring they fit within a given file length.
* @param input list of FileRange objects
* @param fileLength optional file length to validate against
* @return sorted list of validated FileRange objects or null if empty
*/","* Validate a list of ranges (including overlapping checks) and
   * return the sorted list.
   * <p>
   * Two ranges overlap when the start offset
   * of second is less than the end offset of first.
   * End offset is calculated as start offset + length.
   * @param input input list
   * @param fileLength file length if known
   * @return a new sorted list.
   * @throws IllegalArgumentException if there are overlapping ranges or
   * a range element is invalid (other than with negative offset)
   * @throws EOFException if the last range extends beyond the end of the file supplied
   *                          or a range offset is negative",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,readVectored,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)",319,345,"/**
* Performs asynchronous file I/O operations on a list of ranges.
* @param ranges list of FileRange objects to process
* @param allocate function to allocate memory buffers for each range
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData),49,53,"/**
* Initializes a new instance of BlockManager with given BlockData.
* @param blockData non-null BlockData object
*/","* Constructs an instance of {@code BlockManager}.
   *
   * @param blockData information about each block of the underlying file.
   *
   * @throws IllegalArgumentException if blockData is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),107,111,"/**
* Applies mask to buffer data.
* @param data BufferData object to modify
*/","* Releases resources allocated to the given block.
   *
   * @param data the {@code BufferData} to release.
   *
   * @throws IllegalArgumentException if data is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,release,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object),88,111,"/**
* Removes the specified item from the pool, synchronizing with createdItems.
* @param item item to be removed
*/","* Releases a previously acquired resource.
   *
   * @throws IllegalArgumentException if item is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,throwIfStateIncorrect,org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),259,275,"/**
* Validates a variable number of states against expected values.
* @param states array of State objects
*/","* Helper that asserts the current state is one of the expected values.
   *
   * @param states the collection of allowed states.
   *
   * @throws IllegalArgumentException if states is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)",103,109,"/**
* Validates and sanitizes string argument.
* @param arg input value to validate
* @param argName name of the argument being validated
*/","* Validates that the given string is not null and has non-zero length.
   * @param arg the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNumberOfElements,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)",182,192,"/**
* Validates the number of elements in a collection.
* @param collection input collection
* @param numElements expected count
* @param argName name of the collection
*/","* Validates that the given set is not null and has an exact number of items.
   * @param <T> the type of collection's elements.
   * @param collection the argument reference to validate.
   * @param numElements the expected number of elements in the collection.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExists,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)",346,350,"/**
* Validates and sets file system mask for the given path.
* @param path Path to validate
* @param argName Name of the argument being validated
*/","* Validates that the given path exists.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,<init>,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int),57,65,"/**
* Initializes a bounded resource pool with the specified capacity.
* @param size maximum number of resources in the pool
*/","* Constructs a resource pool of the given size.
   *
   * @param size the size of this pool. Cannot be changed post creation.
   *
   * @throws IllegalArgumentException if size is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)",90,108,"/**
* Initializes a BufferPool with specified size and bufferSize.
* @param size total number of buffers in pool
* @param bufferSize size of each buffer
* @param prefetchingStatistics statistics for memory allocation tracking
*/","* Initializes a new instance of the {@code BufferPool} class.
   * @param size number of buffer in this pool.
   * @param bufferSize size in bytes of each buffer.
   * @param prefetchingStatistics statistics for this stream.
   * @throws IllegalArgumentException if size is zero or negative.
   * @throws IllegalArgumentException if bufferSize is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int),120,124,"/**
 * Sets mask bit for a given block.
 * @param blockNumber unique block identifier
 */","* Requests optional prefetching of the given block.
   *
   * @param blockNumber the id of the block to prefetch.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)",111,118,"/**
* Initializes a BufferData object with the specified block number and data.
* @param blockNumber unique block identifier
* @param buffer ByteBuffer containing block data
*/","* Constructs an instances of this class.
   *
   * @param blockNumber Number of the block associated with this buffer.
   * @param buffer The buffer associated with this block.
   *
   * @throws IllegalArgumentException if blockNumber is negative.
   * @throws IllegalArgumentException if buffer is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Retryer.java,<init>,"org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)",55,63,"/**
* Initializes a Retryer with customizable delay and update intervals.
* @param perRetryDelay delay between retries in milliseconds
* @param maxDelay maximum total retry delay in milliseconds
* @param statusUpdateInterval interval to update retry status in seconds
*/","* Initializes a new instance of the {@code Retryer} class.
   *
   * @param perRetryDelay per retry delay (in ms).
   * @param maxDelay maximum amount of delay (in ms) before retry fails.
   * @param statusUpdateInterval time interval (in ms) at which status update would be made.
   *
   * @throws IllegalArgumentException if perRetryDelay is zero or negative.
   * @throws IllegalArgumentException if maxDelay is less than or equal to perRetryDelay.
   * @throws IllegalArgumentException if statusUpdateInterval is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int),243,245,"/**
* Masks the specified block number within valid range.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long),247,249,"/**
* Masks file content at specified offset.
* @param offset starting position to mask in bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)",117,120,"/**
* Applies masks to array elements and updates internal state.
* @param array input array of type T
* @param argName name of the parameter for logging purposes
*/","* Validates that the given array is not null and has at least one element.
   * @param <T> the type of array's elements.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)",127,130,"/**
* Applies mask operations to input byte array and logs result.
* @param array input data to be processed
* @param argName name of the input parameter for logging purposes
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)",137,140,"/**
* Applies masks to short array using multiple methods.
* @param array input short array
* @param argName name of the argument being masked
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)",147,150,"/**
* Applies mask operations to an integer array.
* @param array input array of integers
* @param argName name of the argument being masked (for logging purposes)
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)",157,160,"/**
* Applies mask operations to an array and updates its length.
* @param array input array to modify
* @param argName name of the argument being processed
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)",168,173,"/**
* Masks function arguments based on iterable size.
* @param iter Iterable of elements to check
* @param argName Name of the argument being masked
*/","* Validates that the given buffer is not null and has non-zero capacity.
   * @param <T> the type of iterable's elements.
   * @param iter the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/DefaultBulkDeleteOperation.java,bulkDelete,org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection),74,91,"/**
* Processes a collection of paths and returns a list of entries containing the processed path and any errors encountered.
*@param paths Collection of paths to process
*/","* {@inheritDoc}.
     * The default impl just calls {@code FileSystem.delete(path, false)}
     * on the single path in the list.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,applyToIOStatisticsSnapshot,"org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)",339,344,"/**
* Applies function to IO statistics snapshot of given source.
* @param source data source (must be serializable)
* @param fun function to apply
* @return result of applying function or null if fails
*/","* Apply a function to an object which may be an IOStatisticsSnapshot.
   * @param <T> return type
   * @param source statistics snapshot
   * @param fun function to invoke if {@code source} is valid.
   * @return the applied value
   * @throws UncheckedIOException Any IO exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,<init>,"org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)",83,92,"/**
* Initializes FlagSet with given class, prefix and optional initial flag set.
* @param enumClass the enumeration class
* @param prefix the prefix for flag names
* @param flags the initial EnumSet of flags (null to create an empty one)
*/","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags. A copy of these are made.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seek,org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long),60,67,"/**
* Sets the current file position to a specified offset.
* @param position the desired file offset
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,available,org.apache.hadoop.fs.sftp.SFTPInputStream:available(),69,77,"/**
* Calculates the maximum mask value based on remaining content length.
* @return Maximum integer value or Integer.MAX_VALUE if too large
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,close,org.apache.hadoop.fs.FileSystem:close(),2701,2712,"/**
* Closes the object and updates cache with object identity hash. 
* @throws IOException if an I/O error occurs during closing
*/","* Close this FileSystem instance.
   * Will release any held locks, delete all files queued for deletion
   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance
   * from the cache, if cached.
   *
   * After this operation, the outcome of any method call on this FileSystem
   * instance, or any input/output stream created by it is <i>undefined</i>.
   * @throws IOException IO failure",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object),153,160,"/**
* Compares this node with another for equality based on the m1() result.
* @param o object to compare against
* @return true if both nodes have equal m1() values, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listStatus,org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path),567,580,"/**
* Retrieves file statuses for a given path, filtering out unresolved files.
* @param f Path to query
* @return Array of resolved FileStatus objects or empty array if none found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object),425,429,"/**
* Recursively calls m1 with FileStatus parameter.
* @param o FileStatus object to process
*/","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * This method was added back by HADOOP-14683 to keep binary compatibility.
   *
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.
   * @throws ClassCastException if the specified object is not FileStatus",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,compareTo,org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),181,184,"/**
* Calls superclass's implementation of m1.
* @param o FileStatus object to process
* @return result of superclass's m1 invocation
*/","* Compare this FileStatus to another FileStatus
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,"org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)",133,138,"/**
* Recursively generates an array of Paths containing the provided path and its ancestors.
* @param stats FileStatus array (not used in this implementation)
* @param path the starting file or directory path
* @return Path[] containing the original path and its parent directories
*/","* convert an array of FileStatus to an array of Path.
   * If stats if null, return path
   * @param stats
   *          an array of FileStatus objects
   * @param path
   *          default path to return in stats is null
   * @return an array of paths corresponding to the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),1300,1303,"/**
* Calls file system's m1 method on the given path.
* @param f Path object to process
* @return short result from file system's m1 method
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),447,450,"/**
* Calls underlying file system's m1 method.
* @param f the Path object to operate on
* @return the result of m1 operation (short value)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run(),4166,4181,"/**
* Runs a loop until thread termination is signaled.
* Waits for and processes StatisticsDataReference objects from queue,
* periodically cleaning up resources.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,openConnection,org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL),46,49,"/**
* Creates a new FsUrlConnection instance.
* @param url URL to connect to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,read,"org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)",66,89,"/**
* Reads data from the underlying resource.
* @param position current file position
* @param buffer byte array to fill with data
* @param offset starting offset in the buffer
* @param length number of bytes to read
* @return actual number of bytes read or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",896,905,"/**
* Returns true if the given file system supports read-only connector.
* @param path file system path
* @param capability file system capability
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,serializer,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer(),262,264,"/**
* Creates a JSON serializer for IO statistics snapshot.
* @return JsonSerialization object for IO statistics snapshot
*/","* Get a JSON serializer for this class.
   * @return a serializer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,publishAsStorageStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",701,704,"/**
* Creates StorageStatistics object from given IOStatistics.
* @param name storage identifier
* @param scheme storage scheme (e.g. file system)
* @param source input statistics
*/","* Publish the IOStatistics as a set of storage statistics.
   * This is dynamic.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param source IOStatistics source.
   * @return a dynamic storage statistics object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStorageStatistics,org.apache.hadoop.fs.FileSystem:getStorageStatistics(),4651,4653,"/**
* Creates an empty storage statistics object with specified masks.
* @return EmptyStorageStatistics object
*/","* Get the StorageStatistics for this FileSystem object.  These statistics are
   * per-instance.  They are not shared with any other FileSystem object.
   *
   * <p>This is a default method which is intended to be overridden by
   * subclasses. The default implementation returns an empty storage statistics
   * object.</p>
   *
   * @return    The StorageStatistics for this FileSystem instance.
   *            Will never be null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String),24,26,"/**
* Constructs a PathIsDirectoryException with specified file path and reason.
* @param path the file path that caused the exception
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String),24,26,"/**
* Constructs an exception indicating that a given path is not a directory.
* @param path the invalid file path
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathOperationException.java,<init>,org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String),24,26,"/**
* Constructs an exception for unsupported file system operations.
* @param path the affected file path
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String),23,25,"/**
* Constructs an exception indicating that a directory with specified path is not empty.
* @param path absolute or relative path to non-empty directory
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,bufferSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int),175,178,"/**
* Initializes buffer size and returns mask value.
* @param bufSize new buffer size
* @return mask value
*/","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,replication,org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short),190,193,"/**
* Calculates and returns the FUNC_MASK value based on replication settings.
* @param replica short integer representing the replication configuration
*/","* Set replication factor.
   *
   * @param replica replica.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,blockSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long),205,208,"/**
* Calculates and returns the function mask value based on block size.
* @param blkSize block size in bytes
*/","* Set block size.
   *
   * @param blkSize block size.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,recursive,org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive(),224,227,"/**
* Returns a mask based on recursive calculations.
*/","* Create the parent directory if they do not exist.
   *
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,create,org.apache.hadoop.fs.FSDataOutputStreamBuilder:create(),254,257,"/**
* Creates and returns a function mask.
* Returns the result of calling m2().","* Create an FSDataOutputStream at the specified path.
   *
   * @return return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,overwrite,org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean),267,274,"/**
* Creates a mask with optional overwrite flag.
* @param overwrite true to create overwrite flag, false otherwise
*/","* Set to true to overwrite the existing file.
   * Set it to false, an exception will be thrown when calling {@link #build()}
   * if the file exists.
   *
   * @param overwrite overrite.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,append,org.apache.hadoop.fs.FSDataOutputStreamBuilder:append(),281,284,"/**
 * Generates and returns a mask based on specified flag settings.
 */","* Append to an existing file (optional operation).
   *
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,hashCode,org.apache.hadoop.fs.permission.FsCreateModes:hashCode(),103,108,"/**
* Recursively calculates and returns the result of m1() by combining 
* the parent class's result with the result of m2().m1().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)",161,197,"/**
* Initializes BlockLocation object with provided data.
* @param names       array of block names
* @param hosts       array of hostnames
* @param cachedHosts array of cached hostnames
* @param topologyPaths array of topology paths
* @param storageIds    array of storage IDs
* @param storageTypes  array of storage types
* @param offset        offset value
* @param length        block length
* @param corrupt       flag indicating corrupted block
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setHosts,org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[]),312,318,"/**
* Interprets and stores host array, either from input or interning empty array.
* @param hosts input host array (or null to replace with empty array)
*/","* Set the hosts hosting this block.
   * @param hosts hosts array.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setCachedHosts,org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[]),324,330,"/**
* Initializes or updates the cached hosts array.
* @param cachedHosts optional array of host strings to interner
*/","* Set the hosts hosting a cached replica of this block.
   * @param cachedHosts cached hosts.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setNames,org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[]),337,343,"/**
* Interns and caches an array of names for efficient reuse.
* @param names array of strings to interner
*/","* Set the names (host:port) hosting this block.
   * @param names names.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setTopologyPaths,org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[]),351,357,"/**
* Initializes or updates the topology paths array.
* @param topologyPaths array of topology path strings
*/","* Set the network topology paths of the hosts.
   *
   * @param topologyPaths topology paths.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setStorageIds,org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[]),359,365,"/**
* Updates internal storage IDs array.
* @param storageIds array of external storage IDs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathInternal,org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData),382,388,"/**
* Performs masked operations on a PathData object.
* @param item PathData object to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),250,272,"/**
* Applies mask operation to PathData based on configuration flags.
* @param item PathData object to modify
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processPath,org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData),91,155,"/**
* Formats file metadata into a string using the provided format template.
*@param item PathData object containing file status and path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isFile,org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path),1895,1902,"/**
* Attempts to evaluate file metadata. If file not found, returns false.
* @param f Path to the file
*/","True iff the named path is a regular file.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by {@link #getFileStatus(Path)} or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is file true, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isFile,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile(),479,482,"/**
* Delegates to realStatus.m1() and returns result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isFile,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile(),55,58,"/**
* Calls underlying file system's m1 method and returns result.
* @return true if successful, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,toString,org.apache.hadoop.fs.FileStatus:toString(),458,488,"/**
* Builds and returns a JSON string representing file information.
* @return JSON string or null if building fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink(),539,542,"/**
* Returns real status path.
* @throws IOException on error 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink(),115,118,"/**
 * Calls underlying file system implementation to perform operation 'm1'.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)",64,71,"/**
* Constructs FsServerDefaults object with specified configuration.
* @param blockSize block size in bytes
* @param bytesPerChecksum checksum size in bytes
* @param writePacketSize write packet size
* @param replication number of replicas
* @param fileBufferSize file buffer size
* @param encryptDataTransfer whether to encrypt data transfer
* @param trashInterval interval for trashing files
* @param checksumType type of checksum
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)",73,80,"/**
* Initializes FsServerDefaults object with provided parameters.
* @param blockSize block size
* @param bytesPerChecksum bytes per checksum
* @param writePacketSize write packet size
* @param replication replication factor
* @param fileBufferSize file buffer size
* @param encryptDataTransfer whether to encrypt data transfer
* @param trashInterval trash interval
* @param checksumType checksum type
* @param keyProviderUri URI of key provider
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",359,363,"/**
* Invokes MyFS operation 1 with given parameters.
* @param path file system path
* @param name operation identifier
* @param value operation data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)",28,30,"/**
* Constructs a new instance of PathAccessDeniedException with the specified path and underlying cause.
* @param path the denied file system path
* @param cause the root cause of the access denial (may be null)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
* Constructs a custom exception with a specific file system path and optional underlying cause.
* @param path the affected file system path
* @param cause the underlying cause of this exception (optional)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
* Constructs a PathNotFoundException with the given path and underlying cause.
* @param path the path that caused the exception
* @param cause the root cause of this exception (if any)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,processArguments,org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList),49,85,"/**
* Concatenates files from source paths to a target path.
* @param args list of source paths and target path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,wrapException,"org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)",460,480,"/**
* Wraps IOException with additional context and logs.
* @param path file/directory path
* @param methodName method performing the operation
* @param exception original IOException instance
* @return wrapped IOException or null if interrupted
*/","* Takes an IOException, file/directory path, and method name and returns an
   * IOException with the input exception as the cause and also include the
   * file,method details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics
   * information.
   *
   * Return instance of same exception if exception class has a public string
   * constructor; Otherwise return an PathIOException.
   * InterruptedIOException and PathIOException are returned unwrapped.
   *
   * @param path file/directory path
   * @param methodName method name
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Reader:sync(long),2831,2864,"/**
* Verifies and reads data block at specified position.
* @param position offset to check
*/","* Seek to the next sync mark past a given position.
     * @param position position.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,reset,org.apache.hadoop.io.MapFile$Reader:reset(),638,640,"/**
* Writes masked data to file.
* @throws IOException on write error
*/","* Re-positions the reader before its first key.
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[]),82,85,"/**
 * Calls m1 with default offset and length (0 to array length).
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerExpressions,org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory),102,106,"/**
* Initializes expression factories with predefined masks.
* @param factory ExpressionFactory instance to configure
*/",Register the expressions with the expression factory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,registerCommands,org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),52,74,"/**
* Registers various commands with the CommandFactory.
* @param factory instance of CommandFactory to register commands with
*/","* Register the command classes used by the fs subcommand
   * @param factory where to register the class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,registerCommands,org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),111,118,"/**
* Creates FsCommand instance using CommandFactory.
* @param factory command creation factory
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expand,org.apache.hadoop.fs.GlobExpander:expand(java.lang.String),63,77,"/**
* Expands file patterns into a list of matching filenames.
* @param filePattern pattern to expand
* @return List of filenames that match the pattern
*/","* Expand globs in the given <code>filePattern</code> into a collection of
   * file patterns so that in the expanded set no file pattern has a slash
   * character (""/"") in a curly bracket pair.
   * <p>
   * Some examples of how the filePattern is expanded:<br>
   * <pre>
   * <b>
   * filePattern         - Expanded file pattern </b>
   * {a/b}               - a/b
   * /}{a/b}             - /}a/b
   * p{a/b,c/d}s         - pa/bs, pc/ds
   * {a/b,c/d,{e,f}}     - a/b, c/d, {e,f}
   * {a/b,c/d}{e,f}      - a/b{e,f}, c/d{e,f}
   * {a,b}/{b,{c/d,e/f}} - {a,b}/b, {a,b}/c/d, {a,b}/e/f
   * {a,b}/{c/\d}        - {a,b}/c/d
   * </pre>
   * 
   * @param filePattern file pattern.
   * @return expanded file patterns
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fetchMore,org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore(),2326,2330,"/**
* Updates internal state and file entries based on token.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,printXAttr,"org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])",120,128,"/**
* Writes a function attribute to the output stream.
* @param name attribute name
* @param value attribute value (null or empty for no value)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2119,2124,"/**
* Retrieves file statuses for a given path and filter.
* @param f the directory to traverse
* @param filter optional filter for file selection
* @return array of FileStatus objects
*/","* Filter files/directories in the given path using the user-supplied path
   * filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param f
   *          a path name
   * @param filter
   *          the user-supplied path filter
   * @return an array of FileStatus objects for the files under the given path
   *         after applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",2161,2168,"/**
* Retrieves status of multiple files matching a filter.
* @param files array of file paths to process
* @param filter filter criteria for files
* @return array of FileStatus objects or null if an error occurs
*/","* Filter files/directories in the given list of paths using user-supplied
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @param filter
   *          the user-supplied path filter
   * @return a list of statuses for the files under the given paths after
   *         applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",36,39,"/**
 * Initializes counter with specified value and metrics info.
 * @param info Metrics information
 * @param initValue Initial counter value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
* Initializes a MutableCounterLong with an initial value.
* @param info MetricsInfo instance
* @param initValue initial count value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
* Initializes mutable gauge with given initial value.
* @param info MetricsInfo object
* @param initValue initial gauge value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",33,36,"/**
* Initializes a mutable gauge with an initial float value.
* @param info Metrics configuration
* @param initValue starting value of the gauge
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",37,40,"/**
* Initializes mutable gauge with specified initial value.
* @param initValue initial integer value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Initializes a metric counter with a given value.
 * @param info Metrics information
 * @param value Long value to be counted
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
* Initializes metric gauge with specified value and metrics information.
* @param info MetricsInfo object containing relevant details
* @param value The numeric value to be measured by the gauge
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
* Initializes an integer metric counter with given value.
* @param info MetricsInfo object
* @param value integer value to be counted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",29,32,"/**
 * Initializes a Gauge with a float metric value.
 * @param info MetricsInfo object containing gauge metadata
 * @param value float value to be measured by the gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeDouble.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)",29,32,"/**
* Constructs a MetricGaugeDouble instance with specified metrics and value.
* @param info MetricsInfo object containing metric details
* @param value the gauge's numeric value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Initializes a metric gauge with an integer value.
 * @param info Metrics information
 * @param value Integer value to be stored in the gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",369,373,"/**
* Wraps a call to the underlying m1 method with null as the last parameter.
* @param url URL object
* @param token Token object
* @param renewer Renewer string
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",416,419,"/**
* Calls m1 with default parameters.
* @param url URL to access
* @param token authentication token
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",457,460,"/**
* Calls m1 with provided URL and token, passing null as a third parameter.
* @param url the URL to process
* @param token the authentication token
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @throws IOException if an IO error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",262,266,"/**
* Initializes ValueQueue with specified parameters.
* @param numValues number of values to store
* @param lowWaterMark minimum queue size threshold
* @param expiry time-to-live for stored values
* @param numFillerThreads number of threads for refilling the queue
* @param fetcher object for fetching new values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileEncryptionInfo.java,<init>,"org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)",57,74,"/**
* Initializes FileEncryptionInfo object with provided parameters.
* @param suite CipherSuite instance
* @param version CryptoProtocolVersion instance
* @param edek encrypted data encryption key (EDEK)
* @param iv initialization vector
* @param keyName string identifier for encryption key
* @param ezKeyVersionName string identifier for EZ key version","* Create a FileEncryptionInfo.
   *
   * @param suite CipherSuite used to encrypt the file
   * @param edek encrypted data encryption key (EDEK) of the file
   * @param iv initialization vector (IV) used to encrypt the file
   * @param keyName name of the key used for the encryption zone
   * @param ezKeyVersionName name of the KeyVersion used to encrypt the
   *                         encrypted data encryption key.
   * @param version version.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getFS,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS(),106,109,"/**
* Returns file system with applied function mask.
* @param fs existing file system instance
* @return modified file system instance or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,permission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission),121,126,"/**
* Computes and returns a mask value based on the given file system permissions.
* @param perm FsPermission object representing file system permissions
*/",* Set permission for the file.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,checksumOpt,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),211,216,"/**
* Computes and returns the functional mask based on the provided checksum options.
* @param chksumOpt checksum optimization parameters
*/",* Set checksum opt.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WrappedIOException.java,<init>,org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException),47,49,"/**
* Wraps an IOException in a custom exception.
* @param cause underlying IOException to be wrapped
*/","* Construct from a non-null IOException.
   * @param cause inner cause
   * @throws NullPointerException if the cause is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,<init>,org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction),50,52,"/**
* Initializes an instance with a custom file system link resolution function. 
* @param fn the custom resolution function to use
*/","* Construct an instance with the given function.
   * @param fn function to invoke.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext),437,440,"/**
* Initializes GlobBuilder with FileContext.
* @param fc FileContext to use
*/","* Construct bonded to a file context.
     * @param fc file context.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem),446,449,"/**
* Initializes GlobBuilder with a file system.
* @param fs the file system to operate on
*/","* Construct bonded to a filesystem.
     * @param fs file system.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getFS,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS(),141,144,"/**
 * Returns a file system instance based on the current configuration.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,permission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission),159,163,"/**
* Returns a mask value based on the given file system permissions.
* @param perm FsPermission object representing file system permissions
* @return A unique mask value for the specified permissions
*/","* Set permission for the file.
   *
   * @param perm permission.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,progress,org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable),239,243,"/**
* Calculates and returns the function mask.
* @param prog Progressable object to track progress
* @return Function mask value
*/","* Set the facility of reporting progress.
   *
   * @param prog progress.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,checksumOpt,org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),296,300,"/**
* Computes and returns the function mask based on given checksum options.
* @param chksumOpt checksum options
* @return computed function mask
*/","* Set checksum opt.
   *
   * @param chksumOpt check sum opt.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,validateWriteArgs,"org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)",110,118,"/**
* Validates and bounds byte array write operation.
* @param b input byte array
* @param off starting offset
* @param len length to write
*/","* Validate args to a write command. These are the same validation checks
   * expected for any implementation of {@code OutputStream.write()}.
   *
   * @param b   byte array containing data.
   * @param off offset in array where to start.
   * @param len number of bytes to be written.
   * @throws NullPointerException      for a null buffer
   * @throws IndexOutOfBoundsException if indices are out of range
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,set,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object),219,224,"/**
* Updates function mask with new value.
* @param v new function mask value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getLowerLayerAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn(),78,83,"/**
* Retrieves an asynchronous get operation with a default mask.
* @return AsyncGet object or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,setGcTimeMonitor,org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor),107,110,"/**
* Initializes and sets GcTimeMonitor instance.
* @param gcTimeMonitor GcTimeMonitor object to initialize
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])",136,142,"/**
* Initializes and configures the cipher instance with provided key and IV.
* @param key encryption key
* @param iv initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])",128,138,"/**
* Initializes the cryptographic engine with key and IV.
* @param key encryption key
* @param iv initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,equalsIgnoreCase,"org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)",1259,1264,"/**
* Calls M2 method on object referenced by string s1.
* @param s1 reference to an object with M2 method
* @param s2 input for M2 method
* @return result of M2 method call
*/","* Compare strings locale-freely by using String#equalsIgnoreCase.
   *
   * @param s1  Non-null string to be converted
   * @param s2  string to be converted
   * @return     the str, converted to uppercase.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LimitInputStream.java,<init>,"org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)",43,48,"/**
* Initializes a Limited InputStream with a specified byte count.
* @param in underlying input stream
* @param limit maximum bytes to read from the stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)",432,439,"/**
* Initializes deprecation delta with key, new keys and optional custom message.
* @param key unique identifier
* @param newKeys array of new values
* @param customMessage custom message for deprecation (optional)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,setReconfigurationUtil,org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil),88,91,"/**
* Sets ReconfigurationUtil instance, allowing for testing.
* @param ru ReconfigurationUtil object to be set
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,isStaleClient,org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object),1173,1181,"/**
* Checks if the current ZooKeeper client has a valid session.
* @param ctx ZooKeeper object containing session information
* @return true if client is stale, false otherwise
*/","* The callbacks and watchers pass a reference to the ZK client
   * which made the original call. We don't want to take action
   * based on any callbacks from prior clients after we quit
   * the election.
   * @param ctx the ZK client passed into the watcher
   * @return true if it matches the current client",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,"org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)",4586,4605,"/**
* Retrieves or initializes Statistics instance for the given file system class.
* @param scheme unique file system identifier
* @param cls Class of the file system
* @return Statistics object associated with the file system
*/","* Get the statistics for a particular file system.
   * @param scheme scheme.
   * @param cls the class to lookup
   * @return a statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedByteArray,"org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])",61,83,"/**
* Compresses input data and writes output size to DataOutput.
* @param out DataOutput stream for compressed output
* @param bytes byte array to compress, null if no data
* @return compression ratio (0-100) or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)",64,81,"/**
* Closes and flushes input/output streams, handling buffering.
* @param in InputStream to close
* @param out OutputStream to close
* @param buffSize buffer size (not used)
* @param close whether to actually close the streams
*/","* Copies from one stream to another.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param buffSize the size of the buffer 
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)",145,175,"/**
* Copies data from input stream to output stream, optionally closing streams.
* @param in InputStream containing data
* @param out OutputStream for copied data
* @param count Total bytes to copy
* @param close Whether to close the streams after copying
*/","* Copies count bytes from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param count number of bytes to copy
   * @param close whether to close the streams
   * @throws IOException if bytes can not be read or written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$IpcStreams:close(),1955,1959,"/**
* Flushes output and input buffers.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,<init>,org.apache.hadoop.util.VersionInfo:<init>(java.lang.String),41,55,"/**
* Loads version information from a properties file for the given component.
* @param component name of the component (e.g. ""my-component"")
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks(),471,479,"/**
* Stops and removes all active metrics sinks.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,finalize,org.apache.hadoop.crypto.random.OsSecureRandom:finalize(),128,131,"/**
* Calls function m1 to perform a specific action.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,doDiskIo,org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File),255,274,"/**
* Recursively checks a directory for errors and returns on first success.
* @param dir Directory to check
*/","* Performs some disk IO by writing to a new file in the given directory
   * and sync'ing file contents to disk.
   *
   * This increases the likelihood of catching catastrophic disk/controller
   * failures sooner.
   *
   * @param dir directory to be checked.
   * @throws DiskErrorException if we hit an error while trying to perform
   *         disk IO against the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)",44,46,"/**
 * Initializes a new PartialListing object from a given path and listing.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)",48,50,"/**
 * Initializes a partial listing with the specified path and an associated remote exception.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object),50,52,"/**
* Initializes the call state with a return value.
* @param r The returned object.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable),53,56,"/**
 * Initializes call context with an exception. 
 * @param none
 * @throws NullPointerException if input is null
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State),57,59,"/**
 * Initializes CallReturn with default values and given State.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",66,70,"/**
* Calls parent method with additional data from m1() and m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",52,56,"/**
* Calls superclass's m3 method with additional data from m1 and m2 methods.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Calls superclass's m3() with additional data from m1() and m2().
* @param initIV initial initialization vector
* @param counter counter value
* @param iv current initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Calls superclass's m3() with additional initialization from m1() and m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,build,org.apache.hadoop.util.GcTimeMonitor$Builder:build(),95,98,"/**
* Creates a GcTimeMonitor instance with specified parameters.
* @param observationWindowMs time window for garbage collection monitoring
* @param sleepIntervalMs interval between monitoring checks
* @param maxGcTimePercentage threshold percentage for garbage collection time
* @param handler callback handler for monitoring results
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeQuotaSet,org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet(),193,202,"/**
* Checks if there are any non-zero type quotas.
* @return true if any quota is set, false otherwise
*/","* Return true if any storage type quota has been set.
   *
   * @return if any storage type quota has been set true, not false.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeConsumedAvailable,org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable(),210,219,"/**
* Checks if the type mask is set.
* @return true if any storage types are consumed, false otherwise
*/","* Return true if any storage type consumption information is available.
   *
   * @return if any storage type consumption information
   * is available, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,getAndCheckStorageTypes,org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String),180,193,"/**
* Retrieves a list of storage types from input string.
* @param types comma-separated list of storage types or ""all""
* @return list of StorageType objects or empty list if invalid input
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,equals,org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object),190,193,"/**
* Calls superclass's version of m1.","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object),549,552,"/**
* Delegates to realStatus's m1 method.
* @param o object to pass to m1 (purpose unclear)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object),40,43,"/**
 * Delegates invocation to superclass's implementation of the method. 
 * @param o object to pass to superclass's method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,hashCode,org.apache.hadoop.fs.LocatedFileStatus:hashCode(),201,204,"/**
* Calls superclass implementation of m1(). 
* @return result from superclass's m1() method.","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode(),554,557,"/**
* Calls real status's m1 method and returns result.
* @return integer value returned by m1 method of realStatus object.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode(),45,48,"/**
* Calls superclass implementation of method m1.
* @return result from superclass implementation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,getFolderUsage,org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String),34,36,"/**
* Calculates a function mask based on the provided folder.
* @param folder directory path to calculate mask from
* @return calculated mask value as a long integer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clear,org.apache.hadoop.fs.statistics.MeanStatistic:clear(),147,149,"/**
 * Initializes mask with default values.
 */",* Set the values to 0.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,set,org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic),168,170,"/**
* Updates local M3 value using another MeanStatistic instance.
* @param other MeanStatistic object to update from
*/","* Set the statistic to the values of another.
   * Synchronized.
   * @param other the source.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics),77,91,"/**
* Formats IOStatistics object into a string.
* @param statistics IOStatistics data or null
* @return formatted string or empty if no data
*/","* Convert IOStatistics to a string form.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToSortedString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)",159,164,"/**
* Formats data from a map into a string using the specified type and delimiter.
* @param sb StringBuilder to append formatted data
* @param type Type of data (e.g. ""key"" or ""value"")
* @param map Map containing data to format
* @param isEmpty Predicate to check for empty values
*/","* Given a map, produce a string with all the values, sorted.
   * Needs to create a treemap and insert all the entries.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param <E> type of values of the map",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String),60,62,"/**
* Returns a duration tracker instance with default start time.
* @param key unique identifier for tracking purposes
*/","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   * The expected use is within a try-with-resources clause.
   * @param key statistic key
   * @return an object to close after an operation completes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)",50,55,"/**
* Combines global and local duration trackers with given key and count.
* @param key unique identifier for tracking
* @param count value to track durations for
* @return combined DurationTracker object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLongStatistics,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics(),66,78,"/**
* Retrieves and processes LongStatistics from a map of counters.
* @return Iterator<LongStatistic> containing processed data
*/","* Take a snapshot of the current counter values
   * and return an iterator over them.
   * @return all the counter statistics.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)",440,445,"/**
* Updates mask settings for mean, min, and max values.
* @param prefix unique identifier prefix
* @param durationMillis time duration in milliseconds
*/","* Add a duration to the min/mean/max statistics, using the
   * given prefix and adding a suffix for each specific value.
   * <p>
   * The update is non -atomic, even though each individual statistic
   * is updated thread-safely. If two threads update the values
   * simultaneously, at the end of each operation the state will
   * be correct. It is only during the sequence that the statistics
   * may be observably inconsistent.
   * </p>
   * @param prefix statistic prefix
   * @param durationMillis duration in milliseconds.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,build,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build(),51,56,"/**
 * Retrieves and returns dynamic I/O statistics.
 * @return DynamicIOStatistics object
 */","* Build the IOStatistics instance.
   * @return an instance.
   * @throws IllegalStateException if the builder has already been built.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)",74,78,"/**
* Adds dynamic IO statistics function with specified key and evaluation logic.
* @param key unique identifier for the statistic
* @param eval function to evaluate IO values as long integers
*/","* Add a new evaluator to the counter statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)",125,129,"/**
* Adds dynamic I/O statistics builder with custom evaluation function.
* @param key unique identifier
* @param eval function to evaluate statistic value from input string
* @return DynamicIOStatisticsBuilder instance for chaining
*/","* Add a new evaluator to the gauge statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)",163,167,"/**
* Configures dynamic IO statistics builder with custom evaluation function.
* @param key unique statistic identifier
* @param eval long-value evaluator function for the key
*/","* Add a new evaluator to the minimum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)",202,206,"/**
* Adds dynamic IO statistics builder function with given key and evaluation function.
* @param key unique identifier for the statistics
* @param eval function to evaluate long value from input string
*/","* Add a new evaluator to the maximum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)",242,246,"/**
* Configures dynamic I/O statistics with a custom evaluation function.
* @param key unique key for the configuration
* @param eval function to evaluate mean statistic values
* @return this builder instance for method chaining
*/","* Add a new evaluator to the mean statistics.
   *
   * This is a function which must return the mean and the sample count.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,register,org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String),142,146,"/**
* Initializes and schedules an IRQ handler with the specified name.
* @param signalName unique identifier for the IRQ handler
*/","* Register an interrupt handler.
   * @param signalName signal name
   * @throws IllegalArgumentException if the registration failed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,unreference,org.apache.hadoop.net.unix.DomainSocket:unreference(boolean),176,182,"/**
* Checks and updates reference count based on channel closure status.
* @param checkClosed true to decrement ref count, false to increment it
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,snapshot,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot(),146,148,"/**
 * Returns a mapping of function names to their corresponding mask values.
 */","* Take a snapshot.
   * @return a map snapshot.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map),200,204,"/**
* Transform map entries using passthrough function.
* @param source input map to transform
*/","* Take a snapshot of a supplied map, where the copy option simply
   * uses the existing value.
   *
   * For this to be safe, the map must refer to immutable objects.
   * @param source source map
   * @param <E> type of values.
   * @return a new map referencing the same values.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot(),44,47,"/**
 * Creates an instance of IOStatisticsSnapshot with default values. 
 */","* Create a new empty snapshot.
   * A new one is always created for isolation.
   *
   * @return a statistics snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)",64,67,"/**
* Initializes IO statistics context with given thread and ID.
* @param threadId unique identifier of the thread
* @param id unique identifier of the IO statistics context
*/","* Constructor.
   * @param threadId thread ID
   * @param id instance ID.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(),59,63,"/**
* Creates an empty IOStatisticsSnapshot instance.
*/","* Create a snapshot statistics instance ready to aggregate data.
   *
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @return an empty snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled(),279,281,"/**
 * Returns whether to mask IO statistics.
 * @return true to mask IO statistics, false otherwise
 */","* Static probe to check if the thread-level IO statistics enabled.
   * @return true if the thread-level IO statistics are enabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsOutputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics(),86,89,"/**
* Returns I/O statistics with specified output mask.
* @return IOStatistics object representing current I/O state
*/","* Ask the inner stream for their IOStatistics.
   * @return any IOStatistics offered by the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsInputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics(),64,67,"/**
 * Returns IO statistics with function mask applied.
 * @return IOStatistics object with updated mask value
 */","* Return any IOStatistics offered by the inner stream.
   * @return inner IOStatistics or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,getIOStatistics,org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics(),156,159,"/**
* Returns IO statistics based on input parameters.
* @return IOStatistics object with calculated values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics(),315,318,"/**
* Retrieves I/O statistics based on provided input data.
* @return IOStatistics object representing system performance metrics
*/","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics(),676,679,"/**
* Returns IO statistics masked with the specified function. 
* @return IOStatistics object with applied mask","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataInputStream:getIOStatistics(),289,292,"/**
* Returns IO statistics based on input mask.
* @return IOStatistics object reflecting current state
*/","* Get the IO Statistics of the nested stream, falling back to
   * null if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics(),167,170,"/**
* Returns I/O statistics mask.
* @return IOStatistics object or null if not supported
*/","* Get the IO Statistics of the nested stream, falling back to
   * empty statistics if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics(),107,110,"/**
* Returns IO statistics with a specific mask value.
* @return IOStatistics object with mask applied
*/","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics(),81,84,"/**
* Returns IO statistics with specified mask.
* @return IOStatistics object representing current IO state
*/","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics(),882,885,"/**
* Returns IO statistics based on input parameters.
* @return IOStatistics object representing system I/O metrics.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics(),321,324,"/**
* Returns I/O statistics with mask applied.
* @return IOStatistics object with mask applied
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,getIOStatistics,org.apache.hadoop.util.LineReader:getIOStatistics(),159,162,"/**
* Returns IO statistics with mask applied.
* @return IOStatistics object with mask applied
*/","* Return any IOStatistics provided by the source.
   * @return IO stats from the input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics(),351,354,"/**
* Returns IO statistics with mask applied.
* @return IOSTatistics object with modified data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics(),405,408,"/**
* Returns I/O statistics with mask applied.
* @return IOStatistics object with mask applied
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics(),449,452,"/**
* Returns IO statistics for the given source.
* @return IOStatistics object representing the source's I/O metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)",429,472,"/**
* Verifies checksums in the provided ByteBuffer data against the given CRC values.
* @param type Type of checksum
* @param algorithm Checksum calculation algorithm
* @param data ByteBuffer containing data to verify
* @param bytesPerCrc Number of bytes per CRC value
* @param crcs ByteBuffer containing expected CRC values
* @param filename Name of the file being verified
* @param basePos Base position for error reporting
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)",478,512,"/**
* Verifies checksums for the given data and CRC values.
* @param type Type of verification
* @param algorithm Checksum algorithm to use
* @param data Data being verified
* @param dataOffset Offset into data where verification starts
* @param dataLength Length of data being verified
* @param bytesPerCrc Number of bytes per CRC value
* @param crcs Array of expected CRC values
* @param crcsOffset Offset into crcs array where verification starts
* @param filename Name of file being verified (for error reporting)
* @param basePos Base position in file for relative error reporting
*/","* Implementation of chunked verification specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,updateDecryptor,"org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])",297,302,"/**
* Updates IV and key for decryption using provided parameters.
* @param decryptor Decryptor object
* @param position Position value used in calculations
* @param iv Initialization vector to be updated
*/","Calculate the counter and iv, update the decryptor.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,encrypt,org.apache.hadoop.crypto.CryptoOutputStream:encrypt(),178,217,"/**
* Encrypts and writes data from input buffer to output.
* @throws IOException on write errors
*/","* Do the encryption, input is {@link #inBuffer} and output is 
   * {@link #outBuffer}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,hasNext,org.apache.hadoop.fs.BatchedRemoteIterator:hasNext(),98,102,"/**
* Checks if function mask is available.
* @return true if entries are loaded, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,next,org.apache.hadoop.fs.BatchedRemoteIterator:next(),111,120,"/**
* Retrieves a functional mask entry from the index, advancing to next entry.
* @return Functional mask object or throws IOException if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(),27,29,"/**
* Initializes checksum calculator with default values.
* @param unknown MD5 hash value
* @param unknown CRC-32 hash value
* @param unknown Gzip compression status
*/","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(),27,29,"/**
* Initializes an empty MD5MD5CRC32CastagnoliFileChecksum object.","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,<init>,org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String),41,43,"/**
* Initializes a GlobPattern instance from a string representation.
* @param globPattern string pattern following glob syntax rules
*/","* Construct the glob pattern object with a glob pattern string
   * @param globPattern the glob pattern string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File),668,670,"/**
 * Returns a string representation of the specified file.
 * @param file the file to convert to a string
 */","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeSecureShellPath,org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File),679,686,"/**
* Generates a mask string for the given file.
* @param file File object to generate mask for
*/","* Convert a os-native filename to a path that works for the shell
   * and avoids script injection attacks.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File),108,116,"/**
* Extracts file metadata and link count into an array.
* @param file the input file to process
* @return array of file metadata strings or throws IOException if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)",739,775,"/**
* Extracts zip archive to specified directory, preserving timestamps and permissions.
* @param inputStream zip archive input stream
* @param toDir target extraction directory
*/","* Given a stream input it will unzip the it in the unzip directory.
   * passed as the second parameter
   * @param inputStream The zip file as input
   * @param toDir The unzip directory where to unzip the zip file.
   * @throws IOException an exception occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)",827,871,"/**
* Unzips the specified archive file to the target directory, handling platform-specific permissions.
* @param inFile   input Zip Archive
* @param unzipDir target unzipping directory
*/","* Given a File input it will unzip it in the unzip directory.
   * passed as the second parameter
   * @param inFile The zip file as input
   * @param unzipDir The unzip directory where to unzip the zip file.
   * @throws IOException An I/O exception has occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)",1086,1109,"/**
* Extracts and writes tar archive to directory.
* @param inFile input file containing the tar archive
* @param untarDir target directory for extraction
* @param gzipped true if input is gzipped, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)",1111,1128,"/**
* Extracts and processes tar archive from input stream.
* @param inputStream source of the tar archive
* @param untarDir directory to extract files into
* @param gzipped whether the archive is gzip-compressed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,tryFence,"org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",57,105,"/**
* Executes PowerShell script on HA Service Target.
* @param target HAServiceTarget instance
* @param argsStr PowerShell script to execute as string
* @return true if execution succeeds, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,toString,org.apache.hadoop.fs.permission.FsCreateModes:toString(),82,86,"/**
* Formats and concatenates masked and unmasked phone numbers.
* @return formatted string with both phone number representations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,disconnect,org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp),165,167,"/**
* Executes SFTP operation using pooled connections. 
* @param channel SFTP channel object 
* @throws IOException if an I/O error occurs
*/","* Logout and disconnect the given channel.
   *
   * @param client
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,shutdown,org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown(),87,113,"/**
* Closes and cleans up all active connections, updating internal state.
*/",Shutdown the connection pool and close all open connections.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)",82,84,"/**
* Constructs an FSDataOutputStream with default buffer size.
* @param out underlying OutputStream
* @param stats FileSystem statistics (optional)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,<init>,org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum),53,58,"/**
* Initializes FSOutputSummer with checksum data.
* @param sum DataChecksum object containing checksum information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,setChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int),257,261,"/**
* Initializes buffer and checksum arrays with specified size.
* @param size target buffer size
*/","* Resets existing buffer with a new one of the specified size.
   *
   * @param size size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,read,org.apache.hadoop.fs.sftp.SFTPInputStream:read(),108,124,"/**
* Reads and returns the next byte from the wrapped stream.
* @return int value of the read byte, -1 on end-of-stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,org.apache.hadoop.fs.ftp.FTPInputStream:read(),70,84,"/**
* Reads bytes from the underlying stream and updates position and statistics.
* @throws IOException if the stream is closed or an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,"org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)",86,101,"/**
* Reads data from the underlying stream and updates position.
* @param buf input buffer
* @param off offset in buffer
* @param len number of bytes to read
* @return number of bytes read, or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(),217,231,"/**
* Reads and processes a single data unit from the input stream.
* @return integer value or throws IOException
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)",233,249,"/**
* Reads data from the stream using m3() method and updates position and statistics.
* @param b input byte array
* @param off offset in bytes
* @param len number of bytes to read
* @return number of bytes read or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)",251,272,"/**
* Reads data from the file system and updates statistics.
* @param position current file position
* @param b byte array containing file data
* @param off offset in the byte array
* @param len length of data to read
* @return number of bytes read or 0 if end-of-file reached
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int),51,58,"/**
* Processes input and updates internal state.
* @param b integer value to process
* @throws IOException on write failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,"org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)",60,67,"/**
* Writes data to output and updates internal state.
* @param b byte array to write
* @param off starting offset in the array
* @param len number of bytes to write
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics),4110,4125,"/**
* Copies and aggregates statistics from another instance.
* @param other source Statistics object
*/","* Copy constructor.
     *
     * @param other    The input Statistics object which is cloned.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead(),4308,4321,"/**
* Calculates the total bytes read by aggregating statistics.
*/","* Get the total number of bytes read.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten(),4327,4340,"/**
* Calculates the total bytes written by aggregating statistics.
*/","* Get the total number of bytes written.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getReadOps(),4346,4360,"/**
* Calculates the total read operations across all statistics data.
* @return total read operations count
*/","* Get the number of file system read operations such as list files.
     * @return number of read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps(),4367,4380,"/**
* Calculates the total number of large read operations.
*/","* Get the number of large file system read operations such as list files
     * under a large directory.
     * @return number of large read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps(),4387,4400,"/**
* Calculates total write operations mask.
* @return integer value representing the total write operations
*/","* Get the number of file system write operations such as create, append
     * rename etc.
     * @return number of write operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime(),4436,4449,"/**
* Calculates the cumulative remote read time in milliseconds.
*/","* Get total time taken in ms for bytes read from remote.
     * @return time taken in ms for remote bytes read.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getData,org.apache.hadoop.fs.FileSystem$Statistics:getData(),4456,4469,"/**
* Aggregates statistics using a custom aggregator.
* @return aggregated StatisticsData object
*/","* Get all statistics data.
     * MR or other frameworks can use the method to get all statistics at once.
     * @return the StatisticsData",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded(),4475,4488,"/**
* Calculates the total bytes read for erasure coded data.
*/","* Get the total number of bytes read on erasure-coded files.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Statistics:toString(),4490,4504,"/**
* Aggregates statistics for a given type, applying callback to each item.
* @return aggregated string representation or null if empty
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,reset,org.apache.hadoop.fs.FileSystem$Statistics:reset(),4524,4539,"/**
* Aggregates statistics and updates root data.
*/","* Resets all statistics to 0.
     *
     * In order to reset, we add up all the thread-local statistics data, and
     * set rootData to the negative of that.
     *
     * This may seem like a counterintuitive way to reset the statistics.  Why
     * can't we just zero out all the thread-local data?  Well, thread-local
     * data can only be modified by the thread that owns it.  If we tried to
     * modify the thread-local data from this thread, our modification might get
     * interleaved with a read-modify-write operation done by the thread that
     * owns the data.  That would result in our update getting lost.
     *
     * The approach used here avoids this problem because it only ever reads
     * (not writes) the thread-local data.  Both reads and writes to rootData
     * are done under the lock, so we're free to modify rootData from any thread
     * that holds the lock.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,toString,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString(),113,116,"/**
* Returns a formatted string representation of the MD5 hash.
* @return String in format ""m1:md5"" where m1 is the first part and md5 is the MD5 hash
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,"org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)",172,187,"/**
* Validates and configures file/path operation flags.
* @param path Object representing a file or directory
* @param pathExists whether the file/directory already exists
* @param flag EnumSet of CreateFlag options to consider
*/","* Validate the CreateFlag for create operation
   * @param path Object representing the path; usually String or {@link Path}
   * @param pathExists pass true if the path exists in the file system
   * @param flag set of CreateFlag
   * @throws IOException on error
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validateForAppend,org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet),195,201,"/**
* Validates and initializes flags for create operation.
* @param flag set of CreateFlags to validate
*/","* Validate the CreateFlag for the append operation. The flag must contain
   * APPEND, and cannot contain OVERWRITE.
   *
   * @param flag enum set flag.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUri,"org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)",316,341,"/**
* Constructs a URI with scheme and port masking.
* @param uri input URI
* @param supportedScheme supported scheme
* @param authorityNeeded whether authority is required
* @param defaultPort default port to use if not specified in URI
* @return masked URI or null if invalid
*/","* Get the URI for the file system based on the given URI. The path, query
   * part of the given URI is stripped out and default file system port is used
   * to form the URI.
   * 
   * @param uri FileSystem URI.
   * @param authorityNeeded if true authority cannot be null in the URI. If
   *          false authority must be null.
   * @param defaultPort default port to use if port is not specified in the URI.
   * 
   * @return URI of the file system
   * 
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,96,"/**
* Encrypts and updates output buffers using GF algorithm.
* @param inputs input buffer
* @param erasedIndexes indexes of elements to process
* @param outputs output buffers to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])",36,50,"/**
* Initializes ByteArrayEncodingState with given encoder and input/output buffers.
* @param encoder RawErasureEncoder instance
* @param inputs byte[][] of encoded input data
* @param outputs byte[][] of output data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",35,47,"/**
* Initializes ByteBufferEncodingState with encoder and input/output buffers.
* @param encoder RawErasureEncoder instance
* @param inputs array of ByteBuffer instances to encode
* @param outputs array of ByteBuffer instances for encoded output
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class),113,116,"/**
* Creates an ArrayPrimitiveWritable instance with the specified primitive type.
* @param componentType Class of the primitive type (e.g., Integer.class, Float.class)
*/","* Construct an instance of known type but no value yet
   * for use with type-specific wrapper classes.
   *
   * @param componentType componentType.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,set,org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object),142,150,"/**
* Initializes mask components from provided Object.
* @param value input object containing mask data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,close,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close(),55,58,"/**
* Sends m1 request to remote proxy. 
* @throws IOException on communication error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close(),49,52,"/**
* Sends an RPC request to mask user data.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close(),54,57,"/**
* Sends an M1 request to the RPC proxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close(),52,55,"/**
* Sends an m1 request to the RPC proxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close(),49,52,"/**
* Sends an RPC request to update the mask.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close(),47,50,"/**
* Sends an RPC request to mask a user's profile.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close(),72,75,"/**
* Sends an M1 request to the RPC proxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close(),165,168,"/**
* Sends an M1 request to the remote procedure call (RPC) service.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,getPermFromString,org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String),44,71,"/**
* Parses a permission string into a bitmask.
* @param permString string of 'r', 'w', 'c', 'd', and 'a' characters
* @return bitmask value or throws exception on invalid input
*/","* Parse ACL permission string, partially borrowed from
   * ZooKeeperMain private method",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)",339,342,"/**
* Combines default and user checksum options.
* @param defaultOpt default checksum option
* @param userOpt user-provided checksum option
*/","* A helper method for processing user input and default value to 
     * create a combined checksum option. 
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option
     *
     * @return ChecksumOpt.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getUriDefaultPort,org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort(),174,177,"/**
* Returns functional mask value using fsImpl.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI),323,326,"/**
* Calls underlying file system's m1 function to process input URI.
* @param uri input URI object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FileSystem:getCanonicalUri(),383,385,"/**
* Calculates the functional mask URI using nested calls to m1 and m2.
* @return calculated URI object
*/","* Return a canonicalized form of this FileSystem's URI.
   *
   * The default implementation simply calls {@link #canonicalizeUri(URI)}
   * on the filesystem's own URI, so subclasses typically only need to
   * implement that method.
   *
   * @see #canonicalizeUri(URI)
   * @return the URI of this filesystem.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI),118,121,"/**
 * Delegates file system operation to underlying implementation.
 * @param uri input URI object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)",162,165,"/**
* Constructs a ContentSummary object with given metrics.
* @param length total content length in bytes
* @param fileCount number of files in the content
* @param directoryCount number of directories in the content
*/","*  Constructor, deprecated by ContentSummary.Builder
   *  This constructor implicitly set spaceConsumed the same as length.
   *  spaceConsumed and length must be set explicitly with
   *  ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,toString,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString(),160,166,"/**
* Returns a string describing the token renewal status.
* @return description of token renewal status or evaporated token
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,<init>,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem),71,75,"/**
* Initializes RenewAction with a given object.
* @param fs the object to be referenced
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getStatus,org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path),283,286,"/**
* Delegates file system status retrieval to underlying FS implementation.
* @param p path to query
* @return FsStatus object or null if error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path),153,156,"/**
* Retrieves file system status for the given path.
* @param f Path to query
* @return FsStatus object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(),3026,3028,"/**
* Retrieves file system status with default parameters.
* @throws IOException on I/O error
*/","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the root partition is reflected.
   *
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStatus,org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path),329,332,"/**
* Delegates file system status query to underlying implementation.
* @param p file path to query
* @return FsStatus object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,toString,org.apache.hadoop.fs.impl.CombinedFileRange:toString(),91,96,"/**
* Concatenates superclass result with formatted string containing range and data size information. 
* @return combined string or null if superclass returns null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder),151,180,"/**
* Initializes HttpReferrerAuditHeader from builder parameters.
* @param builder configuration builder for audit header properties
*/","* Instantiate.
   * <p>
   * All maps/enums passed down are copied into thread safe equivalents.
   * as their origin is unknown and cannot be guaranteed to
   * not be shared.
   * <p>
   * Context and operationId are expected to be well formed
   * numeric/hex strings, at least adequate to be
   * used as individual path elements in a URL.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,<init>,"org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)",36,39,"/**
* Creates a thread-local map with custom key-to-value mapping factory.
* @param factory function to create values for given long keys
* @param referenceLost callback when a value is garbage collected
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,hasCapability,org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String),128,131,"/**
* Checks capability using underlying store implementation.
* @param capability capability to check
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String),316,319,"/**
* Executes M1 operation on store implementation.
* @param capability capability string to use in M1 operation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,hasCapability,org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String),242,245,"/**
* Calls the M1 implementation utility to determine whether the given capability is supported.
* @param capability string identifier of the capability to check
* @return true if capability is supported, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getPrefetched,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int),168,172,"/**
* Retrieves operation with mask by block number.
* @param blockNumber unique block identifier
* @return Operation object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getCached,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int),174,178,"/**
* Retrieves an operation mask by block number.
* @param blockNumber unique block identifier
* @return Operation object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getRead,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int),180,184,"/**
* Creates an operation to fetch data from a specific mask.
* @param blockNumber unique identifier of the mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,release,org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int),186,190,"/**
* Creates an operation with RELEASE kind and specified block number.
* @param blockNumber unique block identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int),192,196,"/**
* Generates an Operation object with REQUEST_PREFETCH kind and specified block number.
* @param blockNumber the target block number
* @return Operation object or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,prefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int),198,202,"/**
* Creates an Operation with PREFETCH Kind and specified blockNumber.
* @param blockNumber block number for the operation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches(),204,206,"/**
* Creates an operation to cancel all prefetches.
* @return Operation object representing the cancellation request
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,close,org.apache.hadoop.fs.impl.prefetch.BlockOperations:close(),208,210,"/**
* Creates an operation to close all masks.
* @return Operation object with Kind.CLOSE and negative mask ID
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int),212,216,"/**
* Creates an operation of type REQUEST_CACHING with the given block number.
* @param blockNumber the block number for the operation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,addToCache,org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int),218,222,"/**
* Creates an Operation instance with CACHE_PUT kind and given block number.
* @param blockNumber cache block index
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,end,org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),224,226,"/**
* Returns an operation with its mask applied.
* @param op original operation to be masked
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,fromSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String),377,424,"/**
* Parses summary into BlockOperations.
* @param summary string containing operations to parse
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,release,org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData),236,255,"/**
* Releases and cleans up allocated buffer data.
* @param data BufferData object to release
*/","* Releases a previously acquired resource.
   * @param data the {@code BufferData} instance to release.
   * @throws IllegalArgumentException if data is null.
   * @throws IllegalArgumentException if data cannot be released due to its state.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,toString,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString(),153,158,"/**
* Returns a formatted string containing size and queue metrics.
* @return Formatted string with size, created, in-queue, and available counts
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDurationInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder),252,295,"/**
* Updates StringBuilder with operation duration statistics.
* @param sb StringBuilder to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,createCache,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",554,556,"/**
* Creates a single-file-per-block cache instance with prefetching statistics.
* @param maxBlocksCount maximum number of blocks to cache
* @param trackerFactory factory for tracking cache usage
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)",90,95,"/**
* Constructs a semaphored delegating executor with given settings.
* @param executorDelegatee the underlying executor service
* @param permitCount initial permit count for semaphore
* @param fair whether to use fair locking in semaphore
*/","* Instantiate without collecting executor aquisition duration information.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getEntry,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int),297,307,"/**
* Retrieves cached Entry object by block number.
* @param blockNumber unique identifier for the block
* @return cached Entry object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseReadyBlock,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int),205,224,"/**
* Finds and releases the highest-priority 'ready' BufferData by blockNumber.
* @param blockNumber unique block identifier
*/","* If no blocks were released after calling releaseDoneBlocks() a few times,
   * we may end up waiting forever. To avoid that situation, we try releasing
   * a 'ready' block farthest away from the given block.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferPool:toString(),278,292,"/**
* Assembles user profile details into a formatted string.
* @return concatenated String containing user info
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,buffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer(),130,133,"/**
* Returns a ByteBuffer representing the mask.
* @return ByteBuffer containing mask data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,data,org.apache.hadoop.fs.impl.prefetch.FilePosition:data(),135,138,"/**
* Returns buffer data with applied mask.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,relative,org.apache.hadoop.fs.impl.prefetch.FilePosition:relative(),172,175,"/**
* Returns a mask value by executing m1() and retrieving result from buffer. 
* @return integer mask value
*/","* Gets the current position within this file relative to the start of the associated buffer.
   *
   * @return the current position within this file relative to the start of the associated buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isWithinCurrentBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long),183,187,"/**
* Checks if position is within buffer range.
* @param pos position to check
*/","* Determines whether the given absolute position lies within the current buffer.
   *
   * @param pos the position to check.
   * @return true if the given absolute position lies within the current buffer, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferStartOffset,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset(),231,234,"/**
* Returns a mask value based on internal calculations.
*/","* Gets the start of the current block's absolute offset.
   *
   * @return the start of the current block's absolute offset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext(),122,126,"/**
* Returns an IO statistics context based on thread-level stats enablement.
* @return Active or empty IO statistics context depending on thread settings
*/","* Get the current thread's IOStatisticsContext instance. If no instance is
   * present for this thread ID, create one using the factory.
   * @return instance of IOStatisticsContext.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),133,145,"/**
* Updates active IO stats context with given statistics.
* @param statisticsContext IOStatisticsContext object
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,mergeSortedRanges,"org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)",380,398,"/**
* Merges overlapping file ranges into combined ranges.
* @param sortedRanges list of file ranges to merge
* @return list of merged combined file ranges
*/","* Merge sorted ranges to optimize the access from the underlying file
   * system.
   * The motivations are that:
   * <ul>
   *   <li>Upper layers want to pass down logical file ranges.</li>
   *   <li>Fewer reads have better performance.</li>
   *   <li>Applications want callbacks as ranges are read.</li>
   *   <li>Some file systems want to round ranges to be at checksum boundaries.</li>
   * </ul>
   *
   * @param sortedRanges already sorted list of ranges based on offset.
   * @param chunkSize round the start and end points to multiples of chunkSize
   * @param minimumSeek the smallest gap that we should seek over in bytes
   * @param maxSize the largest combined file range in bytes
   * @return the list of sorted CombinedFileRanges that cover the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,findChecksumRanges,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)",344,362,"/**
* Merges file ranges into combined segments based on CRC offset.
* @param dataRanges list of FileRange objects
* @param bytesPerSum size of each CRC sum chunk
* @param minSeek minimum seek position for merging
* @param maxSize maximum size of merged segment
* @return List of CombinedFileRange objects representing merged segments
*/","* Find the checksum ranges that correspond to the given data ranges.
     * @param dataRanges the input data ranges, which are assumed to be sorted
     *                   and non-overlapping
     * @return a list of AsyncReaderUtils.CombinedFileRange that correspond to
     *         the checksum ranges",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(),48,50,"/**
* Initializes a new instance of the Name class with default settings.
*/",Creates a case sensitive name expression.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,subset,org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String),144,147,"/**
* Creates a new instance of MetricsConfig with the given prefix.
* @param prefix filter prefix for metrics configuration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,apply,"org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)",57,68,"/**
* Recursively traverses the expression tree and performs operation m1 on each node.
* @param item PathData object to process
* @param depth current recursion depth (not used in this implementation)
*/","* Applies child expressions to the {@link PathData} item. If all pass then
   * returns {@link Result#PASS} else returns the result of the first
   * non-passing expression.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getOptions,org.apache.hadoop.fs.shell.find.Find:getOptions(),235,241,"/**
* Returns cached FindOptions instance or initializes it with m1() values. 
*/","Returns the current find options, creating them if necessary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List),99,140,"/**
* Parses command line arguments and sets options.
* @param args collection of argument strings
*/","Parse parameters from the given list of args.  The list is
   *  destructively modified to remove the options.
   * 
   * @param args as a list of input arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getDescription,org.apache.hadoop.fs.shell.Command:getDescription(),547,551,"/**
* Returns a string describing the function mask based on conditions.
* @return descriptive string or null if condition is met
*/","* The long usage suitable for help output
   * @return text of the usage",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayWarning,org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String),510,512,"/**
* Logs error message with prefix.
* @param message error description to be logged
*/","* Display an warning string prefaced with the command name.
   * @param message warning message to display",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getUsage,org.apache.hadoop.fs.shell.Command:getUsage(),537,541,"/**
* Generates a function mask string based on usage flag and command options.
* @return Function mask string or empty if no options available
*/","* The short usage suitable for the synopsis
   * @return ""name options""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)",71,88,"/**
* Initializes a MetricsSourceAdapter with specified configuration.
* @param prefix prefix for metrics source name
* @param name name of metrics source
* @param description description of metrics source
* @param source underlying metrics source
* @param injectedTags optional tags to inject into metrics records
* @param recordFilter filter for metric records
* @param metricFilter filter for individual metrics
* @param jmxCacheTTL time-to-live for JMX cache
* @param startMBeans whether to start MBean registration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo),42,50,"/**
* Builds a Metric Record Builder based on filter criteria.
* @param info Metric information object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,org.apache.hadoop.util.ChunkedArrayList:<init>(),94,96,"/**
* Initializes ChunkedArrayList with default chunk capacity and size.
* @param initialChunkCapacity initial array list chunk capacity
* @param maxChunkSize maximum allowed chunk size
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,<init>,org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List),47,57,"/**
* Segregates ACL entries into access and default scopes.
* @param aclEntries list of AclEntry objects to process
*/","* Creates a new ScopedAclEntries from the given list.  It is assumed that the
   * list is already sorted such that all access entries precede all default
   * entries.
   *
   * @param aclEntries List&lt;AclEntry&gt; to separate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,printToStream,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream),312,334,"/**
* Formats and prints masked data to the specified output stream.
* @param out target output stream
*/","* Render the table to a stream.
     * @param out PrintStream for output",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,moved,org.apache.hadoop.fs.Options$HandleOpt:moved(boolean),432,434,"/**
* Creates a Location instance based on the given boolean flag.
* @param allow true to create an enabled location, false otherwise
*/","* @param allow If true, resolve references to this entity anywhere in
     *              the namespace.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,changed,org.apache.hadoop.fs.Options$HandleOpt:changed(boolean),423,425,"/**
* Creates a new Data instance with the specified mask.
* @param allow boolean flag indicating whether to allow the data
*/","* @param allow If true, resolve references to this entity even if it has
     *             been modified.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,long)",54,59,"/**
* Initializes a Data Fetcher object with the specified directory and interval.
* @param path directory to fetch data from
* @param dfInterval time interval for fetching data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(),901,903,"/**
 * Initializes a new instance of the Shell class with default values.
 * @param initialDelay initial delay value (default is 0) 
 */","* Create an instance with no minimum interval between runs; stderr is
   * not merged with stdout.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,"org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)",204,207,"/**
* Initializes thread to refresh cache with specified space used data.
* @param spaceUsed caching space used details
* @param runImmediately whether to execute immediately or schedule for later
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",255,263,"/**
* Creates a private token by cloning an existing public token.
* @param publicToken original public token to clone
* @param newService service associated with the cloned token
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token),1144,1153,"/**
* Creates a delegated authentication token from the given DelegationToken.
* @param dToken DelegationToken object to create token from
*/","* Generate a DelegationTokenAuthenticatedURL.Token from the given generic
   * typed delegation token.
   *
   * @param dToken The delegation token.
   * @return The DelegationTokenAuthenticatedURL.Token, with its delegation
   *         token set to the delegation token passed in.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path),847,852,"/**
 * Delegates file status iteration to superclass implementation.
 * @param f Path to iterate over
 * @return Iterator of LocatedFileStatus objects or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),284,288,"/**
* Delegates file status iteration to underlying FS instance.
* @param f Path to iterate over
* @return Iterator of LocatedFileStatus objects or null if empty
*/",List files and its block locations in a directory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setVerifyChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean),1368,1372,"/**
* Throws access control exception to enable or disable checksum verification.
* @param verifyChecksum true to enable, false to disable checksum verification
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileChecksum,org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),124,128,"/**
* Computes file checksum using M2 algorithm.
* @param f path to file for which checksum is computed
* @return FileChecksum object or null on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),502,505,"/**
* Calls underlying file system service to compute checksum of given file.
* @param f path to the file
* @return FileChecksum object or null if an error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),179,186,"/**
* Updates path data with mask values.
* @param item PathData object to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",623,627,"/**
* Calls underlying file system to store data.
* @param path file system path
* @param name data identifier
* @param value binary data to be stored
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",282,285,"/**
* Opens a file asynchronously using the FS implementation.
* @param path file location
* @param parameters open file parameters
* @return input stream to the opened file or null if failed
*/","* Open a file by delegating to
   * {@link FileSystem#openFileWithOptions(Path, org.apache.hadoop.fs.impl.OpenFileParameters)}.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   *
   * @return a future which will evaluate to the opened file.ControlAlpha
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",721,726,"/**
* Wraps file opening operation with asynchronous completion. 
* @param path file system path to open
* @param parameters open file parameters
* @return FSDataInputStream or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",444,449,"/**
* Wraps native FS data input stream retrieval in a CompletableFuture.
* @param path file system path to access
* @param parameters open file parameters
* @return CompletableFuture containing FSDataInputStream or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",728,733,"/**
* Delegates to FS implementation to open file.
* @param pathHandle unique file identifier
* @param parameters file open parameters
* @return FSDataInputStream or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",249,252,"/**
* Initializes an INodeDirLink object with specified path and user information.
* @param pathToNode directory path
* @param aUgi UserGroupInformation instance
* @param link INodeLink to be associated
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addDir,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",211,220,"/**
* Creates a new directory with the specified component at the given path.
* @param pathComponent name of the directory to create
* @param aUgi user/group information for the created directory
* @return newly created INodeDir object or throws FileAlreadyExistsException if already exists
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems(),1035,1058,"/**
* Retrieves a list of child file systems.
*@return array of child file system objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getFallbackFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem(),400,411,"/**
* Retrieves the fallback file system mask.
* @return FileSystem object or null if not found
*/","* @return Gets the fallback file system configured. Usually, this will be the
   * default cluster.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,addCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall),118,124,"/**
* Adds an asynchronous call to the processing queue and logs it.
* @param call AsyncCall object to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,update,"org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)",231,241,"/**
* Copies data from input to output buffer.
* @param input input ByteBuffer
* @param output output ByteBuffer
*/","* Continues a multiple-part encryption or decryption operation. The data
   * is encrypted or decrypted, depending on how this cipher was initialized.
   * <p>
   * 
   * All <code>input.remaining()</code> bytes starting at 
   * <code>input.position()</code> are processed. The result is stored in
   * the output buffer.
   * <p>
   * 
   * Upon return, the input buffer's position will be equal to its limit;
   * its limit will not have changed. The output buffer's position will have
   * advanced by n, when n is the value returned by this method; the output
   * buffer's limit will not have changed.
   * <p>
   * 
   * If <code>output.remaining()</code> bytes are insufficient to hold the
   * result, a <code>ShortBufferException</code> is thrown.
   * 
   * @param input the input ByteBuffer
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException if there is insufficient space in the
   * output buffer",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,doFinal,org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer),270,277,"/**
* Encrypts and writes data to the given ByteBuffer.
* @param output ByteBuffer to store encrypted data
*/","* Finishes a multiple-part operation. The data is encrypted or decrypted,
   * depending on how this cipher was initialized.
   * <p>
   * The result is stored in the output buffer. Upon return, the output buffer's
   * position will have advanced by n, where n is the value returned by this
   * method; the output buffer's limit will not have changed.
   * </p>
   * If <code>output.remaining()</code> bytes are insufficient to hold the result,
   * a <code>ShortBufferException</code> is thrown.
   * <p>
   * Upon finishing, this method resets this cipher object to the state it was
   * in when previously initialized. That is, the object is available to encrypt
   * or decrypt more data.
   * </p>
   * If any exception is thrown, this cipher object need to be reset before it
   * can be used again.
   *
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException      if there is insufficient space in the output buffer.
   * @throws IllegalBlockSizeException This exception is thrown when the length
   *                                   of data provided to a block cipher is incorrect.
   * @throws BadPaddingException       This exception is thrown when a particular
   *                                   padding mechanism is expected for the input
   *                                   data but the data is not padded properly.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java,create,org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String),41,66,"/**
* Parses and creates a RegexMountPointInterceptor instance from the input string.
* @param interceptorSettingsString configuration settings as a string
*/","* interceptorSettingsString string should be like ${type}:${string},
   * e.g. replaceresolveddstpath:word1,word2.
   *
   * @param interceptorSettingsString
   * @return Return interceptor based on setting or null on bad/unknown config.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,toString,org.apache.hadoop.fs.DF:toString(),129,139,"/**
* Generates a formatted string for displaying disk usage metrics.
* @return A string containing disk usage information in a specific format
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,normalizePath,"org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)",297,318,"/**
* Normalizes and truncates URL path.
* @param scheme protocol scheme
* @param path original URL path
* @return normalized URL path
*/","* Normalize a path string to use non-duplicated forward slashes as
   * the path separator and remove any trailing path separators.
   *
   * @param scheme the URI scheme. Used to deduce whether we
   * should replace backslashes or not
   * @param path the scheme-specific part
   * @return the normalized path string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isWindowsAbsolutePath,"org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)",341,348,"/**
* Validates file system path syntax.
* @param pathString path string to check
* @param slashed whether the path is supposed to be slash-separated
*/","* Determine whether a given path string represents an absolute path on
   * Windows. e.g. ""C:/a/b"" is an absolute path. ""C:a/b"" is not.
   *
   * @param pathString the path string to evaluate
   * @param slashed true if the given path is prefixed with ""/""
   * @return true if the supplied path looks like an absolute path with a Windows
   * drive-specifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isUriPathAbsolute,org.apache.hadoop.fs.Path:isUriPathAbsolute(),388,391,"/**
* Checks URI mask functionality.
* @return true if successful, false otherwise
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.
   *
   * @return whether this URI's path is absolute",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHarHash,org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path),488,490,"/**
 * Extracts the least significant 31 bits from the given Path object.
 * @param p the input Path object
 */","* the hash of the path p inside  the filesystem
   * @param p the path in the harfilesystem
   * @return the hash code of the path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build(),4694,4714,"/**
* Creates an FSDataOutputStream with specified flags.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus),108,135,"/**
* Converts a FileStatus object to a FileStatusProto message.
* @param stat FileStatus object to convert
* @return FileStatusProto message or null if conversion failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,<init>,"org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)",46,58,"/**
* Initializes an SFTP input stream for a given file.
* @param channel SFTP connection
* @param path Path to the file
* @param stats File statistics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path),69,73,"/**
* Validates and normalizes a file system path.
* @param path the input path to validate
*/","* Validate a path.
   * @param path path to check.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,getExecString,org.apache.hadoop.fs.Stat:getExecString(),91,108,"/**
* Returns the command line arguments for the stat function.
* @return Array of strings containing the command and flags
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",103,106,"/**
* Recursively checks access control for the given path.
* @param operation security operation to perform
* @param p directory or file path to check
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",181,184,"/**
* Recursively checks access control for given operation and path.
* @param operation target operation to check
* @param p directory path to check
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getPathToResolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)",217,227,"/**
* Extracts the file mask from a given path.
* @param srcPath input file path
* @param resolveLastComponent whether to include the last component in the result
* @return file extension (mask) or null if invalid input
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDependencies,"org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2284,2298,"/**
* Validates source and destination paths for copying.
* @param qualSrc the source path
* @param qualDst the destination path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compareTo,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object),3794,3805,"/**
* Compares two SegmentDescriptors based on segment length and offset.
* @param o the other SegmentDescriptor to compare with
* @return negative, zero, or positive int if this is less than, equal to, or greater than that
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object),3807,3820,"/**
* Compares two SegmentDescriptors for equality.
* @param o object to compare with current instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNextIdToTry,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)",731,753,"/**
* Calculates a function mask by traversing the directory tree and finding the maximum file ID.
* @param initial Path to start searching from
* @param lastId previous maximum file ID
* @return next available file ID (id + 1)
*/","* Return the next ID suffix to use when creating the log file. This method
   * will look at the files in the directory, find the one with the highest
   * ID suffix, and 1 to that suffix, and return it. This approach saves a full
   * linear probe, which matters in the case where there are a large number of
   * log files.
   *
   * @param initial the base file path
   * @param lastId the last ID value that was used
   * @return the next ID to try
   * @throws IOException thrown if there's an issue querying the files in the
   * directory",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getPathAsString,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString(),144,146,"/**
* Returns the function mask by executing nested methods.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createServiceURL,org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path),447,453,"/**
* Constructs a function-specific URL from the given path.
* @param path input path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seek,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long),313,319,"/**
* Seeks to a specific position in the resource, throwing an exception if past EOF.
* @param pos target file position
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,skip,org.apache.hadoop.fs.FSInputChecker:skip(long),405,413,"/**
* Calculates and updates functional mask value.
* @param n number of units to add
* @return the updated functional mask value or 0 for invalid input
*/","* Skips over and discards <code>n</code> bytes of data from the
   * input stream.
   *
   * <p>This method may skip more bytes than are remaining in the backing
   * file. This produces no exception and the number of bytes skipped
   * may include some number of bytes that were beyond the EOF of the
   * backing file. Attempting to read from the stream after skipping past
   * the end will result in -1 indicating the end of the file.
   *
   *<p>If <code>n</code> is negative, no bytes are skipped.
   *
   * @param      n   the number of bytes to be skipped.
   * @return     the actual number of bytes skipped.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if the chunk to skip to is corrupted",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,fallbackRead,"org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)",57,117,"/**
* Fetches data from InputStream using a ByteBuffer from the provided pool, 
* with optional zero-copy read and direct buffer support.
* @param stream InputStream to read from
* @param bufferPool ByteBufferPool for allocating buffers
* @param maxLength maximum length of data to read
* @return ByteBuffer containing read data or null on failure
*/","* Perform a fallback read.
   *
   * @param stream input stream.
   * @param bufferPool bufferPool.
   * @param maxLength maxLength.
   * @throws IOException raised on errors performing I/O.
   * @return byte buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,reset,org.apache.hadoop.fs.audit.CommonAuditContext:reset(),185,188,"/**
* Performs mask-related operations on evaluated entries.
* Calls methods to evaluate and process masks.
*/","* Rest the context; will set the standard options again.
   * Primarily for testing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,createInstance,org.apache.hadoop.fs.audit.CommonAuditContext:createInstance(),212,216,"/**
* Creates and initializes a CommonAuditContext object with default settings. 
* @return initialized CommonAuditContext instance
*/","* Demand invoked to create the instance for this thread.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,toString,org.apache.hadoop.tools.TableListing:toString(),229,292,"/**
* Formats table data into a string, wrapping long lines if necessary.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)",82,84,"/**
* Constructs an FsPermission object with specified user, group, and other actions.
* @param u user action
* @param g group action
* @param o other action
*/","* Construct by the given {@link FsAction}.
   * @param u user action
   * @param g group action
   * @param o other action",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(short),95,95,"/**
* Constructs FsPermission object from short mode value.
* @param mode short mode value representing file permissions
*/","* Construct by the given mode.
   * @param mode mode.
   * @see #toShort()",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,readFields,org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput),185,189,"/**
* Reads and updates mask value from input stream.
* @param in DataInput stream containing mask data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,read,org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput),214,218,"/**
* Constructs FsPermission object from DataInput stream.
* @param in input stream containing permission byte
*/","* Create and initialize a {@link FsPermission} from {@link DataInput}.
   *
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return FsPermission.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry),230,232,"/**
* Returns the corresponding file system action based on the ACL entry.
* @param entry AclEntry object containing access control information
*/","* Get the effective permission for the AclEntry
   * @param entry AclEntry to get the effective action
   * @return FsAction.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,createImmutable,"org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",49,57,"/**
* Creates a PermissionStatus instance with the given attributes.
* @param user       user identifier
* @param group      group identifier
* @param permission file system permissions
*/","* Create an immutable {@link PermissionStatus} object.
   * @param user user.
   * @param group group.
   * @param permission permission.
   * @return PermissionStatus.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclSpec,"org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)",235,245,"/**
* Parses ACL specification into a list of AclEntry objects.
* @param aclSpec comma-separated string containing ACL entries
* @param includePermission whether to include permission details in each entry
* @return List of parsed AclEntry objects or empty list if invalid input","* Parses a string representation of an ACL spec into a list of AclEntry
   * objects. Example: ""user::rwx,user:foo:rw-,group::r--,other::---""
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclSpec
   *          String representation of an ACL spec.
   * @param includePermission
   *          for setAcl operations this will be true. i.e. AclSpec should
   *          include permissions.<br>
   *          But for removeAcl operation it will be false. i.e. AclSpec should
   *          not contain permissions.<br>
   *          Example: ""user:foo,group:bar""
   * @return Returns list of {@link AclEntry} parsed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,create,"org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",58,63,"/**
* Creates FsCreateModes object from two FsPermission objects.
* @param masked permission bits to mask
* @param unmasked original permission bits
*/","* Create from masked and unmasked modes.
   *
   * @param masked masked.
   * @param unmasked unmasked.
   * @return FsCreateModes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printExtendedAclEntry,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)",137,152,"/**
* Evaluates ACL entry permissions and outputs mask information.
* @param aclStatus current ACL status
* @param fsPerm file system permission
* @param entry ACL entry to evaluate
*/","* Prints a single extended ACL entry.  If the mask restricts the
     * permissions of the entry, then also prints the restricted version as the
     * effective permissions.  The mask applies to all named entries and also
     * the unnamed group entry.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entry AclEntry extended ACL entry to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toString,org.apache.hadoop.fs.permission.AclEntry:toString(),102,108,"/**
* Returns the function mask value using the m1() method.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,org.apache.hadoop.util.StringUtils:getStrings(java.lang.String),407,410,"/**
* Splits input string by comma delimiter.
* @param str input string to split
*/","* Returns an arraylist of strings.
   * @param str the comma separated string values
   * @return the arraylist of the comma separated string values",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/UmaskParser.java,<init>,org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String),41,45,"/**
* Constructs an UmaskParser instance from a string representation of the mask.
* @param modeStr string describing the mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/RawParser.java,<init>,org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String),35,38,"/**
* Initializes a new RawParser instance with specified mode.
* @param modeStr string representation of the mode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,<init>,org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String),38,40,"/**
* Creates a ChmodParser instance from a string representation of a file's permissions.
* @param modeStr string containing permission settings (e.g., ""rwx"", ""755"")",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",530,535,"/**
* Creates a data block with a limited size and upload statistics.
* @param index ignored parameter (not used)
* @param limit maximum byte count
* @param statistics upload statistics to be included in the block
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,available,org.apache.hadoop.fs.store.ByteBufferInputStream:available(),116,120,"/**
* Returns a function mask value by invoking supporting methods and retrieving result from buffer.
* @return integer function mask value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,position,org.apache.hadoop.fs.store.ByteBufferInputStream:position(),126,129,"/**
* Calls m1() and returns result from byteBuffer's m2().
* @return result of byteBuffer.m2()
*/","* Get the current buffer position.
   * @return the buffer position",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,hasRemaining,org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining(),135,138,"/**
* Calls method m1 and returns result of byteBuffer's m2.
* @return true if successful, false otherwise
*/","* Check if there is data left.
   * @return true if there is data remaining in the buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,reset,org.apache.hadoop.fs.store.ByteBufferInputStream:reset(),147,152,"/**
* Resets application state and performs subsequent operations.
* @throws IOException if an I/O error occurs during reset
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload(),454,458,"/**
* Initiates block data upload with specified mask.
* @throws IOException on upload failure
*/","* Switch to the upload state and return a stream for uploading.
     * Base class calls {@link #enterState(DestState, DestState)} to
     * manage the state machine.
     *
     * @return the stream.
     * @throws IOException trouble",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterClosedState,org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState(),466,473,"/**
* Checks if function is in closed state.
* @return true if closed, false otherwise
*/","* Enter the closed state.
     *
     * @return true if the class was in any other state, implying that
     * the subclass should do its close operations.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)",878,885,"/**
* Writes a portion of the input byte array to the output stream.
* @param b input byte array
* @param offset starting position in the array
* @param len number of bytes to write
* @return number of bytes written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)",747,753,"/**
* Writes data to underlying buffer and disk.
* @param b byte array to write
* @param offset starting offset in byte array
* @param len number of bytes to write
* @return number of bytes successfully written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush(),940,943,"/**
* Calls superclass and output methods.
*/","* Flush operation will flush to disk.
     *
     * @throws IOException IOE raised on FileOutputStream",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)",612,618,"/**
* Writes data to underlying storage.
* @param b the byte array to write
* @param offset starting position in array
* @param len number of bytes to write
* @return number of bytes actually written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString(),767,776,"/**
* Returns a human-readable string representation of the ByteBuffer block.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStatistics,org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI),2396,2398,"/**
 * Wraps URI in a call to AbstractFileSystem.m1.
 * @param uri input URI
 */","* Get the statistics for a particular file system
   * 
   * @param uri
   *          the uri to lookup the statistics. Only scheme and authority part
   *          of the uri are used as the key to store and lookup.
   * @return a statistics object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createMultipartUploader,org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path),457,461,"/**
 * Returns an instance of MultipartUploaderBuilder configured with the given base path. 
 * @param basePath directory containing uploaded files
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getCurrentDirectoryIndex,org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex(),257,260,"/**
* Calls m1() to initialize context and then delegates to m2().
* @return result from m2() in initialized context
*/","* Get the current directory index for the given configuration item.
   * @return the current directory index for the given configuration item.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,getPos,org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos(),49,52,"/**
* Calls the equivalent method on the output formatter. 
* @return result from fsOut.m1()",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Writer:sync(),1369,1375,"/**
* Writes sync data to output stream or updates last sync position.
* @throws IOException on write error
*/","* create a sync point.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getLength,org.apache.hadoop.io.SequenceFile$Writer:getLength(),1530,1532,"/**
* Returns a mask value from output stream.
* @throws IOException on I/O error
*/","@return Returns the current length of the output file.
     *
     * <p>This always returns a synchronized position.  In other words,
     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
     * the key may be earlier in the file than key last written when this
     * method was called (e.g., with block-compression, it may be the first key
     * in the block that was being written when this method was called).</p>
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCurrentPos,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos(),156,158,"/**
* Calculates a long value by combining two masked values.
* @return combined mask value
*/","* Get the current position in file.
       * 
       * @return The current byte offset in underlying file.
       * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getContentSummary,org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path),1923,1945,"/**
* Computes the content summary for a given file path.
* @param f the file path to process
* @return ContentSummary object representing the aggregated data
*/","Return the {@link ContentSummary} of a given {@link Path}.
   * @param f path to use
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure
   * @return content summary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,buildACL,org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[]),107,126,"/**
* Processes user group strings to populate users and groups sets.
* @param userGroupStrings array of ACL parts with user/group info
*/","* Build ACL from the given array of strings.
   * The strings contain comma separated values.
   *
   * @param userGroupStrings build ACL from array of Strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,parseEnumSet,"org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)",68,99,"/**
* Retrieves an EnumSet from a string list, applying a mapping and ignoring unknown values if requested.
* @param key unique identifier for the operation
* @param valueString comma-separated string of values to process
* @param enumClass Class representing the enumeration type
* @param ignoreUnknown true to bypass exceptions on unknown values
* @return EnumSet containing processed values, or null if invalid input
*/","* Given a comma separated list of enum values,
   * trim the list, map to enum values in the message (case insensitive)
   * and return the set.
   * Special handling of ""*"" meaning: all values.
   * @param key Configuration object key -used in error messages.
   * @param valueString value from Configuration
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values be ignored?
   * @param <E> enum type
   * @return a mutable set of enum values parsed from the valueString, with any unknown
   * matches stripped if {@code ignoreUnknown} is true.
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,ensureCurrentState,org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE),97,104,"/**
* Validates service state against a specified expected state.
* @param expectedState required service state
*/","* Verify that that a service is in a given state.
   * @param expectedState the desired state
   * @throws ServiceStateException if the service state is different from
   * the desired state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,<init>,org.apache.hadoop.service.AbstractService:<init>(java.lang.String),113,116,"/**
 * Initializes an AbstractService instance with a given name.
 * @param name unique identifier of the service
 */","* Construct the service.
   * @param name service name",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,checkStateTransition,"org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",128,135,"/**
* Validates transition of a service to a proposed state.
* @param name service name
* @param state current service state
* @param proposed target service state
*/","* Check that a state tansition is valid and
   * throw an exception if not
   * @param name name of the service (can be null)
   * @param state current state
   * @param proposed proposed new state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceCreationFailure,org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception),745,747,"/**
* Creates a ServiceLaunchException instance with exit code and original exception.
* @param exception original exception to be wrapped
*/","* Generate an exception announcing a failure to create the service.
   * @param exception inner exception.
   * @return a new exception, with the exit code
   * {@link LauncherExitCodes#EXIT_SERVICE_CREATION_FAILURE}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,verifyConfigurationFilesExist,org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[]),989,1003,"/**
* Validates and logs configuration files.
* @param filenames array of configuration file names
*/","* Verify that all the specified filenames exist.
   * @param filenames a list of files
   * @throws ServiceLaunchException if a file is not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])",1088,1092,"/**
* Creates a new KerberosDiagsFailure instance with formatted error message. 
* @param category error category
* @param message format string for error message
* @param args variable arguments to be used in formatting
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,convertToExitException,org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable),714,737,"/**
* Creates an ExitUtil.ExitException instance based on the provided Throwable.
* @param thrown the exception to wrap
* @return a custom ExitException with additional details or null if not applicable
*/","* Convert an exception to an {@code ExitException}.
   *
   * This process may just be a simple pass through, otherwise a new
   * exception is created with an exit code, the text of the supplied
   * exception, and the supplied exception as an inner cause.
   * 
   * <ol>
   *   <li>If is already the right type, pass it through.</li>
   *   <li>If it implements {@link ExitCodeProvider#getExitCode()},
   *   the exit code is extracted and used in the new exception.</li>
   *   <li>Otherwise, the exit code
   *   {@link LauncherExitCodes#EXIT_EXCEPTION_THROWN} is used.</li>
   * </ol>
   *  
   * @param thrown the exception thrown
   * @return an {@code ExitException} with a status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,<init>,org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service),52,54,"/**
 * Registers a shutdown hook to safely terminate services.
 * @param service service instance to be shut down
 */","* Create an instance.
   * @param service the service",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,toString,org.apache.hadoop.service.launcher.InterruptEscalator:toString(),89,101,"/**
* Constructs InterruptEscalator string representation.
* @return formatted String or null if owner is null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,noteFailure,org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception),257,272,"/**
* Records and handles service failure by updating the failure cause.
* @param exception Exception instance to be recorded
*/","* Failure handling: record the exception
   * that triggered it -if there was not one already.
   * Services are free to call this themselves.
   * @param exception the exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,recordLifecycleEvent,org.apache.hadoop.service.AbstractService:recordLifecycleEvent(),421,426,"/**
* Logs lifecycle event with current time and state.
*/",* Add a state change event to the lifecycle history,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceInit,org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration),104,113,"/**
* Initializes and starts all registered services.
* @param conf Hadoop configuration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,stop,"org.apache.hadoop.service.CompositeService:stop(int,boolean)",147,170,"/**
* Stops specified number of started services in reverse order.
* @param numOfServicesStarted number of services to stop
* @param stopOnlyStartedServices true to only stop started services, false to also stop init'd services
*/","* Stop the services in reverse order
   *
   * @param numOfServicesStarted index from where the stop should work
   * @param stopOnlyStartedServices flag to say ""only start services that are
   * started, not those that are NOTINITED or INITED.
   * @throws RuntimeException the first exception raised during the
   * stop process -<i>after all services are stopped</i>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service),65,67,"/**
* Wraps Service exception in LOG exception.
* @param service instance of Service to wrap exception from
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,progressable,org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable),1036,1038,"/**
* Wraps a Progressable value as an Option.
* @param value Progressable object to be wrapped
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,blockSize,org.apache.hadoop.io.SequenceFile$Writer:blockSize(long),1032,1034,"/**
* Creates an option with a specific function mask.
* @param value long integer representing the function mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncInterval,org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int),1061,1063,"/**
* Returns a SyncIntervalOption instance with the specified mask value.
* @param value integer representing the mask value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,replication,org.apache.hadoop.io.SequenceFile$Writer:replication(short),1024,1026,"/**
* Creates an Option instance from a replication mask value.
* @param value short integer representing a replication mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,bufferSize,org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int),1016,1018,"/**
* Creates an Option instance representing a function mask with the specified value.
* @param value integer value representing the function mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput),126,129,"/**
 * Writes function mask data to output stream.
 * @param out DataOutput stream to write to
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,write,org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput),171,200,"/**
* Writes masked data to output stream based on component type.
*@param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,valueClass,org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class),1044,1046,"/**
 * Creates an option with a given class as its value.
 * @param value Class to be used as the option's value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,keyClass,org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class),285,287,"/**
* Returns an Option instance representing a function mask.
* @param value class type of the writable comparable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,keyClass,org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class),1040,1042,"/**
* Creates a KeyClassOption instance with the given Class value.
* @param value Class object to create option from
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,compareTo,org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8),156,160,"/**
* Calculates function mask by comparing two UTF-8 strings.
* @param o string to compare with this object's bytes
*/",Compare two UTF8s.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,equals,org.apache.hadoop.io.UTF8:equals(java.lang.Object),193,203,"/**
* Compares the bytes of two UTF-8 objects for equality.
* @param o object to compare with this instance
* @return true if byte arrays are equal, false otherwise
*/",Returns true iff <code>o</code> is a UTF8 with the same contents.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,compareTo,org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash),241,245,"/**
* Calculates a mask value using MD5 hashes.
* @param that the other MD5 hash to compare with
* @return an integer mask value
*/",Compares this object with the specified object for order.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable),50,56,"/**
* Calculates the bitwise mask between two binary comparable values.
* @param other The BinaryComparable value to compare with
*/","* Compare bytes from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,"org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)",66,69,"/**
* Computes a bitwise mask from two byte arrays.
* @param other input array to combine with
* @param off offset into other array
* @param len length of input array slice
*/","* Compare bytes from {#getBytes()} to those provided.
   *
   * @param other other.
   * @param off off.
   * @param len len.
   * @return compareBytes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)",89,92,"/**
* Computes the mask value using the provided byte arrays and their respective offsets and lengths.
* @param b1 first byte array
* @param s1 offset in b1
* @param l1 length of b1
* @param b2 second byte array
* @param s2 offset in b2
* @param l2 length of b2
* @return the computed mask value",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,hashCode,org.apache.hadoop.io.UTF8:hashCode(),205,208,"/**
* Calculates the mask value based on input byte array and its length.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,hashCode,org.apache.hadoop.io.BinaryComparable:hashCode(),88,91,"/**
* Calculates a mask value using m1 and m2 values.
* @return calculated mask value as an integer
*/","* Return a hash of the bytes returned from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token:hashCode(),402,405,"/**
* Generates a function mask using the provided identifier.
* @return integer function mask value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readDouble,"org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)",311,313,"/**
* Calculates masked value using m1 and m2 functions.
* @param bytes input byte array
* @param start starting index in bytes
*/","* Parse a double from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return double from a byte array.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setSize,org.apache.hadoop.io.BytesWritable:setSize(int),131,138,"/**
* Sets the array size with optional resizing if necessary.
* @param size new array size
*/","* Change the size of the buffer. The values in the old range are preserved
   * and any new values are undefined. The capacity is changed if it is 
   * necessary.
   * @param size The new number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,<init>,org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator),38,40,"/**
* Initializes Merge Sort algorithm with a custom comparator.
* @param comparator comparison function to sort IntWritable objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canRead,org.apache.hadoop.fs.FileUtil:canRead(java.io.File),1412,1423,"/**
* Checks file accessibility on Windows or calls platform-independent method.
* @param f File to check
* @return true if accessible, false otherwise
*/","* Platform independent implementation for {@link File#canRead()}
   * @param f input file
   * @return On Unix, same as {@link File#canRead()}
   *         On Windows, true if process has read access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canWrite,org.apache.hadoop.fs.FileUtil:canWrite(java.io.File),1431,1442,"/**
* Tests write access to a file on the current OS.
* @param f File object
* @return true if writable, false otherwise
*/","* Platform independent implementation for {@link File#canWrite()}
   * @param f input file
   * @return On Unix, same as {@link File#canWrite()}
   *         On Windows, true if process has write access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canExecute,org.apache.hadoop.fs.FileUtil:canExecute(java.io.File),1450,1461,"/**
* Checks if file has execute permission.
* @param f the File object to check
*/","* Platform independent implementation for {@link File#canExecute()}
   * @param f input file
   * @return On Unix, same as {@link File#canExecute()}
   *         On Windows, true if process has execute access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,assertCodeLoaded,org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded(),364,368,"/**
* Validates native IO loading.
* @throws IOException if NativeIO is not loaded
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,getInstance,org.apache.hadoop.io.ReadaheadPool:getInstance(),55,62,"/**
* Returns an instance of the ReadaheadPool class, initializing it if necessary.
*/",* @return Return the singleton instance for the current process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,verifyCanMlock,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock(),300,302,"/**
* Returns true if mask is enabled.
* @return true if mask is enabled, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,getLoadingFailureReason,org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason(),53,61,"/**
* Returns the function mask as a string, or error message if NativeIO is unavailable or non-UNIX OS.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit(),884,886,"/**
* Calculates a function mask value based on condition evaluation.
* @return non-zero if m1() and m2() both evaluate to true, otherwise 0
*/","* Get the maximum number of bytes that can be locked into memory at any
   * given point.
   *
   * @return 0 if no bytes can be locked into memory;
   *         Long.MAX_VALUE if there is no limit;
   *         The number of bytes that can be locked into memory otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)",576,600,"/**
* Calculates checksum for provided data and stores it in the sums array.
* @param data input data to calculate checksum from
* @param dataOffset offset into data where calculation starts
* @param dataLength length of data to process
* @param sums output array to store calculated checksums
* @param sumsOffset offset into sums array where result will be stored
*/","* Implementation of chunked calculation specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.
   *
   * @param data data.
   * @param dataOffset dataOffset.
   * @param dataLength dataLength.
   * @param sums sums.
   * @param sumsOffset sumsOffset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getCreateForWriteFileOutputStream,"org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)",1001,1037,"/**
* Creates a file output stream with specified permissions.
* @param f the file to create
* @param permissions the desired file permissions
* @return FileOutputStream object for writing to the file
* @throws IOException if an I/O error occurs
*/","* @return Create the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,run,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run(),210,232,"/**
* Performs file read-ahead operation based on the specified conditions.
* @throws IOException if an I/O error occurs during the operation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,cleanBufferPool,org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool(),816,821,"/**
* Releases and reinitializes crypto stream buffers from pool.
*/",Clean direct buffer pool,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers(),311,314,"/**
* Initializes crypto streams with masks. 
* @param inBuffer input buffer to mask
* @param outBuffer output buffer to mask
*/",Forcibly free the direct buffers.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getFstat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor),575,596,"/**
* Fetches file status by descriptor, handling Windows-specific differences.
* @param fd FileDescriptor to retrieve status for
* @return Stat object containing file status or null on failure
*/","* Returns the file stat for a file descriptor.
     *
     * @param fd file descriptor.
     * @return the file descriptor file stat.
     * @throws IOException thrown if there was an IO error while obtaining the file stat.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getStat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String),606,627,"/**
* Retrieves file statistics for the specified path.
* @param path file system path
* @return Stat object containing file attributes or null on error
*/","* Return the file stat for a file path.
     *
     * @param path  file path
     * @return  the file stat
     * @throws IOException  thrown if there is an IO error while obtaining the
     * file stat",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)",55,57,"/**
* Creates a new instance of BoundedByteArrayOutputStream with specified initial capacity and maximum size limit.","* Create a BoundedByteArrayOutputStream with the specified
   * capacity and limit.
   * @param capacity The capacity of the underlying byte array
   * @param limit The maximum limit upto which data can be written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet),80,82,"/**
* Constructs an instance from a given EnumSet.
* @param value the EnumSet to copy
*/","* Construct a new EnumSetWritable. Argument <tt>value</tt> should not be null
   * or empty.
   * 
   * @param value enumSet value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)",107,109,"/**
 * Invokes buffer's m1 method to process input stream data.
 * @param in input stream containing data
 * @param length length of input data
 */","* Writes bytes from a InputStream directly into the buffer.
   * @param in input in.
   * @param length input length.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,nextBytes,org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[]),97,108,"/**
* Synchronized method to process and mask a byte array.
* @param bytes input byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,next,org.apache.hadoop.crypto.random.OsSecureRandom:next(int),110,118,"/**
* Computes a mask of the specified length from a series of 32-bit integers.
* @param nbits desired mask length
* @return int representing the computed mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,fromString,org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String),75,81,"/**
* Decodes and restores object from base64-encoded string.
* @param str base64-encoded string
* @return restored object of type T or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyStream,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream(),1806,1809,"/**
* Reads and processes key data from buffer.
* @return DataInputStream object containing processed key data
*/","* Streaming access to the key. Useful for desrializing the key into
         * user objects.
         * 
         * @return The input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeWritable,"org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)",355,366,"/**
* Encodes and sets new value for writable object using base64 encoding.
* @param obj Writable object to set new value on
* @param newValue new encoded value as string
*/","* Modify the writable to the value from the newValue.
   * @param obj the object to read into
   * @param newValue the string with the url-safe base64 encoded bytes
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeUncompressedBytes,org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream),699,715,"/**
* Writes decompressed data to output stream using codec and buffer.
*@throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)",177,192,"/**
* Computes and returns a result using buffers and keys.
* @param b1 input byte array 1
* @param s1 start index for b1
* @param l1 length of b1
* @param b2 input byte array 2
* @param s2 start index for b2
* @param l2 length of b2
* @return result value (type not specified)
*/","Optimization hook.  Override this to make SequenceFile.Sorter's scream.
   *
   * <p>The default implementation reads the data into two {@link
   * WritableComparable}s (using {@link
   * Writable#readFields(DataInput)}, then calls {@link
   * #compare(WritableComparable,WritableComparable)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
* Constructs an instance of RSErasureCodec with configuration and options.
* @param conf Configuration object
* @param options ErasureCodec options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
* Constructs an instance of HHXORErasureCodec with given configuration and options. 
* @param conf HBase Configuration object
* @param options ErasureCodec-specific options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",32,34,"/**
 * Constructs a DummyErasureCodec instance with the given configuration and options.
 * @param conf overall configuration
 * @param options codec-specific options
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,37,"/**
* Initializes XORErasureCodec with configuration and erasure codec options.
* @param conf Hadoop Configuration instance
* @param options Erasure Codec Options containing schema information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawCoderFactory,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)",154,161,"/**
* Retrieves a Raw Erasure Coder Factory instance based on the provided coder and codec names.
* @param coderName name of the erasure coder
* @param codecName name of the codec to be used with the coder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,anyRecoverable,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup),86,90,"/**
* Checks if the given ECBlockGroup has valid erase count.
* @param blockGroup ECBlockGroup object to verify
* @return true if erase count is within valid range, false otherwise
*/","* Given a BlockGroup, tell if any of the missing blocks can be recovered,
   * to be called by ECManager
   * @param blockGroup a blockGroup that may contain erased blocks but not sure
   *                   recoverable or not
   * @return true if any erased block recoverable, false otherwise",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),132,136,"/**
* Recursively sums up ECBlockGroup values from its sub-groups.
* @param blockGroup ECBlockGroup instance
* @return total value of the block group and its children
*/","* Get the number of erased blocks in the block group.
   * @param blockGroup blockGroup.
   * @return number of erased blocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getErasedIndexes,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[]),159,174,"/**
* Extracts indices of EC blocks that match a certain condition.
* @param inputBlocks array of EC blocks to search
* @return array of indices or empty array if none found
*/","* Get indexes of erased blocks from inputBlocks
   * @param inputBlocks inputBlocks.
   * @return indexes of erased blocks from inputBlocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[]),98,122,"/**
* Validates and counts input buffers against specified conditions.
* @param buffers array of ByteBuffer objects to validate
* @throws HadoopIllegalArgumentException on invalid buffer or insufficient count
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][]),95,115,"/**
* Validates input buffers for decoding.
* @param buffers array of byte arrays to validate
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",52,65,"/**
* Initializes erasure decoding step with input blocks, erased indexes,
* and encoding/decoding tools.
* @param inputBlocks EC block array
* @param erasedIndexes array of erased data unit indices
* @param outputBlocks EC block array for results
* @param rawDecoder Raw Erasure Decoder instance
* @param rawEncoder Raw Erasure Encoder instance
*/","* The constructor with all the necessary info.
   * @param inputBlocks inputBlocks.
   * @param erasedIndexes the indexes of erased blocks in inputBlocks array
   * @param outputBlocks outputBlocks.
   * @param rawDecoder underlying RS decoder for hitchhiker decoding
   * @param rawEncoder underlying XOR encoder for hitchhiker decoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])",38,54,"/**
* Validates input arrays for masking operation.
* @param inputs array of data to mask
* @param erasedIndexes array of indexes to erase from outputs
* @param outputs array of masked data
*/","* Check and validate decoding parameters, throw exception accordingly. The
   * checking assumes it's a MDS code. Other code  can override this.
   * @param inputs input buffers to check
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",48,57,"/**
* Initializes ECBlock-based erasure encoding step with raw encoders.
* @param inputBlocks array of input blocks
* @param outputBlocks array of output blocks
* @param rsRawEncoder raw encoder for Reed-Solomon encoding
* @param xorRawEncoder raw encoder for XOR encoding
*/","* The constructor with all the necessary info.
   *
   * @param inputBlocks inputBlocks.
   * @param outputBlocks outputBlocks.
   * @param rsRawEncoder  underlying RS encoder for hitchhiker encoding
   * @param xorRawEncoder underlying XOR encoder for hitchhiker encoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/EncodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])",36,43,"/**
* Validates input and output array lengths against encoder parameters.
* @param inputs input data array
* @param outputs output data array
*/","* Check and validate decoding parameters, throw exception accordingly.
   * @param inputs input buffers to check
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates an instance of XORRawDecoder with given options.
* @param coderOptions ErasureCoderOptions to configure the decoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,45,"/**
* Creates an ErasureDecodingStep for the given block group.
* @param blockGroup EC block group to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,39,"/**
* Returns a raw decoder instance for masking.
* @param coderOptions Erasure coding options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeXORRawDecoder with given ErasureCoderOptions.
* @param coderOptions configuration options for the decoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeRSRawDecoder with ErasureCoderOptions.
* @param coderOptions options for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBackForDecode,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)",150,201,"/**
* Computes and stores parity information in a ByteBuffer.
* @param inputs input data arrays
* @param outputs output data arrays
* @param pbParityIndex index of parity unit to compute
* @return ByteBuffer containing computed parity information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])",371,384,"/**
* Merges two arrays of integers into a single array.
* @param p first array
* @param q second array
* @return merged array with common elements combined","* Compute the sum of two polynomials. The index in the array corresponds to
   * the power of the entry. For example p[0] is the constant term of the
   * polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p+q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])",327,340,"/**
* Computes the element-wise product of two arrays using a recursive formula. 
* @param p first array
* @param q second array
* @return an array with the element-wise product of p and q","* Compute the multiplication of two polynomials. The index in the array
   * corresponds to the power of the entry. For example p[0] is the constant
   * term of the polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p*q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,gaussianElimination,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][]),549,588,"/**
* Performs row operations to transform the input matrix into upper triangular form.
* @param matrix 2D array to be transformed
*/","* Perform Gaussian elimination on the given matrix. This matrix has to be a
   * fat matrix (number of rows &gt; number of columns).
   *
   * @param matrix matrix.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,getPrimitivePower,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)",38,45,"/**
* Computes the primitive power array of a given size.
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
* @return an array of primitive powers with total size numDataUnits + numParityUnits
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunks,"org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])",80,87,"/**
* Prints header and processes ECChunks.
* @param header input header string
* @param chunks array of ECChunk objects to process
*/","* Print data in hex format in an array of chunks.
   * @param header header.
   * @param chunks chunks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeXORRawEncoder with ErasureCoderOptions.
* @param coderOptions configuration for the encoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeRSRawEncoder with ErasureCoderOptions.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates an instance of XORRawEncoder with given configuration.
* @param coderOptions ErasureCoderOptions to configure encoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,44,"/**
* Creates a masking erasure coding step for the given block group.
* @param blockGroup EC block group object
* @return ErasureCodingStep instance or null if invalid input
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),31,34,"/**
* Creates a raw encoder with dummy functionality.
* @param coderOptions ErasureCoderOptions instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)",87,91,"/**
* Applies function m1 to each ByteBuffer in the array.
* @param buffers array of ByteBuffers
* @param dataLen length of data to process
*/",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,toBuffers,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),108,125,"/**
* Processes ECChunk array and returns a ByteBuffer array with filtered data.
* @param chunks Array of ECChunks to process
*/","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convertToByteArrayState into buffers
   * @return an array of ByteBuffers",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)",96,101,"/**
* Applies function m1 to each buffer array in parallel.
* @param buffers arrays of byte data
* @param offsets initial offset for each array
* @param dataLen length of the data to process
*/",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),83,96,"/**
* Converts ByteArrayEncodingState to ByteBuffer state, then recursively processes outputs.
* @param encodingState state containing output data and offsets
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,99,"/**
* Processes output buffers with ByteBufferDecodingState.
* @param decodingState state object containing output buffers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),57,71,"/**
* Initializes RSRawDecoder with ErasureCoderOptions.
* @param coderOptions encoding options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),42,61,"/**
* Initializes RS raw encoder with ErasureCoderOptions.
* @param coderOptions configuration options for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,generateDecodeMatrix,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[]),143,176,"/**
* Updates the decode matrix based on erased indexes.
* @param erasedIndexes array of indices that have been erased
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(),121,123,"/**
* Initializes a Galois Field with default parameters.
* @return GaloisField object
*/","* Get the object performs Galois field arithmetic with default setting.
   * @return GaloisField.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsR,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long),616,638,"/**
* Extracts a specified number of bits from the input buffer.
* @param n number of bits to extract
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetBit,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit(),640,658,"/**
* Checks the input buffer for a function mask bit.
* @throws IOException if unexpected end of stream encountered
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init(),756,772,"/**
* Initializes data and combined CRC for a 100k block size.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endCompression,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression(),833,849,"/**
* Generates mask values using various operations and updates combined CRC. 
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues(),951,992,"/**
* Initializes and updates internal data structures for mask calculations.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort(),1738,1901,"/**
* Reorders and processes data using a custom algorithm.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",49,51,"/**
* Constructs a CompressorStream instance with default buffer size.
* @param out output stream to write compressed data to
* @param compressor compression algorithm to use
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)",54,58,"/**
* Initializes BlockCompressorStream with output stream and compression settings.
* @param out target OutputStream
* @param compressor Compressor instance for data compression
* @param bufferSize allocated buffer size for input data
* @param compressionOverhead overhead bytes consumed by compression algorithm
*/","* Create a {@link BlockCompressorStream}.
   * 
   * @param out stream
   * @param compressor compressor to be used
   * @param bufferSize size of buffer
   * @param compressionOverhead maximum 'overhead' of the compression 
   *                            algorithm with given bufferSize",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,org.apache.hadoop.io.compress.CompressorStream:write(int),115,119,"/**
* Invokes recursive processing on the byte array with single-byte input.
* @param b single byte to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",68,72,"/**
* Initializes DecompressorStream with given input stream and buffer size.
* @param in InputStream to decompress
* @param decompressor Decompression strategy
* @param bufferSize Buffer size for I/O operations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,<init>,org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream),162,166,"/**
* Initializes decompression stream with an external input source.
* @param input InputStream to pass through to underlying decompression logic
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream),64,66,"/**
 * Initializes the decompressor stream with the given input stream.
 * @param in InputStream to read compressed data from
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,decompress,"org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)",108,173,"/**
* Decompresses a block of data using the given decompressor and buffer.
* @param b compressed byte array
* @param off offset into the array
* @param len length of the block
* @return length of decompressed data or -1 on EOF
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,finish,org.apache.hadoop.io.compress.BlockCompressorStream:finish(),136,145,"/**
* Continues processing until a valid compressed block is encountered.
* @throws IOException on compression or I/O error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int),78,85,"/**
* Initializes ZStandard decompressor with specified buffer size.
* @param bufferSize maximum size of direct buffers for compression and decompression
*/","* Creates a new decompressor.
   * @param bufferSize bufferSize.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finalize,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize(),248,251,"/**
* Performs mask-related functionality. 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset(),307,311,"/**
* Calls superclass method and sets end-of-input flag.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)",94,103,"/**
* Initializes ZStandardCompressor with specified compression level and buffer sizes.
* @param level compression level
* @param inputBufferSize size of direct buffer for uncompressed data
* @param outputBufferSize size of direct buffer for compressed data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,decompress,"org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)",68,112,"/**
* Decompresses data in-place using a provided decompressor.
* @param b the input byte array
* @param off offset into the input array
* @param len length of the input to process
* @return number of bytes decompressed or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(),102,104,"/**
 * Initializes a new instance of the Lz4Compressor class with default direct buffer size.
 */",* Creates a new compressor with the default buffer size.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClassByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String),274,281,"/**
* Retrieves a compression codec instance by name, returning the class type or null on failure. 
* @param codecName name of the desired compression codec
*/","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias and returns its implemetation class.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,"org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)",149,165,"/**
* Creates or retrieves Compressor instance based on provided codec and configuration.
* @param codec CompressionCodec instance
* @param conf Configuration object
* @return Compressor instance or null if creation failed
*/","* Get a {@link Compressor} for the given {@link CompressionCodec} from the 
   * pool or a new one.
   *
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Compressor</code>
   * @param conf the <code>Configuration</code> object which contains confs for creating or reinit the compressor
   * @return <code>Compressor</code> for the given 
   *         <code>CompressionCodec</code> from the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getDecompressor,org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec),180,195,"/**
* Retrieves a Decompressor instance from pool or creates a new one based on codec.
* @param codec CompressionCodec object
* @return Decompressor instance or null if creation fails
*/","* Get a {@link Decompressor} for the given {@link CompressionCodec} from the
   * pool or a new one.
   *  
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Decompressor</code>
   * @return <code>Decompressor</code> for the given 
   *         <code>CompressionCodec</code> the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnCompressor,org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor),202,215,"/**
* Updates or removes a Compressor instance based on its type.
* @param compressor Compressor object to process
*/","* Return the {@link Compressor} to the pool.
   * 
   * @param compressor the <code>Compressor</code> to be returned to the pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnDecompressor,org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),223,236,"/**
* Applies mask to a Decompressor instance based on its type.
* @param decompressor Decompressor instance to process
*/","* Return the {@link Decompressor} to the pool.
   * 
   * @param decompressor the <code>Decompressor</code> to be returned to the 
   *                     pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType(),151,155,"/**
* Returns the compressor class based on function mask.
* @return Compressor subclass (ZStandardCompressor) 
*/","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType(),209,213,"/**
* Returns the decompressor class based on the function mask.
* @return Class of the ZStandard decompressor.","* Get the type of {@link Decompressor} needed by
   * this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getCompressorType,org.apache.hadoop.io.compress.DefaultCodec:getCompressorType(),69,72,"/**
* Returns compressor class based on configuration.
* @return compressor class type (e.g. ZlibCompressor)",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getDecompressorType,org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType(),95,98,"/**
* Returns the Decompressor class for zlib compression. 
* @return Class of decompressor to use (e.g., ZlibDecompressor)",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration),66,68,"/**
* Initializes BuiltInGzipCompressor with configuration.
* @param conf Hadoop Configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration),167,175,"/**
* Resets configuration and initializes CRC calculation.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(),122,126,"/**
 * Initializes a new instance of the GzipZlibCompressor with default compression settings.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),128,133,"/**
* Initializes GzipZlibCompressor with configuration settings.
* @param conf Hadoop Configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(),234,239,"/**
* Initializes Zlib compressor with default settings.
*/","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZLIB format.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),245,250,"/**
* Initializes Zlib compressor with configuration settings.
* @param conf application configuration
*/","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration),283,298,"/**
* Reinitializes the compressor with a new configuration.
* @param conf Configuration object
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   * 
   * @param conf Configuration storing new settings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(),352,354,"/**
* Initializes a new instance of ZlibDirectDecompressor with default compression header.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",356,358,"/**
* Constructs a ZlibDirectDecompressor instance with given compression header and direct buffer size.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(),117,119,"/**
* Initializes ZlibDecompressor with default header and buffer size. 
* @param compressionHeader default zlib compression header
* @param directBufferSize initial direct buffer size for decompression
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>(),137,139,"/**
* Initializes GzipZlibDecompressor with autodetection and buffer size.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset(),365,369,"/**
* Calls superclass's m1() and marks input as ended. 
* @see #endOfInput */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeHeaderState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState(),257,358,"/**
* Processes gzip header and sets the state accordingly.
*/","* Parse the gzip header (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy (some) header bytes to another buffer.  (Filename,
   * comment, and extra-field bytes are simply skipped.)</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.  Note that
   * no version of gzip to date (at least through 1.4.0, 2010-01-20) supports
   * the FHCRC header-CRC16 flagbit; instead, the implementation treats it
   * as a multi-file continuation flag (which it also doesn't support). :-(
   * Sun's JDK v6 (1.6) supports the header CRC, however, and so do we.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedStringArray,org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput),181,189,"/**
* Reads and returns an array of strings from the input stream.
* @param in DataInput object
* @return String array or null if invalid length
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,write,org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput),54,57,"/**
* Writes function mask value to output stream.
* @param out DataOutput stream to write to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,org.apache.hadoop.io.Text:write(java.io.DataOutput),395,399,"/**
* Writes user profile data to output stream.
* @param out DataOutput stream to write to
*/","* Serialize. Write this object to out length uses zero-compressed encoding.
   *
   * @see Writable#write(DataOutput)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,"org.apache.hadoop.io.Text:write(java.io.DataOutput,int)",401,409,"/**
* Writes data to output stream with size check.
* @param out DataOutput stream
* @param maxLength maximum allowed data length
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,write,org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput),94,104,"/**
* Writes user data to output stream.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput),737,747,"/**
* Writes user data to output stream.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVInt,org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput),334,340,"/**
* Extracts and casts a 64-bit value from the input stream into an integer.
* @param stream DataInput stream containing the value
* @return The extracted integer value or throws IOException if out of range
*/","* Reads a zero-compressed encoded integer from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVIntInRange,"org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)",354,370,"/**
* Validates and extracts an integer value from the input stream within a specified range.
* @param stream DataInput stream containing the integer value
* @param lower minimum allowed value
* @param upper maximum allowed value
* @return extracted integer value as int, or throws IOException if invalid
*/","* Reads an integer from the input stream and returns it.
   *
   * This function validates that the integer is between [lower, upper],
   * inclusive.
   *
   * @param stream Binary input stream
   * @param lower input lower.
   * @param upper input upper.
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,readFields,org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput),49,52,"/**
* Reads and sets the FUNC_MASK from the input stream.
* @param in DataInput object containing the mask value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(java.lang.String),95,97,"/**
* Initializes Text instance from input string.
* @param string input text to be stored in this Text object
*/","* Construct from a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,"org.apache.hadoop.io.Text:find(java.lang.String,int)",187,221,"/**
* Finds the position of a substring within a byte array.
* @param what the target substring
* @param start search starting offset in the array
* @return the matched position or -1 if not found
*/","* Finds any occurrence of <code>what</code> in the backing
   * buffer, starting as position <code>start</code>. The starting
   * position is measured in bytes and the return value is in
   * terms of byte position in the buffer. The backing buffer is
   * not converted to a string for this operation.
   *
   * @param what input what.
   * @param start input start.
   * @return byte position of the first occurrence of the search
   *         string in the UTF-8 buffer or -1 if not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)",582,588,"/**
* Writes serialized data to output stream and returns its length.
* @param out output stream
* @param s input string
* @return integer length of written data or -1 on failure
*/","* Write a UTF8 encoded string to out.
   *
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.
   * @return a UTF8 encoded string to out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)",598,610,"/**
* Writes a string to output with length masking.
* @param out DataOutput stream
* @param s String value
* @param maxLength maximum allowed bytes
* @return actual bytes written
*/","* @return Write a UTF8 encoded string with a maximum size to out.
   *
   * @param out input out.
   * @param s input s.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(byte[]),246,254,"/**
* Initializes internal state from UTF-8 encoded byte array.
* @param utf8 input data in UTF-8 encoding
*/","* Set to a utf8 byte array. If the length of <code>utf8</code> is
   * <em>zero</em>, actually clear {@link #bytes} and any existing
   * data is lost.
   *
   * @param utf8 input utf8.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text),260,263,"/**
* Copies text attributes from one Text object to another.
* @param other Text object containing attributes to copy
*/","* Copy a text.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readDefaultLine,"org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)",197,263,"/**
* Parses text up to the next newline or maxBytesToConsume, 
* consuming at most maxLineLength characters per line.
* @param str Text object to append to
* @param maxLineLength maximum length of each line
* @param maxBytesToConsume total bytes to consume before newline
* @return number of consumed bytes
*/","* Read a line terminated by one of CR, LF, or CRLF.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readCustomLine,"org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)",268,371,"/**
* Consumes input bytes until a record delimiter is found or maxBytesToConsume is reached.
* @param maxLineLength maximum number of characters to read per line
* @return total consumed bytes
*/",* Read a line terminated by a custom delimiter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,toString,org.apache.hadoop.io.Text:toString(),337,344,"/**
* Generates a function mask.
* @return encoded string or throws RuntimeException if encoding fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(),45,48,"/**
* Initializes a new instance of SortedMapWritable with an empty TreeMap. 
* @param none
*/",default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(),43,46,"/**
 * Initializes an empty instance of MapWritable.",Default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,write,org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput),182,198,"/**
* Serializes the current object's state to the output stream.
* @param out DataOutput stream for serialization
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,write,org.apache.hadoop.io.MapWritable:write(java.io.DataOutput),147,163,"/**
* Serializes the instance's data to the output stream.
* @param out DataOutput stream to write to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDeserializer,"org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)",2163,2166,"/**
 * Creates a deserializer instance using the provided serialization factory and target class.
 * @param sf Serialization factory
 * @param c Target class to deserialize into
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,org.apache.hadoop.util.bloom.Key:<init>(byte[]),87,89,"/**
* Initializes a new Key instance from a byte array.
* @param value the key value as a byte array
*/","* Constructor.
   * <p>
   * Builds a key with a default weight.
   * @param value The byte value of <i>this</i> key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),1052,1054,"/**
* Creates an option instance with compression type.
* @param value CompressionType enum value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,"org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",302,306,"/**
* Creates a SequenceFile Writer option with specified compression settings. 
* @param type compression type
* @param codec compression codec
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,<init>,org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer),52,57,"/**
* Initializes DeserializerComparator with a custom deserializer.
* @param deserializer Deserializer instance to compare with
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,compare,"org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)",59,73,"/**
* Deserializes and processes two byte arrays using a buffer
* @param b1 first byte array to process
* @param s1 start index of first byte array
* @param l1 length of first byte array
* @param b2 second byte array to process
* @param s2 start index of second byte array
* @param l2 length of second byte array
* @return result of recursive m3(key1, key2) call",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,readFields,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput),99,104,"/**
* Reads and initializes CRC and MD5 settings from input stream.
* @param in input data stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(byte[]),118,120,"/**
* Computes MD5 hash of given byte array.
* @param data input bytes to be hashed
*/","* Construct a hash value for a byte array.
   * @param data data.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8),195,197,"/**
* Computes MD5 hash using provided UTF-8 string.
* @param utf8 UTF-8 encoded string to hash
*/","* Construct a hash value for a String.
   * @param utf8 utf8.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash:<init>(java.lang.String),61,63,"/**
* Sets MD5 hash value from hexadecimal representation.
* @param hex hexadecimal string representing MD5 hash
*/","* Constructs an MD5Hash from a hex string.
   * @param hex input hex.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,stream,org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream),1020,1022,"/**
* Creates an option with a function mask.
* @param value FSDataOutputStream object
* @return StreamOption instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendIfExists,org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean),1028,1030,"/**
* Creates an option with a mask to append only if the given condition is true.
* @param value the boolean condition to apply the mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,file,org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path),994,996,"/**
* Returns a FileOption representing a mask option.
* @param value file path to be masked
* @return FileOption object or null if invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)",421,436,"/**
* Calculates retry action based on current retry attempt and failovers.
* @param e exception
* @param curRetry current retry count
* @param failovers total allowed retries
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at-most-once
* @return RetryAction object with decision and sleep time",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryForeverWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)",82,86,"/**
* Creates a retry policy with fixed sleep duration.
* @param sleepTime duration to wait between retries
* @param timeUnit unit of the sleep time (e.g. seconds, milliseconds)
*/","* <p>
   * Keep trying forever with a fixed time between attempts.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)",99,101,"/**
* Creates a retry policy with fixed sleep between retries.
* @param maxRetries maximum number of retries
* @param sleepTime time to wait before next retry in specified unit
* @return RetryPolicy instance for the configured policy
*/","* <p>
   * Keep trying a limited number of times, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)",346,351,"/**
* Initializes retry logic with maximum attempt time and fixed sleep interval.
* @param maxTime total allowed time in specified unit
* @param sleepTime fixed interval between retries
* @param timeUnit unit of time for maxTime and sleepTime
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,exponentialBackoffRetry,"org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)",148,151,"/**
* Creates an exponential backoff retry policy with specified parameters.
* @param maxRetries maximum number of retries
* @param sleepTime initial delay between retries in the given unit
* @param timeUnit unit of measurement for the sleep time (e.g. milliseconds, seconds)
*/","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by a random
   * number in the range of [0, 2 to the number of retries)
   * </p>
   *
   *
   * @param timeUnit timeUnit.
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithProportionalSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)",130,132,"/**
* Creates a retry policy with proportional backoff up to maximum retries.
* @param maxRetries maximum number of allowed retries
* @param sleepTime initial sleep duration in specified unit
* @param timeUnit time unit for the sleep duration (e.g. SECONDS, MILLISECONDS)
*/","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by the number of tries so far.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param maxRetries maxRetries.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",210,215,"/**
* Creates a retry policy with failover on network exception.
* @param fallbackPolicy default retry policy for failures
* @param maxFailovers maximum number of failovers allowed
* @param delayMillis initial delay in milliseconds
* @param maxDelayBase base value for exponential backoff delay
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,newAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",322,327,"/**
* Creates and returns an instance of AsyncCall.
* @param method the target method to invoke
* @param args arguments for the invocation
* @param isRpc whether the call is a remote procedure call
* @param callId unique identifier for the asynchronous call
* @param retryInvocationHandler handler for retrying failed invocations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newRetryInfo,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)",273,302,"/**
* Determines the maximum retry delay and action based on policy and exceptions.
* @param policy RetryPolicy instance
* @param e Exception(s) to evaluate
* @param counters Counters object for retries and failovers
* @param idempotentOrAtMostOnce Whether operation is idempotent or at most once
* @return RetryInfo object with maximum delay, action, expected failover count, and last exception that caused failure",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStop,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon),186,190,"/**
* Triggers daemon-specific function mask based on GRACE_PERIOD.
* @param d Daemon object to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,read,org.apache.hadoop.security.Groups$TimerToTickerAdapter:read(),292,296,"/**
* Calculates function mask in milliseconds.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,unlock,org.apache.hadoop.util.InstrumentedWriteLock:unlock(),59,69,"/**
* Releases and reports write lock if still held at function entry.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming(),74,79,"/**
* Updates write lock held timestamp.
*/",* Starts timing for the instrumented write lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)",87,99,"/**
* Initializes InstrumentedLock with given parameters.
* @param name lock identifier
* @param logger logging instance
* @param lock underlying synchronization primitive
* @param minLoggingGapMs minimum gap between log messages (ms)
* @param lockWarningThresholdMs threshold for lock warning (ms)
* @param clock timer instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedLock:startLockTiming(),177,179,"/**
 * Acquires timestamp lock and updates lockAcquireTimestamp.",* Starts timing for the instrumented lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,unlock,org.apache.hadoop.util.InstrumentedReadLock:unlock(),65,75,"/**
* Releases and acquires locks while possibly generating a report.
* @param localLockAcquireTime lock acquire time
* @param localLockReleaseTime lock release time
* @param generateReport whether to generate a report
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedReadLock:startLockTiming(),81,86,"/**
* Updates lock held timestamp when write lock is acquired.
*/","* Starts timing for the instrumented read lock.
   * It records the time to ThreadLocal.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",327,330,"/**
* Initializes RetryInvocationHandler with FailoverProxyProvider and RetryPolicy.
* @param proxyProvider provider for failover proxies
* @param retryPolicy policy for retrying failed invocations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)",100,110,"/**
* Creates a proxy instance with failover and retry capabilities.
* @param iface interface type
* @param proxyProvider provider for the underlying proxy object
* @param methodNameToPolicyMap map of method names to retry policies
* @param defaultPolicy default retry policy when no specific policy is defined
*/","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the a set of retry policies
   * specified by method name. If no retry policy is defined for a method then a
   * default of {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param methodNameToPolicyMap map of method names to retry policies
   * @param defaultPolicy defaultPolicy.
   * @param <T> T.
   * @return the retry proxy",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,failover,"org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)",217,229,"/**
* Updates failover count and logs a failover event if necessary.
* @param expectedFailoverCount current expected failover count
* @param method the invoked method
* @param callId unique call identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,log,"org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)",398,430,"/**
* Logs a method invocation failure with details about failover attempts, retries and delay.
* @param method the failed Method invocation
* @param isFailover whether it's a failover attempt
* @param failovers number of failover attempts remaining
* @param retries current retry count
* @param delay time to wait before next attempt in milliseconds
* @param ex the underlying exception causing failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",48,65,"/**
* Simulates a retriable exception based on retry count.
* @param method unused method parameter
* @param args unused method arguments
* @throws RetriableException if retry count is below threshold
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,hashCode,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode(),150,153,"/**
* Delegates to Multiple Linear Random Retry strategy.
* @return result of underlying strategy's implementation
*/","* Similarly, remoteExceptionToRetry is ignored as part of hashCode since it
     * does not affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,equals,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object),135,144,"/**
* Recursively checks for instance of WrapperRetryPolicy.
* @param obj object to check
*/","* remoteExceptionToRetry is ignored as part of equals since it does not
     * affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)",283,291,"/**
* Determines next retry action based on retries and failovers.
* @param e exception to evaluate
* @param retries current retry count
* @param failovers current failover count
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at-most-once
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mayThrow,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List),314,324,"/**
* Logs and optionally re-throws multiple IOExceptions if minimum replication threshold is met.
* @param ioExceptions list of IOExceptions to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path),875,915,"/**
* Retrieves file status for the given path, handling multiple nodes and exceptions.
* @param f the input path
* @return FileStatus object or throws IOException with aggregated errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,build,org.apache.hadoop.io.MultipleIOException$Builder:build(),83,85,"/**
* Returns an IOException instance with error details.
* @return IOException object containing exception information.","* @return null if nothing is added to this builder;
     *         otherwise, return an {@link IOException}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String),878,883,"/**
* Initializes DataIndex object with specified default compression algorithm.
* @param defaultCompressionAlgorithmName unique name of the compression algorithm to use
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getSupportedCompressionAlgorithms,org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms(),198,200,"/**
 * Returns a string array containing compression masks.
 */","* Get names of supported compression algorithms. The names are acceptable by
   * TFile.Writer.
   * 
   * @return Array of strings, each represents a supported compression
   *         algorithm. Currently, the following compression algorithms are
   *         supported.
   *         <ul>
   *         <li>""none"" - No compression.
   *         <li>""lzo"" - LZO compression.
   *         <li>""gz"" - GZIP compression.
   *         </ul>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName(),576,578,"/**
* Returns block state value from associated RBlockState instance. 
* @return Block state value as a string or equivalent default if not set
*/","* Get the name of the compression algorithm used to compress the block.
       * 
       * @return name of the compression algorithm.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)",451,455,"/**
* Creates a new metadata index entry with compressed data in the specified range.
* @param raw raw data to be indexed
* @param begin start of the indexing region
* @param end end of the indexing region (exclusive)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readString,org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput),276,282,"/**
* Reads and processes input data to generate a mask string.
* @param in DataInput object providing input data
*/","* Read a String as a VInt n, followed by n Bytes in Text format.
   * 
   * @param in
   *          The input stream.
   * @return The string
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput),2302,2307,"/**
* Reads a TFileIndexEntry from the given DataInput stream.
* @param in input stream containing the entry data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,readLength,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength(),102,109,"/**
* Calculates and updates mask-related variables based on input data. 
* @throws IOException if input reading fails
*/","* Reading the length of next chunk.
     * 
     * @throws java.io.IOException
     *           when no more data is available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String),2051,2057,"/**
* Initializes a new TFileMeta object with the specified comparator.
* @param comparator string for sorting and filtering files
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String),176,178,"/**
* Returns a comparator instance based on the provided file metadata name.
* @param name string identifier of the file metadata
*/","* Make a raw comparator from a string name.
   * 
   * @param name
   *          Comparator name
   * @return A RawComparable comparator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeChunk,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)",253,266,"/**
* Writes chunk to output stream with length mask.
* @param chunk binary data chunk
* @param offset starting index in chunk
* @param len length of chunk to write
* @param last true if this is the last chunk in a sequence
*/","* Write out a chunk.
     * 
     * @param chunk
     *          The chunk buffer.
     * @param offset
     *          Offset to chunk buffer for the beginning of chunk.
     * @param len
     * @param last
     *          Is this the last call to flushBuffer?",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeBufData,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)",279,287,"/**
* Flushes output and writes data to buffer.
* @param data input data array
* @param offset starting index in data
* @param len number of bytes to write
*/","* Write out a chunk that is a concatenation of the internal buffer plus
     * user supplied data. This will never be the last block.
     * 
     * @param data
     *          User supplied data buffer.
     * @param offset
     *          Offset to user data buffer.
     * @param len
     *          User data buffer size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput),2335,2339,"/**
* Serializes data to output stream.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,<init>,"org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)",377,382,"/**
* Initializes single chunk encoder with output stream and chunk size.
* @param out DataOutputStream to write encoded data
* @param size Chunk size
*/","* Constructor.
     * 
     * @param out
     *          the underlying output stream.
     * @param size
     *          The total # of bytes to be written as a single chunk.
     * @throws java.io.IOException
     *           if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryComparator,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator(),925,944,"/**
* Returns a comparator for sorting Scanner.Entries based on their masked values.
* @throws RuntimeException if entries are not comparable (i.e., TFiles are unsorted)
*/","* Get a Comparator object to compare Entries. It is useful when you want
     * stores the entries in a collection (such as PriorityQueue) and perform
     * sorting or comparison among entries based on the keys without copying out
     * the key.
     * 
     * @return An Entry Comparator..",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)",1007,1012,"/**
* Compares two byte arrays using a custom comparator.
* @param a first array to compare
* @param o1 offset in array a
* @param l1 length of slice from array a
* @param b second array to compare
* @param o2 offset in array b
* @param l2 length of slice from array b
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1014,1019,"/**
* Compares two objects using a custom comparator.
* @param a first object to compare
* @param b second object to compare
* @return result of comparison (e.g. mask value)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,setFirstKey,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)",2250,2253,"/**
* Copies a specified section of the input key into the local firstKey array.
* @param key input key array
* @param offset starting position in key array
* @param length number of bytes to copy
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey(),2255,2260,"/**
* Returns a raw comparable mask as a byte array.
* @return Byte array containing the mask or null if index is invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,atEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd(),1588,1590,"/**
* Checks if current location is within a valid range relative to end location.
* @return true if current location is at or beyond end location, false otherwise
*/","* Is cursor at the end location?
       * 
       * @return true if the cursor is at the end location.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long),1031,1035,"/**
* Calculates a location based on an offset and block index.
* @param offset input offset to calculate from
* @return Location object or end if invalid offset
*/","* Get the location pointing to the beginning of the first key-value pair in
     * a compressed block whose byte offset in the TFile is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the location to the corresponding entry; or end() if no such
     *         entry exists.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,clone,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone(),759,762,"/**
* Returns location based on block and record indices.
* @return Location object representing current position.",* @see java.lang.Object#clone(),,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long),2238,2242,"/**
* Calculates the function mask location for a given record number.
* @param recNum record identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),713,715,"/**
 * Copies location data from another instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[]),1840,1842,"/**
* Returns the integer value from the first four bytes of the buffer.
* @param buf input byte array
*/","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value. The value part of the key-value
         * pair pointed by the current cursor is not cached and can only be
         * examined once. Calling any of the following functions more than once
         * without moving the cursor will result in exception:
         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.
         *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,set,org.apache.hadoop.io.UTF8:set(java.lang.String),96,118,"/**
* Processes input string, truncating if too long and encoding into byte array.
* @param string input string to process
*/","* Set to contain the contents of a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,toByteArray,org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[]),461,472,"/**
* Converts Writable objects into a byte array.
* @param writables variable arguments of Writable objects
* @return byte array representation or null on failure
*/","* Convert writables to a byte array.
   * @param writables input writables.
   * @return ByteArray.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,getBytes,org.apache.hadoop.io.UTF8:getBytes(java.lang.String),238,249,"/**
* Generates a byte array from the input string.
* @param string input string
*/","* @return Convert a string to a UTF-8 encoded byte array.
   * @see String#getBytes(String)
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeBuffer,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer),1623,1635,"/**
* Compresses data using the Deflate algorithm and writes it to output.
* @param uncompressedDataBuffer input buffer containing data to compress
*/",Workhorse to check and write out compressed data/lengths,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,byteArrayForBloomKey,org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer),71,79,"/**
* Extracts and truncates a byte array from the input buffer.
* @param buf DataOutputBuffer instance
* @return Truncated byte array or null on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,lessThan,"org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)",3537,3548,"/**
* Compares two SegmentDescriptors based on their attributes.
* @param a first segment descriptor
* @param b second segment descriptor
* @return true if a is ""less than"" b, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,copy,org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable),125,142,"/**
* Copies the contents of another Map object into this one.
* @param other source Map object
*/","* Used by child copy constructors.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)",1545,1575,"/**
* Sets key-value pair in compressed form.
* @param key unique identifier with correct class
* @param val value with correct class
*/",Append a key/value pair.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,toString,org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object),83,90,"/**
* Serializes object to Base64-encoded string.
* @param obj object to serialize
* @return serialized string or null on IOException
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,checkKey,org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable),418,429,"/**
* Writes a key to the output buffer while maintaining sorted order.
* @param key WritableComparable object to be written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getBytes,org.apache.hadoop.security.token.TokenIdentifier:getBytes(),60,68,"/**
* Generates a byte mask using the DataOutputBuffer.
* @return byte array representing the generated mask
*/","* Get the bytes for the token identifier
   * @return the bytes of the identifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeWritable,org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable),340,347,"/**
* Encodes object as base64 string.
* @param obj Writable object to encode
* @return Base64 encoded string or null on error
*/","* Generate a string with the url-quoted base64 encoded serialized form
   * of the Writable.
   * @param obj the object to serialize
   * @return the encoded string
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,moveData,org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData(),314,316,"/**
* Masks input buffer using output buffer's first two elements.
* @param none (no parameters)
*/",* Move the data from the output buffer to the input buffer.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getAllResolvedHostnameByDomainName,"org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)",64,80,"/**
* Retrieves hostnames or FQDNs from IP addresses.
* @param domainName domain name to resolve
* @param useFQDN whether to fetch full qualified domain names
* @return array of hostnames (or FQDNs) or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistanceByPath,"org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",376,400,"/**
* Calculates the functional distance between two nodes.
* @param node1 first Node object
* @param node2 second Node object
* @return integer distance or MAX_VALUE if null pointers detected
*/","Return the distance between two nodes by comparing their network paths
   * without checking if they belong to the same ancestor node by reference.
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object),108,112,"/**
* Calls parent class's implementation of m1.
* @param o arbitrary object to be passed to m1
* @return true if m1 returns true, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,equals,org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object),316,319,"/**
* Calls superclass's version of m1.
* @param to arbitrary object reference
* @return true if superclass's m1 returns true, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode(),114,118,"/**
* Calls superclass's implementation of m1(). 
* Returns result as is.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,hashCode,org.apache.hadoop.net.InnerNodeImpl:hashCode(),311,314,"/**
* Calls superclass implementation of m1(). 
* @return result from superclass's m1() method
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node),193,195,"/**
* Applies mask operation using m2 on the given node.
* @param node input node to process
* @return masked result or null if failed
*/","* Return a reference to the node given its string representation.
   * Default implementation delegates to {@link #getNode(String)}.
   * 
   * <p>To be overridden in subclasses for specific NetworkTopology 
   * implementations, as alternative to overriding the full {@link #add(Node)}
   *  method.
   * 
   * @param node The string representation of this node's network location is
   * used to retrieve a Node object. 
   * @return a reference to the node; null if the node is not in the tree
   * 
   * @see #add(Node)
   * @see #getNode(String)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getLeaves,org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String),643,655,"/**
* Retrieves a list of leaf nodes within the specified scope.
* @param scope unique identifier for the scope
* @return List of Node objects representing leaf nodes
*/","return leaves in <i>scope</i>
   * @param scope a path string
   * @return leaves nodes under specific scope",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,countNumOfAvailableNodes,"org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)",664,711,"/**
* Calculates a mask value based on the given scope and excluded nodes.
* @param scope Node scope string
* @param excludedNodes Collection of excluded nodes
* @return Calculated mask value
*/","return the number of leaves in <i>scope</i> but not in <i>excludedNodes</i>
   * if scope starts with ~, return the number of nodes that are not
   * in <i>scope</i> and <i>excludedNodes</i>; 
   * @param scope a path string that may start with ~
   * @param excludedNodes a list of nodes
   * @return number of available nodes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interRemoveNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node),1104,1122,"/**
* Updates node's mask based on its rack and associated nodes.
* @param node Node object to process
*/","* Internal function for update empty rack number
   * for remove or decommission a node.
   * @param node node to be removed; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)",970,1000,"/**
* Sorts and processes a subset of nodes based on weights.
* @param reader node reader
* @param nodes array to populate with sorted nodes
* @param activeLen length of nodes to process
* @param secondarySort optional custom sorting consumer
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   * And it helps choose node with more fast storage type.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param nonDataNodeReader True if the reader is not a datanode",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,recommissionNode,org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node),1040,1055,"/**
* Decommissions a leaf node, locking and unlocking network resources.
* @param node Node to be decommissioned
*/","* Update empty rack number when add a node like recommission.
   * @param node node to be added; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)",50,53,"/**
* Creates a new Reader instance from a ReadableByteChannel.
* @param channel input byte channel
* @param timeout read operation timeout in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)",55,58,"/**
* Initializes writer with given WritableByteChannel and timeout.
* @param channel channel to write data into
* @param timeout write operation timeout in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,org.apache.hadoop.net.SocketOutputStream:write(int),101,109,"/**
* Writes an integer to a buffer and calls the next processing stage.
* @param b integer value to write
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)",264,267,"/**
* Reads bytes from the given FileChannel at specified position and count.
* @param fileCh channel to read from
* @param position offset in bytes to start reading from
* @param count number of bytes to read
*/","* Call
   * {@link #transferToFully(FileChannel, long, int, LongWritable, LongWritable)
   * }
   * with null <code>waitForWritableTime</code> and <code>transferToTime</code>.
   *
   * @param fileCh input fileCh.
   * @param position input position.
   * @param count input count.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,resolve,org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List),106,125,"/**
* Resolves and caches host names for the given list of user names.
* @param names list of user names
* @return list of fully resolved host names or empty list if invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,wrapException,"org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)",850,949,"/**
* Wraps a specific IOException type with additional context and error codes.
* @param destHost destination host
* @param destPort destination port
* @param localHost local host
* @param localPort local port
* @param exception underlying IOException instance
* @return wrapped IOException instance
*/","* Take an IOException , the local host port and remote host port details and
   * return an IOException with the input exception as the cause and also
   * include the host details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics information.
   * If the exception is of type BindException, ConnectException,
   * UnknownHostException, SocketTimeoutException or has a String constructor,
   * return a new one of the same type; Otherwise return an IOException.
   *
   * @param destHost target host (nullable)
   * @param destPort target port
   * @param localHost local host (nullable)
   * @param localPort local port
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>(),144,144,"/**
* Creates an instance of RawScriptBasedMappingWithDependency.
*/","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,<init>,org.apache.hadoop.net.TableMapping:<init>(),63,65,"/**
 * Initializes a new instance of the TableMapping class.
 * Uses a RawTableMapping instance as its base mapping.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),95,97,"/**
 * Initializes ScriptBasedMapping with raw DNSToSwitch mapping data.
 * @param rawMap raw DNSToSwitch mapping data to initialize from
 */","* Create an instance from the given raw mapping
   * @param rawMap raw DNSTOSwithMapping",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String),48,50,"/**
 * Initializes an InnerNodeImpl instance with the given file system path.
 */","* Construct an InnerNode from a path-like string.
   * @param path input path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)",99,102,"/**
* Initializes an NflyNode with specified properties.
* @param hostName node hostname
* @param rackName node rack name
* @param fs file system for the node
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,"org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)",60,63,"/**
* Initializes an inner node with specified details.
* @param name node name
* @param location node location
* @param parent parent node (if any)
* @param level nesting level of the node
*/","* Construct an InnerNode
   * from its name, its network location, its parent, and its level.
   * @param name input name.
   * @param location input location.
   * @param parent input parent.
   * @param level input level.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,kick,org.apache.hadoop.net.unix.DomainSocketWatcher:kick(),359,374,"/**
* Sends a masked notification and marks the user as kicked.
* @throws IOException on write failure
*/",* Wake up the DomainSocketWatcher thread.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,handle,org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket),103,130,"/**
* Checks if a DomainSocket is still open.
* @param sock the socket to check
* @return true if the socket was closed, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,bindAndListen,org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String),191,200,"/**
* Creates a DomainSocket instance for the specified path.
* @param path socket path
*/","* Create a new DomainSocket listening on the given path.
   *
   * @param path         The path to bind and listen on.
   * @return             The new DomainSocket.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,socketpair,org.apache.hadoop.net.unix.DomainSocket:socketpair(),210,216,"/**
* Creates an array of anonymous DomainSockets with specified file descriptors.
* @throws IOException if unable to access file descriptors
*/","* Create a pair of UNIX domain sockets which are connected to each other
   * by calling socketpair(2).
   *
   * @return                An array of two UNIX domain sockets connected to
   *                        each other.
   * @throws IOException    on error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,connect,org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String),255,261,"/**
* Creates a DomainSocket instance for the given path.
* @param path socket file path
*/","* Create a new DomainSocket connected to the given path.
   *
   * @param path              The path to connect to.
   * @throws IOException      If there was an I/O error performing the connect.
   *
   * @return                  The new DomainSocket.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallbackAndRemove,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",435,440,"/**
* Updates the entry for a specific file descriptor in the given map.
* @param caller context of the update operation
* @param entries map of file descriptors to their associated data
* @param fdSet set of file descriptors being managed
* @param fd ID of the file descriptor to be updated
*/","* Send callback, and if the domain socket was closed as a result of
   * processing, then also remove the entry for the file descriptor.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen(),592,595,"/**
* Delegates call to DomainSocket instance's m1() method.
* @return result of m1() on DomainSocket instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close(),597,600,"/**
*Makes a recursive call to the superclass's m1 method.
* @throws IOException if an I/O error occurs during invocation.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close(),558,561,"/**
* Calls parent class's implementation of m1().",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,close,org.apache.hadoop.net.unix.DomainSocketWatcher:close(),266,284,"/**
* Closes the resource and notifies watchers.
* @throws IOException on failure
*/","* Close the DomainSocketWatcher and wait for its thread to terminate.
   *
   * If there is more than one close, all but the first will be ignored.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close(),547,550,"/**
* Delegates execution to parent's m1() method.
* @throws IOException if an I/O error occurs during delegation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,get,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel),387,403,"/**
* Retrieves a SelectorInfo object for the given SelectableChannel.
* @param channel SelectableChannel instance
* @return SelectorInfo object or null if not found
*/","* Takes one selector from end of LRU list of free selectors.
     * If there are no selectors awailable, it creates a new selector.
     * Also invokes trimIdleSelectors(). 
     * 
     * @param channel
     * @return 
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,release,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo),411,417,"/**
* Updates selector info with current timestamp and notifies the provider.
* @param info SelectorInfo object to update
*/","* puts selector back at the end of LRU list of free selectos.
     * Also invokes trimIdleSelectors().
     * 
     * @param info",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getLeaf,"org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)",253,300,"/**
* Recursively fetches a Node by leaf index, excluding the specified node.
* @param leafIndex target leaf index
* @param excludedNode Node to exclude from search
* @return Node object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,remove,org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node),189,234,"/**
* Removes a node and its descendants, updating internal data structures.
* @param n Node to be removed
* @return true if removal was successful, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,org.apache.hadoop.net.DNS:getIPs(java.lang.String),152,155,"/**
* Convenience wrapper to fetch device information.
* @param strInterface network interface string
*/","* @return Like {@link DNS#getIPs(String, boolean)}, but returns all
   * IPs associated with the given interface and its subinterfaces.
   *
   * @param strInterface input strInterface.
   * @throws UnknownHostException
   * If no IP address for the local host could be found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,"org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)",248,277,"/**
* Resolves interface hostname and returns an array of potential hostnames.
* @param strInterface interface name
* @param nameserver DNS server to use (optional)
* @param tryfallbackResolution whether to attempt fallback resolution (default: false)
* @return array of possible hostnames or null if not found
*/","* Returns all the host names associated by the provided nameserver with the
   * address bound to the specified network interface
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *            (e.g. eth0 or eth0:0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            if true and if reverse DNS resolution fails then attempt to
   *            resolve the hostname with
   *            {@link InetAddress#getCanonicalHostName()} which includes
   *            hosts file resolution.
   * @return A string vector of all host names associated with the IPs tied to
   *         the specified interface
   * @throws UnknownHostException if the given interface is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,org.apache.hadoop.net.SocketInputStream:read(),112,127,"/**
* Reads a single byte from the underlying stream and returns its value.
* @throws IOException if reading fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,listBeans,"org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)",235,328,"/**
* Generates JSON output listing MBean attributes for a given query.
* @param qry JMX query object
* @param attribute optional attribute name to filter results
* @param response HttpServletResponse object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseArguments,org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[]),141,170,"/**
* Validates and processes command-line arguments for level management.
* @param args array of input arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)",117,132,"/**
* Prints help entries and function mask to the specified output stream.
* @param pStr output stream
* @param helpEntries map of command help information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,org.apache.hadoop.log.LogThrottlingHelper:<init>(long),145,147,"/**
* Initializes LogThrottlingHelper with minimum log period in milliseconds.
* @param minLogPeriodMs threshold time for log throttling
*/","* Create a log helper without any primary recorder.
   *
   * @see #LogThrottlingHelper(long, String)
   * @param minLogPeriodMs input minLogPeriodMs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,org.apache.hadoop.log.LogThrottlingHelper:record(double[]),195,197,"/**
* Creates LogAction instance with default recorder name and execution time.
* @param values log action parameters
*/","* Record some set of values at the current time into this helper. Note that
   * this does <i>not</i> actually write information to any log. Instead, this
   * will return a LogAction indicating whether or not the caller should write
   * to its own log. The LogAction will additionally contain summary information
   * about the values specified since the last time the caller was expected to
   * write to its log.
   *
   * <p>Specifying multiple values will maintain separate summary statistics
   * about each value. For example:
   * <pre>{@code
   *   helper.record(1, 0);
   *   LogAction action = helper.record(3, 100);
   *   action.getStats(0); // == 2
   *   action.getStats(1); // == 50
   * }</pre>
   *
   * @param values The values about which to maintain summary information. Every
   *               time this method is called, the same number of values must
   *               be specified.
   * @return A LogAction indicating whether or not the caller should write to
   *         its log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,getEvent,org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest),366,373,"/**
* Retrieves an Event object based on the 'event' query parameter.
* @return Event object or CPU if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,main,org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[]),215,224,"/**
* Processes each command-line argument, quoting and then unquoting it for demonstration purposes.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameter,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String),1804,1808,"/**
* Escapes HTML characters in user-provided input.
* @param name user-provided string to be sanitized
*/",* Unquote the name and quote the value.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterValues,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String),1810,1822,"/**
* Unquotes and sanitizes string array from raw request.
* @param name the input string to unquote
* @return sanitized String[] or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterMap,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap(),1824,1838,"/**
* Processes request data, sanitizing and transforming entries.
*@return Map of transformed key-value pairs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getRequestURL,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL(),1844,1848,"/**
* Builds HTML-quoted string from API response.
* @return quoted string in StringBuffer format
*/","* Quote the url so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getServerName,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName(),1854,1857,"/**
* Escapes HTML in raw request content.
* @return escaped HTML string or null if failed
*/","* Quote the server name so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addAsyncProfilerServlet,"org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)",797,816,"/**
* Configures the async profiler mask based on asynchronous profiler home configuration.
* @param contexts ContextHandlerCollection instance
* @param conf Configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addNoCacheFilter,org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler),888,891,"/**
* Configures servlet context handler with no-cache filter.
* @param ctxt ServletContextHandler instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,makeConfigurationChangeMonitor,"org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)",634,663,"/**
* Schedules periodic reloading of SSL keystores and truststores.
* @param reloadInterval interval between reloads in milliseconds
* @param sslContextFactory factory to create SSL context
* @return Timer instance for monitoring thread
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)",61,64,"/**
* Initializes file monitoring timer task with single file path.
* @param filePath unique file identifier
* @param onFileChange callback for file change events
* @param onChangeFailure callback for failure events
*/","* See {@link #FileMonitoringTimerTask(List, Consumer, Consumer)}.
   *
   * @param filePath The file to monitor.
   * @param onFileChange What to do when the file changes.
   * @param onChangeFailure What to do when <code>onFileChange</code>
   *                        throws an exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,close,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close(),877,892,"/**
* Closes and releases any held resources on the current output stream.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRatesWithAggregation,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String),331,337,"/**
* Retrieves aggregated rates with synchronization for the specified name.
* @param name unique identifier
* @return MutableRatesWithAggregation object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)",348,351,"/**
*Synchronizes and updates metric mapping for the given user.
*@param name unique user identifier
*@param metric updated metric data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getAnnotatedMetricsFactory,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory(),33,35,"/**
* Creates a mutable metrics factory instance. 
* @return MutableMetricsFactory object instance",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,flush,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush(),863,875,"/**
* Flushes output stream and logs any write errors.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,currentConfig,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig(),348,358,"/**
* Converts configuration to a string mask.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPluginLoader,org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader(),228,264,"/**
* Resolves plugin class loader by searching URLs or delegating to parent.
* @return ClassLoader instance or default loader if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration),287,298,"/**
* Generates a configuration mask string from the given Configuration.
* @param c the Configuration to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,consume,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer),172,200,"/**
* Processes metrics records from the given buffer, applying filters and pushing to sink.
* @param buffer input metrics buffer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,tag,"org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",81,84,"/**
* Creates a new metrics record builder with a specific mask.
* @param info metrics information
* @param value string value to apply as mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),91,95,"/**
* Builds metrics record with 3 components from abstract metric.
* @param metric AbstractMetric object to derive values from
* @return MetricsRecordBuilder instance for chaining
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",102,105,"/**
* Creates a metrics record builder with a FUNC_MASK operation.
* @param info metrics information
* @param value integer value to apply to mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",107,110,"/**
* Creates a MetricsRecordBuilder instance with a FUNC_MASK metric.
* @param info metrics information object
* @param value metric value
* @return MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",112,115,"/**
* Creates a MetricsRecordBuilder with the specified mask value.
* @param info MetricsInfo object
* @param value Mask value to be applied
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",117,120,"/**
* Creates a metrics record builder with a specific mask.
* @param info metrics information object
* @param value masked value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",122,125,"/**
 * Creates a new metrics record builder instance with a function mask.
 * @param info metrics information object
 * @param value function mask value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",127,130,"/**
* Creates a metrics record builder with a specific mask.
* @param info Metrics information object
* @param value Value to be masked
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord),184,186,"/**
* Creates a new MetricsRecord instance.
* @param mr existing MetricsRecord to build upon
* @param unused flag (always set to false)
*/","* Update the cache and return the current cache record
   * @param mr the update record
   * @return the updated cache record",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink:flush(),110,122,"/**
* Flushing metrics to Graphite with fallback to m2() on failure. 
* @throws MetricsException on error closing connection
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,write,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String),167,174,"/**
* Writes message to output stream if condition is met.
* @param msg message to be written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,init,org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration),54,68,"/**
* Initializes Graphite client with configuration settings.
* @param conf SubsetConfiguration object for host, port, and prefix
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,writeMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer),111,163,"/**
* Writes prometheus metrics to the given Writer, masking sensitive tags.
* @param writer output stream for metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consume,org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),65,75,"/**
* Invokes consumer function with result of m1() and ensures cleanup via m2().
* @throws InterruptedException if interrupted while waiting
*/","* Consume one element, will block if queue is empty
   * Only one consumer at a time is allowed
   * @param consumer  the consumer callback object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consumeAll,org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),82,94,"/**
* Iterates and processes data using the provided Consumer.
* @param consumer callback function to process each data element
*/","* Consume all the elements, will block if queue is empty
   * @param consumer  the consumer callback object
   * @throws InterruptedException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit(),64,66,"/**
 * Executes m1() function on cacheHit object.",* One cache hit event,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared(),71,73,"/**
* Clears mask cache by invoking m1 on cacheClearer instance.
*/",* One cache cleared,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated(),78,80,"/**
* Triggers mask update in cache.
* Calls m1() on cacheUpdated object.",* One cache updated,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures(),215,217,"/**
* Triggers mask for authentication failures.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses(),223,225,"/**
* Triggers authentication successes RPC event.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses(),231,233,"/**
* Calls RPC authorization success handler.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures(),239,241,"/**
 * Triggers authorization failure count increment.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoff,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff(),343,345,"/**
 * Executes RPC client backoff with 1 millisecond delay.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected(),350,352,"/**
* Sends RPC request to client with backoff on disconnection.
*/",* Client was disconnected due to backoff,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSlowRpc,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc(),366,368,"/**
* Executes slow RPC calls with mask operation.
*/",* Increments the Slow RPC counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls(),373,375,"/**
* Requeues all calls using RPC mechanism.
*/",* Increments the Requeue Calls counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRpcCallSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses(),380,382,"/**
* Calls RPC function m1 to update success counter.",* One RPC call success event.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelWrite,"org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",3923,3932,"/**
* Reads data from a WritableByteChannel into a ByteBuffer and returns the count of bytes transferred.
* @param channel channel to read from
* @param buffer buffer to write to
* @return number of bytes successfully transferred or -1 on failure
*/","* This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * buffer increases. This also minimizes extra copies in NIO layer
   * as a result of multiple write operations required to write a large 
   * buffer.  
   *
   * @see WritableByteChannel#write(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelRead,"org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)",3943,3952,"/**
* Reads bytes from a channel into a buffer and returns the count.
* @param channel input data stream
* @param buffer output byte array
* @return number of bytes read, or -1 on error
*/","* This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * ByteBuffer increases. There should not be any performance degredation.
   * 
   * @see ReadableByteChannel#read(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,getRecord,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord(),152,157,"/**
* Returns a MetricsRecordImpl instance based on filter criteria.
* @return A valid MetricsRecordImpl object or null if filtered out
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",58,60,"/**
* Wraps MetricsInfo calls to retrieve MBean attribute info.
* @param info MetricsInfo object
* @param type Attribute type (e.g., ""cpu"", ""memory"")
* @return MBeanAttributeInfo object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,get,org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get(),96,112,"/**
* Creates an MBeanInfo object with attributes from metrics records.
* @return MBeanInfo object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateAttrCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable),250,268,"/**
* Calculates the number of updated metrics.
* @param lastRecs iterable collection of MetricsRecordImpl objects
* @return total count of updated metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newObjectName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String),128,137,"/**
* Creates an ObjectName with FUNC_MASK suffix for the given name.
* @param name unique identifier
* @return ObjectName or throws MetricsException if already exists or error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newSourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)",147,156,"/**
* Generates a unique metric source mask, either by returning the original name or generating a new one.
* @param name metric source identifier
* @param dupOK whether duplicate names are allowed
* @return unique String representation of the metric source
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetrics,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)",96,107,"/**
* Determines whether to enqueue MetricsBuffer based on logical time.
* @param buffer Metrics data to process
* @param logicalTimeMs timestamp in milliseconds
* @return true if enqueued, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetricsImmediate,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer),109,126,"/**
* Attempts to add metrics to a queue, handling full queue and timeout scenarios.
* @param buffer MetricsBuffer object to process
* @return true if successful, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,diskCheckFailed,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed(),146,149,"/**
* Updates failure count and last failure time.
* @param none 
* @return none
*/",* Increase the failure count and update the last failure timestamp.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,fetchGroupSet,org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String),413,424,"/**
* Retrieves a set of string flags for the specified user.
* @param user unique username
*/","* Queries impl for groups belonging to the user.
     * This could involve I/O and take awhile.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdown,org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown(),66,69,"/**
* Resets internal metrics state and clears profile implementation. 
* @param none
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,registerIfNeeded,org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded(),72,79,"/**
* Initializes JVM metrics system.
* @param none
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,create,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create(),147,149,"/**
* Returns a new instance of UgiMetrics with default values.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown(),105,107,"/**
* Applies mask to metrics system.
* @param name metric name (not used in this implementation)
*/",* Shutdown the instrumentation process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown(),108,110,"/**
* Applies mask to metrics system.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown(),247,249,"/**
* Updates metrics system with function name.
* @param name the name of the current function",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,registerMetrics2Source,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String),891,894,"/**
* Registers metrics in the specified namespace.
* @param namespace unique identifier for metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,unregister,org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName),136,149,"/**
* Unregisters MBean by name and notifies metrics system.
* @param mbeanName unique identifier of the MBean to unregister
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,remove,org.apache.hadoop.http.HttpServer2Metrics:remove(),161,163,"/**
* Logs server metrics using MetricsSystem.
* @param port current HTTP server port number
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,snapshot,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",228,230,"/**
* Calls internal metrics registry with provided builder and flag.
* @param rb MetricsRecordBuilder instance
* @param all Flag to indicate whether to include all data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",80,83,"/**
 * Delegates metric calculation to underlying registry.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",117,125,"/**
* Generates and logs quantile metrics.
* @param ucName upper-case name prefix
* @param uvName upper-case name suffix
* @param desc metric description
* @param lvName lower-case name for interval
* @param pDecimalFormat decimal format for logging
*/","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param pDecimalFormat Number formatter for percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",75,83,"/**
* Generates and processes multiple inverse percentile metrics.
* @param ucName upper case name prefix
* @param uvName upper/lower case name suffix
* @param desc metric description
* @param lvName lower case name component
* @param df DecimalFormat object for formatting values
*/","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param df Number formatter for inverse percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,<init>,org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String),49,51,"/**
* Initializes MetricsRegistry with given name.
* @param name unique identifier for registry
*/","* Construct the registry with a record name
   * @param name  of the record of the metrics",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",169,197,"/**
* Accumulates metrics from averaged records and updates the MetricsRecordBuilder.
* @param builder MetricsRecordBuilder to update
* @param all whether to include all averages or only those updated recently
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)",162,164,"/**
* Creates and caches a MetricsTag instance with given name, description, and value.
* @param name tag name
* @param description tag description
* @param value tag value
*/","* Get a metrics tag.
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)",141,146,"/**
* Creates MetricsInfo object with extracted metrics data.
* @param cls class to extract from
* @param annotation metrics annotation with name and about fields
* @return MetricsInfo object or null if extraction fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)",162,174,"/**
* Creates a MetricsInfo object from the given annotation.
* @param annotation Metric object with m1() method
* @param defaultName fallback name if annotation has incorrect format
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",64,85,"/**
* Initializes MutableStat with specified name, description and sample information.
* @param name unique identifier
* @param description descriptive text
* @param sampleName sample name
* @param valueName value name
* @param extended whether to include additional statistics
*/","* Construct a sample statistics metric
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")
   * @param extended    create extended stats (stdev, min/max etc.) by default.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,metricInfo,org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method),145,147,"/**
* Returns metrics info for a given method.
* @param method Method object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcInfo,org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String),209,222,"/**
* Retrieves or creates metrics info for a garbage collector by name.
* @param gcName unique GC identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder),1030,1033,"/**
* Sets M3 metric in MetricsRecordBuilder.
* @param rb builder instance to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1036,1039,"/**
* Adds decayed call volume metric to MetricsRecordBuilder. 
* @param rb builder for creating metrics records
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1041,1044,"/**
* Sets call volume metric in MetricsRecordBuilder.
* @param rb record builder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1047,1051,"/**
* Sets M3 metric for ServiceUserDecayedCallVolume in the MetricsRecordBuilder.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1054,1058,"/**
* Adds service user call volume metrics to the record builder.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCallVolumePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1061,1067,"/**
* Adds call volumes for each priority level to the metrics record.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addAvgResponseTimePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1070,1076,"/**
* Adds average response times for each priority to the metrics record.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSystem,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem(),488,490,"/**
* Injects host-specific tags into the system.
* @param none
* @return none
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,tag,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",73,79,"/**
* Adds metric information with specified value.
* @param info Metric details
* @param value Value to associate with the metric
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)",415,420,"/**
* Registers metrics with the given info and value.
* @param info MetricsInfo object
* @param value metric value to register
* @param override whether to override existing values
* @return this MetricsRegistry instance
*/","* Add a tag to the metrics
   * @param info  metadata of the tag
   * @param value of the tag
   * @param override existing tag if true
   * @return the registry (for keep adding tags etc.)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(),42,45,"/**
 * Calls overloaded version of m1 with default float value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr(),47,50,"/**
* Applies a mask to all functional units.
* @see #m1(float)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double),178,180,"/**
*Synchronously updates internal state using input value.
*@param x input value to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,org.apache.hadoop.metrics2.lib.MutableStat:add(long),132,136,"/**
* Updates internal statistics with new value and triggers secondary processing.
* @param value new data point to process
*/","* Add a snapshot to the metric.
   * @param value of the metric",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,"org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)",48,53,"/**
* Initializes statistics and minimum/maximum values.
* @param numSamples1 number of samples
* @param mean1 sample mean
* @param s1 standard deviation
* @param minmax1 MinMax object to initialize
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshotInto,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate),182,187,"/**
* Updates rate metric with synchronization.
* @param metric MutableRate object to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newImpl,org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type),55,70,"/**
* Returns a MutableMetric instance based on the specified Metric.Type.
* @param metricType Type of metric to fetch
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,toString,org.apache.hadoop.metrics2.util.SampleStat:toString(),145,156,"/**
* Combines results from various helper methods into a single string.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev(),412,414,"/**
* Calculates and returns the m1-m2 value from rpcProcessingTime.","* Return Standard Deviation of the Processing Time.
   * @return  double",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev(),445,447,"/**
* Calculates and returns the m1-m2 value of deferred RPC processing time.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insert,org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long),113,123,"/**
* Adds a long value to the buffer, incrementing its size and triggering overflow handling when full.
* @param v the long value to add
*/","* Add a new value from the stream.
   * 
   * @param v v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,snapshot,org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot(),236,250,"/**
* Computes and returns a map of quantile values.
* @return Map<Quantile, Long> containing computed values or null if failed
*/","* Get a snapshot of the current values of all the tracked quantiles.
   * 
   * @return snapshot of the tracked quantiles. If no items are added
   * to the estimator, returns null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTopTokenRealOwners,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int),880,898,"/**
* Retrieves top N name-value pairs from token owner statistics.
* @param n the number of items to retrieve
* @return List of NameValuePair objects, or empty if n is 0
*/","* Return top token real owners list as well as the tokens count.
   *
   * @param n top number of users
   * @return map of owners to counts",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTopCallers,org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int),1099,1112,"/**
* Returns the top N caller functions based on their costs.
* @param n number of top callers to retrieve
* @return TopN object containing the top N caller function names and costs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),90,103,"/**
* Updates Netgroup cache based on provided groups.
* @param groups list of group names to process
*/","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTokens,org.apache.hadoop.security.UserGroupInformation:getTokens(),1724,1729,"/**
* Returns collection of tokens based on subject's data.
* @return Collection of Token objects for the subject
*/","* Obtain the collection of tokens associated with this user.
   * 
   * @return an unmodifiable collection of tokens associated with user",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,org.apache.hadoop.security.User:<init>(java.lang.String),42,44,"/**
* Constructs a new User instance with the given name.
* @param name user's full name
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroups,org.apache.hadoop.security.Groups:getGroups(java.lang.String),213,217,"/**
* Retrieves a list of function masks for the specified user.
* @param user unique user identifier
* @return list of function mask strings or empty list if not found
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * Note this method can be expensive as it involves Set {@literal ->} List
   * conversion. For user with large group membership
   * (i.e., {@literal >} 1000 groups), we recommend using getGroupSet
   * to avoid the conversion and fast membership look up via contains().
   * @param user User's name
   * @return the group memberships of the user as list
   * @throws IOException if user does not exist
   * @deprecated Use {@link #getGroupsSet(String user)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupsSet,org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String),231,233,"/**
* Retrieves a set of function masks for the specified user.
* @param user unique user identifier
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * This provide better performance when user has large group membership via
   * <br>
   * 1) avoid {@literal set->list->set} conversion for the caller
   * UGI/PermissionCheck <br>
   * 2) fast lookup using contains() via Set instead of List
   * @param user User's name
   * @return the group memberships of the user as set
   * @throws IOException if user does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),67,73,"/**
* Fetches and caches user group memberships by ID.
* @param user unique user identifier
* @return list of group names or empty list if not found
*/","* Gets unix groups and netgroups for the user.
   *
   * It gets all unix groups as returned by id -Gn but it
   * only returns netgroups that are used in ACLs (there is
   * no way to get all netgroups for a given user, see
   * documentation for getent netgroup)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,org.apache.hadoop.security.KDiag:println(),868,870,"/**
 * Recursively invokes itself with an empty string argument. 
 */",* Print a new line,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printSysprop,org.apache.hadoop.security.KDiag:printSysprop(java.lang.String),897,900,"/**
 * Logs and sets field mask to unset value.
 * @param property name of the field to modify
 */","* Print a system property, or {@link #UNSET} if unset.
   * @param property property to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printEnv,org.apache.hadoop.security.KDiag:printEnv(java.lang.String),916,919,"/**
* Retrieves environment variable value and updates local variable.
* @param variable name of the variable to fetch
*/","* Print an environment variable's name and value; printing
   * {@link #UNSET} if it is not set.
   * @param variable environment variable",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dump,org.apache.hadoop.security.KDiag:dump(java.io.File),926,932,"/**
* Prints the contents of a file to the console.
* @param file input file to read from
*/","* Dump any file to standard out.
   * @param file file to dump
   * @throws IOException IO problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,error,"org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])",1011,1013,"/**
* Logs an error with formatted message and arguments.
* @param category error type (e.g. ""Database"")
* @param message error description
*/","* Print a message as an error
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,warn,"org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])",1020,1022,"/**
* Logs warning message with formatted string.
* @param category log category
* @param message warning message to be logged
*/","* Print a message as an warning
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullUserMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap(),359,370,"/**
* Updates the user ID to name mapping and timestamps.
* @throws IOException on input/output errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullGroupMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap(),372,384,"/**
* Initializes and updates the group ID name mapping.
* @throws IOException on input/output errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1844,1846,"/**
* Executes authentication method-specific logic.
* @param authMethod type of authentication to perform
*/","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(int),123,131,"/**
* Writes an integer to the output stream using either native write or wrapped buffer. 
* @param b the integer value to be written
*/","* Writes the specified byte to this output stream.
   * 
   * @param b
   *          the <code>byte</code>.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(byte[]),148,151,"/**
 * Invokes m1 with default offset and length using entire input array.
 */","* Writes <code>b.length</code> bytes from the specified byte array to this
   * output stream.
   * <p>
   * The <code>write</code> method of <code>SASLOutputStream</code> calls the
   * <code>write</code> method of three arguments with the three arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the data.
   * @exception NullPointerException
   *              if <code>b</code> is null.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers(),1078,1088,"/**
* Initializes and returns an array of KeyManagers from the configured keystore.
* @throws IOException if keystore access fails
* @throws GeneralSecurityException on security-related errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createTrustManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers(),1090,1100,"/**
* Returns custom trust managers for SSL/TLS connections.
* @throws IOException if trust store cannot be accessed
* @throws GeneralSecurityException if trust manager factory fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,doFilter,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",205,212,"/**
* Filters incoming HTTP request and handles the response.
* @param request servlet request object
* @param response servlet response object
* @param chain filter chain to pass control to next filter
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",92,98,"/**
* Calls m1 and continues filter chain processing.
* @param req servlet request
* @param res servlet response
* @param chain filter chain to be executed next
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,init,org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig),84,90,"/**
 * Initializes and configures filter functionality.
 * @param filterConfig Filter configuration object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,getKey,org.apache.hadoop.security.token.delegation.DelegationKey:getKey(),75,82,"/**
* Generates a secret key from provided byte array.
* @return SecretKey object or null if input is invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,checkToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),528,546,"/**
* Retrieves DelegationTokenInformation for the given token identifier.
* @param identifier TokenIdent object containing token information
* @return DelegationTokenInformation object or null if not found/invalid
*/","* Find the DelegationTokenInformation for the given token id, and verify that
   * if the token is expired. Note that this method should be called with 
   * acquiring the secret manager's monitor.
   *
   * @param identifier identifier.
   * @throws InvalidToken invalid token exception.
   * @return DelegationTokenInformation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(),703,705,"/**
* Constructs an empty DelegationTokenInformation object.
* @param creationTimestamp default timestamp (0)
* @param tokenInfo default token info (null) 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,logExpireTokens,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection),786,793,"/**
* Removes expired tokens and performs associated cleanup.
* @param expiredTokens collection of expired TokenIdent objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),143,146,"/**
 * Delegates execution of m1 to the provided AbstractDelegationTokenSecretManager. 
 */","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy(),189,193,"/**
 * Executes M1 operations on Token Manager and Auth Handler.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateCurrentKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey(),440,456,"/**
* Updates the current master key for generating delegation tokens.
* @throws IOException if an I/O error occurs
*/","* Update the current master key 
   * This is called once by startThreads before tokenRemoverThread is created, 
   * and only by tokenRemoverThread afterwards.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,org.apache.hadoop.security.token.delegation.DelegationKey:<init>(),47,49,"/**
* Creates a new DelegationKey instance with default values.
* @param delegateId default delegate ID
* @param timestamp default timestamp in milliseconds
* @param secretKey default secret key or null for none
*/",Default constructore required for Writable,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",183,188,"/**
* Creates a new authenticated URL using a delegation token.
* @param authenticator DelegationTokenAuthenticator instance
* @param connConfigurator ConnectionConfigurator for the underlying connection
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.
   * @param connConfigurator a connection configurator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,renew,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew(),126,151,"/**
* Tries to refresh user session with existing FS instance.
* @return true if refresh was successful, false otherwise
*/","* Renew or replace the delegation token for this file system.
     * It can only be called when the action is not in the queue.
     * @return
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,cancel,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel(),153,158,"/**
* Executes action on file system instance and updates token.
* @throws IOException I/O error
* @throws InterruptedException thread interruption
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(),568,573,"/**
* Reads a single byte from the underlying input stream.
* @return the read byte or -1 on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[]),575,578,"/**
* Calls m1 with offset and length of entire byte array.
* @param b input byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setSaslClient,org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient),1905,1910,"/**
* Flushes SaslRpcClient input/output buffers.
* @param client SaslRpcClient instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning(),348,352,"/**
* Retrieves password from environment variable or file key. 
* @return password string or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning(),315,319,"/**
* Retrieves password from environment variable and file.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError(),354,358,"/**
* Retrieves password from environment variable or file.
* @return password string or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError(),321,325,"/**
* Retrieves keystore password using environment variable and file key.
* @return the retrieved password or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(),193,207,"/**
* Reads a byte from the input stream, either directly or by wrapping.
* @throws IOException if an I/O error occurs
*/","* Reads the next byte of data from this input stream. The value byte is
   * returned as an <code>int</code> in the range <code>0</code> to
   * <code>255</code>. If no byte is available because the end of the stream has
   * been reached, the value <code>-1</code> is returned. This method blocks
   * until input data is available, the end of the stream is detected, or an
   * exception is thrown.
   * <p>
   * 
   * @return the next byte of data, or <code>-1</code> if the end of the stream
   *         is reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,"org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)",248,275,"/**
* Reads data from input stream into buffer.
* @param b output byte array
* @param off output offset
* @param len length to read
* @return number of bytes read or -1 on error
*/","* Reads up to <code>len</code> bytes of data from this input stream into an
   * array of bytes. This method blocks until some input is available. If the
   * first argument is <code>null,</code> up to <code>len</code> bytes are read
   * and discarded.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @param off
   *          the start offset of the data.
   * @param len
   *          the maximum number of bytes read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",51,58,"/**
* Wraps call to m2 with InetAddress lookup, throwing AuthorizationException if address unknown.
*/","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should
   *             be preferred to avoid possibly re-resolving the ip address.
   * @param user ugi of the effective or proxy user which contains a real user.
   * @param remoteAddress the ip address of client.
   * @throws AuthorizationException Authorization Exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKeytab,org.apache.hadoop.security.UserGroupInformation:getKeytab(),814,819,"/**
* Retrieves function mask using hadoop login context.
* @return String representation of function mask or null on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isHadoopLogin,org.apache.hadoop.security.UserGroupInformation:isHadoopLogin(),825,828,"/**
* Checks if mask is present using M1 result.
* @return true if M1 returns non-null, false otherwise
*/","* Is the ugi managed by the UGI or an external subject?
   * @return true if managed by UGI.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUser,"org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1522,1537,"/**
* Creates a UserGroupInformation instance with the given user and real user.
* @param user user name
* @param realUser actual user identity
* @return UserGroupInformation object
*/","* Create a proxy user using username of the effective user and the ugi of the
   * real user.
   * @param user user.
   * @param realUser realUser.
   * @return proxyUser ugi",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getName,org.apache.hadoop.security.UserGroupInformation$RealUser:getName(),472,475,"/**
* Returns the FUNC_MASK value from the underlying user data.
* @return FUNC_MASK string representation or null if not available
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,shouldBackOff,org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable),720,745,"/**
* Determines whether to apply a mask to the given Schedulable object based on its priority level and response time history.
* @param obj the schedulable object to evaluate
* @return true if the object should be masked, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUserOrSelf,org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation),1558,1564,"/**
* Returns a functional mask of the given UserGroupInformation, 
* or the original object if no valid mask exists.
* @param user UserGroupInformation to process
* @return UserGroupInformation with functional mask applied, or original object
*/","* If this is a proxy user, get the real user. Otherwise, return
   * this user.
   * @param user the user to check
   * @return the real user or self",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation:toString(),1819,1827,"/**
* Formats user authentication information into a string.
* @return formatted string or null if not found
*/",* Return the username.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(),1863,1869,"/**
* Returns authentication method mask for current user or group.
* @return AuthenticationMethod value or null if unavailable
*/","* Get the authentication method from the real user's subject.  If there
   * is no real user, return the given user's authentication method.
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation),1878,1885,"/**
* Retrieves the authentication method for a given UserGroupInformation.
* @param ugi UserGroupInformation object
* @return AuthenticationMethod enum value
*/","* Returns the authentication method of a ugi. If the authentication method is
   * PROXY, returns the authentication method of the real user.
   * 
   * @param ugi ugi.
   * @return AuthenticationMethod",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeIpcConnectionContext,"org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",91,122,"/**
* Creates IpcConnectionContextProto instance with user information and protocol.
* @param protocol optional protocol string
* @param ugi UserGroupInformation object to include in context
* @param authMethod authentication method used for ugi (KERBEROS or TOKEN)
*/","* This method creates the connection context  using exactly the same logic
   * as the old connection context as was done for writable where
   * the effective and real users are set based on the auth method.
   *
   * @param protocol protocol.
   * @param ugi ugi.
   * @param authMethod authMethod.
   * @return IpcConnectionContextProto.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection),4110,4126,"/**
* Checks for and disconnects an existing client.
* @param connection database connection object
* @return true if client existed, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UserIdentityProvider.java,makeIdentity,org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable),28,35,"/**
* Retrieves and returns the function mask from a Schedulable object.
* @param obj Schedulable object to extract function mask from
* @return function mask as a string or null if object is null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,verify,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)",262,273,"/**
* Verifies SSL/TLS session by certificate.
* @param host server hostname
* @param session SSLSession object
* @return true if verified, false on exception or failure
*/","* The javax.net.ssl.HostnameVerifier contract.
         *
         * @param host    'hostname' we used to create our socket
         * @param session SSLSession with the remote server
         * @return true if the host matched the one in the certificate.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)",280,284,"/**
* Overloads m1() to accept single host and certificate.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)",292,347,"/**
* Verifies host against SSL session and certificate chain.
* @param host array of hosts to verify
* @throws IOException if verification fails or other I/O issues occur
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadResource,org.apache.hadoop.util.FindClass:loadResource(java.lang.String),172,180,"/**
* Retrieves a resource identifier by name.
* @param name unique resource identifier
* @return resource ID or E_NOT_FOUND if not found
*/","* Load a resource
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,<init>,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),132,151,"/**
* Initializes the SSLSocketFactory with the specified preferred channel mode.
* @param preferredChannelMode desired SSL channel mode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration),86,88,"/**
 * Initializes a new Command instance with the given configuration.
 */","* Constructor.
   *
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration),53,55,"/**
* Initializes CommandFactory with configuration.
* @param conf application configuration","* Factory constructor for commands
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem:<init>(),777,779,"/**
* Initializes a new instance of FileSystem with no file system provider.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration),75,77,"/**
 * Initializes an instance of FsShell with the given Hadoop configuration.
 */","* Construct a FsShell with the given configuration.  Commands can be
   * executed via {@link #run(String[])}
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(),109,109,"/**
 * Initializes an instance of Null with a null value. 
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,"org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",110,113,"/**
 * Initializes a new NullInstance with the specified class and configuration.
 * @param declaredClass Class that declares this instance
 * @param conf configuration for this instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)",171,184,"/**
* Initializes KDiag with configuration, output stream, and authentication details.
* @param conf Hadoop Configuration object
* @param out PrintWriter for logging diagnostics
* @param keytab File containing Kerberos keytabs
* @param principal User principal for authentication
* @param minKeyLength Minimum length of encryption keys
* @param securityRequired Flag indicating required security features",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration),131,133,"/**
 * Initializes the FindClass instance with the provided Hadoop Configuration.
 * @param conf Hadoop configuration object
 */","* Create a class with a specified configuration
   * @param conf configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,"org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)",53,56,"/**
 * Initializes group retrieval with configuration and output stream.
 * @param conf Hadoop Configuration object
 * @param out print stream to write output to
 */","* Used exclusively for testing.
   * 
   * @param conf The configuration to use.
   * @param out The PrintStream to write to, instead of System.out",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(),32,34,"/**
 * Initializes Configured instance with default configuration.
 */",Construct a Configured.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration),103,105,"/**
 * Initializes HAAdmin with Hadoop Configuration.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByNameWithSearch,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String),706,718,"/**
* Resolves InetAddress by searching hostname and configured domains.
* @param host Input hostname
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAppConfigurationEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String),2211,2230,"/**
* Retrieves application configuration entries based on the provided app name.
* @param appName name of the application to fetch config for
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,parseStaticMap,org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File),588,630,"/**
* Reads user ID and group ID mappings from a static mapping file.
* @param staticMapFile input file containing mappings
* @return StaticMapping object with uid and gid mappings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getAclString,org.apache.hadoop.security.authorize.AccessControlList:getAclString(),301,312,"/**
* Generates a function mask string.
* @return function mask string with '*' or 'm1() m2()' content
*/","* Returns the access control list as a String that can be used for building a
   * new instance by sending it to the constructor of {@link AccessControlList}.
   * @return acl string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])",228,244,"/**
* Creates a new CredentialEntry for the given alias.
* @param alias unique identifier for the credential
* @param credential character array containing the actual credential data
* @return CredentialEntry object or throws IOException if an error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute(),438,465,"/**
* Creates or updates a credential using the given provider and credentials.
* @throws IOException, NoSuchAlgorithmException if an error occurs during creation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTGT,org.apache.hadoop.security.UserGroupInformation:getTGT(),852,861,"/**
* Retrieves the first valid Kerberos ticket from the subject.
* @return The first valid KerberosTicket object or null if none found
*/","* Get the Kerberos TGT
   * @return the user's TGT or null if none was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",819,823,"/**
* Initializes ZK client using provided configuration and trust store.
* @param zkClientConfig ZooKeeper client configuration
* @param truststoreKeystore Trust store and keystore for SSL/TLS authentication
*/","* Configure ZooKeeper Client with SSL/TLS connection.
   * @param zkClientConfig ZooKeeper Client configuration
   * @param truststoreKeystore truststore keystore, that we use to set the SSL configurations
   * @throws ConfigurationException if the SSL configs are empty",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,unprotectedRelogin,"org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1347,1377,"/**
* Performs logout and re-login for the given Hadoop login context.
* @param login HadoopLoginContext to process
* @param ignoreLastLoginTime whether to ignore last login time
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String),124,129,"/**
* Resolves client address to a map of properties.
* @param clientAddress IP address or hostname
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,handle,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]),294,348,"/**
* Processes SASL DIGEST-MD5 callbacks, extracting credentials and authenticating the user.
* @param callbacks array of Callback objects to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)",97,118,"/**
* Initializes a CryptoOutputStream instance with encryption settings.
* @param out OutputStream to wrap
* @param codec CryptoCodec to use for encryption
* @param bufferSize buffer size in bytes
* @param key encryption key
* @param iv initialization vector
* @param streamOffset initial stream offset
* @param closeOutputStream whether to close the underlying OutputStream",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor(),54,58,"/**
* Creates an SM4-based encryptor instance.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor(),60,64,"/**
* Returns an instance of Decryptor using SM4 cipher. 
* @throws GeneralSecurityException if instantiation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor(),54,58,"/**
* Creates an AES encryptor instance.
* @return Encryptor object using AES cipher and mask values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor(),60,64,"/**
* Creates an AES decryptor instance.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,"org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)",131,140,"/**
* Creates an OpenSSL cipher instance based on the given transformation and engine ID.
* @param transformation encryption algorithm string
* @param engineId name of the cryptographic engine to use (can be null)
*/","* Return an <code>OpensslCipher</code> object that implements the specified
   * transformation.
   * 
   * @param transformation the name of the transformation, e.g., 
   * AES/CTR/NoPadding.
   * @param engineId the openssl engine to use.if not set,
   * defalut engine will be used.
   * @return OpensslCipher an <code>OpensslCipher</code> object
   * @throws NoSuchAlgorithmException if <code>transformation</code> is null, 
   * empty, in an invalid format, or if Openssl doesn't implement the 
   * specified algorithm.
   * @throws NoSuchPaddingException if <code>transformation</code> contains 
   * a padding scheme that is not available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,isSupported,org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite),181,193,"/**
* Determines whether a cipher suite is supported based on its algorithm and padding.
* @param suite CipherSuite object containing encryption details
* @return true if the suite is supported, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String),376,398,"/**
* Retrieves a list of KeyVersions for the given name.
* @param name unique identifier
* @return List of KeyVersion objects or empty list if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,createKey,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",433,460,"/**
* Creates a new key with given name and material.
* @param name unique key name
* @param material byte array representing the key's material
* @param options Options object containing metadata settings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])",508,527,"/**
* Generates a new key version based on provided name and material.
* @param name unique key identifier
* @param material byte array to generate key from
* @return KeyVersion object or throws IOException if invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])",246,250,"/**
* Creates a KMSEncryptedKeyVersion instance with specified key and version details.
* @param keyName the name of the key
* @param keyVersionName the name of the key version
* @param iv initialization vector for encryption
* @param encryptedVersionName name of the encrypted key version
* @param keyMaterial encrypted key material bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONKeyVersion,org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map),185,202,"/**
* Retrieves KeyProvider.KeyVersion from the provided Map.
* @param valueMap input map containing key metadata
* @return KeyProvider.KeyVersion object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONMetadata,org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map),204,218,"/**
* Retrieves KeyProvider.Metadata from the provided Map.
* @param valueMap map containing key provider metadata values
* @return KeyProvider.Metadata object or null if data is invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String),66,69,"/**
* Retrieves a KeyVersion using the provider's m1 method.
* @param name unique key identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,createKey,"org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",567,571,"/**
* Generates key version based on provided name and material.
* @param name unique key identifier
* @param options configuration settings for algorithm and parameters
*/","* Create a new key generating the material for it.
   * The given key must not already exist.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #createKey(String, byte[], Options)} method.
   *
   * @param name the base name of the key
   * @param options the options for the new key.
   * @return the version name of the first version of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String),612,621,"/**
* Generates a KeyVersion object using the M5 algorithm.
* @param name unique key identifier
*/","* Roll a new version of the given key generating the material for it.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #rollNewVersion(String, byte[])} method.
   *
   * @param name the basename of the key
   * @return the name of the new version of the key
   * @throws IOException              raised on errors performing I/O.
   * @throws NoSuchAlgorithmException This exception is thrown when a particular
   *                                  cryptographic algorithm is requested
   *                                  but is not available in the environment.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])",140,146,"/**
* Performs M2 operation on a given material with specified name.
* @param name material name
* @param material material bytes
* @return KeyVersion object or throws IOException if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),110,122,"/**
* Builds JSON map for EncryptedKeyVersion object.
* @param encryptedKeyVersion Encrypted key version to serialize
* @return JSON map or empty map if input is null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),288,308,"/**
* Attempts to warm up keys across multiple providers.
* @param keyNames variable number of key names to warm up
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close(),544,554,"/**
* Closes all KMS client providers and logs any IO exceptions encountered.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readLock,org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String),115,117,"/**
 * Invokes nested methods on UserProfile object retrieved by name.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String),119,121,"/**
 * Applies a series of operations (m1, m2, m3) on the specified key.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String),123,125,"/**
* Applies mask functionality to specified key.
* @param keyName unique identifier of key to mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeLock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String),127,129,"/**
 * Calls chained methods on the result of m1() operation.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String),139,141,"/**
* Initializes builder with given context and default caller context separator.
* @param context user-provided context
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto),70,80,"/**
* Converts a GenericRefreshResponseCollectionProto to a Collection of RefreshResponse objects.
* @param collection Generic refresh response collection protocol buffer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)",44,62,"/**
* Processes a refresh request and returns a collection of refresh responses.
* @param controller RPC controller instance
* @param request Refresh request object containing identifier
* @return Collection of refresh response objects or throws ServiceException on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)",335,339,"/**
* Creates a cache entry with a unique function mask.
* @param expirationTime time until the entry expires
* @param clientId client identifier
* @param callId unique call ID
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)",155,159,"/**
* Creates a cache entry with custom payload and expiration time.
* @param clientId unique client identifier
* @param callId call identifier
* @param payload user-defined data object
* @param expirationTime timestamp for cache entry to expire
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)",84,88,"/**
* Constructs a new cache entry with specified client ID, call ID, and expiration time.
* Sets the state to either SUCCESS or FAILED based on the provided success flag.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue(),3885,3888,"/**
* Calls queue's m1 method and returns result.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable),289,299,"/**
* Handles the given event E, potentially delegating to other methods based on conditions. 
* @param e the event to handle
*/","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable),301,304,"/**
* Applies mask criteria to entity.
* @param e entity to evaluate
* @return true if matches, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,callQueueLength,org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength(),167,169,"/**
 * Calculates the length of the call queue.
 * @return integer value representing the queue length
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,initialize,org.apache.hadoop.ipc.WritableRpcEngine:initialize(),80,84,"/**
* Initializes writable IPC server with invocation handler.
*/",* Register the rpcRequest deserializer for WritableRpcEngine,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerProtocolEngine,org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine(),70,77,"/**
* Initializes server protocol buffer RPC if not already set up.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,setExpirationTime,"org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)",147,149,"/**
* Updates entry expiration time by adding specified period.
* @param e Entry object
* @param expirationPeriod additional seconds to add
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,start,org.apache.hadoop.util.StopWatch:start(),58,65,"/**
* Initializes and starts the stopwatch.
* @return current instance for method chaining
*/","* Start to measure times and make the state of stopwatch running.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,stop,org.apache.hadoop.util.StopWatch:stop(),71,79,"/**
* Stops the StopWatch and updates elapsed time.
* @return this StopWatch object
*/","* Stop elapsed time and make the state of stopwatch stop.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(),105,109,"/**
* Calculates elapsed time in nanoseconds since function invocation.
* @return elapsed time as a long integer
*/",* @return current elapsed time in nanosecond.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Call:sendResponse(),1086,1094,"/**
* Processes response wait count, sending data if zero.
* @throws IOException on processing error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,abortResponse,org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable),1096,1103,"/**
* Handles throwable by invoking secondary method if response wait count is positive.
* @throws IOException on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprint,org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[]),140,142,"/**
 * Recursively counts the total number of methods in the provided array.
 * @param methods array of Method objects
 */","* Get the hash code of an array of methods
   * Methods are sorted before hashcode is calculated.
   * So the returned value is irrelevant of the method order in the array.
   * 
   * @param methods an array of methods
   * @return the hash code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getSigFingerprint,"org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)",186,200,"/**
* Computes protocol signature fingerprint based on server version and method hashes.
* @param protocol target protocol class
* @param serverVersion server version number
* @return ProtocolSigFingerprint object or null if not cached
*/","* Return a protocol's signature and finger print from cache
   * 
   * @param protocol a protocol class
   * @param serverVersion protocol version
   * @return its signature and finger print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,valueOf,org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes),131,134,"/**
* Creates a RemoteException with class and message details from attributes.
* @param attrs Attributes object containing class and message values
*/","* Create RemoteException from attributes.
   * @param attrs may not be null.
   * @return RemoteException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,readResponse,org.apache.hadoop.ipc.Client$IpcStreams:readResponse(),1922,1944,"/**
* Reads and returns the response buffer from input stream.
*@throws IOException on read error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldFailoverOnException,org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception),773,781,"/**
* Checks if the exception is a remote-related standby exception.
* @param e the exception to check
* @return true if it's a remote standby exception, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getWrappedRetriableException,org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception),795,803,"/**
* Extracts RetriableException from RemoteException.
* @param e remote exception to unwrap
* @return RetriableException or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,monitorHealth,"org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",34,42,"/**
* Calls HA service protocol method m2.
* @param svc HAServiceProtocol instance
* @throws IOException on remote exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToActive,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",44,52,"/**
* Invokes HA service protocol method m2 and handles RemoteException.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToStandby,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",54,62,"/**
* Invokes HAServiceProtocol's m2 method and handles RemoteException.
* @param svc HA service protocol instance
* @param reqInfo State change request info
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToObserver,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",64,71,"/**
* Invokes HA service protocol method m2 with provided request info.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo object
* @throws IOException on remote exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,isHealthCheckFailedException,org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable),229,235,"/**
* Checks if an exception is a health check failure or nested remote exception.
* @param t the exception to check
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,get,org.apache.hadoop.fs.PartialListing:get(),67,72,"/**
* Returns a list of items with mask applied.
* @throws IOException if an error occurs
*/","* Partial listing of the path being listed. In the case where the path is
   * a file. The list will be a singleton with the file itself.
   *
   * @return Partial listing of the path being listed.
   * @throws IOException if there was an exception getting the listing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call),985,988,"/**
* Creates a new instance of Call with specified attributes from an existing Call object.
* @param call the Call object to create a new instance from
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",990,992,"/**
* Constructs a new Call instance with default headers.
* @param id call identifier
* @param retryCount maximum retries for the call
* @param kind type of RPC operation (e.g. request, response)
* @param clientId client ID associated with this call
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",994,998,"/**
* Constructs a Call instance with default values for timeout and max attempts.
* @param id unique call identifier
* @param retryCount maximum number of retries
* @param kind RPC operation type
* @param clientId client ID as byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",65,78,"/**
* Invokes m3() with extracted timing metrics.
* @param callName name of the scheduled task
* @param schedulable Schedulable object
* @param details ProcessingDetails containing timing data
*/","* Store a processing time value for an RPC call into this scheduler.
   *
   * @param callName The name of the call.
   * @param schedulable The schedulable representing the incoming call.
   * @param details The details of processing time.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numDroppedConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections(),171,173,"/**
* Returns the number of dropped connections.
* @return count of dropped connections on the server.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,register,"org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)",4097,4108,"/**
* Establishes a server connection via the provided SocketChannel.
* @param channel underlying socket channel
* @param ingressPort port number for the connection
* @param isOnAuxiliaryPort whether this is an auxiliary port connection
* @return established Connection object or null if unsuccessful
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections(),153,155,"/**
* Returns the number of open connections.
* @return The current count of open connections.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueues,"org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)",257,267,"/**
* Checks function mask based on priority, element, and include last flag.
* @param priority current priority level
* @param e element to check against
* @param includeLast whether to consider the last priority
* @return true if match found, false otherwise
*/","* Offer the element to queue of the given or lower priority.
   * @param priority - starting queue priority
   * @param e - element to add
   * @param includeLast - whether to attempt last queue
   * @return boolean if added to a queue",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",269,279,"/**
* Submits element to the appropriate priority queue with a timeout.
* @param e the element to submit
* @param timeout maximum waiting time in specified units
* @param unit TimeUnit for the timeout value
* @return true if submitted successfully, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object),281,290,"/**
* Processes an element based on its priority level.
* @param e the element to process
* @return true if processing is successful, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,populateResponseParamsOnError,"org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",1279,1302,"/**
* Processes an exception and populates a ResponseParams object.
* @param t the exception to process
* @param responseParams the ResponseParams object to update
*/","* @param t              the {@link java.lang.Throwable} to use to set
     *                       errorInfo
     * @param responseParams the {@link ResponseParams} instance to populate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersionForRpcKind,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",106,120,"/**
* Retrieves version numbers of the specified protocol.
* @param rpcKind RPC kind
* @param protocol protocol name
* @return array of version numbers or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchProtocolException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String),29,31,"/**
 * Constructs an RpcNoSuchProtocolException with the specified protocol-related error message.
 * @param message descriptive text of the exception
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchMethodException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String),30,32,"/**
* Constructs an RpcNoSuchMethodException with the specified error message.
* @param message detailed description of the exception cause
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)",248,255,"/**
* Constructs a VersionMismatch exception with specified protocol and version details.
* @param interfaceName name of the protocol interface
* @param clientVersion client-side version number
* @param serverVersion server-side version number
*/","* Create a version mismatch exception
     * @param interfaceName the name of the protocol mismatch
     * @param clientVersion the client's version of the protocol
     * @param serverVersion the server's version of the protocol",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)",1986,1989,"/**
* Constructs an RpcServerException with error code and underlying IOException.
* @param errCode RPC error code
* @param ioe underlying IOException
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int),78,81,"/**
 * Initializes a new FramedBuffer instance with the specified capacity.
 * @param capacity maximum buffer size (excluding framing overhead)
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer:reset(),70,74,"/**
* Resets response buffer and calls underlying FramedBuffer's m1() method.
* @return This instance for method chaining
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,writeTo,org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream),48,50,"/**
* Calls m1() and delegates output stream to its m2() method.
* @param out OutputStream object for writing data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,toByteArray,org.apache.hadoop.ipc.ResponseBuffer:toByteArray(),52,54,"/**
* Calls and returns result of m2() on UserProfile object. 
* @return byte array or null if m1() returns null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,recomputeScheduleCache,org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache(),545,560,"/**
* Computes and schedules cache levels for each ID.
* @param callCosts map of IDs with atomic cost values
*/",* Update the scheduleCache to match current conditions in callCosts.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,cachedOrComputedPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object),642,661,"/**
* Computes and returns the priority of an object based on its schedule cache or call costs.
* @param identity unique identifier of the object
*/","* Returns the priority level for a given identity by first trying the cache,
   * then computing it.
   * @param identity an object responding to toString and hashCode
   * @return integer scheduling decision from 0 to numLevels - 1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,setPriorityLevel,"org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",272,276,"/**
* Calls DecayRpcScheduler's m1 method to adjust RPC scheduling.
* @param user UserGroupInformation object
* @param priority scheduling priority value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary(),914,922,"/**
* Retrieves scheduler state by delegating to the current scheduler.
* @return scheduler state as a string or error message
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)",296,299,"/**
* Creates an RPC request with a protobuf header for the given method.
* @param method target Method object
* @param theRequest message to be sent
* @return RpcProtobufRequest instance or null if invalid input
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)",306,309,"/**
* Creates an RPC request with function mask from a given method and message.
* @param method Method to generate function mask for
* @param theRequest Message containing request data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,refreshServiceAcl,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl(),54,58,"/**
* Invokes RPC proxy to refresh service ACL.
* @throws IOException on RPC failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshUserToGroupsMappings,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings(),59,63,"/**
* Calls remote procedure via RPC proxy.
* @throws IOException on communication failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration(),65,69,"/**
* Invokes RPC proxy to refresh superuser groups configuration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,refreshCallQueue,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue(),54,58,"/**
* Calls RPC proxy to refresh call queue.
* @throws IOException if RPC operation fails.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,getGroupsForUser,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String),52,59,"/**
* Makes M4 RPC call to retrieve groups for a given user.
* @param user user identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,cedeActive,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int),56,63,"/**
* Initiates ceding of active request after specified time.
* @param millisToCede time in milliseconds to wait before ceding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,gracefulFailover,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover(),65,69,"/**
* Executes RPC call to m2 on remote controller.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,monitorHealth,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth(),84,87,"/**
 * Sends RPC request to monitor health using the provided proxy.
 * @throws IOException if communication with the remote endpoint fails.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,getServiceStatus,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus(),114,128,"/**
* Retrieves HA service status using RPC.
* @return HAServiceStatus object with updated status
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod(),165,171,"/**
* Executes RPC call with retries and async handling.
* @throws Throwable if an error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,close,org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close(),264,270,"/**
* Closes the client connection and notifies all clients.
* @param client Client object to be notified
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close(),325,331,"/**
* Closes client connection and sends mask message.
* @throws IOException on I/O error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close(),335,341,"/**
* Closes client connection and notifies clients.
* @throws IOException on I/O error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getMetrics,"org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",462,477,"/**
* Records fair call queue metrics for each namespace.
* @param collector MetricsCollector instance
* @param all whether to include all namespaces or not
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object),40,53,"/**
* Wraps the given object in a RpcWritable instance, 
* or throws exception if unable to wrap.
*@param o Object to be wrapped
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode(),174,177,"/**
* Calls superclass implementation of m1.
*/",Override hashcode to avoid findbugs warnings,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,getAndAdvanceCurrentIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex(),145,149,"/**
* Computes and returns a function mask index.
* @return integer index value
*/",* Use the mux by getting and advancing index.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,registerForDeferredResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse(),421,426,"/**
* Creates and initializes a protobuf RPC engine callback.
* @return ProtobufRpcEngineCallback instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerForDeferredResponse2,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2(),453,458,"/**
* Creates and configures a protobuf RPC engine callback.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer),110,116,"/**
* Writes function mask data to response buffer.
* @param out ResponseBuffer object for writing
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer),159,163,"/**
* Writes functional mask values to output buffer.
* @param out ResponseBuffer to write to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,writeTo,org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer),63,70,"/**
* Writes encoded response data to buffer using protocol buffer encoding.
* @param out ResponseBuffer to write to
* @throws IOException on write error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,cleanupCalls,org.apache.hadoop.ipc.Client$Connection:cleanupCalls(),1307,1314,"/**
* Iterates over call records and marks them as closed.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteAddress,org.apache.hadoop.ipc.Server:getRemoteAddress(),436,439,"/**
* Retrieves and returns a mask value from the specified IP address.
* @return null if address is invalid or null, otherwise the mask value
*/","@return Returns remote address as a string when invoked inside an RPC.
   *  Returns null in case of an error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnectionsPerUser,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser(),162,165,"/**
 * Returns the number of open connections per user.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAsyncWrite,org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey),1798,1821,"/**
* Handles RpcCall selection key and updates operations.
* @param key SelectionKey object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRespond,org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall),1927,1939,"/**
* Executes asynchronous RPC operation and updates response queue.
* @param call RpcCall object containing operation context
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsTracer.java,get,org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration),39,47,"/**
* Returns a synchronized instance of the Tracer with configuration applied.
* @param conf Hadoop Configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)",77,79,"/**
* Initializes MachineList with trimmed and parsed host entries.
* @param hostEntries comma-separated list of machine hosts
* @param addressFactory factory for creating InetAddress objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.util.Collection),85,87,"/**
* Initializes machine list with provided host entries and default address factory.
* @param hostEntries collection of host entries
*/","*
   * @param hostEntries collection of separated ip/cidr/host addresses",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,isIn,org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String),71,77,"/**
* Checks if IP address matches any mask in the list.
* @param ipAddress target IP address to check
* @return true if match found, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,org.apache.hadoop.util.SysInfoLinux:<init>(),180,183,"/**
* Initializes Linux system info with default file paths.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(),215,217,"/**
 * Recursively calls itself with an initial flag value.
 */","* Read /proc/meminfo, parse and compute memory information only once.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize(),609,616,"/**
* Calculates total available disk space in bytes.
* @return total available space, or -1 if calculation fails
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime(),646,650,"/**
* Calls m1() and returns result of m2() tracked by CPU time tracker.
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage(),653,661,"/**
* Calculates the system-wide CPU usage as a float value.
* @return normalized CPU usage or UNAVAILABLE if failed
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed(),664,672,"/**
* Calculates overall VCore usage as a percentage.
* @return Overall VCore usage (0.0 - 1.0) or CpuTimeTracker.UNAVAILABLE if unavailable
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead(),688,692,"/**
* Calculates and returns the function mask value.
* @return 64-bit integer representing the function mask",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten(),694,698,"/**
* Calculates and returns the function mask value based on disks bytes written. 
* @return Function mask value as a long integer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,<init>,org.apache.hadoop.util.IdentityHashStore:<init>(int),62,72,"/**
* Initializes IdentityHashStore with specified capacity.
* @param capacity maximum number of identity hashes to store
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,put,"org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)",118,126,"/**
* Inserts a key-value pair into the buffer, potentially resizing it if necessary.
* @param k key to be inserted
* @param v value associated with the key
*/","* Add a new (key, value) mapping.
   *
   * Inserting a new (key, value) never overwrites a previous one.
   * In other words, you can insert the same key multiple times and it will
   * lead to multiple entries.
   *
   * @param k Generics Type k.
   * @param v Generics Type v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,releaseBuffer,org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer),222,235,"/**
* Releases and reclaims buffer, handling potential ClassCastException.
* @param buffer ByteBuffer instance to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,hasNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext(),329,333,"/**
* Checks if there's a next mask to process.
* @return true if a next mask is available, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,next,org.apache.hadoop.util.LightWeightGSet$SetIterator:next(),335,344,"/**
* Fetches and returns the next element, throwing exception if none exist.
* @return Element object or throws IllegalStateException if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,put,org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object),149,177,"/**
* Updates and returns existing LinkedElement at given index.
* @param element updated LinkedElement instance
* @return existing LinkedElement instance or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object),221,228,"/**
* Recursively resolves value of type E associated with given key.
* @param key unique identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int),3253,3256,"/**
* Copies and sorts a specified range of integers.
* @param count number of elements to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory(),148,154,"/**
* Creates a TransformerFactory instance with secure processing enabled.
* @return configured TransformerFactory object
*/","* This method should be used if you need a {@link TransformerFactory}. Use this method
   * instead of {@link TransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link TransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureSAXTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory(),165,171,"/**
* Creates a secure SAX transformer factory instance. 
* @return SAXTransformerFactory with secure processing enabled
*/","* This method should be used if you need a {@link SAXTransformerFactory}. Use this method
   * instead of {@link SAXTransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link SAXTransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,formatSize,"org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)",477,481,"/**
* Formats byte count to human-readable string or binary mask.
* @param size total bytes
* @param humanReadable true for human-readable format (e.g., KB, MB), false for binary mask (e.g., B, KiB)
*/","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,formatSize,org.apache.hadoop.fs.shell.Ls:formatSize(long),126,130,"/**
* Formats file size as human-readable string (e.g., ""1.5 MB"") or binary prefix.
* @param size file size in bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,formatSize,org.apache.hadoop.fs.shell.FsUsage:formatSize(long),55,59,"/**
* Converts file size to human-readable format.
* @param size file size in bytes
* @return formatted string (e.g. ""1 MB"", ""5 KB"") or ""1"" if traditional binary prefix is off
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,formatSize,"org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)",394,398,"/**
* Formats file size as human-readable string or binary mask.
* @param size file size in bytes
* @param humanReadable true for human-readable format (e.g., KB, MB), false for binary mask (e.g., 1Kb)
*/","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,humanReadableInt,org.apache.hadoop.util.StringUtils:humanReadableInt(long),132,135,"/**
* Converts binary number to string representation with traditional binary prefix.
* @param number binary number value
*/","* Given an integer, return a string that is in an approximate, but human 
   * readable format. 
   * @param number the number to format
   * @return a human readable form of the integer
   *
   * @deprecated use {@link TraditionalBinaryPrefix#long2String(long, String, int)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteDesc,org.apache.hadoop.util.StringUtils:byteDesc(long),1022,1024,"/**
* Returns a string representation of binary prefix mask.
* @param len length in bits
*/","* a byte description of the given long interger value.
   *
   * @param len len.
   * @return a byte description of the given long interger value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)",379,417,"/**
* Calculates memory capacity for a map based on max memory and percentage.
* @param maxMemory maximum available memory
* @param percentage proportion of max memory to use (0-100)
* @param mapName name of the map
* @return calculated capacity in entries, or 0 if invalid inputs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,addToUsagesTable,"org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)",114,127,"/**
* Calculates and sets disk usage metrics for the given URI.
* @param uri file system URI
* @param fsStatus file system status object
* @param mountedOnPath path where the file is mounted
*/","* Add a new row to the usages table for the given FileSystem URI.
     *
     * @param uri - FileSystem URI
     * @param fsStatus - FileSystem status
     * @param mountedOnPath - FileSystem mounted on path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readChecksumChunk,"org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)",293,334,"/**
* Reads function data from a byte array, handling checksum errors with retries.
* @param b input byte array
* @param off starting offset in the array
* @param len length of data to be read
* @return number of bytes read or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readChars,"org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)",279,332,"/**
* Processes and appends valid UTF-8 encoded bytes to the StringBuilder.
* @param in input stream
* @param buffer destination StringBuilder
* @param nBytes number of bytes to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte[]),199,201,"/**
 * Returns a hexadecimal string representation of the given byte array.
 * @param bytes input byte array
 */","* Same as byteToHexString(bytes, 0, bytes.length).
   * @param bytes bytes.
   * @return byteToHexString.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",51,54,"/**
* Calls overloaded version of m1 with null as optional parameter.","* Sort the given range of items using heap sort.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException),874,876,"/**
* Handles exit exception by invoking m1 function.
* @param ee ExitException object containing error details
*/","* Exit the JVM using an exception for the exit code and message,
   * invoking {@link ExitUtil#terminate(ExitUtil.ExitException)}.
   *
   * This is the standard way a launched service exits.
   * An error code of 0 means success -nothing is printed.
   *
   * If {@link ExitUtil#disableSystemExit()} has been called, this
   * method will throw the exception.
   *
   * The method <i>may</i> be subclassed for testing
   * @param ee exit exception
   * @throws ExitUtil.ExitException if ExitUtil exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)",1024,1026,"/**
* Logs and exits with a service launch exception.
* @param status error code
* @param message descriptive error message
*/","* Exit with a printed message. 
   * @param status status code
   * @param message message message to print before exiting
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)",338,344,"/**
* Recursively handles exit exceptions by either propagating or wrapping the exception.
* @param status error code
* @param t thrown exception (will be wrapped in ExitException if not already so)
*/","* Like {@link #terminate(int, String)} but uses the given throwable to
   * build the message to display or throw as an
   * {@link ExitException}.
   * <p>
   * @param status exit code to use if the exception is not an ExitException.
   * @param t throwable which triggered the termination. If this exception
   * is an {@link ExitException} its status overrides that passed in.
   * @throws ExitException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)",380,382,"/**
 * Throws an ExitException with the specified status and message.
 * @param status exit status
 * @param msg error message
 */","* Terminate the current process. Note that terminate is the *only* method
   * that should be used to terminate the daemon processes.
   *
   * @param status exit code
   * @param msg message used to create the {@code ExitException}
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)",354,360,"/**
* Recursively wraps or unwraps HaltExceptions as needed.
* @param status error code
* @param t Throwable to process
* @throws HaltException if wrapping is required
*/","* Forcibly terminates the currently running Java virtual machine.
   *
   * @param status exit code to use if the exception is not a HaltException.
   * @param t throwable which triggered the termination. If this exception
   * is a {@link HaltException} its status overrides that passed in.
   * @throws HaltException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)",399,401,"/**
* Wraps an exception with provided status and message.
* @param status error code
* @param message detailed error description
*/","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @param message message
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,unregister,org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister(),69,75,"/**
* Unregisters shutdown hook in a thread-safe manner.
*/",* Unregister the hook.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",63,67,"/**
* Recursively masks elements in a sorted array.
* @param s the input array to mask
* @param p start index of masking range
* @param r end index of masking range (inclusive)
* @param rep progress callback for updates
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(),83,85,"/**
* Constructs an empty LightWeightResizableGSet with default initial capacity and load factor.
* @param defaultInitialCapacity default initial size of the set
* @param defaultLoadFactor default threshold for resizing
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(int),87,89,"/**
* Initializes a new instance of LightWeightResizableGSet with specified initial capacity.
* @param initCapacity initial number of elements the set can hold
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable),91,98,"/**
* Returns a list from an iterable, using recursion for non-collection iterables.
* @param elements input iterable to convert
* @return ArrayList of type E or throws NullPointerException if null
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list then
   * calling Iterables#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newLinkedList,org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable),185,190,"/**
* Creates and populates a linked list with elements from an iterable.
* @param elements iterable containing elements to add to the list
*/","* Creates a <i>mutable</i> {@code LinkedList} instance containing the given
   * elements; a very thin shortcut for creating an empty list then calling
   * Iterables#addAll.
   *
   * <p><b>Performance note:</b> {@link ArrayList} and
   * {@link java.util.ArrayDeque} consistently
   * outperform {@code LinkedList} except in certain rare and specific
   * situations. Unless you have spent a lot of time benchmarking your
   * specific needs, use one of those instead.</p>
   *
   * @param elements elements.
   * @param <E> Generics Type E.
   * @return Generics Type E List.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getAclFromPermAndEntries,"org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)",42,90,"/**
* Creates a masked ACL entry list from the given permission and entries.
* @param perm FsPermission object
* @param entries List of AclEntry objects
* @return List of AclEntry objects with added mask entry
*/","* Given permissions and extended ACL entries, returns the full logical ACL.
   *
   * @param perm FsPermission containing permissions
   * @param entries List&lt;AclEntry&gt; containing extended ACL entries
   * @return List&lt;AclEntry&gt; containing full logical ACL",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,addChunk,org.apache.hadoop.util.ChunkedArrayList:addChunk(int),156,160,"/**
* Initializes chunking data structure with specified capacity.
* @param capacity desired chunk size
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[]),70,80,"/**
* Creates an ArrayList from a variable number of elements.
* @param elements array of elements to add
* @return populated ArrayList or null if input is null
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the given
   * elements.
   *
   * <p>Note that even when you do need the ability to add or remove,
   * this method provides only a tiny bit of syntactic sugar for
   * {@code newArrayList(}
   * {@link Arrays#asList asList}
   * {@code (...))}, or for creating an empty list then calling
   * {@link Collections#addAll}.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithExpectedSize,org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int),148,151,"/**
* Creates an ArrayList with estimated initial capacity based on provided size.
* @param estimatedSize initial capacity estimate
*/","* Creates an {@code ArrayList} instance to hold {@code estimatedSize}
   * elements, <i>plus</i> an unspecified amount of padding;
   * you almost certainly mean to call {@link
   * #newArrayListWithCapacity} (see that method for further advice on usage).
   *
   * @param estimatedSize an estimate of the eventual {@link List#size()}
   *     of the new list.
   * @return a new, empty {@code ArrayList}, sized appropriately to hold the
   *     estimated number of elements.
   * @throws IllegalArgumentException if {@code estimatedSize} is negative.
   *
   * @param <E> Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String),155,158,"/**
* Convenience wrapper around m1(String, boolean), always using default loading.
* @param name class name to load
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)",201,204,"/**
* Masks a file's contents using the provided instance.
* @param file File to mask
* @param instance Instance used for masking
*/","* Save to a local file. Any existing file is overwritten unless
   * the OS blocks that.
   * @param file file
   * @param instance instance
   * @throws IOException IO exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)",72,81,"/**
* Initializes StatisticDurationTracker instance and increments counter in IOStatisticsStore.
* @param iostats IOStatisticsStore to update
* @param key unique counter identifier
* @param count positive value to increment the counter by
*/","* Constructor.
   * If the supplied count is greater than zero, the counter
   * of the key name is updated.
   * @param iostats statistics to update
   * @param key Key to use as prefix of values.
   * @param count #of times to increment the matching counter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])",69,83,"/**
* Logs a duration information message with optional logging level.
* @param log Logger instance
* @param logAtInfo whether to log at INFO level or not
* @param format log format string
* @param args variable arguments for the log format
*/","* Create the duration text from a {@code String.format()} code call
   * and log either at info or debug.
   * @param log log to write to
   * @param logAtInfo should the log be at info, rather than debug
   * @param format format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,toString,org.apache.hadoop.util.OperationDuration:toString(),91,94,"/**
* Returns function mask string using m1().","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newStripedCrcComposer,"org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)",83,92,"/**
* Creates a CRC composer with specified parameters.
* @param type checksum type
* @param bytesPerCrcHint hint for CRC calculation
* @param stripeLength stripe length
*/","* Returns a CrcComposer which will collapse CRCs for every combined
   * underlying data size which aligns with the specified stripe boundary. For
   * example, if ""update"" is called with 20 CRCs and bytesPerCrc == 5, and
   * stripeLength == 10, then every two (10 / 5) consecutive CRCs will be
   * combined with each other, yielding a list of 10 CRC ""stripes"" in the
   * final digest, each corresponding to 10 underlying data bytes. Using
   * a stripeLength greater than the total underlying data size is equivalent
   * to using a non-striped CrcComposer.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @param stripeLength stripeLength.
   * @return a CrcComposer which will collapse CRCs for every combined.
   * underlying data size which aligns with the specified stripe boundary.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,compose,"org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)",102,105,"/**
* Computes a function mask using the given CRC values and parameters.
* @param crcA first CRC value
* @param crcB second CRC value
* @param lengthB data length
* @param mod modulus for calculation
* @return computed function mask
*/","* compose.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param lengthB length of content corresponding to {@code crcB}, in bytes.
   * @param mod mod.
   * @return compose result.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getBytes,org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes(),64,67,"/**
* Generates CRC mask.
* @return byte array representing CRC mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,digest,org.apache.hadoop.util.CrcComposer:digest(),204,213,"/**
* Computes and returns a digest value for the current stripe.
* @return Digest value as a byte array
*/","* Returns byte representation of composed CRCs; if no stripeLength was
   * specified, the digest should be of length equal to exactly one CRC.
   * Otherwise, the number of CRCs in the returned array is equal to the
   * total sum bytesPerCrc divided by stripeLength. If the sum of bytesPerCrc
   * is not a multiple of stripeLength, then the last CRC in the array
   * corresponds to totalLength % stripeLength underlying data bytes.
   *
   * @return byte representation of composed CRCs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJarAndSave,"org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)",168,178,"/**
 * Unpacks and saves a compressed archive from InputStream to specified directory.
 * @param inputStream input stream containing the compressed archive
 * @param toDir target directory for unpacking
 * @param name file name for the unpacked archive
 * @param unpackRegex pattern for extracting specific files or directories
 */","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped. Keep also a copy
   * of the entire jar in the same directory for backward compatibility.
   * TODO remove this feature in a new release and do only unJar
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   * @param name name.
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)",104,106,"/**
* Extracts contents of a JAR file.
* @param jarFile JAR file to extract from
* @param toDir directory to extract into
*/","* Unpack a jar file into a directory.
   *
   * This version unpacks all files inside the jar regardless of filename.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)",98,100,"/**
* Extracts specified number of bytes from UTF-8 encoded array starting at offset 0.
* @param utf UTF-8 encoded byte array
* @param b extracted byte count
* @return extracted byte count or -1 if invalid
*/","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,get,org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object),147,171,"/**
* Retrieves a value from the cache using FUNC_MASK strategy.
* @param key unique identifier for cache lookup
*/","* Get the value, creating if needed.
   * @param key key.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,check,"org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)",190,226,"/**
* Logs lock hold duration and updates suppressed statistics.
* @param acquireTime lock acquisition timestamp
* @param releaseTime lock release timestamp
* @param checkLockHeld true for held lock, false for waiting lock
*/","* Log a warning if the lock was held for too long.
   *
   * Should be invoked by the caller immediately AFTER releasing the lock.
   *
   * @param acquireTime  - timestamp just after acquiring the lock.
   * @param releaseTime - timestamp just before releasing the lock.
   * @param checkLockHeld checkLockHeld.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)",390,400,"/**
* Formats a time string with optional duration.
* @param formattedFinishTime time string to display
* @param finishTime timestamp of completion (0 for no duration)
* @param startTime timestamp of start (0 for no duration)
* @return Formatted time string or empty if no duration
*/","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   * @param formattedFinishTime formattedFinishTime to use
   * @param finishTime finish time
   * @param startTime start time
   * @return formatted value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,org.apache.hadoop.util.StringUtils:split(java.lang.String),570,572,"/**
* Escapes string and converts to comma-separated array.
* @param str input string to process
*/","* Split a string using the default separator
   * @param str a string that may have escaped separator
   * @return an array of strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,camelize,org.apache.hadoop.util.StringUtils:camelize(java.lang.String),1094,1102,"/**
* Replaces special characters in a string with underscores.
* @param s input string
* @return modified string with special chars replaced
*/","* Convert SOME_STUFF to SomeStuff
   *
   * @param s input string
   * @return camelized string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)",678,681,"/**
* Escapes a character in a string.
* @param str input string
* @param escapeChar character used for escaping
* @param charToEscape character to be escaped
* @return modified string with character escaped
*/","* Escape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the char to be escaped
   * @return an escaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)",736,739,"/**
* Escapes a single character in a string.
* @param str input string
* @param escapeChar the character used for escaping
* @param charToEscape the character to be escaped
*/","* Unescape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the escaped char
   * @return an unescaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,createStartupShutdownMessage,"org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])",837,851,"/**
* Generates functional mask message for startup.
* @param classname class name
* @param hostname host machine name
* @param args command line arguments array
* @return formatted string containing system and application information
*/","* Generate the text for the startup/shutdown message of processes.
   * @param classname short classname of the class
   * @param hostname hostname
   * @param args Command arguments
   * @return a string to log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBuildVersion,org.apache.hadoop.util.VersionInfo:getBuildVersion(),162,164,"/**
* Returns version info mask string.","* Returns the buildVersion which includes version,
   * revision, user and date.
   * @return the buildVersion",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next(),650,658,"/**
* Returns the function mask value and consumes input data if available.
* @throws IOException if an I/O error occurs
*/","* Return the next value.
     * Will retrieve the next elements if needed.
     * This is where the mapper takes place.
     * @return true if there is another data element.
     * @throws IOException failure in fetch operation or the transformation.
     * @throws NoSuchElementException no more data",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close(),695,707,"/**
* Closes the object and its subordinate, invoking superclass and helper methods. 
* @throws IOException if an I/O error occurs during closing process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext(),466,479,"/**
* Checks for the existence of a next item in the data stream.
* @return true if there is a next item, false otherwise
*/","* Check for the source having a next element.
     * If it does not, this object's close() method
     * is called and false returned
     * @return true if there is a new value
     * @throws IOException failure to retrieve next value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,lazyAutoCloseablefromSupplier,org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier),99,101,"/**
* Creates a lazy auto-closeable reference to an object managed by the given supplier.
* @param supplier creates instances of T, which must implement AutoCloseable
*/","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",72,79,"/**
* Calculates file system mask using BulkDelete.
* @param fs FileSystem instance
* @param path File system path
* @return integer mask value
*/","* Get the maximum number of objects/files to delete in a single request.
   * @param fs filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",104,113,"/**
* Applies mask to files in the specified directory and subdirectories.
* @param fs FileSystem instance
* @param base base directory path
* @param paths collection of file paths to process
* @return list of file paths with associated metadata or null if failed
*/","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other
   *          than ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws UncheckedIOException if an IOE was raised.
   * @throws IllegalArgumentException if a path argument is invalid.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",165,191,"/**
* Opens a file with specified policy and options.
* @param fs FileSystem instance
* @param path Path to the file
* @param policy Read policy (e.g. ""read-only"")
* @param status Optional FileStatus object
* @param length Optional file length
* @param options Additional file open options
* @return FSDataInputStream or null on failure
*/","* OpenFile assistant, easy reflection-based access to
   * {@link FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",213,224,"/**
* Applies mask operation to input stream at specified position.
* @param in InputStream to process
* @param position Position to apply mask from
* @param buf ByteBuffer to store result
*/","* Delegate to {@link ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * Note: that is the default behaviour of {@link FSDataInputStream#readFully(long, ByteBuffer)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",134,139,"/**
* Computes and returns a mask of file system statistics.
* @param fs FileSystem object
* @param path Path to compute statistics for
* @return Serializable result or null if failed
*/","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),192,196,"/**
* Processes JSON input to produce a serializable result.
* @param json input JSON string
*/","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,get,org.apache.hadoop.util.functional.LazyAtomicReference:get(),120,123,"/**
* Evaluates and returns the mask value.
* @return The evaluated mask value
*/","* Implementation of {@code Supplier.get()}.
   * <p>
   * Invoke {@link #eval()} and convert IOEs to
   * UncheckedIOException.
   * <p>
   * This is the {@code Supplier.get()} implementation, which allows
   * this class to passed into anything taking a supplier.
   * @return the value
   * @throws UncheckedIOException if the constructor raised an IOException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable),581,583,"/**
* Creates a Builder instance from an iterable of items.
* @param items input iterable
*/","* Create a task builder for the iterable.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[]),595,597,"/**
* Creates a new builder instance from an array of items.
* @param items array of items to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException),106,110,"/**
* Wraps an ExecutionException in an IOException and delegates to FutureIO. 
* @param e exception containing execution result 
*/","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * See {@link FutureIO#raiseInnerCause(ExecutionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future),97,110,"/**
* Retrieves the result of a future, handling cancellation and execution exceptions.
* @param future Future object to retrieve result from
* @return Result T or throws exception if cancelled or failed
*/","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * If this thread is interrupted while waiting for the future to complete,
   * an {@code InterruptedIOException} is raised.
   * However, if the future is cancelled, a {@code CancellationException}
   * is raised in the {code Future.get()} call. This is
   * passed up as is -so allowing the caller to distinguish between
   * thread interruption (such as when speculative task execution is aborted)
   * and future cancellation.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,"org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",129,145,"/**
* Retrieves result from Future with optional timeout.
* @param future Future object to retrieve result from
* @param timeout Timeout duration in specified TimeUnit
* @param unit Time unit for timeout duration
* @return Result object or throws exception if failed
*/","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * @param future future to evaluate
   * @param timeout timeout to wait.
   * @param unit time unit.
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException),123,127,"/**
* Wraps a CompletionException in an IO operation.
* @param e CompletionException to be handled
* @throws IOException if IO operation fails
*/","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * See {@link FutureIO#raiseInnerCause(CompletionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setConf,"org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",74,81,"/**
* Invokes configurable methods on the given object based on configuration settings.
* @param theObject object to invoke methods on
* @param conf configuration settings
*/","* Check and set 'configuration' if necessary.
   * 
   * @param theObject object for which to set configuration
   * @param conf Configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableName.java,getClass,"org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)",91,103,"/**
* Fetches the writable class for a given name from NAME_TO_CLASS or Configuration.
* @param name The name of the writable class
* @param conf The configuration to use for loading the class
* @return The loaded Class object, or null if not found
*/","* Return the class for a name.
   * Default is {@link Class#forName(String)}.
   *
   * @param name input name.
   * @param conf input configuration.
   * @return class for a name.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createCodec,"org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",232,254,"/**
* Creates ErasureCodec instance from specified class name and options.
* @param conf configuration object
* @param codecClassName name of the ErasureCodec implementation class
* @param options ErasureCodecOptions for codec creation
* @return ErasureCodec instance or null if creation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,loadClass,"org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)",412,424,"/**
* Retrieves a class by name from configuration or default lookup.
* @param conf Hadoop Configuration object
* @param className Class name to fetch
* @return Loaded Class object or null if not found
*/","* Find and load the class with given name <tt>className</tt> by first finding
   * it in the specified <tt>conf</tt>. If the specified <tt>conf</tt> is null,
   * try load it directly.
   *
   * @param conf configuration.
   * @param className classname.
   * @return Class.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocolClass,"org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)",331,339,"/**
* Loads and caches a protocol class based on the given name.
* @param protocolName unique protocol identifier
* @param conf configuration object
* @return Class<?> instance or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getClass,org.apache.hadoop.util.FindClass:getClass(java.lang.String),154,156,"/**
* Retrieves a function mask class by name.
* @param name class name to fetch
* @return FunctionMask class or null if not found
*/","* Get a class fromt the configuration
   * @param name the class name
   * @return the class
   * @throws ClassNotFoundException if the class was not found
   * @throws Error on other classloading problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)",232,254,"/**
* Logs function usage with a timestamped message.
* @param log logging object
* @param title function name
* @param minInterval minimum interval between logs in seconds
*/","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last
   * @deprecated to be removed with 3.4.0. Use {@link #logThreadInfo(Logger, String, long)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)",262,283,"/**
* Logs stack trace with specified title at minimum interval.
* @param log Logger instance
* @param title Log message title
* @param minInterval Minimum time interval between logs in seconds
*/","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)",105,113,"/**
* Initializes FSBuilder with a path and/or path handle.
* @param optionalPath file system path (optional)
* @param optionalPathHandle file system path handle (optional)","* Constructor with both optional path and path handle.
   * Either or both argument may be empty, but it is an error for
   * both to be defined.
   * @param optionalPath a path or empty
   * @param optionalPathHandle a path handle/empty
   * @throws IllegalArgumentException if both parameters are set.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(),819,821,"/**
 * Initializes a new instance of the Configuration class with default settings.
 */",A new configuration.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,validateResponse,"org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)",143,191,"/**
* Handles HTTP status mismatch by throwing a custom exception.
* @param conn HttpURLConnection object
* @param expectedStatus expected HTTP status code
*/","* Validates the status of an <code>HttpURLConnection</code> against an
   * expected HTTP status code. If the current status code is not the expected
   * one it throws an exception with a detail message using Server side error
   * messages if available.
   * <p>
   * <b>NOTE:</b> this method will throw the deserialized exception even if not
   * declared in the <code>throws</code> of the method signature.
   *
   * @param conn the <code>HttpURLConnection</code>.
   * @param expectedStatus the expected HTTP status code.
   * @throws IOException thrown if the current status code does not match the
   * expected one.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newCrc32C,org.apache.hadoop.util.DataChecksum:newCrc32C(),102,112,"/**
* Selects and initializes CRC32C implementation based on configuration.
* @return CRC32C instance or fallback to PureJavaCrc32C if initialization fails
*/","* The flag is volatile to avoid synchronization here.
   * Re-entrancy is unlikely except in failure mode (and inexpensive).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,removeAll,org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection),361,370,"/**
* Checks if any element in the collection matches the condition.
* @param collection Collection of objects to check
* @return True if at least one element matches, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[]),266,278,"/**
* Recursively fills an array with elements from a dynamic iterator.
* @param array array to fill
* @return the filled array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String),143,145,"/**
* Calls Shell.m1() with user name and returns result as an array.
* @param userName unique user identifier
*/","* Returns just the shell command to be used to fetch a user's groups list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsIDForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String),164,166,"/**
 * Calls M1 shell command with specified username.
 * @param userName user name to pass to shell command
 */","* Returns just the shell command to be used to fetch a user's group IDs list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)",311,318,"/**
* Generates command with file path based on permissions and recursion.
* @param perm permission level
* @param recursive whether to recurse into directories
* @param file file path to append to the command
*/","* Return a command to set permission for specific file.
   *
   * @param perm String permission to set
   * @param recursive boolean true to apply to all sub-directories recursively
   * @param file String file to set
   * @return String[] containing command and arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getCheckProcessIsAliveCommand,org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String),362,364,"/**
* Returns mask array based on provided process ID.
* @param pid unique process identifier
* @return array of masks or null if invalid PID
*/","* Return a command for determining if process with specified pid is alive.
   * @param pid process ID
   * @return a <code>kill -0</code> command or equivalent",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHome,org.apache.hadoop.util.Shell:getHadoopHome(),610,612,"/**
* Generates function mask by invoking nested methods.
*/","* Get the Hadoop home directory. Raises an exception if not found
   * @return the home dir
   * @throws IOException if the home directory cannot be located.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBin,org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String),642,646,"/**
* Returns the mask file associated with an executable.
* @param executable name of the executable
*/","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File),137,146,"/**
* Extracts file metadata and command identifiers.
* @param file input file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell),1402,1404,"/**
 * Initializes a new ShellTimeoutTimerTask instance with the specified shell.
 * @param shell the shell associated with this timer task
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(),71,77,"/**
* Calculates and returns current progress phase with weightages.
*/","* Adds a node to the tree. Gives equal weightage to all phases.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhases,org.apache.hadoop.util.Progress:addPhases(int),130,137,"/**
* Executes and configures the mask phase based on the given iteration count.
* @param n number of iterations for the mask phase
*/","* Adds n nodes to the tree. Gives equal weightage to all phases.
   *
   * @param n n.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,"org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)",94,99,"/**
* Creates and configures a Progress object with given weightage.
* @param weightage relative importance value
* @return configured Progress object or null if creation failed","* Adds a named node with a specified progress weightage to the tree.
   *
   * @param status status.
   * @param weightage weightage.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,get,org.apache.hadoop.util.Progress:get(),225,231,"/**
* Calculates the function mask value by traversing up the progress tree.
* @return function mask value or 0 if tree root is reached
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getProgress,org.apache.hadoop.util.Progress:getProgress(),239,241,"/**
* Returns a float value based on the result of calling m1().","* Returns progress in this node. get() would give overall progress of the
   * root node(not just given current node).
   *
   * @return progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(),275,280,"/**
* Recursively appends and returns concatenated string.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String),332,334,"/**
* Calls m1 with default configuration (null).
* @param path file system path to process
*/","* Create a ZNode.
   * @param path Path of the ZNode.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,"org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)",372,384,"/**
* Applies ACL permissions to a file system path.
* @param path file system path
* @param zkAcl list of access control lists (ACLs)
*/","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @param zkAcl ACLs for ZooKeeper.
   * @throws Exception If it cannot create the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])",364,378,"/**
* Creates a builder instance for a dynamic constructor with specified class and argument types.
* @param className name of the class
* @param argClasses classes of method arguments (variable arity)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstance,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[]),68,75,"/**
* Wraps function call to m2 with error handling.
* @throws RuntimeException if an exception occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])",84,89,"/**
* Dynamically calls a method on the target object with variable arguments.
* @param target object to invoke method on
* @param args variable number of arguments for the method call
* @return result of the method invocation, cast to type R
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[]),215,217,"/**
* Invokes method 'm1' with input arguments.
* @param args variable number of input arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])",91,98,"/**
* Calls generic method m2 with provided arguments and target.
* @param target the target object
* @param args variable number of additional arguments
* @return result of method call
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[]),198,200,"/**
 * Invokes method m1 on receiver with variable arguments.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])",284,298,"/**
* Loads and invokes a method on a target class.
* @param className name of the class to invoke
* @param methodName name of the method to invoke
* @param argClasses classes of method arguments (varargs)
*/","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])",343,346,"/**
* Configures builder with target class and variable classes.
* @param targetClass target class to build
* @param varClasses variable classes to include
*/","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])",387,401,"/**
* Tries to resolve a method by loading the target class and recursively invoking itself.
* @param className name of the class containing the desired method
* @param methodName name of the method to invoke
* @param argClasses classes of the method's arguments (varargs)
*/","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",448,451,"/**
* Configures builder with target class and optional argument classes.
* @param targetClass class to build
* @param argClasses variable number of argument classes
*/","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadInvocation,"org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",101,121,"/**
* Creates an unbound method instance for the specified class and name.
* @param source Class containing the method (optional)
* @return UnboundMethod instance or null if not found
*/","* Get an invocation from the source class, which will be unavailable() if
   * the class is null or the method isn't found.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or ""unavailable""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,requireAllMethodsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable(),225,242,"/**
* Verifies availability of specific unbound methods.
* @throws UnsupportedOperationException for any unavailable method
*/","* For testing: verify that all methods were found.
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available(),249,251,"/**
* Checks bulk delete deletion method flag.
* @return true if enabled, false otherwise
*/","* Are the bulk delete methods available?
   * @return true if the methods were found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available(),306,308,"/**
* Checks file system open file method flag.
* @return true/false indicating status of flag
*/","* Is the {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)}
   * method available.
   * @return true if the optimized open file method can be invoked.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available(),380,382,"/**
* Checks if mask is available.
* @param byteBufferPositionedReadableReadFullyAvailableMethod 
*      result of availability check for readable and fully readable
* @return true if mask is available, false otherwise
*/","* Are the ByteBufferPositionedReadable methods loaded?
   * This does not check that a specific stream implements the API;
   * use {@link #byteBufferPositionedReadable_readFullyAvailable(InputStream)}.
   * @return true if the hadoop libraries have the method.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),392,400,"/**
* Checks if input stream is fully available and readable.
* @param in InputStream to check
* @return true if stream is fully available and readable, false otherwise
*/","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the API is available, the stream implements the interface
   * (including the innermost wrapped stream) and that it declares the stream capability.
   * @throws IOException if the operation was attempted and failed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable(),360,362,"/**
* Returns a boolean indicating whether to mask function calls.
* @return true if masking is enabled, false otherwise
*/","* Are the core IOStatistics methods and classes available.
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable(),368,370,"/**
* Returns true if iostatistics context is enabled.
* @return true if enabled, false otherwise
*/","* Are the IOStatisticsContext methods and classes available?
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,checkAvailable,org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),183,188,"/**
* Validates and registers an unbound method.
* @param method UnboundMethod object to validate
*/","* Require a method to be available.
   * @param method method to probe
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object),490,492,"/**
* Invokes method m2 on an instance of class M1.
* @param receiver instance of class M1
*/","* Returns the first valid implementation as a BoundMethod or throws a
     * NoSuchMethodException if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,build,org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object),503,505,"/**
* Invokes m2() on the result of m1(), passing the given receiver.","* Returns the first valid implementation as a BoundMethod or throws a
     * RuntimeError if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStaticChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked(),514,516,"/**
 * Returns a mask value by executing m1() and then calling m2() on its result. 
 * @throws NoSuchMethodException if either m1() or m2() is not found
 */","* Returns the first valid implementation as a StaticMethod or throws a
     * NoSuchMethodException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStatic,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic(),525,527,"/**
* Returns a mask value by calling nested methods.
*/","* Returns the first valid implementation as a StaticMethod or throws a
     * RuntimeException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,loadFileSystems,org.apache.hadoop.fs.FileSystem:loadFileSystems(),3516,3545,"/**
* Loads and registers available file systems.
* @throws ServiceConfigurationError if any file system fails to load
*/","* Load the filesystem declarations from service resources.
   * This is a synchronized operation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,main,org.apache.hadoop.util.VersionInfo:main(java.lang.String[]),182,193,"/**
* Prints system information and build details.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])",171,176,"/**
* Creates an RPC request header with default metadata.
* @param rpcKind type of RPC request
* @param operation operation to be performed
* @param callId unique ID for the call
* @param retryCount number of retries
* @param uuid unique identifier (defaults to null)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseVersion,org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String),360,453,"/**
* Parses the given version string into a hierarchical structure of Version Items.
* @param version input version string
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)",120,145,"/**
* Initializes LightWeightCache with specified parameters.
* @param recommendedLength recommended cache length
* @param sizeLimit maximum allowed queue size
* @param creationExpirationPeriod time (ms) items remain in cache after creation
* @param accessExpirationPeriod time (ms) items remain in cache after last access
* @param timer used for expiration logic
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,get,org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object),98,101,"/**
* Calls superclass's m1() method to perform operation.
* @param key unique identifier (type K)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object),144,147,"/**
* Checks if a key exists in the mapping.
* @param key unique identifier to search for
* @return true if key is present, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,toString,org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString(),158,166,"/**
* Formats the executor service as a string.
* @return formatted string representation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",130,144,"/**
* Processes input data based on file type.
* @param type data type
* @param filename input file name
* @param inputStream input stream
* @param map data mapping object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,main,org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[]),252,266,"/**
* Computes Jenkins hash for a file.
* @param args array containing single filename
*/","* Compute the hash of the specified file
   * @param args name of file to compute hash of.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,<init>,"org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)",83,97,"/**
* Initializes a hash function with specified max value, number of hashes, and type.
* @param maxValue maximum input value
* @param nbHash number of hash iterations
* @param hashType identifier for the hash algorithm to use
*/","* Constructor.
   * <p>
   * Builds a hash function that must obey to a given maximum number of returned values and a highest value.
   * @param maxValue The maximum highest returned value.
   * @param nbHash The number of resulting hashed values.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(),102,102,"/**
* Initializes a new instance of the RetouchedBloomFilter class.
*/",Default constructor - use with readFields,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,write,org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput),248,257,"/**
* Serializes the object's state to the output stream.
* @param out DataOutput stream to write to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,write,org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput),407,427,"/**
* Writes object data to output stream, including FP vectors and key lists.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,add,org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key),104,127,"/**
* Updates the FUNC_MASK in the hash table for a given Key.
* @param key the Key object to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),178,200,"/**
* Verifies a key is present in the hash table.
* @param key unique key to search for
* @return true if key exists, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,approximateCount,org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key),220,238,"/**
* Finds the minimum non-masked value in a hash bucket.
* @param key input to be hashed
*/","* This method calculates an approximate count of the key, i.e. how many
   * times the key was added to the filter. This allows the filter to be
   * used as an approximate <code>key -&gt; count</code> map.
   * <p>NOTE: due to the bucket size of this filter, inserting the same
   * key more than 15 times will cause an overflow at all filter positions
   * associated with this key, and it will significantly increase the error
   * rate for this and other keys. For this reason the filter can only be
   * used to store small count values <code>0 &lt;= N &lt;&lt; 15</code>.
   * @param key key to be tested
   * @return 0 if the key is not present. Otherwise, a positive value v will
   * be returned such that <code>v == count</code> with probability equal to the
   * error rate of this filter, and <code>v &gt; count</code> otherwise.
   * Additionally, if the filter experienced an underflow as a result of
   * {@link #delete(Key)} operation, the return value may be lower than the
   * <code>count</code> with the probability of the false negative rate of such
   * filter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,add,org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key),116,128,"/**
* Updates the functional mask based on the provided key.
* @param key unique identifier to compute the mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),142,156,"/**
* Checks if a key matches the FUNC_MASK.
* @param key Key object to check
* @return True if key matches, False otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,add,org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key),118,131,"/**
* Processes a given Key object by hashing and updating its vector.
* @param key the input Key to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key),139,150,"/**
* Applies a mask operation to the key using hash functions.
* @param key Key object to process
*/","* Adds a false positive information to <i>this</i> retouched Bloom filter.
   * <p>
   * <b>Invariant</b>: if the false positive is <code>null</code>, nothing happens.
   * @param key The false positive key to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,removeKey,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])",351,365,"/**
* Updates the given Key's hash values in the provided hash vector.
* @param k Key to be updated
* @param vector Hash array of Keys
*/","* Removes a given key from <i>this</i> filer.
   * @param k The key to remove.
   * @param vector The counting vector associated to the key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,equals,org.apache.hadoop.util.bloom.Key:equals(java.lang.Object),137,143,"/**
* Checks if input object has key mask set to zero.
* @param o the object to check (must be of type Key)
* @return true if key mask is zero, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,minimumFnRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[]),250,265,"/**
* Finds the index of the hash with the minimum key weight.
* @param h array of hash indices
*/","* Chooses the bit position that minimizes the number of false negative generated.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,maximumFpRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[]),272,286,"/**
* Finds the index of the hash that maximizes the FP-weight.
* @param h array of hash indices
* @return index with maximum FP-weight or -Integer.MIN_VALUE if empty","* Chooses the bit position that maximizes the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that maximizes the number of false positive removed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,computeRatio,org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio(),370,379,"/**
* Calculates the weight ratios for each vector element.
* @param vectorSize size of the input vectors
*/",* Computes the ratio A/FP.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,dumpResource,org.apache.hadoop.util.FindClass:dumpResource(java.lang.String),187,209,"/**
* Retrieves and processes a resource by its name.
* @param name unique identifier for the resource
* @return SUCCESS or error code if processing fails
*/","* Dump a resource to out
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,usage,org.apache.hadoop.util.FindClass:usage(java.lang.String[]),342,361,"/**
* Returns error code for usage mismatch.
*/","* Print a usage message
   * @param args the command line arguments
   * @return an exit code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,run,org.apache.hadoop.util.GcTimeMonitor:run(),153,172,"/**
* Continuously monitors and logs GC data, triggering alerts when threshold exceeded.
* @throws InterruptedException if thread is interrupted while sleeping
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,put,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3510,3520,"/**
* Validates and initializes compression settings from the input segment.
* @param stream SegmentDescriptor to fetch compression settings from
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,insert,org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object),73,85,"/**
* Inserts or updates an element in the heap data structure.
* @param element element to insert/replace
*/","* Adds element to the PriorityQueue in log(size) time if either
   * the PriorityQueue is not full, or not lessThan(element, top()).
   * @param element element.
   * @return true if element is added, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newTreeSet,org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable),154,159,"/**
* Creates a sorted TreeSet from an iterable collection of comparable elements.
* @param elements input iterable containing elements to be added to the set
*/","* Creates a <i>mutable</i> {@code TreeSet} instance containing the given
   * elements sorted by their natural ordering.
   *
   * <p><b>Note:</b> if mutability is not required, use
   * ImmutableSortedSet#copyOf(Iterable) instead.
   *
   * <p><b>Note:</b> If {@code elements} is a {@code SortedSet} with an
   * explicit comparator, this method has different behavior than
   * {@link TreeSet#TreeSet(SortedSet)}, which returns a {@code TreeSet}
   * with that comparator.
   *
   * <p><b>Note for Java 7 and later:</b> this method is now unnecessary and
   * should be treated as deprecated. Instead, use the {@code TreeSet}
   * constructor directly, taking advantage of the new
   * <a href=""http://goo.gl/iz2Wi"">""diamond"" syntax</a>.
   *
   * <p>This method is just a small convenience for creating an empty set and
   * then calling Iterables#addAll. This method is not very useful and will
   * likely be deprecated in the future.
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain
   * @return a new {@code TreeSet} containing those elements (minus duplicates)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable),123,127,"/**
* Converts an iterable to a set, leveraging recursive structure if necessary.
* @param elements input iterable
* @return Set of unique elements or null if not found
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set then calling
   * {@link Collection#addAll} or Iterables#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterable) instead. (Or, change
   * {@code elements} to be a FluentIterable and call {@code elements.toSet()}.)</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, use
   * newEnumSet(Iterable, Class) instead.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[]),100,105,"/**
* Creates a hash set from an array of elements.
* @param elements variable number of elements to include
* @return HashSet containing all input elements
*/","* Creates a <i>mutable</i> {@code HashSet} instance initially containing
   * the given elements.
   *
   * <p><b>Note:</b> if elements are non-null and won't be added or removed
   * after this point, use ImmutableSet#of() or ImmutableSet#copyOf(Object[])
   * instead. If {@code E} is an {@link Enum} type, use
   * {@link EnumSet#of(Enum, Enum[])} instead. Otherwise, strongly consider
   * using a {@code LinkedHashSet} instead, at the cost of increased memory
   * footprint, to get deterministic iteration behavior.</p>
   *
   * <p>This method is just a small convenience, either for
   * {@code newHashSet(}{@link Arrays#asList}{@code (...))}, or for creating an
   * empty set then calling {@link Collections#addAll}.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,run,org.apache.hadoop.util.ProgramDriver:run(java.lang.String[]),120,146,"/**
* Executes a program by ID, handling invalid inputs and unknown programs.
* @param args array of command-line arguments
* @return 0 on success, -1 otherwise
*/","* This is a driver for the example programs.
   * It looks at the first command line argument and tries to find an
   * example program with that name.
   * If it is found, it calls the main method in that class with the rest 
   * of the command line arguments.
   * @param args The argument from the user. args[0] is the command to run.
   * @return -1 on error, 0 on success
   * @throws NoSuchMethodException  when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.
   * @throws IllegalAccessException for backward compatibility.
   * @throws IllegalArgumentException if the arg is invalid.
   * @throws Throwable Anything thrown by the example program's main",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",151,155,"/**
* Adds a column to the builder with specified title and justification.
* @param title column title
* @param justification text alignment (e.g. left, center, right)
* @param wrap whether to wrap long texts in the column
*/","* Add a new field to the Table under construction.
     *
     * @param title Field title.
     * @param justification Right or left justification. Defaults to left.
     * @param wrap Width at which to auto-wrap the content of the cell.
     *        Defaults to Integer.MAX_VALUE.
     * @return This Builder object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,logDeprecationOnce,"org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)",1465,1470,"/**
* Logs deprecation of a function based on its name and source.
* @param name the function name
* @param source the source of the function
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1936,1938,"/**
* Converts time value to milliseconds based on specified unit.
* @param name unused parameter
* @param vStr time value string
* @param unit time unit to convert to (e.g. TimeUnit.SECONDS)
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param unit Unit to convert the stored property, if it exists.
   * @return time duration in given time unit.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStreamReader,"org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)",3149,3177,"/**
* Parses a resource into an XML stream.
* @param wrapper Resource wrapper object
* @param quiet whether to suppress logging if parsing fails
* @return XMLStreamReader2 instance or null on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartElement,org.apache.hadoop.conf.Configuration$Parser:handleStartElement(),3240,3265,"/**
* Handles XML stream tokens and performs corresponding actions.
* @throws XMLStreamException if parsing error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendXMLProperty,"org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3695,3733,"/**
* Updates a property in the document with given configuration and redactor.
* @param doc Document to update
* @param conf Configuration element
* @param propertyName Property name
* @param redactor Optional ConfigRedactor for value transformation
*/","*  Append a property with its attributes to a given {#link Document}
   *  if the property is found in configuration.
   *
   * @param doc
   * @param conf
   * @param propertyName",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecations,org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]),566,572,"/**
* Iterates to find a valid DeprecationContext.
* @param deltas array of DeprecationDelta objects
*/","* Adds a set of deprecated keys to the global deprecations.
   *
   * This method is lockless.  It works by means of creating a new
   * DeprecationContext based on the old one, and then atomically swapping in
   * the new context.  If someone else updated the context in between us reading
   * the old context and swapping in the new one, we try again until we win the
   * race.
   *
   * @param deltas   The deprecations to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndElement,org.apache.hadoop.conf.Configuration$Parser:handleEndElement(),3374,3413,"/**
* Parses configuration parameters based on token type.
* @throws IOException if include parameter is encountered without a fallback
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForPortRange,"org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)",1502,1531,"/**
* Iterates through port ranges to establish connections and perform m1 operation.
* @param listener ServerConnector instance
* @param startPort starting port for iteration
* @throws Exception if any connection attempt fails
*/","* Bind using port ranges. Keep on looking for a free port in the port range
   * and throw a bind exception if no port in the configured range binds.
   * @param listener jetty listener.
   * @param startPort initial port which is set in the listener.
   * @throws Exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fatalError,org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String),768,772,"/**
* Logs and notifies client of an error.
* @param errorMessage detailed error message
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToActive,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),89,95,"/**
* Sends transition to active request.
* @param reqInfo StateChangeRequestInfo object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToStandby,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),97,103,"/**
* Transitions the system to standby mode.
* @param reqInfo StateChangeRequestInfo object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToObserver,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),105,112,"/**
* Sends a Transition to Observer request to the controller.
* @param reqInfo StateChangeRequestInfo object containing request details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToActive,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)",107,117,"/**
* Executes M3 business logic.
* @param controller RPC controller for error handling
* @return TransitionToActiveResponseProto object or throws ServiceException on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToStandby,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)",119,129,"/**
* Executes M3 server-side operation and returns response.
* @param controller RPC controller
* @param request TransitionToStandbyRequestProto object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToObserver,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)",131,141,"/**
* Executes M3 RPC service operation.
* @param controller RPC controller object
* @param request TransitionToObserverRequestProto object containing input data
* @return TransitionToObserverResponseProto object or throws ServiceException on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,<init>,org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String),237,256,"/**
* Parses optional user and SSH port from input string.
* @param arg input string containing user:port or just user
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)",151,153,"/**
* Prints usage message to output stream.
* @param pStr PrintStream object
* @param cmd command string
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)",491,503,"/**
* Parses command line options and executes a command.
* @param cmdName name of the command to execute
* @param opts Options object for parsing
* @param argv array of command line arguments
* @param helpEntries map of available commands with usage info
* @return true if execution was successful, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,doFence,"org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)",130,171,"/**
* Attempts to kill a process running on the specified port and verify its status.
* @param session SSH connection
* @param serviceAddr InetSocketAddress containing the host and port
* @return true if the process was successfully killed or is not running, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,addTargetInfoAsEnvVars,"org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)",220,243,"/**
* Configures environment variables with HA service-specific settings.
* @param target HAServiceTarget instance
* @param environment Map of environment variables to update
*/","* Add information about the target to the the environment of the
   * subprocess.
   * 
   * @param target
   * @param environment",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setAclsWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String),1124,1138,"/**
* Updates ACL on the specified ZK path.
* @param path absolute ZooKeeper path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction),1140,1143,"/**
* Executes the given ZooKeeper action with default parameters.
* @param action ZooKeeper action to execute
* @return result of the action or throws an exception if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readNonByteBufferPositionedReadable,"org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)",151,170,"/**
* Reads data from a PositionedReadable stream into a ByteBuffer.
* @param stream PositionedReadable stream to read from
* @param range FileRange specifying the data to read
* @param buffer ByteBuffer to hold the read data
*/","* Read into a direct tor indirect buffer using {@code PositionedReadable.readFully()}.
   * @param stream stream
   * @param range file range
   * @param buffer destination buffer
   * @throws IOException IO problems.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateVectoredReadRanges,org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List),82,85,"/**
* Applies mask function to each file range in the list.
* @param ranges collection of file ranges
*/","* Validate a list of vectored read ranges.
   * @param ranges list of ranges.
   * @throws EOFException any EOF exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setCaching,org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future),195,201,"/**
* Updates internal state to caching mode and associates given future with current object.
* @param actionFuture Future object containing pending task
*/","* Indicates that a caching operation is in progress.
   *
   * @param actionFuture the {@code Future} of a caching action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,updateState,"org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",243,250,"/**
* Updates the current state with a new one while validating and applying mask.
* @param newState new state to apply
* @param expectedCurrentState one or more expected current states
*/","* Updates the current state to the specified value.
   * Asserts that the current state is as expected.
   * @param newState the state to transition to.
   * @param expectedCurrentState the collection of states from which
   *        transition to {@code newState} is allowed.
   *
   * @throws IllegalArgumentException if newState is null.
   * @throws IllegalArgumentException if expectedCurrentState is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsDir,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)",357,364,"/**
* Validates and sets mask for the given file path.
* @param path File system path
* @param argName Name of argument being validated (for error messages)
*/","* Validates that the given path exists and is a directory.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsFile,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)",371,375,"/**
* Validates and processes a file path with the given argument name.
* @param path file system path
* @param argName name of the argument being validated
*/","* Validates that the given path exists and is a file.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int),127,135,"/**
* Checks if the given block number is the last block in a file.
* @param blockNumber index of the block to check
* @return true if blockNumber is the last block, false otherwise
*/","* Indicates whether the given block is the last block in the associated file.
   * @param blockNumber the id of the desired block.
   * @return true if the given block is the last block in the associated file, false otherwise.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStartOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int),181,185,"/**
* Calculates function mask value based on block number and size.
* @param blockNumber unique block identifier
* @return calculated long mask value
*/","* Gets the start offset of the given block.
   * @param blockNumber the id of the given block.
   * @return the start offset of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getState,org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int),206,210,"/**
* Retrieves the current state at a specified block number.
* @param blockNumber specific block identifier
*/","* Gets the state of the given block.
   * @param blockNumber the id of the given block.
   * @return the state of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,setState,"org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)",218,222,"/**
* Updates the state of a block with the given block number and state.
* @param blockNumber unique block identifier
* @param blockState current state of the block
*/","* Sets the state of the given block to the given value.
   * @param blockNumber the id of the given block.
   * @param blockState the target state.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long),143,147,"/**
* Calculates function mask by dividing block size.
* @param offset memory address offset
* @return function mask value as integer
*/","* Gets the id of the block that contains the given absolute offset.
   * @param offset the absolute offset to check.
   * @return the id of the block that contains the given absolute offset.
   * @throws IllegalArgumentException if offset is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",94,107,"/**
* Evaluates snapshot with optional statistics for masking.
* @param snapshot serialized data to evaluate
* @param statistics optional IOStatistics instance
* @return true if snapshot can be masked, false otherwise
*/","* Aggregate an existing {@link IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",162,171,"/**
* Applies a mask to the given filesystem at the specified path.
* @param snapshot snapshot data
* @param fs file system object
* @param path file system location
* @param overwrite whether to override existing files
*/","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable),203,206,"/**
* Creates a map of function masks by counting counters in the given snapshot.
* @param source input snapshot
*/","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable),213,216,"/**
* Computes gauges mask from source object.
* @param source input object to process
* @return map of gauge names to their corresponding IDs
*/","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable),223,226,"/**
* Computes mask values from given source object.
* @param source input data to process
*/","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable),233,236,"/**
* Extracts maximum values from snapshot and returns them as a map.
* @param source Serializable object containing statistics data
*/","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable),245,253,"/**
* Extracts and maps statistics from the given source.
* @param source input data
*/","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample count is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,copy,org.apache.hadoop.fs.impl.FlagSet:copy(),253,255,"/**
 * Creates and returns a new FlagSet instance with default values.
 */","* Create a copy of the FlagSet.
   * @return a new mutable instance with a separate copy of the flags",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,createFlagSet,"org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)",276,281,"/**
* Creates a FlagSet instance from an EnumSet.
* @param enumClass the type of enums
* @param prefix     prefix for flag names
* @param flags      set of enums to include in FlagSet
*/","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags
   * @param <E> enum type
   * @return a mutable FlagSet",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,close,org.apache.hadoop.fs.HarFileSystem:close(),733,744,"/**
* Calls superclass method and optionally executes child file system method.
* @throws IOException if error occurs in child file system operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,close,org.apache.hadoop.fs.RawLocalFileSystem:close(),895,898,"/**
* Calls parent class's implementation of this method.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeAll,org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll(),155,163,"/**
* Closes all FileSystem instances in the map.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,close,org.apache.hadoop.fs.FsShell:close(),371,376,"/**
* Clears and invokes file system operation.
* @throws IOException if an I/O error occurs
*/","*  Performs any necessary cleanup
   * @throws IOException upon error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,close,org.apache.hadoop.fs.FilterFileSystem:close(),527,531,"/**
 * Calls superclass and file system methods M1.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean),3799,3831,"/**
* Performs file system operations with optional auto-closing.
* @param onlyAutomatic true to skip non-automatic file systems
* @throws IOException if any operation fails
*/","* Close all FileSystem instances in the Cache.
     * @param onlyAutomatic only close those that are marked for automatic closing
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation),3844,3868,"/**
* Filters and executes operations on target file systems for the given user.
* @param ugi UserGroupInformation object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,compareTo,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),117,120,"/**
* Calls superclass's implementation of m1.
* @param o FileStatus object to process
* @return result from superclass's m1() method
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)",118,133,"/**
* Reads data from file at specified position.
* @param position starting offset in the file
* @param buffer destination byte array
* @param offset initial offset within the buffer
* @param length total bytes to read
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,read,"org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)",115,118,"/**
* Delegates FSInputStream#m1 call to underlying input stream.
* @param position file position
* @param buffer data buffer
* @param offset buffer offset
* @param length number of bytes to read
* @return result from underlying FSInputStream#m1
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),180,184,"/**
* Converts an M2 snapshot to JSON format.
* @param snapshot M2 snapshot object
*/","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])",1878,1890,"/**
* Writes data to a Hadoop FileContext.
* @param fileContext Hadoop context
* @param path file location
* @param bytes data to be written
* @return the input fileContext
*/","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1948,1966,"/**
* Writes Iterable of CharSequence to a FileContext.
* @param fileContext FileContext to write to
* @param path Path to write at
* @param lines Iterable of lines to write
* @param cs Charset for encoding
* @return the original FileContext
*/","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",2014,2028,"/**
* Writes CharSequence to FileContext using specified Charset.
* @param fs FileContext instance
* @param path Path to write to
* @param charseq CharSequence to write
* @param cs Charset to use for encoding
* @return the modified FileContext instance
*/","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file context with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFile,org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path),4732,4735,"/**
* Creates an FSDataOutputStreamBuilder with specified file mask.
* @param path Path to create output stream for
*/","* Create a new FSDataOutputStreamBuilder for the file with path.
   * Files are overwritten by default.
   *
   * @param path file path
   * @return a FSDataOutputStreamBuilder object to build the file
   *
   * HADOOP-14384. Temporarily reduce the visibility of method before the
   * builder interface becomes stable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createFile,org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path),1109,1112,"/**
* Creates an output stream builder with specified file system mask.
* @param path file system path
*/","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,appendFile,org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path),4742,4744,"/**
* Creates an FSDataOutputStreamBuilder instance with a mask.
* @param path file system path
*/","* Create a Builder to append a file.
   * @param path file path.
   * @return a {@link FSDataOutputStreamBuilder} to build file append request.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,appendFile,org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path),1120,1122,"/**
 * Creates an FSDataOutputStreamBuilder instance with the specified file system mask.
 * @param path Path object representing the target location
 */","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",155,159,"/**
* Constructs BlockLocation object with provided details.
* @param names array of block names
* @param hosts array of hostnames for blocks
* @param cachedHosts array of cached hostnames
* @param topologyPaths array of topology paths
* @param offset start position in bytes
* @param length total size in bytes
* @param corrupt whether the data is corrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,toString,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString(),559,562,"/**
* Calls m1() on realStatus object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults(),60,71,"/**
* Creates and returns FsServerDefaults object with default values.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults(),60,71,"/**
* Creates FsServerDefaults object with default values.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(),75,80,"/**
* Extracts and returns a single byte value from the input buffer.
* @return The extracted byte value or -1 on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,next,org.apache.hadoop.fs.FileSystem$DirListingIterator:next(),2332,2342,"/**
* Retrieves next item from iterator and returns as type T.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),102,118,"/**
* Processes a file's mask (extended attributes) or value based on the dump flag.
* @param item PathData object containing file information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listStatus,org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path),959,962,"/**
* Retrieves file statuses by path with default filter.
* @param f Path to query
* @return array of FileStatus objects or null if not found
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given path
   * @throws IOException if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[]),2140,2143,"/**
* Retrieves status of multiple files.
* @param files array of file paths
*/","* Filter files/directories in the given list of paths using default
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @return a list of statuses for the files under the given paths after
   *         applying the filter default Path filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",103,108,"/**
* Returns a MutableCounterInt instance with specified value and MetricsInfo.
* @param info MetricsInfo object
* @param iVal integer value to be masked in the counter
*/","* Create a mutable integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",127,133,"/**
* Initializes and registers a mutable counter with the given metric info and value.
* @param info MetricsInfo object
* @param iVal initial value for the counter
* @return MutableCounterLong instance representing the initialized counter
*/","* Create a mutable long integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",176,181,"/**
* Retrieves and stores a mutable gauge with the given value for the specified metric.
* @param info MetricsInfo object containing metric details
* @param iVal initial value of the gauge
*/","* Create a mutable long integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",200,205,"/**
* Calculates a gauge value by applying the FUNC_MASK function to the given metrics info and input value.
* @param info MetricsInfo object containing relevant data
* @param iVal Input float value to process
* @return MutableGaugeFloat result or null if failed
*/","* Create a mutable float gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",152,157,"/**
* Generates a mutable gauge integer for the given metrics info and value.
* @param info MetricsInfo object
* @param iVal int value to be wrapped in MutableGaugeInt
* @return MutableGaugeInt instance or null if creation fails
*/","* Create a mutable integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",102,109,"/**
* Applies a filter mask to the given MetricsInfo and updates the metrics.
* @param info MetricsInfo object
* @param value long value to be processed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",120,127,"/**
* Updates the builder with a gauge metric for the specified value.
* @param info MetricsInfo object describing the metric
* @param value long value to be recorded
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",93,100,"/**
* Applies a mask to the given MetricsInfo with the specified integer value.
* @param info MetricsInfo object
* @param value integer value to apply as mask
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",129,136,"/**
* Applies a gauge metric with the given value to the metrics builder.
* @param info MetricsInfo instance
* @param value float value of the metric
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",138,145,"/**
* Adds a gauge metric to the builder with specified info and value.
* @param info MetricsInfo object
* @param value double value of the metric
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",111,118,"/**
* Sets a metric mask value for the specified MetricsInfo.
* @param info Metrics information
* @param value integer value to set
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFS,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS(),54,57,"/**
* Calls superclass's implementation of m1() to retrieve a File System. 
* @return File system object as returned by superclass's m1()
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,resolve,"org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",91,96,"/**
* Resolves a filesystem link to the specified type.
* @param fileContext context for resolving links
* @param path location of the link
* @param fn function to resolve the link to the desired type
* @return object of type T or throws exceptions if resolution fails
*/","* Apply the given function to the resolved path under the the supplied
   * FileContext.
   * @param fileContext file context to resolve under
   * @param path path to resolve
   * @param fn function to invoke
   * @param <T> return type.
   * @return the return value of the function as revoked against the resolved
   * path.
   * @throws UnresolvedLinkException link resolution failure
   * @throws IOException other IO failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext),413,415,"/**
 * Creates a GlobBuilder instance from the given FileContext.
 * @param fileContext FileContext object containing relevant data
 */","* Create a builder for a Globber, bonded to the specific file
   * context.
   * @param fileContext file context.
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem),403,405,"/**
 * Creates a new GlobBuilder instance with the specified file system.
 * @param filesystem the file system to associate with the GlobBuilder
 */","* Create a builder for a Globber, bonded to the specific filesystem.
   * @param filesystem filesystem
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone(),246,266,"/**
* Handles asynchronous function call result.
* @return true if handled successfully, false otherwise
*/","@return true if the call is done; otherwise, return false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn(),56,66,"/**
* Returns an AsyncGet instance from a cache or generates a new one.
* @return cached or newly created AsyncGet object
*/","* @return the async return value from {@link AsyncCallHandler}.
   * @param <T> T.
   * @param <R> R.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)",441,443,"/**
* Convenience constructor to create deprecation delta with single replacement key. 
* @param key original deprecated key
* @param newKey replacement key
* @param customMessage custom message for deprecation notice
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)",445,447,"/**
* Creates a deprecation delta with a single replacement mapping.
* @param key original key
* @param newKey new key to replace the original one with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedString,"org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)",94,96,"/**
* Generates mask value from input string.
* @param out DataOutput object
* @param s input string (may be null)
* @return int mask value or 0 if input is invalid
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,concat,"org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",612,622,"/**
* Copies multiple source files to a single target file.
* @param trg the target file path
* @param psrcs array of source file paths
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,invoke,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke(),279,316,"/**
* Invokes asynchronous call with lower layer and potentially initiates a new async call.
* @throws Throwable if an error occurs during invocation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object),122,125,"/**
* Invokes superclass's m1() method.
* @param o arbitrary object to pass to superclass
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode(),127,130,"/**
* Calls superclass implementation of m1(). 
* @return result from superclass's m1() method
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,main,org.apache.hadoop.fs.DUHelper:main(java.lang.String[]),85,90,"/**
* Prints operating system-specific message based on input argument.
* @param args array containing single input value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,refresh,org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh(),45,48,"/**
* Updates mask value based on helper function result.
* @param m1() result of unknown helper function
*/",* Override to hook in DUHelper class.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,<init>,org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic),107,111,"/**
 * Copies all data from another MeanStatistic instance.
 * @param that MeanStatistic to copy from
 */","* Create from another statistic.
   * @param that source",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",245,251,"/**
* Updates existing MeanStatistic object in map.
* @param key unique identifier for MeanStatistic
* @param value new MeanStatistic value to update
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsSourceToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object),63,70,"/**
* Generates a mask string from the given source object.
* @param source nullable object to process
* @return generated mask string or empty if processing fails
*/","* Extract the statistics from a source object -or """"
   * if it is not an instance of {@link IOStatistics},
   * {@link IOStatisticsSource} or the retrieved
   * statistics are null.
   * <p>
   * Exceptions are caught and downgraded to debug logging.
   * @param source source of statistics.
   * @return a string for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,toString,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString(),103,106,"/**
* Generates function mask using wrapped value.
* @return function mask string
*/","* Return the statistics dump of the wrapped statistics.
   * @return the statistics for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString(),325,330,"/**
* Returns function mask based on available statistics.
* @return Function mask string or NULL_SOURCE if no statistics are available.","* Evaluate and stringify the statistics.
     * @return a string value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString(),253,256,"/**
* Returns the function mask value.
* @return Function mask string from m1 method invocation.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToPrettyString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics),102,121,"/**
* Formats IOStatistics into a human-readable mask.
* @param statistics IOStatistics object to process (null for empty result)
* @return String representation of the statistics or an empty string if null
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * This is more expensive than the simple conversion, so should only
   * be used for logging/output where it's known/highly likely that the
   * caller wants to see the values. Not for debug logging.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,createTracker,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)",672,678,"/**
* Creates a duration tracker instance based on provided factory and statistic name.
* @param factory optional factory for custom tracking implementation
* @param statistic name of the tracked statistic
* @return DurationTracker object or stub implementation if no factory is provided
*/","* Create the tracker. If the factory is null, a stub
   * tracker is returned.
   * @param factory tracker factory
   * @param statistic statistic to track
   * @return a duration tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,deleteBlockFileAndEvictCache,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),446,471,"/**
* Purges single entry from the cache.
* @param elementToPurge Entry object containing path and block number to purge
*/","* Delete cache file as part of the block cache LRU eviction.
   *
   * @param elementToPurge Block entry to evict.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable),128,138,"/**
* Executes a task with permit release and retries interrupted tasks.
* @param task callable task to execute
* @return Future result of the task, or error future if interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)",140,150,"/**
* Executes a task with permit release and retries on interrupt.
* @param task the task to execute
* @param result the desired result type
* @return Future containing the result or exception
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable),152,162,"/**
* Executes task on executor with permit release.
* @param task the task to execute
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,execute,org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable),164,173,"/**
* Executes a command with acquired queueing permits.
* @param command Runnable to be executed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,iterator,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator(),56,59,"/**
* Returns an iterator over LongStatistics.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)",447,450,"/**
* Recursively calls m2 with transformed duration.
* @param prefix unused prefix parameter
* @param duration input time duration to transform
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,fromStorageStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics),72,83,"/**
* Calculates IO statistics with a custom mask.
* @param storageStatistics input storage statistics
* @return DynamicIOStatistics object or null if invalid
*/","* Create  IOStatistics from a storage statistics instance.
   *
   * This will be updated as the storage statistics change.
   * @param storageStatistics source data.
   * @return an IO statistics source.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)",87,91,"/**
* Sets dynamic I/O statistics with key and value from atomic long source.
* @param key unique identifier
* @param source atomic long with initial value 
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",100,104,"/**
* Sets IO statistics mask using specified key and updates source counter.
* @param key unique identifier
* @param source source counter to update
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMutableCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)",113,117,"/**
* Creates DynamicIOStatisticsBuilder with counter data from source.
* @param key unique identifier
* @param source MutableCounterLong instance for statistics
*/","* Build a dynamic counter statistic from a
   * {@link MutableCounterLong}.
   * @param key key of this statistic
   * @param source mutable long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)",138,142,"/**
* Updates statistics with values from a source.
* @param key identifier for the statistics
* @param source AtomicLong containing initial values
*/","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",151,155,"/**
* Builds DynamicIOStatistics with specified key and initialized statistics from source.
* @param key unique identifier
* @param source initial statistics
* @return this builder instance for chaining
*/","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",176,180,"/**
* Builds dynamic I/O statistics with given key and source value.
* @param key unique identifier
* @param source atomic long value
* @return DynamicIOStatisticsBuilder instance for chaining
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",189,193,"/**
* Updates DynamicIOStatisticsBuilder with statistics from the given source.
* @param key statistic key
* @param source source of statistics (AtomicInteger)
* @return this builder instance for chaining
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",215,219,"/**
* Adds dynamic I/O statistics builder with custom mask.
* @param key unique identifier
* @param source atomic counter
*/","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",228,232,"/**
* Builds dynamic IO statistics with specified key and source.
* @param key unique identifier
* @param source source of statistics (AtomicInteger)
*/","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,registerFailureHandling,org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling(),760,772,"/**
* Initializes interrupt handling and uncaught exception handler.
* @throws IllegalArgumentException if configuration fails
*/","* Override point: register this class as the handler for the control-C
   * and SIGINT interrupts.
   *
   * Subclasses can extend this with extra operations, such as
   * an exception handler:
   * <pre>
   *  Thread.setDefaultUncaughtExceptionHandler(
   *     new YarnUncaughtExceptionHandler());
   * </pre>",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,accept,org.apache.hadoop.net.unix.DomainSocket:accept(),233,243,"/**
* Creates a new DomainSocket instance with the given path and file descriptor.
* @return DomainSocket object or throws IOException if creation fails
*/","* Accept a new UNIX domain connection.
   *
   * This method can only be used on sockets that were bound with bind().
   *
   * @return                The new connection.
   * @throws IOException    If there was an I/O error performing the accept--
   *                        such as the socket being closed from under us.
   *                        Particularly when the accept is timed out, it throws
   *                        SocketTimeoutException.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,setAttribute,"org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)",308,317,"/**
* Executes mask operation with specified type and size.
* @param type mask operation type
* @param size size for the operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,getAttribute,org.apache.hadoop.net.unix.DomainSocket:getAttribute(int),321,332,"/**
* Retrieves a function mask for the given type.
* @param type identifier of the function to fetch mask for
* @return function mask value or throws IOException if error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,shutdown,org.apache.hadoop.net.unix.DomainSocket:shutdown(),395,404,"/**
* Performs mask-related operations and cleanup.
* @throws IOException if an I/O error occurs
*/","* Call shutdown(SHUT_RDWR) on the UNIX domain socket.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,sendFileDescriptors,"org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)",421,431,"/**
* Executes a function on the provided file descriptors.
* @param descriptors array of FileDescriptor objects
* @param jbuf byte buffer for data storage
* @param offset starting position in the buffer
* @param length number of bytes to process
*/","* Send some FileDescriptor objects to the process on the other side of this
   * socket.
   * 
   * @param descriptors       The file descriptors to send.
   * @param jbuf              Some bytes to send.  You must send at least
   *                          one byte.
   * @param offset            The offset in the jbuf array to start at.
   * @param length            Length of the jbuf array to use.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,recvFileInputStreams,"org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)",448,487,"/**
* Reads data from multiple file streams into a buffer.
* @param streams array of FileInputStream objects
* @param buf target byte array
* @param offset starting offset in the buffer
* @param length number of bytes to read
* @return result code or throws IOException on failure
*/","* Receive some FileDescriptor objects from the process on the other side of
   * this socket, and wrap them in FileInputStream objects.
   *
   * @param streams input stream.
   * @param buf input buf.
   * @param offset input offset.
   * @param length input length.
   * @return wrap them in FileInputStream objects.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,createNewInstance,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long),102,107,"/**
* Creates and returns an IOStatisticsContext instance with the given key.
* @param key unique identifier for the context
*/","* Creating a new IOStatisticsContext instance for a FS to be used.
   * @param key Thread ID that represents which thread the context belongs to.
   * @return an instance of IOStatisticsContext.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunkedSums,"org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",401,427,"/**
* Validates and updates user profile by ID.
* @param data ByteBuffer containing user data
* @param checksums ByteBuffer with checksums for validation
* @param fileName name of the file being processed
* @param basePos starting position in the file
*/","* Verify that the given checksums match the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,.
   * but the position is maintained.
   *  
   * @param data the DirectByteBuffer pointing to the data to verify.
   * @param checksums the DirectByteBuffer pointing to a series of stored
   *                  checksums
   * @param fileName the name of the file being read, for error-reporting
   * @param basePos the file position to which the start of 'data' corresponds
   * @throws ChecksumException if the checksums do not match",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,afterDecryption,"org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])",271,286,"/**
* Computes and applies decryption mask based on provided parameters.
* @param decryptor Decryptor object
* @param inBuffer Input buffer containing encrypted data
* @param position Position within the input buffer
* @param iv Initialization vector for decryption
* @return Decryption mask byte or 0 if not applicable
*/","* This method is executed immediately after decryption. Check whether 
   * decryptor should be updated and recalculate padding if needed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,resetStreamOffset,org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long),309,317,"/**
* Encrypts and decrypts data using a mask function.
* @param offset input/output buffer address
*/","* Reset the underlying stream offset; clear {@link #inBuffer} and 
   * {@link #outBuffer}. This Typically happens during {@link #seek(long)} 
   * or {@link #skip(long)}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,"org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)",151,172,"/**
* Writes data to buffer using chunked writes.
* @param b byte array to write
* @param off starting offset in the array
* @param len number of bytes to write
*/","* Encryption is buffer based.
   * If there is enough room in {@link #inBuffer}, then write to this buffer.
   * If {@link #inBuffer} is full, then do encryption and write data to the
   * underlying stream.
   * @param b the data.
   * @param off the start offset in the data.
   * @param len the number of bytes to write.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,flush,org.apache.hadoop.crypto.CryptoOutputStream:flush(),262,269,"/**
* Override of superclass method to perform additional initialization.
*/","* To flush, we need to encrypt the data in the buffer and write to the 
   * underlying stream, then do the flush.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,compile,org.apache.hadoop.fs.GlobPattern:compile(java.lang.String),57,59,"/**
* Compiles a glob pattern into a regular expression mask.
* @param globPattern glob pattern string
* @return compiled Pattern object
*/","* Compile glob pattern string
   * @param globPattern the glob pattern
   * @return the pattern object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,init,"org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)",64,73,"/**
* Initializes the function mask with a glob pattern and optional path filter.
* @param filePattern glob pattern for file filtering
* @param filter optional PathFilter instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,prepare,org.apache.hadoop.fs.shell.find.Name:prepare(),73,80,"/**
* Initializes glob pattern from user input, optionally converting to case-insensitive. 
* @throws IOException if error occurs during initialization
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)",1035,1051,"/**
* Extracts and uncompresses tarball to specified directory.
* @param inputStream input stream containing tarball
* @param untarDir target directory for extraction
* @param gzipped whether tarball is compressed with gzip
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem:close(),710,722,"/**
* Calls superclass method, attempts to close closed resource, and releases database connection.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,create,"org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",723,729,"/**
* Creates a file output stream with specified permissions and settings.
* @param f the path to the file
* @param permission file system permissions
* @param overwrite whether to allow overwriting existing files
* @param bufferSize buffer size for I/O operations
* @param replication block replication factor
* @param blockSize block size in bytes
* @param progress progress monitor
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,resetChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize(),263,265,"/**
* Calculates and masks function values.
* @param sum result of previous calculation
* @param BUFFER_NUM_CHUNKS buffer size constant
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getAllStatistics,org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics(),234,244,"/**
* Returns a synchronized map of statistics for each URI.
* @return Map<URI, Statistics> containing aggregated statistics
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,<init>,"org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)",118,125,"/**
* Initializes storage statistics with name and data.
* @param name unique identifier for the storage
* @param stats FileSystem statistics object, must be non-null and contain valid data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLongStatistics,org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics(),132,135,"/**
* Returns an iterator over long statistics.
* @return Iterator of LongStatistic objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLong,org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String),137,140,"/**
* Calculates function mask based on given key.
* @param key input key to compute mask from
* @return calculated Long value or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadByDistance,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int),4411,4430,"/**
* Calculates the byte count for a given distance.
* @param distance integer value representing the distance
*/","* In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller
     * ask for bytes read for distance 2, the function will return the value
     * for group {1, 2}.
     * @param distance the network distance
     * @return the total number of bytes read by the network distance",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,reset,org.apache.hadoop.fs.FileSystemStorageStatistics:reset(),157,160,"/**
 * Calls and delegates to stats.m1(). 
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,clearStatistics,org.apache.hadoop.fs.AbstractFileSystem:clearStatistics(),218,222,"/**
* Iterates over statistics table and applies mask operation.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",1334,1359,"/**
* Creates or appends to a file based on the specified flags and permissions.
* @param f Path to the file
* @param absolutePermission File permissions
* @param flag Create mode flags
* @return FSDataOutputStream instance or null if failed
*/","* This create has been added to support the FileContext that processes
   * the permission with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f path.
   * @param absolutePermission permission.
   * @param flag create flag.
   * @param bufferSize buffer size.
   * @param replication replication.
   * @param blockSize block size.
   * @param progress progress.
   * @param checksumOpt check sum opt.
   * @return output stream.
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,<init>,"org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)",278,283,"/**
* Initializes an AbstractFileSystem instance from a URI and scheme parameters.
* @param uri URI to initialize the file system
* @param supportedScheme supported protocol scheme (e.g. ""s3"", ""hdfs"")
* @param authorityNeeded whether authority is required for this file system
* @param defaultPort default port number for this file system
*/","* Constructor to be called by subclasses.
   * 
   * @param uri for this file system.
   * @param supportedScheme the scheme supported by the implementor
   * @param authorityNeeded if true then theURI must have authority, if false
   *          then the URI must have null authority.
   * @param defaultPort default port to use if port is not specified in the URI.
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])",117,127,"/**
* Encodes and masks input byte arrays into output byte arrays.
* @param inputs input byte arrays to be encoded
* @param outputs output byte arrays to store the result
*/","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",68,99,"/**
* Encodes input buffers and writes output to direct or byte array state.
* @param inputs input ByteBuffer arrays
* @param outputs output ByteBuffer array
*/","* Encode with inputs and generates outputs.
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation. Anyway the positions of input buffers will move forward.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after encoding
   * @param outputs output buffers to put the encoded data into, ready to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object),122,124,"/**
* Constructs an ArrayPrimitiveWritable with the given primitive value.
* @param value primitive value to be wrapped in the writable object
*/","* Wrap an existing array of primitives
   * @param value - array of primitives",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.HarFileSystem:getCanonicalUri(),318,321,"/**
* Returns file system URI using underlying implementation.
*/","* Used for delegation token related functionality. Must delegate to
   * underlying file system.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri(),113,116,"/**
* Returns the first URI from file system.
* @return First URI in file system or null if empty
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(),148,151,"/**
* Returns file system status with mask information.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String),687,693,"/**
* Checks if capability is not supported by this instance or data source.
* @param capability unique capability identifier
* @return true if capability is not supported, false otherwise
*/","* Probe the inner stream for a capability.
     * Syncable operations are rejected before being passed down.
     * @param capability string to query the stream support for.
     * @return true if a capability is known to be supported.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hasCapability,org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String),1410,1416,"/**
* Delegates call to out instance's m1 method.
* @param capability capability string to pass to out.m1
* @return result of out.m1 or false if out is null/capability is null
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String),484,487,"/**
 * Calls underlying data service to perform operation 'm1'.
 * @param capability the capability being processed
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,addToCacheAndRelease,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)",484,552,"/**
* Processes a block of data, potentially caching it.
* @param data BufferData object to process
* @param blockFuture Future object for waiting on the block's completion
* @throws Exception if an error occurs during processing or caching
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),215,226,"/**
* Performs Buffer operations based on provided data.
* @param data BufferData object containing relevant information
*/","* Releases resources allocated to the given block.
   *
   * @throws IllegalArgumentException if data is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseDoneBlocks,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks(),192,198,"/**
* Processes and removes completed BufferData objects.
*/",* Releases resources for any blocks marked as 'done'.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean),232,250,"/**
* Generates a string representation of operations, optionally including debug info.
* @param showDebugInfo true to include debug information in output
* @return generated string or null if ops is empty
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters),113,137,"/**
* Initializes CachingBlockManager with given parameters.
* @param blockManagerParameters configuration and settings for the manager
*/","* Constructs an instance of a {@code CachingBlockManager}.
   *
   * @param blockManagerParameters params for block manager.
   * @throws IllegalArgumentException if bufferPoolSize is zero or negative.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,<init>,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)",104,108,"/**
* Initializes a custom thread pool executor with specified permit count.
* @param permitCount maximum number of threads allowed in the pool
* @param eventProcessingExecutor underlying thread pool executor for event processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,get,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)",265,283,"/**
* Processes block data from ByteBuffer.
* @param blockNumber unique block identifier
* @param buffer ByteBuffer containing block data
*/","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if buffer is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,toString,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString(),635,647,"/**
* Constructs and returns a string representation of cache and pool status.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,absolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute(),145,148,"/**
* Calculates function mask value based on internal calculations.
* @return calculated mask value
*/","* Gets the current absolute position within this file.
   *
   * @return the current absolute position within this file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferFullyRead,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead(),241,246,"/**
* Verifies buffer consistency by comparing start offset, data value, and byte count.
*/","* Determines whether the current buffer has been fully read.
   *
   * @return true if the current buffer has been fully read, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setAbsolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long),157,165,"/**
* Checks if a position is within the buffer and triggers a mask operation.
* @param pos position to check
* @return true if mask operation triggered, false otherwise
*/","* If the given {@code pos} lies within the current buffer, updates the current position to
   * the specified value and returns true; otherwise returns false without changing the position.
   *
   * @param pos the absolute position to change the current position to if possible.
   * @return true if the given current position was updated, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext(),71,77,"/**
* Returns an instance of IOStatisticsContext.
* @return IOStatisticsContext object or null if invalid
*/","* Get the context's IOStatisticsContext.
   *
   * @return instance of IOStatisticsContext for the context.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),84,88,"/**
 * Integrates metrics into the provided IO statistics context.
 * @param statisticsContext context to integrate metrics into
 */","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getInstanceConfigs,org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String),157,171,"/**
* Builds a metrics configuration map for the specified type.
* @param type string identifier of the metric type
* @return Map of instance IDs to MetricsConfig objects
*/","* Return sub configs for instance specified in the config.
   * Assuming format specified as follows:<pre>
   * [type].[instance].[option] = [value]</pre>
   * Note, '*' is a special default instance, which is excluded in the result.
   * @param type  of the instance
   * @return  a map with [instance] as key and config object as value",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,applyItem,org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData),412,419,"/**
* Applies functional mask to path data based on condition.
* @param item PathData object to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processArguments,org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList),421,429,"/**
* Processes PathData list using custom expression evaluation.
* @param args list of PathData objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processOptions,org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList),59,75,"/**
* Parses and validates test flags from command arguments.
* @param args list of string arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList),81,90,"/**
* Configures command options from the provided list of arguments.
* @param args list of strings containing command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList),189,195,"/**
* Formats and displays file size information.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList),85,92,"/**
* Processes command-line arguments for mask functionality.
* @param args list of strings representing input arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processOptions,org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList),50,54,"/**
* Formats command arguments based on mask syntax.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList),132,153,"/**
* Processes command-line arguments using the CommandFormat class.
* @param args list of strings representing command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processOptions,org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList),63,78,"/**
* Executes function mask command with arguments.
* @param args list of string arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList),235,242,"/**
* Parses command-line arguments for immediate flag.
* @param args list of string arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList),197,203,"/**
* Parses command-line arguments to generate a mask.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList),234,249,"/**
* Processes FUNC_MASK command with CRC verification.
* @param args list of arguments for the command
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processOptions,org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList),124,178,"/**
* Parses command line arguments and initializes quota display settings.
* @param args list of user input strings
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processOptions,org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList),51,56,"/**
* Processes command mask using given arguments.
* @param args list of string arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList),134,146,"/**
* Initializes file timestamp and permissions from command-line arguments.
* @param args LinkedList of strings containing command-line options
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList),276,292,"/**
* Processes a command with flags and parameters.
* @param args list of input arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList),175,189,"/**
* Executes function mask command with specified arguments.
* @param args list of string arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList),377,390,"/**
* Executes command with custom format and logging.
* @param args list of arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList),80,86,"/**
* Validates command arguments and checksum.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList),64,75,"/**
* Validates and executes a command with mask functionality.
* @param args list of input arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processOptions,org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList),82,89,"/**
* Processes a command with a mask and optional formatting.
* @param args list of strings containing command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList),61,65,"/**
* Formats command arguments using specified mask.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,"org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)",87,92,"/**
* Processes command-line arguments and returns a list of strings up to the specified position.
* @param args array of command-line arguments
* @param pos maximum number of arguments to process
*/","Parse parameters starting from the given position
   * Consider using the variant that directly takes a List
   * 
   * @param args an array of input arguments
   * @param pos the position at which starts to parse
   * @return a list of parameters",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList),104,109,"/**
* Executes mask-related command with provided arguments.
* @param args list of strings containing command parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processOptions,org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList),51,66,"/**
* Validates and updates the mask length.
* @param args list of arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processOptions,org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList),56,72,"/**
* Validates and updates FUNC_MASK parameters.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList),183,192,"/**
* Parses command-line arguments and updates instance variables.
*@param args list of string arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList),144,150,"/**
* Processes a mask command with optional arguments.
* @param args list of strings containing command parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.String),501,504,"/**
* Increments error counter and logs message with mask.
* @param message error message to be logged
*/","* Display an error string prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param message error message to display",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceUsage,"org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",249,251,"/**
* Formats and prints command mask to output stream.
* @param out output stream for printing
* @param instance Command object with mask data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String),52,55,"/**
* Creates a new MetricsRecord with the given name and its own ID.
* @param name unique identifier for the record
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList),94,105,"/**
* Executes filesystem statistics and logs results.
* @throws IOException on data access failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList),194,207,"/**
* Processes and displays PathData list, possibly with header line.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createPathHandle,"org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1148,1172,"/**
* Creates a path handle for the given file status, validating its authority and handling options.
* @param stat FileStatus object
* @param opts Options.HandleOpt array
* @return LocalFileSystemPathHandle instance
*/","* Hook to implement support for {@link PathHandle} operations.
   * @param stat Referent in the target FileSystem
   * @param opts Constraints that determine the validity of the
   *            {@link PathHandle} reference.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,exact,org.apache.hadoop.fs.Options$HandleOpt:exact(),384,386,"/**
* Initializes an array of handle options with default values.
* @return Array of HandleOpt objects
*/","* Handle is valid iff the referent is neither moved nor changed.
     * Equivalent to changed(false), moved(false).
     * @return Options requiring that the content and location of the entity
     * be unchanged between calls.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,content,org.apache.hadoop.fs.Options$HandleOpt:content(),394,396,"/**
* Returns an array of handle options with default values.
* @return Array containing two HandleOpt instances with different configurations
*/","* Handle is valid iff the content of the referent is the same.
     * Equivalent to changed(false), moved(true).
     * @return Options requiring that the content of the entity is unchanged,
     * but it may be at a different location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,path,org.apache.hadoop.fs.Options$HandleOpt:path(),404,406,"/**
 * Returns an array of handle options with specific mask values.
 * @return Array of HandleOpt objects with pre-set flags
 */","* Handle is valid iff the referent is unmoved in the namespace.
     * Equivalent to changed(true), moved(false).
     * @return Options requiring that the referent exist in the same location,
     * but its content may have changed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,reference,org.apache.hadoop.fs.Options$HandleOpt:reference(),414,416,"/**
* Returns an array of handle options with mask enabled.
* @return Array of HandleOpt objects with mask flag set to true
*/","* Handle is valid iff the referent exists in the namespace.
     * Equivalent to changed(true), moved(true).
     * @return Options requiring that the implementation resolve a reference
     * to this entity regardless of changes to content or location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)",1248,1259,"/**
* Initializes a ShellCommandExecutor with the given command, working directory, environment variables, and execution settings.
* @param execString array of executable strings
* @param dir working directory for the shell command (optional)
* @param env map of environment variables to set (optional)
* @param timeout maximum allowed execution time in milliseconds
* @param inheritParentEnv whether to inherit parent process environment variables
*/","* Create a new instance of the ShellCommandExecutor to execute a command.
     *
     * @param execString The command to execute with arguments
     * @param dir If not-null, specifies the directory which should be set
     *            as the current working directory for the command.
     *            If null, the current working directory is not modified.
     * @param env If not-null, environment of the command will include the
     *            key-value pairs specified in the map. If null, the current
     *            environment is not modified.
     * @param timeout Specifies the time in milliseconds, after which the
     *                command will be killed and the status marked as timed-out.
     *                If 0, the command will not be timed out.
     * @param inheritParentEnv Indicates if the process should inherit the env
     *                         vars from the parent process or not.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,initRefreshThread,org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean),108,118,"/**
* Initializes or stops the refresh thread based on the interval setting.
* @param runImmediately whether to start immediately if enabled
*/","* RunImmediately should set true, if we skip the first refresh.
   * @param runImmediately The param default should be false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,privateClone,org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text),243,245,"/**
* Creates a private token instance with specified service.
* @param newService Text representation of service
*/","* Create a private clone of a public token.
   * @param newService the new service name
   * @return a private token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies(),1125,1139,"/**
* Retrieves a collection of all BlockStoragePolicySpi instances.
* @return Collection of BlockStoragePolicySpi objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,initAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)",333,353,"/**
* Executes asynchronous call with masking, returning result or error.
* @param asyncCall asynchronous call object
* @param asyncCallReturn asynchronous call result value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,process,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",164,182,"/**
* Encrypts data using provided Cipher instance.
* @param inBuffer input ByteBuffer
* @param outBuffer output ByteBuffer
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initializeInterceptors,org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors(),98,115,"/**
* Parses and initializes interceptors from settings string.
* @throws IOException on invalid settings or interceptor creation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,main,org.apache.hadoop.fs.DF:main(java.lang.String[]),216,223,"/**
* Initializes DFS with default interval from specified directory.
* @param args array of directories to initialize, first element used
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,initialize,"org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",258,266,"/**
* Constructs a URI object from the provided components.
* @param scheme URI scheme
* @param authority URI authority
* @param path URI path
* @param fragment URI fragment
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,uriToString,"org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)",466,487,"/**
* Constructs a URI mask from the given URI and scheme inference flag.
* @param uri input URI
* @param inferredSchemeFromPath true if scheme was inferred from path, false otherwise
* @return string representation of URI or decoded remainder if no scheme present
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotSchemeWithRelative,org.apache.hadoop.fs.Path:checkNotSchemeWithRelative(),85,90,"/**
* Validates function mask parameters and throws exception on unsupported configuration.
*/","* Test whether this Path uses a scheme and is relative.
   * Pathnames with scheme and relative path are illegal.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsoluteAndSchemeAuthorityNull,org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull(),377,380,"/**
* Checks if function mask conditions are met.
* @return true if all conditions match, false otherwise
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute <strong>and</strong> the scheme is null, <b>and</b> the authority
   * is null.
   *
   * @return whether the path is absolute and the URI has no scheme nor
   * authority parts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsolute,org.apache.hadoop.fs.Path:isAbsolute(),399,401,"/**
* Returns true if mask is enabled.","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.  This method is a wrapper for {@link #isUriPathAbsolute()}.
   *
   * @return whether this URI's path is absolute",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkPath,org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path),369,413,"/**
* Validates the file system of a given Path.
* @param path input Path to validate
*/","* Check that a Path belongs to this FileSystem.
   * 
   * If the path is fully qualified URI, then its scheme and authority
   * matches that of this file system. Otherwise the path must be 
   * slash-relative name.
   * @param path the path.
   * @throws InvalidPathException if the path is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,write,org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput),530,537,"/**
* Writes user profile data to output stream.
* @param out DataOutput stream to write to
*/","* Write instance encoded as protobuf to stream.
   * @param out Output stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPutArguments,"org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",114,124,"/**
* Validates parameters for file upload functionality.
* @param filePath the path to the file
* @param inputStream input stream containing file data
* @param partNumber part number of the uploaded file (1-indexed)
* @param uploadId unique identifier for the upload operation
* @param lengthInBytes total length of the file in bytes
*/","* Check all the arguments to the
   * {@link MultipartUploader#putPart(UploadHandle, int, Path, InputStream, long)}
   * operation.
   * @param filePath Target path for upload (as {@link #startUpload(Path)}).
   * @param inputStream Data for this part. Implementations MUST close this
   * stream after reading in the data.
   * @param partNumber Index of the part relative to others.
   * @param uploadId Identifier from {@link #startUpload(Path)}.
   * @param lengthInBytes Target length to read from the stream.
   * @throws IllegalArgumentException invalid argument",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,abortUploadsUnderPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path),132,138,"/**
* Performs asynchronous operation on file at given Path, returning a future result.
* @param path location of the file to process
*/","* {@inheritDoc}.
   * @param path path to abort uploads under.
   * @return a future to -1.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",1452,1456,"/**
* Creates an FSDataOutputStream with append mode and specified buffer size.
* @param f file path
* @param bufferSize buffer size for I/O operations
* @param progress progress listener (optional)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1498,1503,"/**
* Deletes file or directory with optional recursion.
* @param f Path to delete
* @param recursive whether to delete subdirectories and files
* @throws AccessControlException if access is denied
* @throws IOException on IO error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1739,1745,"/**
* Checks if file system operation is allowed.
* @param src source file path
* @param dst destination file path
* @return true if operation allowed, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1747,1750,"/**
* Truncates file to specified length.
* @param f Path to file
* @param newLength desired file size in bytes
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1752,1757,"/**
* Applies access control mask to file and updates owner.
* @param f Path object representing the file
* @param username new owner's username
* @param groupname new owner's group name
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1759,1764,"/**
* Sets file system permissions on a Path.
* @param f Path to set permissions for
* @param permission FsPermission object specifying new permissions
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1766,1771,"/**
* Sets file replication mask based on path and replication factor.
* @param f file path to set replication for
* @param replication new replication factor value
* @throws AccessControlException if access control fails
* @throws IOException on I/O operation error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1773,1778,"/**
* Sets file times (mtime and atime) with provided values.
* @param f the file to modify
* @param mtime last modified time in milliseconds
* @param atime last accessed time in milliseconds
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1800,1805,"/**
* Modifies ACL entries on specified file or directory.
* @param path Path object to modify ACL entries for
* @param aclSpec List of AclEntry objects specifying new ACL permissions
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1807,1812,"/**
* Removes ACL entries from a file or directory.
* @param path file system path to modify
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1814,1818,"/**
* Removes ACL mask from the specified file or directory.
* @param path Path to the file or directory
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1820,1824,"/**
* Removes ACL mask from file or directory.
* @param path file system path to operate on
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1826,1830,"/**
* Applies ACL (Access Control List) mask to the specified file or directory.
* @param path Path object for the target resource
* @param aclSpec List of AclEntry objects defining the ACL specification
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1841,1846,"/**
* Sets XAttr on a file or directory.
* @param path Path to the file/directory
* @param name Name of the attribute
* @param value Attribute value in bytes
* @param flag Set flags for the attribute
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1869,1873,"/**
* Removes extended attribute from file.
* @param path file Path object
* @param name name of attribute to remove
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1875,1880,"/**
* Creates snapshot with specified name and returns error if failed.
* @param path filesystem path to operate on
* @param snapshotName unique name for the snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1882,1887,"/**
* Renames a snapshot in the specified directory.
* @param path Path to the snapshot directory
* @param snapshotOldName Old name of the snapshot
* @param snapshotNewName New name for the snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1889,1894,"/**
* Deletes a snapshot by name and masks the given Path.
* @param path Path to delete the snapshot from
* @param snapshotName Name of the snapshot to delete
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1901,1905,"/**
* Applies mask to source file and throws exception if storage policy is not satisfied.
* @param src file path to be masked
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1907,1912,"/**
* Applies storage policy mask to specified source Path.
* @param src Path object to apply mask to
* @param policyName name of the policy to be applied
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),1914,1918,"/**
* Throws an exception with error message and path if storage policy is unset.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1044,1049,"/**
* Deletes files and directories matching given path.
* @param f path to delete from
* @param recursive whether to delete recursively or not
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1308,1313,"/**
* Truncates a file to a specified length.
* @param f file path to truncate
* @param newLength target file size in bytes
* @throws FileNotFoundException if file not found
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1315,1321,"/**
* Renames a file by applying a mask to its path.
* @param src source file path
* @param dst destination file path
* @throws AccessControlException if access is denied
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1328,1332,"/**
* Creates symbolic link from target to link.
* @param target file path to be linked
* @param link symbolic link path
* @param createParent whether to create parent directory if it doesn't exist
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1340,1345,"/**
* Applies functional mask to the specified file path.
* @param f the file path to apply mask to
* @param username the owner username
* @param groupname the group name
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1347,1352,"/**
* Sets file permissions and masks function result.
* @param f path to set permissions for
* @param permission new file permissions
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1354,1359,"/**
* Checks if file system replication is valid.
* @param f file path to check
* @param replication desired replication factor
* @return true if valid, false otherwise (throws exception)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1361,1366,"/**
* Sets file modification and access times.
* @param f file path
* @param mtime last modified time in milliseconds
* @param atime last accessed time in milliseconds
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1374,1379,"/**
* Modifies ACL entries for the specified file or directory.
* @param path Path to the file/directory
* @param aclSpec List of ACL entry specifications to apply
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1381,1386,"/**
* Removes ACL entries from file system object.
* @param path file system object path
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1388,1392,"/**
* Removes ACLs from the specified directory.
* @param path Path to the directory
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1394,1398,"/**
* Removes ACL mask from the specified file or directory.
* @param path Path to the resource
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1400,1404,"/**
* Sets ACL mask on specified file or directory.
* @param path file system path to set ACL mask for
* @throws IOException if an I/O error occurs during operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1415,1420,"/**
* Sets an extended attribute on a file or directory.
* @param path Path to the file/directory
* @param name Attribute name
* @param value Attribute value as byte array
* @param flag Flags for the set operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1443,1447,"/**
* Removes extended attributes from the specified file.
* @param path file path to remove attributes from
* @param name attribute name to be removed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1449,1454,"/**
* Creates a function mask for the given path and snapshot name.
* @param path file system path
* @param snapshotName unique snapshot identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1456,1461,"/**
* Renames a snapshot on a given filesystem path.
* @param path filesystem path to rename the snapshot for
* @param snapshotOldName original name of the snapshot
* @param snapshotNewName new name to assign to the snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1463,1468,"/**
* Deletes a snapshot by ID and associated data.
* @param path Path to delete
* @param snapshotName unique snapshot identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1470,1473,"/**
* Throws an exception to satisfy storage policy at specified path.
* @param path file system path where policy is being satisfied
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1475,1479,"/**
* Throws exception to set storage policy based on policy name.
* @param path directory path
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",340,364,"/**
* Creates or appends to a file with specified flags and permissions.
* @param f file path
* @param flag creation flags
* @param absolutePermission permission bits
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress tracking
* @param checksumOpt checksum option
* @param createParent whether to create parent directories if needed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",651,667,"/**
* Resolves symbolic link and creates target if not found.
* @param target target file path
* @param link symbolic link path
* @param createParent whether to create parent directory if missing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object),3923,3929,"/**
* Checks if the given object is an instance of LinkedSegmentsDescriptor and delegates to superclass.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String),173,198,"/**
* Retrieves a credential entry by alias, decrypting the key if necessary.
* @param alias unique identifier for the credential
* @return CredentialEntry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getAliases,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases(),206,226,"/**
* Retrieves a list of key aliases from the keystore.
* @return List of String aliases
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,skip,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long),291,299,"/**
* Computes the remaining bytes to read from a file, ensuring we don't exceed its length.
* @param n number of additional bytes to consider
* @return remaining bytes that can be safely read
*/","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     * The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getPermissions,org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile),455,461,"/**
* Returns a FsPermission object combining user, group, and others access.
* @param ftpFile FTP file object to determine permissions from
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,applyUMask,org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission),297,301,"/**
* Creates a new FsPermission object with the specified permissions.
* @param umask existing FsPermission object
*/","* Apply a umask to this permission and return a new one.
   *
   * The umask is used by create, mkdir, and other Hadoop filesystem operations.
   * The mode argument for these operations is modified by removing the bits
   * which are set in the umask.  Thus, the umask limits the permissions which
   * newly created files and directories get.
   *
   * @param umask              The umask to use
   * 
   * @return                   The effective permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto),38,41,"/**
* Converts FsPermissionProto to FsPermission object.
* @param proto FsPermissionProto instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getPermissions,org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry),305,307,"/**
* Converts SFTP file permissions to FS permission.
* @param sftpFile SFTP file entry
* @return FsPermission object representing the file's permissions
*/","* Return file permission.
   *
   * @param sftpFile
   * @return file permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData),98,110,"/**
* Updates file system permissions for the given PathData item.
* @param item PathData object to modify
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(int),127,129,"/**
* Constructs FsPermission object from Unix-style file mode.
* @param mode Unix-style file mode (permissions)
*/","* Construct by the given mode.
   *
   * octal mask is applied.
   *
   *<pre>
   *              before mask     after mask    file type   sticky bit
   *
   *    octal     100644            644         file          no
   *    decimal    33188            420
   *
   *    octal     101644           1644         file          yes
   *    decimal    33700           1420
   *
   *    octal      40644            644         directory     no
   *    decimal    16804            420
   *
   *    octal      41644           1644         directory     yes
   *    decimal    17316           1420
   *</pre>
   *
   * 100644 becomes 644 while 644 remains as 644
   *
   * @param mode Mode is supposed to come from the result of native stat() call.
   *             It contains complete permission information: rwxrwxrwx, sticky
   *             bit, whether it is a directory or a file, etc. Upon applying
   *             mask, only permission and sticky bit info will be kept because
   *             they are the only parts to be used for now.
   * @see #FsPermission(short mode)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDefault,org.apache.hadoop.fs.permission.FsPermission:getDefault(),416,418,"/**
* Creates an FsPermission instance with default file permissions.
* @return FsPermission object representing read/write/execute permissions for owner, group, and others.","* Get the default permission for directory and symlink.
   * In previous versions, this default permission was also used to
   * create files, so files created end up with ugo+x permission.
   * See HADOOP-9155 for detail. 
   * Two new methods are added to solve this, please use 
   * {@link FsPermission#getDirDefault()} for directory, and use
   * {@link FsPermission#getFileDefault()} for file.
   * This method is kept for compatibility.
   *
   * @return Default FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDirDefault,org.apache.hadoop.fs.permission.FsPermission:getDirDefault(),425,427,"/**
* Constructs FsPermission with execute and write permissions for owner.
* @return FsPermission object representing the specified permissions
*/","* Get the default permission for directory.
   *
   * @return DirDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getFileDefault,org.apache.hadoop.fs.permission.FsPermission:getFileDefault(),434,436,"/**
* Creates an FsPermission instance with execute permissions for owner and group.","* Get the default permission for file.
   *
   * @return FileDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getCachePoolDefault,org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault(),443,445,"/**
* Creates file system permissions mask with read and execute access for owner. 
* @return FsPermission object representing the mask
*/","* Get the default permission for cache pools.
   *
   * @return CachePoolDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,valueOf,org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String),452,475,"/**
* Converts Unix symbolic permission string to FsPermission object.
* @param unixSymbolicPermission string representation of permissions
*/","* Create a FsPermission from a Unix symbolic permission string
   * @param unixSymbolicPermission e.g. ""-rw-rw-rw-""
   * @return FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short),479,481,"/**
 * Constructs an ImmutableFsPermission with the specified permissions.
 * @param permission short integer value representing file system permissions.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList),188,248,"/**
* Validates and processes ACL flags and entries.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printAclEntriesForSingleScope,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)",112,126,"/**
* Processes ACL entries based on type and permissions.
* @param aclStatus Acl status
* @param fsPerm File system permission
* @param entries List of AclEntry objects
*/","* Prints all the ACL entries in a single scope.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entries List<AclEntry> containing ACL entries of file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,aclSpecToString,org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List),330,337,"/**
* Concatenates ACL entries into a comma-separated string.
* @param aclSpec list of AclEntry objects
*/","* Convert a List of AclEntries into a string - the reverse of parseAclSpec.
   * @param aclSpec List of AclEntries to convert
   * @return String representation of aclSpec",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String),148,150,"/**
 * Constructs an FsPermission instance based on the provided access control string.
 * @param mode access control string representing file system permissions
 */","* Construct by given mode, either in octal or symbolic format.
   * @param mode mode as a string, either in octal or symbolic format
   * @throws IllegalArgumentException if <code>mode</code> is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList),81,96,"/**
* Processes chmod command with specified arguments.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,org.apache.hadoop.fs.store.ByteBufferInputStream:read(),94,100,"/**
* Returns a function mask value based on current conditions.
* If m1() returns a positive value, it's masked with the last byte of byteBuffer.m2(); otherwise, returns -1. 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,skip,org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long),102,114,"/**
* Calculates the file offset based on the given offset and current position.
* @param offset additional offset to apply
* @return calculated file offset or throws IOException if invalid
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,mark,org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int),140,145,"/**
* Performs mark operation with specified read limit.
* @param readlimit maximum bytes to be read before marking",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,"org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)",169,189,"/**
* Extracts a specified length of bytes from the given buffer starting at the indicated offset.
* @param b input buffer
* @param offset starting position in buffer
* @param length number of bytes to extract
* @return extracted byte count or -1 on error
*/","* Read in data.
   * @param b destination buffer.
   * @param offset offset within the buffer.
   * @param length length of bytes to read.
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the
   * amount of data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload(),887,897,"/**
* Executes m1() and returns BlockUploadData.
* @return BlockUploadData object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload(),593,600,"/**
* Initializes and returns a BlockUploadData object with data from the buffer.
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload(),722,731,"/**
* Initializes and returns a BlockUploadData object with data from the buffer.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$DataBlock:close(),475,481,"/**
* Marks the function as closed and performs subsequent actions.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,checkAndWriteSync,org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync(),1445,1450,"/**
* Updates function mask if synchronization interval has passed.
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize(),170,173,"/**
* Calculates the function mask by subtracting start position from result of m1(). 
* @return difference between m1() and posStart
*/","* Current size of compressed data.
       * 
       * @return
       * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength(),507,512,"/**
* Calculates and returns the file length as a bitmask value.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPathArgument,org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData),232,246,"/**
* Handles PathData item for erasure coding; checks file system compatibility and recurses directories as needed. 
* @param item PathData object containing file system and path information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,adjustColumnWidths,org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[]),327,355,"/**
* Formats and updates display format for PathData items.
* @param items array of PathData objects
*/","* Compute column widths and rebuild the format string
   * @param items to find the max field width for each column",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData),219,230,"/**
* Calculates and updates the masked file statistics for a given PathData item.
* @param item PathData object containing file information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getQuotaUsage,org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1952,1954,"/**
* Retrieves quota usage information from the specified file.
* @param f Path to the file to query
*/","Return the {@link QuotaUsage} of a given {@link Path}.
   * @param f path to use
   * @return the quota usage
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path),2730,2732,"/**
* Computes functional mask value based on file system path.
* @param path file system path
* @return computed functional mask value
*/","* Return the total size of all files from a specified path.
   * @param path the path.
   * @throws IOException IO failure
   * @return the number of path content summary.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/AbstractLaunchableService.java,<init>,org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String),48,50,"/**
 * Initializes an instance of AbstractLaunchableService with the specified name.
 */","* Construct an instance with the given name.
   *
   * @param name input name.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,<init>,org.apache.hadoop.service.CompositeService:<init>(java.lang.String),53,55,"/**
* Initializes a new instance of the CompositeService class with the specified name.
* @param name unique service identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,<init>,org.apache.hadoop.util.JvmPauseMonitor:<init>(),70,72,"/**
 * Initializes JVM pause monitor with class name. 
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,enterState,org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE),113,119,"/**
* Updates service state and returns previous state.
* @param proposed new service state
* @return the old service state
*/","* Enter a state -thread safe.
   *
   * @param proposed proposed new state
   * @return the original state
   * @throws ServiceStateException if the transition is not permitted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,instantiateService,org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration),658,694,"/**
* Instantiates and initializes a Service object from the configuration.
* @param conf Configuration object
* @return initialized Service object or null if creation fails
*/","* @return Instantiate the service defined in {@code serviceClassName}.
   *
   * Sets the {@code configuration} field
   * to the the value of {@code conf},
   * and the {@code service} field to the service created.
   *
   * @param conf configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])",1094,1098,"/**
* Constructs a KerberosDiagsFailure exception with specified details.
* @param category error category
* @param throwable underlying cause of failure (optional)
* @param message descriptive error message
* @param args additional message arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStop,org.apache.hadoop.service.CompositeService:serviceStop(),128,136,"/**
* Stops services and calls superclass method.
* @throws Exception if an error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,run,org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run(),184,187,"/**
* Performs mask operation on composite service.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,progressable,org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable),308,310,"/**
* Wraps Progressable instance in SequenceFile.Writer Option. 
* @param value Progressable object to be wrapped
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,valueClass,org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class),293,295,"/**
* Creates a writer option for a sequence file based on the given class. 
* @param value the class to use for writing",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,equals,org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object),74,82,"/**
* Compares this object to another using binary comparison.
* @param other Object to compare with
* @return true if objects are equal, false otherwise
*/",* Return true if bytes from {#getBytes()} match.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,hashCode,org.apache.hadoop.io.BytesWritable:hashCode(),207,210,"/**
* Calls superclass's implementation of m1.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,hashCode,org.apache.hadoop.io.Text:hashCode(),422,425,"/**
* Calls superclass's implementation of m1().",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,hashCode,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode(),95,98,"/**
* Delegates call to token's m1() method.
* @return result of token.m1()
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,"org.apache.hadoop.io.BytesWritable:set(byte[],int,int)",178,182,"/**
* Updates mask data in the underlying buffer.
* @param newData new byte array
* @param offset starting index for update
* @param length number of bytes to update
*/","* Set the value to a copy of the given byte range.
   *
   * @param newData the new values to copy in
   * @param offset the offset in newData to start at
   * @param length the number of bytes to copy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,readFields,org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput),184,189,"/**
* Reads and processes data from input stream using FUNC_MASK protocol.
* @param in DataInput object containing protocol data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable),1690,1694,"/**
* Processes BytesWritable object and returns an integer value.
* @throws IOException if processing fails
*/","* Copy the key into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual key size.
         * 
         * @param key
         *          BytesWritable to hold the key.
         * @throws IOException raised on errors performing I/O.
         * @return the key into BytesWritable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,list,org.apache.hadoop.fs.FileUtil:list(java.io.File),1619,1630,"/**
* Retrieves an array of file names within the specified directory.
* @param dir target directory
* @return array of file names, or null if access is denied or an error occurs
*/","* A wrapper for {@link File#list()}. This java.io API returns null
   * when a dir is not a directory or for any I/O error. Instead of having
   * null check everywhere File#list() is used, we will add utility API
   * to get around this problem. For the majority of cases where we prefer
   * an IOException to be thrown.
   * @param dir directory for which listing should be performed
   * @return list of file names or empty string list
   * @exception AccessDeniedException for unreadable directory
   * @exception IOException for invalid directory or for bad disk",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkAccessByFileMethods,org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File),153,174,"/**
* Verifies directory permissions and throws exceptions for any issues.
* @param dir File object representing the directory to check
*/","* Checks that the current running process can read, write, and execute the
   * given directory by using methods of the File object.
   * 
   * @param dir File to check
   * @throws DiskErrorException if dir is not readable, not writable, or not
   *   executable",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)",470,477,"/**
* Initializes and locks memory for the given ByteBuffer.
* @param buffer direct ByteBuffer to lock
* @param len length of the buffer
*/","* Locks the provided direct ByteBuffer into memory, preventing it from
     * swapping out. After a buffer is locked, future accesses will not incur
     * a page fault.
     * 
     * See the mlock(2) man page for more information.
     * 
     * @throws NativeIOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,create,"org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])",74,100,"/**
* Creates a SharedFileDescriptorFactory instance for each path, 
* throwing an exception on failure or if no paths are configured.
* @param prefix factory prefix
* @param paths array of file descriptor paths
* @return the first successfully created SharedFileDescriptorFactory
* @throws IOException on loading or configuration error
*/","* Create a new SharedFileDescriptorFactory.
   *
   * @param prefix       The prefix to prepend to all the file names created
   *                       by this factory.
   * @param paths        An array of paths to use.  We will try each path in 
   *                       succession, and return a factory using the first 
   *                       usable path.
   * @return             The factory.
   * @throws IOException If a factory could not be created for any reason.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit(),285,287,"/**
* Wraps NativeIO.m1() call to provide native functionality.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,writeChecksumChunks,"org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)",212,228,"/**
* Computes and updates checksum for a byte array chunk.
* @param b the input byte array
* @param off starting offset in the array
* @param len length of the chunk to process
*/","Generate checksums for the given data chunks and output chunks & checksums
   * to the underlying output stream.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)",534,564,"/**
* Updates checksums using CRC32 algorithm.
* @param data input buffer
* @param checksums output buffer for updated checksums
*/","* Calculate checksums for the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,
   * but the position is maintained.
   * 
   * @param data the DirectByteBuffer pointing to the data to checksum.
   * @param checksums the DirectByteBuffer into which checksums will be
   *                  stored. Enough space must be available in this
   *                  buffer to put the checksums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoInputStream:freeBuffers(),809,813,"/**
* Encrypts input and decrypts output using crypto stream utilities.
*/",Forcibly free the direct buffers.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,unbuffer,org.apache.hadoop.crypto.CryptoInputStream:unbuffer(),853,858,"/**
 * Invokes methods m1 and m2, then executes StreamCapabilitiesPolicy.m3 with input parameter.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int),45,47,"/**
 * Initializes a new instance of BoundedByteArrayOutputStream with specified capacity.
 * @param capacity maximum allowed size in bytes.","* Create a BoundedByteArrayOutputStream with the specified
   * capacity
   * @param capacity The capacity of the underlying byte array",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeFromUrlString,org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String),382,384,"/**
* Applies mask to current value and updates it.
* @param newValue new value to apply mask to
*/","* Decode the given url safe string into this token.
   * @param newValue the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),89,107,"/**
* Filters EC blocks from input group based on predicate m3().
* @param blockGroup ECBlockGroup to filter
* @return array of filtered EC blocks or null if empty
*/","* Which blocks were erased ?
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",36,49,"/**
* Initializes ByteBufferDecodingState with decoder and input/output buffers.
* @param decoder RawErasureDecoder instance
* @param inputs array of ByteBuffer inputs
* @param erasedIndexes array of erased indexes (ignored)
* @param outputs array of ByteBuffer outputs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])",37,52,"/**
* Initializes ByteArrayDecodingState with decoder and input/output buffers.
* @param decoder RawErasureDecoder instance
* @param inputs byte[][] of input data buffers
* @param erasedIndexes int[] of erased index positions
* @param outputs byte[][] of output data buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
* Creates a native XOR decoder instance.
* @param coderOptions Erasure coding options
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
* Returns native RS decoder instance for raw erasure decoding.
* @param coderOptions ErasureCoderOptions object for configuration
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,53,"/**
* Initializes RSLegacyRawDecoder with ErasureCoderOptions.
* @param coderOptions ErasureCoderOptions object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,53,"/**
* Initializes RSLegacyRawEncoder with ErasureCoderOptions.
* @param coderOptions configuration options for erasure coding. 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
* Creates a native XOR encoder instance. 
* @param coderOptions Erasure coder configuration options",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
* Creates a native raw encoder instance with given options.
* @param coderOptions configuration settings for encoding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),39,62,"/**
* Computes XOR mask from inputs and stores result in outputs.
* @param decodingState decoding state object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),39,60,"/**
* Encrypts input data using XOR operation.
* @param encodingState state object containing input and output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),167,217,"/**
* Processes null masks by decoding input lengths and adjusting output buffers.
* @param decodingState state object containing inputs and outputs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),55,88,"/**
* Processes encoding state to generate output array.
* @param encodingState current encoding state
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),63,68,"/**
* Updates output buffer and applies GF table operations.
* @param encodingState stateful data for encoding process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),64,86,"/**
* Performs XOR masking operation on input data.
* @param decodingState state object containing input/output buffers and indices
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),62,85,"/**
* Processes input data using the provided ByteArrayEncodingState.
* @param encodingState state object containing inputs, outputs and offsets
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),111,165,"/**
* Updates inputs array based on erased indexes and decoded length.
* @param decodingState state object containing input/output arrays
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),90,128,"/**
* Computes and stores the masked output for the given encoding state.
* @param encodingState current encoding state with outputs, inputs, offsets, etc.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),70,79,"/**
* Applies masks to output and input byte arrays.
* @param encodingState ByteArrayEncodingState object containing relevant data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates a Raw Erasure decoder instance based on provided options.
* @param coderOptions configuration settings for the decoder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates a Raw Erasure Encoder instance with custom options.
* @param coderOptions configuration settings for the encoder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,processErasures,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[]),117,140,"/**
* Initializes and updates erasure-related data structures based on given indexes.
* @param erasedIndexes array of indices to mark for erasure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextMarker,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)",217,257,"/**
* Searches for a specific marker in compressed data.
* @param marker target value to search for
* @param markerBitLength length of the marker in bits
* @return true if found, false otherwise
*/","* This method tries to find the marker (passed to it as the first parameter)
   * in the stream. It can find bit patterns of length &lt;= 63 bits.
   * Specifically this method is used in CBZip2InputStream to find the end of
   * block (EOB) delimiter in the stream, starting from the current position
   * of the stream. If marker is found, the stream position will be at the
   * byte containing the starting bit of the marker.
   * @param marker The bit pattern to be found in the stream
   * @param markerBitLength No of bits in the marker
   * @return true if the marker was found otherwise false
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException if marketBitLength is greater than 63",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetUByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte(),660,662,"/**
* Retrieves a single character from the file system.
* @return Character representing function mask or throws IOException if error occurs.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetInt,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt(),664,666,"/**
* Calculates a specific integer mask value using the m1 function.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int),1006,1037,"/**
* Computes FUNC_MASK value based on groupNo and current state.
* @param groupNo unique group identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,recvDecodingTables,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables(),709,788,"/**
* Updates internal state and masks data based on various conditions.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)",627,643,"/**
* Initializes CBZip2OutputStream with specified block size and output stream.
* @param out target output stream
* @param blockSize compression block size (1-9)
*/","* Constructs a new <tt>CBZip2OutputStream</tt> with specified blocksize.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  *
  * @param out
  *            the destination stream.
  * @param blockSize
  *            the blockSize as 100k units.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws IllegalArgumentException
  *             if {@code (blockSize < 1) || (blockSize > 9)}
  * @throws NullPointerException
  *             if {@code out == null}.
  *
  * @see #MIN_BLOCKSIZE
  * @see #MAX_BLOCKSIZE",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,moveToFrontCodeAndSend,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend(),1391,1395,"/**
* Initializes mask functionality.
* 
* Performs three-stage initialization: calls m1 with offset and origPtr,
* executes m2, and then calls m3.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,blockSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort(),1605,1629,"/**
* Resets work limits and attempts to find origin pointer.
* @param none
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",69,71,"/**
* Creates a new BlockCompressorStream instance with default buffer size and compression level.
* @param out output stream to write compressed data
* @param compressor compressor instance to use for compression
*/","* Create a {@link BlockCompressorStream} with given output-stream and 
   * compressor.
   * Use default of 512 as bufferSize and compressionOverhead of 
   * (1% of bufferSize + 12 bytes) =  18 bytes (zlib algorithm).
   * 
   * @param out stream
   * @param compressor compressor to be used",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",74,77,"/**
* Initializes DecompressorStream with input stream and decompressor.
* @param in InputStream to compress
* @param decompressor Decompression strategy
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",48,51,"/**
* Initializes BlockDecompressorStream with input stream and decompression settings.
* @param in InputStream to read from
* @param decompressor Decompressor instance for compression algorithm
* @param bufferSize Buffer size for efficient I/O operations
*/","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @param bufferSize size of buffer
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,"org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",138,142,"/**
* Wraps input stream with decompression functionality.
* @param in input stream to be decompressed
* @param decompressor decompression instance
* @return CompressionInputStream object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,"org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)",95,106,"/**
* Computes a function mask from the specified byte array and offset.
* @param b input byte array
* @param off starting index within the array
* @param len number of bytes to process
* @return computed mask value or 0 for empty input
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,write,"org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)",81,134,"/**
* Writes a byte array to the compressor in chunks.
* @param b input byte array
* @param off starting offset within the array
* @param len number of bytes to write
*/","* Write the data provided to the compression codec, compressing no more
   * than the buffer size less the compression overhead as specified during
   * construction for each block.
   *
   * Each block contains the uncompressed length for the block, followed by
   * one or more length-prefixed blocks of compressed data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(),70,72,"/**
 * Initializes decompressor with default stream size.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int),298,300,"/**
 * Initializes a new instance of ZStandardDirectDecompressor with specified direct buffer size.
 * @param directBufferSize size of the direct buffer to use for decompression
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)",90,92,"/**
* Constructs a new ZStandard compressor with specified compression level and buffer sizes.
* @param level Zstandard compression level
* @param bufferSize input/output buffer size
*/","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZStandard format.
   * @param level level.
   * @param bufferSize bufferSize.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec),167,169,"/**
 * Returns an instance of Compressor using the specified CompressionCodec. 
 * @param codec compression codec to use
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor(),319,343,"/**
* Retrieves and initializes a Decompressor instance from CodecPool.
* @return initialized Decompressor object or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createOutputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)",128,143,"/**
* Creates a compression output stream using the specified codec and configuration.
* @param codec CompressionCodec instance
* @param conf Configuration object
* @param out OutputStream to compress data to
* @return CompressionOutputStream or null on failure
*/","* Create an output stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the output stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param out         The output stream to wrap.
     * @return            The new output stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,close,org.apache.hadoop.io.compress.CompressionOutputStream:close(),61,75,"/**
* Releases compressor resources, ensuring proper cleanup on exceptions.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Writer:close(),1422,1443,"/**
* Serializes and cleans up resources after compression.
*/",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor),310,317,"/**
* Returns a compressor to the pool.
* @param compressor Compressor object to return
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createInputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)",154,169,"/**
* Creates a compression-aware input stream from the given codec and input stream.
* @param codec CompressionCodec instance
* @param conf Configuration object (not used)
* @param in InputStream to compress
* @return CompressionInputStream instance or null on failure
*/","* Create an input stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the input stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param in          The input stream to wrap.
     * @return            The new input stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,close,org.apache.hadoop.io.compress.CompressionInputStream:close(),65,75,"/**
* Decompresses data using a decompressor and returns it to the pool.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Reader:close(),2169,2188,"/**
* Releases decompressors and deserializers, and notifies input stream.
*/",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),345,352,"/**
* Releases a Decompressor object back into the pool.
* @param decompressor Decompressor instance to release
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createCompressor,org.apache.hadoop.io.compress.GzipCodec:createCompressor(),62,67,"/**
* Returns compressor instance based on configuration.
* @return Gzip or built-in gzip compressor, depending on m1 flag in config
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration),109,113,"/**
* Returns a Compressor instance based on configuration settings.
* @param conf Configuration object
*/","* Return the appropriate implementation of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib compressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDirectDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration),144,147,"/**
* Returns direct decompressor instance based on configuration.
* @param conf input configuration
* @return ZlibDirectDecompressor or null if not applicable
*/","* Return the appropriate implementation of the zlib direct decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor(),109,114,"/**
* Returns a DirectDecompressor instance based on configuration.
* @return ZlibDecompressor or null if not configured
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration),133,136,"/**
* Returns decompressor instance based on configuration settings.
* @param conf Configuration object
* @return ZlibDecompressor or BuiltInZlibInflater instance","* Return the appropriate implementation of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDecompressor(),95,100,"/**
* Returns decompressor instance based on configuration.
* @return Gzip decompressor implementation depending on m1 flag
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,decompress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)",189,243,"/**
* Processes input data and returns available bytes.
* @param b input byte array
* @param off offset into the input array
* @param len length of the input data
* @return number of available bytes or -1 on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,write,org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput),759,769,"/**
* Writes metadata and associated key-value pairs to output stream.
* @param out DataOutput stream for writing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,writeImpl,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput),205,215,"/**
* Writes functional metadata to output stream.
* @param out DataOutput stream
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,write,org.apache.hadoop.security.token.Token:write(java.io.DataOutput),323,331,"/**
* Writes user credentials and metadata to output stream.
* @param out DataOutput stream for writing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),333,345,"/**
* Stores delegation key using SQL secret manager.
* @param key DelegationKey object
*/","* Persists a DelegationKey into the SQL database. The delegation keyId
   * is expected to be unique and any duplicate key attempts will result
   * in an IOException.
   * @param key DelegationKey to persist into the SQL database.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),351,363,"/**
* Updates delegation key in SQL secret manager.
* @param key DelegationKey object
*/","* Updates an existing DelegationKey in the SQL database.
   * @param key Updated DelegationKey.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,addOrUpdateDelegationKey,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)",691,725,"/**
* Stores or updates a ZKDTSMDelegationKey instance as a ZooKeeper node.
* @param key DelegationKey to store
* @param isUpdate true to update existing node, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,readFields,org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput),49,52,"/**
* Reads and sets the mask value from input stream.
* @param in DataInput stream containing mask value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringSafely,"org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)",484,497,"/**
* Reads and decodes a variable-length string from input stream.
* @param in DataInput source
* @param maxLength maximum allowed encoded size
* @return decoded String or null if malformed
*/","* Read a string, but check it for sanity. The format consists of a vint
   * followed by the given number of bytes.
   * @param in the stream to read from
   * @param maxLength the largest acceptable length of the encoded string
   * @return the bytes as a string
   * @throws IOException if reading from the DataInput fails
   * @throws IllegalArgumentException if the encoded byte size for string 
             is negative or larger than maxSize. Only the vint is read.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,org.apache.hadoop.io.Text:readFields(java.io.DataInput),346,350,"/**
* Initializes mask data from input stream.
* @param in DataInput stream containing mask data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,"org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)",352,362,"/**
* Validates and deserializes masked data.
* @param in DataInput stream
* @param maxLength maximum allowed length
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,skip,org.apache.hadoop.io.Text:skip(java.io.DataInput),369,372,"/**
* Reads and writes a masked data value.
* @param in input stream
*/","* Skips over one Text in the input.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBuffer,"org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)",2274,2291,"/**
* Reads and processes mask data from input stream.
* @param buffer DataInputBuffer instance
* @param filter CompressionInputStream instance
*/",Read a compressed buffer,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput),749,758,"/**
* Reads and populates user profile fields from input stream.
* @param in DataInput stream containing user data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,"org.apache.hadoop.io.Text:readString(java.io.DataInput,int)",566,572,"/**
* Reads and decodes a FUNC_MASK from the input stream.
* @param in input stream
* @param maxLength maximum allowed length
* @return encoded mask as string or throws IOException if an error occurs.","* @return Read a UTF8 encoded string with a maximum size.
   * @param in input datainput.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,readFields,org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput),108,119,"/**
* Reads and initializes FUNC_MASK fields from input stream.
*@param in DataInput to read from
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeString,"org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)",256,266,"/**
* Writes masked string to output stream.
* @param out DataOutput stream
* @param s String to mask (null for no-op)
*/","* Write a String as a VInt n, followed by n Bytes as in Text format.
   * 
   * @param out out.
   * @param s s.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String),54,62,"/**
* Retrieves a CredentialEntry instance by alias.
* @param alias unique identifier for the credential
* @return CredentialEntry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])",64,75,"/**
* Creates a new CredentialEntry instance with the given name and credential.
* @param name unique identifier for the credential
* @param credential character array containing the actual credential data
* @return a CredentialEntry object representing the newly created entry
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,deleteCredentialEntry,org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String),77,87,"/**
* Retrieves and updates credential by name.
* @param name unique credential identifier
* @throws IOException if credential is missing or unavailable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress),474,487,"/**
* Constructs a Text object from an InetSocketAddress.
* @param addr network address to extract host and port
* @return Text representation of the address in ""host:port"" format
*/","* Construct the service key for a token
   * @param addr InetSocketAddress of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String),58,66,"/**
* Retrieves KeyVersion object for given FUNC_MASK version.
* @param versionName name of the version
* @return KeyVersion object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getMetadata,org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String),68,80,"/**
* Fetches metadata for a given function mask by name.
* @param name unique function mask identifier
* @return Metadata object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,createKey,"org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",82,100,"/**
* Creates a key with the given name and material, returning its version.
* @param name unique key identifier
* @param material key data (byte array)
* @param options metadata configuration
* @return KeyVersion object containing key details
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDtService,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI),430,441,"/**
* Constructs a Text object from the given URI.
* @param uri input URI
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),123,131,"/**
* Creates a Token object from the provided TokenProto.
* @param tokenProto Token prototype with data to be extracted
* @return Token object with converted data or null if failed
*/","* Create a hadoop token from a protobuf token.
   * @param tokenProto token
   * @return a new token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,org.apache.hadoop.io.Text:find(java.lang.String),171,173,"/**
* Calls m1 with default offset of 0.
* @param what parameter passed to m1
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeEnum,"org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)",432,435,"/**
* Writes an enumeration value to output stream.
* @param out DataOutputStream instance
* @param enumVal enumeration value to be written
*/","* writes String value of enum to DataOutput. 
   * @param out Dataoutput stream
   * @param enumVal enum value
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,"org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",128,135,"/**
* Writes user and group information to output stream.
* @param out DataOutput stream
* @param username user identifier
* @param groupname group identifier
* @param permission FsPermission object
*/","* Serialize a {@link PermissionStatus} from its base components.
   * @param out out.
   * @param username username.
   * @param groupname groupname.
   * @param permission FsPermission.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(byte[]),112,114,"/**
 * Creates a new Text instance from a UTF-8 encoded byte array.
 * @param utf8 byte array containing text data in UTF-8 encoding
 */","* Construct from a byte array.
   *
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text),103,105,"/**
* Constructs a new Text instance from UTF-8 encoded string.
* @param utf8 UTF-8 encoded text content
*/","* Construct from another text.
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)",180,187,"/**
* Determines the mask for a given text based on record delimiter bytes.
* @param str input Text object
* @param maxLineLength maximum allowed line length
* @param maxBytesToConsume maximum allowed bytes to consume
* @return integer mask value or throws IOException if error occurs","* Read one line from the InputStream into the given Text.
   *
   * @param str the object to store the given line (without newline)
   * @param maxLineLength the maximum number of bytes to store into str;
   *  the rest of the line is silently discarded.
   * @param maxBytesToConsume the maximum number of bytes to consume
   *  in this call.  This is only a hint, because if the line cross
   *  this threshold, we allow it to happen.  It can overshoot
   *  potentially by as much as one buffer length.
   *
   * @return the number of bytes read including the (longest) newline
   * found.
   *
   * @throws IOException if the underlying stream throws",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,getTextLength,org.apache.hadoop.io.Text:getTextLength(),146,151,"/**
* Calculates and returns the text length mask value.
*/","* @return Returns the length of this text. The length is equal to the number of
   * Unicode code units in the text.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,toString,org.apache.hadoop.io.SequenceFile$Metadata:toString(),828,840,"/**
* Formats metadata into a string representation.
* @return formatted string or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRenewer,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text),105,116,"/**
* Initializes or updates the renewer field with a valid Hadoop Kerberos name.
* @param renewer input Text object containing a Kerberos principal
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getAliases,org.apache.hadoop.security.alias.UserProvider:getAliases(),111,119,"/**
* Retrieves a list of function masks.
* @return list of function mask strings
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName(),1020,1023,"/**
* Returns the function mask using canonical service.
* @return Function mask string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName(),246,249,"/**
* Returns functional mask value from canonical service.
* @return string representation of functional mask value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable),55,58,"/**
* Copies contents from another SortedMapWritable.","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable),53,56,"/**
 * Constructs a new MapWritable instance by copying from another.
 * @param other MapWritable object to copy values from
 */","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),297,300,"/**
* Returns a SequenceFile Writer option based on the specified compression type. 
* @param type Compression type to use when writing sequence files
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,<init>,org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>(),42,45,"/**
 * Initializes JavaSerializationComparator instance with deserializer.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumTimeWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)",114,116,"/**
* Creates a retry policy with fixed sleep interval and maximum execution time.
* @param maxTime maximum allowed execution time
* @param sleepTime fixed sleep interval between retries
* @param timeUnit unit of time for maxTime and sleepTime
*/","* <p>
   * Keep trying for a maximum time, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param timeUnit timeUnit.
   * @param sleepTime sleepTime.
   * @param maxTime maxTime.
   * @return RetryPolicy.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,run,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run(),977,1053,"/**
* Runs the renewal loop for Kerberos ticket, attempting to fetch a new TGT.
* @throws InterruptedException if interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)",205,208,"/**
* Returns a retry policy with specified maximum failovers.
* @param fallbackPolicy default retry policy to use
* @param maxFailovers maximum number of failovers allowed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newCall,"org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)",349,356,"/**
* Creates a Call object based on client configuration.
* @param method the method to be called
* @param args method arguments
* @param isRpc whether it's an RPC call
* @param callId unique call identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",50,57,"/**
* Constructs an InstrumentedWriteLock instance with specified parameters.
* @param name Lock identifier
* @param logger Logger instance for logging events
* @param readWriteLock Shared lock object wrapping the write lock
* @param minLoggingGapMs Minimum gap between log messages
* @param lockWarningThresholdMs Threshold for issuing lock warnings
* @param clock Timer instance for tracking time-related events
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)",81,85,"/**
* Initializes an instrumented lock with the specified settings.
* @param name unique lock identifier
* @param logger logging instance for lock events
* @param lock underlying synchronization lock
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",56,63,"/**
* Constructs an InstrumentedReadLock with the specified parameters.
* @param name unique identifier for the lock
* @param logger logging instance for warnings and errors
* @param readWriteLock shared lock instance
* @param minLoggingGapMs minimum time gap between log messages
* @param lockWarningThresholdMs threshold for issuing lock warnings
* @param clock timer instance for timing-related calculations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,org.apache.hadoop.util.InstrumentedLock:tryLock(),117,124,"/**
* Tries to acquire lock and execute subsequent operation.
* @return true if successful, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",58,65,"/**
* Creates a proxied object with failover and retry functionality.
* @param iface interface to proxy
* @param proxyProvider provider for failover proxy instances
* @param retryPolicy policy for retries on invocation failures
*/","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the same retry policy for each
   * method in the interface.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param retryPolicy the policy for retrying or failing over method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",35,39,"/**
* Constructs a lossy retry invocation handler with specified parameters.
* @param numToDrop number of invocations to drop during retries
* @param proxyProvider provider for failover proxies
* @param retryPolicy policy for retrying failed invocations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)",79,85,"/**
* Creates a proxy instance for the given interface and implementation.
* @param iface target interface
* @param implementation concrete class to proxy
* @param methodNameToPolicyMap map of method names to retry policies
*/","* Create a proxy for an interface of an implementation class
   * using the a set of retry policies specified by method name.
   * If no retry policy is defined for a method then a default of
   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param <T> T.
   * @param implementation the instance whose methods should be retried
   * @param methodNameToPolicyMap a map of method names to retry policies
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo(),151,159,"/**
* Updates retry counters and invokes failover logic based on retry conditions.
* @param none
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,handleException,"org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)",376,396,"/**
* Retrieves RetryInfo for the specified method invocation.
* @param method target method to invoke
* @param callId unique call identifier
* @param policy retry policy settings
* @param counters statistics counters
* @param expectFailoverCount expected failover count
* @param e exception occurred (if any)
* @return RetryInfo object or throws Exception if not retriable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int),327,340,"/**
* Writes data to multiple output streams while handling exceptions.
* @param d the data to be written
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)",348,361,"/**
* Writes data to multiple output streams concurrently.
* @param bytes data to write
* @param offset starting position in the data array
* @param len number of bytes to write
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,flush,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush(),363,376,"/**
* Iterates over output streams, flushing and handling exceptions.
* @throws IOException if any exception occurs during iteration
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,close,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close(),133,149,"/**
* Handles exceptions from multiple Closeable proxies.
* @throws IOException if any proxy operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)",285,297,"/**
* Initializes a Writer object from an FSDataOutputStream.
* @param fout output stream with desired compression
* @param compressionName name of the compression algorithm
* @param conf configuration settings for this writer
*/","* Constructor
     * 
     * @param fout
     *          FS output stream.
     * @param compressionName
     *          Name of the compression algorithm, which will be used for all
     *          data blocks.
     * @throws IOException
     * @see Compression#getSupportedAlgorithms",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput),809,822,"/**
* Reads and initializes a MetaIndexEntry from input.
* @param in DataInput stream containing index data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput),2060,2068,"/**
* Reads and validates TFile metadata from input stream.
* @param in DataInput stream containing metadata
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput),864,875,"/**
* Initializes DataIndex object from input stream.
* @param in input stream containing data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",2145,2179,"/**
* Initializes a TFileIndex with the specified entry count and reads 
* key entries from the input stream, indexed by record numbers.
* @param entryCount number of index entries
* @param in input stream containing key data
* @param comparator byte comparator to use for comparisons
*/","* For reading from file.
     * 
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,checkEOF,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF(),119,126,"/**
* Checks for function mask presence based on iterative chunk processing.
* @throws IOException on I/O errors
*/","* Check whether we reach the end of the stream.
     * 
     * @return false if the chunk encoded stream has more data to read (in which
     *         case available() will be greater than 0); true otherwise.
     * @throws java.io.IOException
     *           on I/O errors.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flushBuffer,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer(),296,301,"/**
* Resets buffer and clears remaining data.
* @throws IOException if an I/O error occurs
*/","* Flush the internal buffer.
     * 
     * Is this the last call to flushBuffer?
     * 
     * @throws java.io.IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close(),338,348,"/**
* Flushes the buffer to disk, freeing resources.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)",316,330,"/**
* Copies data from input buffer to current buffer,
* handling partial writes and buffer overflow.
* @param b input byte array
* @param off offset in input buffer
* @param len length of data to copy
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput),2272,2290,"/**
* Writes TFileIndex entries to output stream.
* @param out DataOutput stream to write to
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable),1957,1961,"/**
* Computes a mask value using provided RawComparable key.
* @param key input key for computation
* @return integer mask value
*/","* Compare an entry with a RawComparable object. This is useful when
         * Entries are stored in a collection, and we want to compare a user
         * supplied key.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close(),432,473,"/**
* Processes a key with associated metadata and appends to the block writer.
* @throws IOException if incorrect key length or unsorted keys
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(java.lang.String),70,72,"/**
 * Initializes UTF-8 encoded String instance with given input. 
 * @param string input string to be encoded and stored.","* Construct from a given string.
   * @param string input string.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getBytes,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes(),80,83,"/**
* Returns the mask in M1 format.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.lang.String),186,188,"/**
* Computes MD5 hash of input string.
* @param string input string to be hashed
*/","* Construct a hash value for a String.
   * @param string string.
   * @return MD5Hash.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync(),1638,1665,"/**
* Flushes buffered records and updates internal state.
* @throws IOException if an I/O error occurs
*/",Compress and flush contents to dfs,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getTrackingId,org.apache.hadoop.security.token.TokenIdentifier:getTrackingId(),78,83,"/**
* Generates and returns the function mask as a string.
* If trackingId is null, it will be computed using m1().
*/","* Returns a tracking identifier that can be used to associate usages of a
   * token across multiple client sessions.
   *
   * Currently, this function just returns an MD5 of {{@link #getBytes()}.
   *
   * @return tracking identifier",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeToUrlString,org.apache.hadoop.security.token.Token:encodeToUrlString(),373,375,"/**
* Generates function mask using m1 service.
* @throws IOException if an error occurs during execution.","* Encode this token as a url safe string.
   * @return the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,cloneWritableInto,"org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",363,371,"/**
* Copies data from src to dst, using a temporary buffer for intermediate storage.
* @param dst destination writable object
* @param src source writable object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode(),162,166,"/**
* Calls parent class's implementation of m1(). 
* @return result of parent class's m1() method.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,add,org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node),133,169,"/**
* Adds a new node to the network topology.
* @param node Node object to be added
*/","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
                                         or node to be added is not a leaf",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)",497,554,"/**
* Chooses a random datanode for the given scope and excluded scope.
* @param scope target scope
* @param excludedScope scope to exclude from selection
* @param excludedNodes collection of nodes to exclude from selection
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,remove,org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node),221,241,"/**
* Removes a node from the network topology.
* @param node Node to be removed
*/","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,decommissionNode,org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node),1061,1076,"/**
* Decommissions a node, handling inner nodes and network locks.
* @param node Node to be decommissioned
*/","* Update empty rack number when remove a node like decommission.
   * @param node node to be added; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",912,915,"/**
* Invokes recursive node processing with specified parameters.
* @param reader initial node to traverse from
* @param nodes array of nodes to process
* @param activeLen length of active nodes
* @param secondarySort callback for secondary sorting
*/","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * </p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",953,956,"/**
* Performs masked function on a node array with secondary sorting.
* @param reader input node
* @param nodes array of nodes to process
* @param activeLen length of active node range
* @param secondarySort callback for secondary sorting results
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)",72,76,"/**
* Initializes this socket input stream with a readable byte channel and timeout.
* @param channel the underlying channel to read from
* @param timeout the maximum time in milliseconds to wait for data
*/","* Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for reading, should also be a {@link SelectableChannel}.
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)",77,81,"/**
* Initializes SocketOutputStream with a writable channel and timeout.
* @param channel WritableByteChannel for output
* @param timeout connection timeout in milliseconds
*/","* Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for writing, should also be a {@link SelectableChannel}.  
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,createSocket,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket(),184,196,"/**
* Establishes a UDP socket connection to the server.
* @throws IOException if connection setup fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcResponse,"org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)",1566,1598,"/**
* Retrieves a Writable object associated with a Call instance.
* @param call the Call instance
* @param connection the Connection instance
* @param timeout maximum wait time in specified unit
* @param unit TimeUnit for timeout value
* @return Writable object or null if not found or interrupted
*/","@return the rpc response or, in case of timeout, null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(),87,89,"/**
* Constructs a ScriptBasedMapping instance using a RawScriptBasedMapping.
*/","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>(),59,61,"/**
* Constructs ScriptBasedMappingWithDependency instance using RawScriptBasedMappingWithDependency.
*/","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,newInnerNode,org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String),32,35,"/**
 * Creates an InnerNodeImpl instance with the specified path.
 * @param path input path to initialize the node with
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String),306,308,"/**
* Initializes an InnerNodeWithNodeGroup instance with the given file path.
* @param path absolute or relative path to the node group
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode),127,129,"/**
* Constructs an MRNflyNode instance from an existing NflyNode.
* @param n the NflyNode to copy
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,createParentNode,org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String),184,187,"/**
* Creates an inner node with specified mask properties.
* @param parentName name of parent node
*/","* Creates a parent node to be added to the list of children.
   * Creates a node using the InnerNode four argument constructor specifying
   * the name, location, parent, and level of this node.
   *
   * <p>To be overridden in subclasses for specific InnerNode implementations,
   * as alternative to overriding the full {@link #add(Node)} method.
   *
   * @param parentName The name of the parent node
   * @return A new inner node
   * @see InnerNodeImpl(String, String, InnerNode, int)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,add,"org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",304,332,"/**
* Adds a new entry to the processing queue and handles socket closure.
* @param sock DomainSocket object
* @param handler Handler object for error handling
*/","* Add a socket.
   *
   * @param sock     The socket to add.  It is an error to re-add a socket that
   *                   we are already watching.
   * @param handler  The handler to associate with this socket.  This may be
   *                   called any time after this function is called.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,remove,org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket),339,354,"/**
* Removes and processes DomainSocket instance.
* @param sock DomainSocket object to process
*/","* Remove a socket.  Its handler will be called.
   *
   * @param sock     The socket to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,<init>,"org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)",241,259,"/**
 * Initializes a DomainSocketWatcher with the specified interrupt check period and source.
 * @param interruptCheckPeriodMs time interval (in ms) to periodically check for interrupts
 * @param src source identifier used in thread naming
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,select,"org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)",321,376,"/**
* Waits for IO on a selectable channel with specified operations and timeout.
* @param channel the SelectableChannel to wait on
* @param ops the desired operations (e.g. READ, WRITE)
* @param timeout the maximum time in millis to wait
* @return 0 if timed out, or non-zero if an event occurred
*/","* Waits on the channel with the given timeout using one of the 
     * cached selectors. It also removes any cached selectors that are
     * idle for a few seconds.
     * 
     * @param channel
     * @param ops
     * @param timeout
     * @return
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultIP,org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String),224,228,"/**
* Extracts and returns the first IP address from the given interface string.
* @param strInterface network interface or IP address (e.g. ""eth0"" or ""192.168.1.1"")
*/","* Returns the first available IP address associated with the provided
   * network interface or the local host IP if ""default"" is given.
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *             (e.g. eth0 or eth0:0) or the string ""default""
   * @return The IP address in text form, the local host IP is returned
   *         if the interface name ""default"" is specified
   * @throws UnknownHostException
   *             If the given interface is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,org.apache.hadoop.net.DNS:getHosts(java.lang.String),340,343,"/**
* Returns an array of strings based on the given interface string.
* @param strInterface interface string to process
*/","* Returns all the host names associated by the default nameserver with the
   * address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @return The list of host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)",360,374,"/**
* Resolves hostname from interface and/or nameserver.
* @param strInterface network interface
* @param nameserver DNS server to query (null for default)
* @param tryfallbackResolution whether to attempt fallback resolution
* @return resolved hostname or cached value if applicable
*/","* Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            Input tryfallbackResolution.
   * @return The default host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream),134,136,"/**
* Logs usage statistics to the provided output stream.
* @param pStr PrintStream object to write log data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,"org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)",363,385,"/**
* Validates a command line argument.
* @param argv array of command line arguments
* @param helpEntries map of available commands and their usage
* @return true if valid, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,"org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)",512,537,"/**
* Parses command line arguments and prints help if needed.
* @param argv array of command line arguments
* @param helpEntries map of available commands with their info
* @return 0 on success, -1 otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addContext,"org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)",997,1001,"/**
* Configures ServletContextHandler with default contexts and masks.
* @param ctxt ServletContextHandler instance
* @param isFiltered flag indicating if filtering is enabled
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPlugin,org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String),202,216,"/**
* Creates a metrics plugin instance by class name.
* @param name unique plugin identifier
* @return T the created plugin object or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,loadFirst,"org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])",113,142,"/**
* Loads and combines metrics configurations from multiple files.
* @param prefix unique identifier for combined config
* @param fileNames one or more file names to load properties from
* @return combined MetricsConfig object or default config if none found
*/","* Load configuration from a list of files until the first successful load
   * @param conf  the configuration object
   * @param files the list of filenames to try
   * @return  the configuration object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(),282,285,"/**
* Calls recursive implementation of m1 with this instance as parameter.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,putMetrics,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),108,193,"/**
* Processes a metrics record and generates Ganglia output.
* @param record input metrics data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,putMetrics,org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),70,108,"/**
* Sends metrics data to Graphite.
* @param record MetricsRecord object containing data and tags
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/PrometheusServlet.java,doGet,"org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",40,46,"/**
* Processes HTTP request and generates response mask.
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,publishMetricsFromQueue,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue(),128,166,"/**
* Retries queue operations with exponential backoff on exceptions.
* @throws InterruptedException if thread is interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,incrCacheClearedCounter,org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter(),221,223,"/**
* Initializes retry cache metrics.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,requeueCall,org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call),3232,3240,"/**
* Executes m4 and m5 operations for a given Call object.
* @param call the Call object to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,getRecords,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords(),57,66,"/**
* Retrieves metrics records from builders and aggregates them into a list.
* @return List of MetricsRecordImpl objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",62,65,"/**
* Sets a function mask attribute with an Integer value.
* @param info MetricsInfo object
* @param value integer value to be set
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",67,70,"/**
* Updates attribute mask with Long type metric value.
* @param info MetricsInfo object containing metric details
* @param value Long value to update the mask with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",72,75,"/**
* Updates attribute with float value and metadata.
* @param info MetricsInfo object containing metadata
* @param value float value to be assigned
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",77,80,"/**
* Updates function mask attribute with Double value.
* @param info MetricsInfo object containing metadata
* @param value numeric value to be masked as a Double
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",82,85,"/**
* Sets a mask attribute based on an integer value.
* @param info MetricsInfo object
* @param value integer value to set as mask
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",87,90,"/**
* Updates attribute with a Long value.
* @param info MetricsInfo object containing attribute metadata
* @param value Long value to be assigned
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateInfoCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable),243,248,"/**
* Updates the info cache with metrics records.
* @param lastRecs iterable of MetricsRecordImpl objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String),108,111,"/**
* Returns an ObjectName instance representing the FUNC_MASK resource.
* @param name Resource name (must be ""FUNC_MASK"")",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,sourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)",123,126,"/**
* Generates function mask by name and duplicate OK status.
* @param name the name to generate mask for
* @param dupOK whether duplicates are allowed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,load,org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String),342,370,"/**
* Fetches user group masks by ID.
* @param user unique user identifier
* @return Set of group masks or null on failure
*/","* This method will block if a cache entry doesn't exist, and
     * any subsequent requests for the same user will wait on this
     * request to return. If a user already exists in the cache,
     * and when the key expires, the first call to reload the key
     * will block, but subsequent requests will return the old
     * value until the blocking thread returns.
     * If reloadGroupsInBackground is true, then the thread that
     * needs to refresh an expired key will not block either. Instead
     * it will return the old cache value and schedule a background
     * refresh
     * @param user key of cache
     * @return List of groups belonging to user
     * @throws IOException to prevent caching negative entries",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdownSingleton,org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton(),139,141,"/**
* Calls instance-specific method m1 on singleton instance.
*/","* Shutdown the JvmMetrics singleton. This is not necessary if the JVM itself
   * is shutdown, but may be necessary for scenarios where JvmMetrics instance
   * needs to be re-created while the JVM is still around. One such scenario
   * is unit-testing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattach,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach(),151,153,"/**
 * Initializes global metrics with default values using m1() function. 
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,stop,org.apache.hadoop.ipc.Server:stop(),3696,3719,"/**
* Shuts down the server, notifying listeners and cleaning up resources.
*/",Stops the service.  No new calls will be handled after this is called.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stopMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans(),226,231,"/**
* Resets and notifies MBean using MBeans interface.
* @param mbeanName name of the managed bean to reset
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,unregisterSource,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String),896,902,"/**
* Initializes DecayRpcSchedulerMetrics2 with the given namespace.
* @param namespace name of metrics system to initialize
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,create,"org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)",151,159,"/**
* Creates and registers HTTP server 2 metrics with the given handler.
* @param handler StatisticsHandler instance
* @param port port number for the HTTP server
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,stop,org.apache.hadoop.http.HttpServer2:stop(),1559,1609,"/**
* Stops and cleans up resources associated with a web app.
*/","* stop the server.
   *
   * @throws Exception exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",569,581,"/**
* Collects and registers metrics for active and total sources/sinks.
* @param builder MetricsCollector instance
* @param all true to include inactive data, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",87,106,"/**
* Initializes a MutableQuantiles instance with provided name, description, sample name,
* value name, and interval.
* @param name                display name
* @param description         descriptive text
* @param sampleName           sample identifier
* @param valueName            value identifier
* @param interval             calculation interval in seconds
*/","* Instantiates a new {@link MutableQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   * 
   * @param name
   *          of the metric
   * @param description
   *          long-form textual description of the metric
   * @param sampleName
   *          type of items in the stream (e.g., ""Ops"")
   * @param valueName
   *          type of the values
   * @param interval
   *          rollover interval (in seconds) of the estimator",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>(),993,1000,"/**
* Initializes metrics for DelegationTokenSecretManager.
* @param ioStatistics initialized IO statistics
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache),42,48,"/**
* Initializes metrics registry for the given RetryCache instance.
* @param retryCache RetryCache object to initialize metrics for
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)",129,131,"/**
* Recursively fetches metric information.
* @param annotation Metric object to process
* @param field Field object to derive from
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)",137,139,"/**
* Recursively resolves metrics info based on method and its annotations.
* @param annotation metric annotation
* @param method method to resolve metrics for
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",262,269,"/**
* Creates and registers a mutable statistic with the given name and description.
* @param name unique identifier for the statistic
* @param desc human-readable description of the statistic
* @param sampleName optional sample-related identifier
* @param valueName optional value-related identifier
* @param extended whether to enable extended statistics functionality
* @return MutableStat object representing the newly created statistic
*/","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @param extended    produce extended stat (stdev, min/max etc.) if true.
   * @return a new mutable stat metric object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",94,97,"/**
* Constructs a new MutableStat instance with default visibility.
* @param name        Statistic name
* @param description Statistic description
* @param sampleName   Sample name for the statistic
* @param valueName    Value name for the statistic
*/","* Construct a snapshot stat metric with extended stat off by default
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRate.java,<init>,"org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)",31,33,"/**
* Creates a new mutable rate with specified parameters.
* @param name unique identifier
* @param description descriptive text
* @param extended true for extended rate properties
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),180,207,"/**
* Populates MetricsRecordBuilder with GC metrics.
* @param rb MetricsRecordBuilder instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,setContext,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String),147,150,"/**
* Converts string to metrics mask.
* @param value input string value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,setContext,org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String),380,382,"/**
* Retrieves metrics registry mask for the given context and name.
* @param name unique metric identifier
* @return MetricsRegistry object or null if not found
*/","* Set the metrics context tag
   * @param name of the context
   * @return the registry itself as a convenience",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)",403,406,"/**
* Creates or updates metrics registry entry.
* @param name metric name
* @param description metric description
* @param value metric value
* @param override whether to override existing entry
* @return MetricsRegistry object
*/","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @param override  existing tag if true
   * @return the registry (for keep adding tags)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",422,424,"/**
* Wraps call to m1 with default flag set to false.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)",100,114,"/**
* Updates user profile statistics by elapsed time.
* @param name unique user identifier
* @param elapsed elapsed time in milliseconds
*/","* Add a rate sample for a rate metric.
   * @param name of the rate metric
   * @param elapsed time",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)",435,449,"/**
* Processes metric buffer and publishes statistics.
* @param buffer MetricsBuffer object
* @param immediate Whether to process immediately or with logical time
*/","* Publish a metrics snapshot to all the sinks
   * @param buffer  the metrics snapshot to publish
   * @param immediate  indicates that we should publish metrics immediately
   *                   instead of using a separate thread.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,copyTo,org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat),59,61,"/**
 * Applies mask to provided SampleStat instance.
 * @param other SampleStat object to modify
 */","* Copy the values to other (saves object creation and gc.)
   * @param other the destination to hold our values",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,<init>,"org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)",46,53,"/**
* Initializes a MethodMetric with the given object, method, metrics info, and metric type.
* @param obj Object to be measured
* @param method Metric calculation method
* @param info Metrics information
* @param type Type of metric (e.g. execution time)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,toString,org.apache.hadoop.metrics2.lib.MutableStat:toString(),187,190,"/**
* Calls and returns result of m2() from the returned value of m1().
* The returned value is expected to be an object that can call m2().
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logSlowRpcCalls,"org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)",593,615,"/**
* Logs slow RPCs based on performance metrics.
* @param methodName method name
* @param call client call object
* @param details ProcessingDetails object containing timing information
*/","* Logs a Slow RPC Request.
   *
   * @param methodName - RPC Request method name
   * @param details - Processing Detail.
   *
   * If a request took significant more time than other requests,
   * and its processing time is at least `logSlowRPCThresholdMs` we consider that as a slow RPC.
   *
   * The definition rules for calculating whether the current request took too much time
   * compared to other requests are as follows:
   * 3 is a magic number that comes from 3 sigma deviation.
   * A very simple explanation can be found by searching for 68-95-99.7 rule.
   * We flag an RPC as slow RPC if and only if it falls above 99.7% of requests.
   * We start this logic only once we have enough sample size.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,toString,org.apache.hadoop.metrics2.util.SampleQuantiles:toString(),280,288,"/**
* Returns a formatted string representing the quantiles and their corresponding values.
* @return string representation of quantile data or ""[no samples]"" if none exist
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addTopNCallerSummary,org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder),1079,1096,"/**
* Populates metrics record builder with top N caller data and priorities.
* @param rb MetricsRecordBuilder to populate
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),78,83,"/**
* Executes mask operation on network groups.
*/",* Refresh the netgroup cache,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String),1585,1592,"/**
* Retrieves a set of group names for the given user, first attempting to use
* the user-to-groups mapping and falling back to an underlying implementation.
* @param user unique user identifier
* @return Set of group names or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,endln,org.apache.hadoop.security.KDiag:endln(),875,878,"/**
 * Calls function m1 with and without a delimiter string.
 */",* Print something at the end of a section,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,title,"org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])",886,891,"/**
* Formats and logs message with masked arguments.
* @param format formatting string
* @param args variable number of argument values to mask
*/","* Print a title entry.
   *
   * @param format format string
   * @param args any arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,fail,"org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])",942,946,"/**
* Throws a custom exception with formatted error message.
* @param category diagnostic category
* @param message error description
* @param args variable arguments for message formatting
*/","* Format and raise a failure.
   *
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullMaps,org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps(),386,389,"/**
* Synchronizes and executes m1() and m2() methods.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,"org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1451,1462,"/**
* Creates a UserGroupInformation instance with the specified user and authentication method.
* @param user unique user identifier
* @param authMethod authentication method to use
* @return UserGroupInformation object
*/","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @param authMethod authMethod.
   * @return the UserGroupInformation for the remote user.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDefault,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault(),1052,1067,"/**
* Returns the default SSL socket factory instance.
* @return LdapSslSocketFactory object or null if not initialized
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,retrievePassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),548,552,"/**
* Computes mask value for given token.
* @param identifier unique token identifier
* @return raw mask bytes or null on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads(),169,177,"/**
* Initializes expired token removal and starts the daemon thread.
* @throws IOException if an I/O error occurs
*/","* should be called before this object is used.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,rollMasterKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey(),463,476,"/**
* Updates and saves data to the current key.
*/","* Update the current master key for generating delegation tokens 
   * It should be called only by tokenRemoverThread.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(),150,152,"/**
* Constructs an unauthenticated URL with default delegation token.","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   * <p>
   * An instance of the default {@link DelegationTokenAuthenticator} will be
   * used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator),160,163,"/**
* Constructs an authenticated URL using the provided delegationToken authenticator.
* @param authenticator DelegationTokenAuthenticator instance
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator),171,174,"/**
* Constructs an authenticated URL using a delegationToken.
* @param connConfigurator configuration for establishing connections
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code> using the default
   * {@link DelegationTokenAuthenticator} class.
   *
   * @param connConfigurator a connection configurator.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(byte[]),225,228,"/**
 * Calls m1 with default offset and length parameters.
 * @param b input byte array
 */","* Reads up to <code>b.length</code> bytes of data from this input stream into
   * an array of bytes.
   * <p>
   * The <code>read</code> method of <code>InputStream</code> calls the
   * <code>read</code> method of three arguments with the arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         is there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromKeytab,org.apache.hadoop.security.UserGroupInformation:isFromKeytab(),834,838,"/**
* Evaluates mask conditions and checks for valid result.","* Is this user logged in from a keytab file managed by the UGI?
   * @return true if the credentials are from a keytab file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromTicket,org.apache.hadoop.security.UserGroupInformation:isFromTicket(),844,846,"/**
* Evaluates all condition methods and returns true if they pass and m3 is null.
*/","*  Is this user logged in from a ticket (but no keytab) managed by the UGI?
   * @return true if the credentials are from a ticket cache.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,shouldRelogin,org.apache.hadoop.security.UserGroupInformation:shouldRelogin(),869,873,"/**
* Evaluates functional mask condition.
* @return true if both m1 and m2 conditions are met, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Cache$Key:toString(),3915,3918,"/**
* Constructs a URI string from existing components.
* @return URI string in format ""(ugiURI)@scheme://authority""
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation$RealUser:toString(),497,500,"/**
* Returns the value from the real user's M1 data.
* @return Value from RealUser object or null if not set
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeIdle,org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean),4130,4149,"/**
* Closes idle connections based on time and thresholds.
* @param scanAll true to scan all connections, false to stop after first match
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeAll,org.apache.hadoop.ipc.Server$ConnectionManager:closeAll(),4151,4157,"/**
* Executes a database operation on each connection in a set.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeConnection,org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection),3493,3495,"/**
* Applies database mask to specified connection.
* @param connection active database connection
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)",275,278,"/**
* Overloads m1 to accept single-host and SSLSocket parameters.
* @param host the target server hostname
* @param ssl the SSL socket instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeDefaultFactory,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),103,108,"/**
* Initializes or reinitializes the SSL channel factory with a specific mode.
* @param preferredMode desired SSL channel mode
*/","* Initialize a singleton SSL socket factory.
   *
   * @param preferredMode applicable only if the instance is not initialized.
   * @throws IOException if an error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration),78,80,"/**
* Initializes an FsCommand instance with configuration settings.
* @param conf Hadoop Configuration object containing command parameters.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(),45,47,"/**
* Initializes CommandFactory with default settings.
* @see #CommandFactory(Long)
*/",Factory constructor for commands,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(),83,85,"/**
 * Initializes the HarFileSystem instance.
 */",* public construction of harfilesystem,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem),103,106,"/**
 * Initializes a new instance of HarFileSystem with the specified file system.
 * @param fs the underlying file system
 */","* Constructor to create a HarFileSystem with an
   * underlying filesystem.
   * @param fs underlying file system",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(),71,72,"/**
* Initializes an empty filter file system.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem),74,77,"/**
 * Initializes the filter file system with the given file system.
 * @param fs the underlying file system to filter
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(),66,68,"/**
* Constructs an empty FsShell instance.","* Default ctor with no configuration.  Be sure to invoke
   * {@link #setConf(Configuration)} with a valid configuration prior
   * to running commands.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration),43,45,"/**
* Initializes and returns the base groups.
* @param conf configuration object
*/","* Create an instance of this tool using the given configuration.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(),76,79,"/**
 * Initializes console output and error streams.
 */",Constructor,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),39,43,"/**
* Initializes ErasureEncoder with given configuration.
* @param options ErasureCoderOptions containing encoding parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,42,"/**
* Initializes ErasureDecoder with given configuration.
* @param options ErasureCoderOptions containing data and parity unit counts
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,<init>,"org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",48,51,"/**
* Initializes deserializer with configuration and target class.
* @param conf Hadoop Configuration object
* @param c target class for deserialization
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,<init>,org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration),105,107,"/**
 * Initializes configuration for this CLI instance.
 * @param conf configuration to be used
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,org.apache.hadoop.security.KDiag:<init>(),186,187,"/**
* Constructs an empty KDiag instance.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(),99,101,"/**
* Protected constructor to prevent direct instantiation of this class.
* Intended for subclassing only.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String),647,685,"/**
* Resolves host to InetAddress.
* @param host hostname or IP address
*/","* Create an InetAddress with a fully qualified hostname of the given
     * hostname.  InetAddress does not qualify an incomplete hostname that
     * is resolved via the domain search list.
     * {@link InetAddress#getCanonicalHostName()} will fully qualify the
     * hostname, but it always return the A record whereas the given hostname
     * may be a CNAME.
     * 
     * @param host a hostname or ip address
     * @return InetAddress with the fully qualified hostname or ip
     * @throws UnknownHostException if host does not exist",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateStaticMapping,org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping(),304,336,"/**
* Reloads or initializes static UID/GID mapping from file if it exists.
* @throws IOException if file access fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,write,org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput),317,321,"/**
* Writes ACL string to output stream.
* @param out DataOutput stream to write to
*/",* Serializes the AccessControlList object,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper(),752,762,"/**
* Initializes ZooKeeper client with truststore and keystore settings.
* @throws IOException if configuration or initialization fails
*/","* Get a new zookeeper client instance. protected so that test class can
   * inherit and pass in a mock object for zookeeper
   *
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,"org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1334,1345,"/**
* Updates login context with mask functionality.
* @param login HadoopLoginContext object
* @param ignoreLastLoginTime whether to ignore last login time
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)",34,40,"/**
* Creates a CryptoFSDataOutputStream instance wrapping the given FSDataOutputStream.
* @param out underlying output stream
* @param codec encryption codec to use
* @param bufferSize buffer size for encryption
* @param key encryption key
* @param iv initialization vector for encryption
* @param closeOutputStream whether to close the underlying output stream when this is closed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",91,95,"/**
* Initializes a CryptoOutputStream instance with the given parameters.
* @param out OutputStream to write encrypted data to
* @param codec CryptoCodec instance for encryption/decryption
* @param bufferSize Buffer size for optimized writing
* @param key Encryption/decryption key
* @param iv Initialization vector for encryption
* @param streamOffset Initial offset in the stream
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",124,128,"/**
* Initializes OpenSSL CTR cipher with specified mode and engine.
* @param mode encryption mode
* @param suite selected cipher suite
* @param engineId ID of the underlying engine
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String),111,114,"/**
* Returns an instance of OpensslCipher with specified transformation.
* @param transformation cipher transformation string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>(),39,48,"/**
* Initializes the OpensslSm4CtrCryptoCodec instance, verifying OpenSSL and SM4 CTR support.
* @throws RuntimeException if initialization fails due to loading failure or unsupported feature.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersion,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)",157,183,"/**
* Creates an EncryptedKeyVersion object from a map of key metadata.
* @param keyName unique key identifier
* @param valueMap map containing encrypted key data and metadata
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute(),456,474,"/**
* Creates a masked item with specified options.
* @throws IOException, NoSuchAlgorithmException if creation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,createKey,"org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",71,75,"/**
* Delegates M1 operation to underlying key provider.
* @param name input name
* @param options operation options
* @return KeyVersion object or null on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String),148,154,"/**
* Computes and returns a KeyVersion object based on the given name.
* Calls m1() to perform initial computations and m3() for additional processing.
* @return KeyVersion object or throws exception if computation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String),77,81,"/**
* Calls key provider to retrieve a key version by name.
* @param name unique key identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute(),308,328,"/**
* Rolls a key using the provided KeyProvider.
* @throws NoSuchAlgorithmException if algorithm not found
* @throws IOException on failure to access key storage
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getSize,org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String),323,340,"/**
* Retrieves a function mask value based on the provided key name.
* @param keyName unique identifier for the key
* @return non-zero integer value or 0 if not found
*/","* Get size of the Queue for keyName. This is only used in unit tests.
   * @param keyName the key name
   * @return int queue size. Zero means the queue is empty or the key does not exist.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getAtMost,"org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)",354,399,"/**
* Retrieves 'num' values of type E from a queue for the given keyName.
* @param keyName unique identifier
* @param num number of values to fetch
* @return List<E> of fetched values or null if not found
*/","* This removes the ""num"" values currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist
   * How many values are actually returned is governed by the
   * <code>SyncGenerationPolicy</code> specified by the user.
   * @param keyName String key name
   * @param num Minimum number of values to return.
   * @return {@literal List<E>} values returned
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException execution exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,drain,org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String),302,316,"/**
* Removes and processes tasks from queue for given key.
* @param keyName unique identifier
*/","* Drains the Queue for the provided key.
   *
   * @param keyName the key to drain the Queue for",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])",57,68,"/**
* Issues a refresh request to the server.
* @param identifier unique identifier
* @param args optional arguments array
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)",341,345,"/**
* Creates a cache entry with payload and expiration time.
* @param payload object to be cached
* @param expirationTime duration until cache item expires
* @param clientId unique client identifier
* @param callId unique function invocation ID
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)",161,165,"/**
* Creates a cache entry with additional payload.
* @param clientId unique client identifier
* @param callId unique call identifier
* @param payload arbitrary object to store as payload
* @param expirationTime timestamp for cache entry expiration
* @param success flag indicating cache entry validity
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object),289,299,"/**
* Handles event E based on conditions and delegates to other methods accordingly.
* @param e the incoming event
*/","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object),301,304,"/**
* Checks if an element matches the function mask.
* @param e Element to check
* @return True if the element matches, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,"org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)",3111,3141,"/**
* Enqueues a call to the queue, optionally blocking until space is available.
* @param call RPC call object
* @param blocking whether to block if queue is full
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,ensureInitialized,org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized(),71,75,"/**
* Initializes mask functionality if not already done.
* @throws Exception if initialization fails 
*/",* Initialize this class if it isn't already.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,get,org.apache.hadoop.util.LightWeightCache:get(java.lang.Object),186,199,"/**
* Retrieves and updates cache entry for given key.
* @param key unique identifier
* @return cached E object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,close,org.apache.hadoop.util.StopWatch:close(),116,121,"/**
* Performs function mask operation when the system is started.
* @implSpec This method should only be called after the system has been initialized.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit),97,100,"/**
* Converts this duration to the specified unit.
* @param timeUnit target time unit (e.g. NANOSECONDS, MICROSECONDS)
*/","* now.
   *
   * @param timeUnit timeUnit.
   * @return current elapsed time in specified timeunit.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,toString,org.apache.hadoop.util.StopWatch:toString(),111,114,"/**
* Generates a function mask string by concatenating m1 and m2 substrings.
* @return concatenated string or null if either substring is null
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])",104,120,"/**
* Initializes invocation with method details and parameters.
* @param method Method object to invoke
* @param parameters Array of parameter values for the method call
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)",210,223,"/**
* Generates a protocol signature based on client methods hash code and server version.
* @param clientMethodsHashCode unique hash of client methods
* @param serverVersion current server version
* @param protocol protocol class to generate signature for
* @return ProtocolSignature object or null if mismatched
*/","* Get a server protocol's signature
   * 
   * @param clientMethodsHashCode client protocol methods hashcode
   * @param serverVersion server protocol version
   * @param protocol protocol
   * @return the server's protocol signature",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)",225,229,"/**
* Generates a protocol signature based on name and version.
* @param protocolName unique protocol identifier
* @param version protocol version number
* @return ProtocolSignature object or throws ClassNotFoundException if protocol not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)",697,750,"/**
* Determines the retry action based on the exception type and policy conditions.
* @param e the Exception to evaluate
* @param retries total number of allowed retries
* @param failovers total number of allowed failovers
* @param isIdempotentOrAtMostOnce whether the invoked method is idempotent or at-most-once
* @return RetryAction object indicating the decision and next retry/failover count",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,shouldRetry,"org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)",98,129,"/**
* Determines retry policy based on exception type and returns a RetryAction.
* @param e the exception to determine policy for
* @param retries number of allowed retries
* @param failovers number of allowed failovers
* @param isMethodIdempotent whether method can be safely retried
* @return RetryAction object or null if not applicable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,doHealthChecks,org.apache.hadoop.ha.HealthMonitor:doHealthChecks(),195,227,"/**
* Periodically checks the health of a monitored service and updates its status.
* @throws InterruptedException if thread is interrupted while waiting for service check
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(),980,983,"/**
* Initializes an invalid call with default values.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addResponseTime,"org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",256,258,"/**
* Calls scheduler's m1 method with provided parameters.
* @param name user-provided string
* @param e Schedulable object
* @param details ProcessingDetails object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAccept,org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey),1616,1639,"/**
* Processes server socket channel and establishes connections.
* @param key SelectionKey object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable),194,213,"/**
* Handles function mask for the given event.
* @param e event to process
* @return true if successful, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable),215,222,"/**
 * Executes the masked function on a task with the specified priority level.
 * @param e Task object
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$RpcCall:run(),1234,1272,"/**
* Processes the RPC request and generates a response.
* @throws Exception if processing fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredError,org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable),1367,1391,"/**
* Handles uncaught exceptions by sending a deferred error response.
* @param t the exception or null if no exception was thrown
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersions,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)",44,68,"/**
* Retrieves protocol versions for each kind, aggregating versions into a single response.
* @param controller RPC controller
* @param request GetProtocolVersions request
* @return GetProtocolVersionsResponseProto object or throws ServiceException if error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,fetchServerMethods,org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method),57,78,"/**
* Validates and fetches server methods based on client version.
* @param method Method object to validate
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolImpl,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)",510,527,"/**
* Retrieves the ProtoClassProtoImpl instance for a given protocol name and version.
* @param server RPC.Server instance
* @param protoName protocol name to fetch implementation for
* @param clientVersion client-side protocol version
* @return ProtoClassProtoImpl instance or null if not found; throws exceptions otherwise.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)",1990,1992,"/**
* Constructs a FatalRpcServerException with specified error code and message.
* @param errCode RPC error code
* @param message human-readable error description
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(int),37,39,"/**
 * Initializes a new ResponseBuffer with specified capacity.
 * @param capacity maximum size of buffer in bytes.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run(),1115,1150,"/**
* Continuously sends RPC requests to the queue and processes responses until connection should close.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,decayCurrentCosts,org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts(),479,540,"/**
* Decays current costs and updates totals.
* @throws Exception if an error occurs during decay
*/","* Decay the stored costs for each user and clean as necessary.
   * This method should be called periodically in order to keep
   * costs current.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),677,684,"/**
* Calculates functional mask value based on schedulable object properties.
* @param obj Schedulable object to process
* @return Functional mask value as integer
*/","* Compute the appropriate priority for a schedulable based on past requests.
   * @param obj the schedulable obj to query and remember
   * @return the level index which we recommend scheduling in",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),686,691,"/**
* Calculates function mask based on user group information.
* @param ugi UserGroupInformation object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setPriorityLevel,"org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",732,735,"/**
* Calls M1 on the call queue with given user group information and priority.
* @param ugi UserGroupInformation object
* @param priority integer priority value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke(),161,163,"/**
* Creates a CallReturn instance with result of m1() function.
* @throws Throwable any exception thrown by m1()
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,getValue,org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object),190,192,"/**
* Applies RPC operation to mask input value.
* @param value input value of type T
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message),405,410,"/**
* Processes and logs a Message by calling remote RPC service.
* @param message input message to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message),437,442,"/**
* Processes message and logs execution time.
* @param message input Message object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForWritable,"org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3564,3580,"/**
* Serializes RPC response header and data to a byte array.
* @param header RpcResponseHeaderProto object
* @param rv Writable object or null
* @return serialized byte array or null on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,removeNextElement,org.apache.hadoop.ipc.FairCallQueue:removeNextElement(),165,178,"/**
* Retrieves an element from the queue based on a priority mask.
* @return E object or null if not found
*/","* Returns an element first non-empty queue equal to the priority returned
   * by the multiplexer or scans from highest to lowest priority queue.
   *
   * Caller must always acquire a semaphore permit before invoking.
   *
   * @return the first non-empty queue with less priority, or null if
   * everything was empty",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$Connection:close(),1266,1304,"/**
* Closes IPC connection and performs cleanup.
* @throws IOException if unexpected closure occurs
*/",Close the connection.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Responder:doRunLoop(),1727,1796,"/**
* Performs periodic cleanup and response checking for RPC calls.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall),3062,3064,"/**
* Processes RpcCall with m1 function from responder.
* @param call RpcCall object to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",56,63,"/**
* Initializes a Globber instance with the specified file system, 
* pattern to match files, and optional filter.
* @param fs File system object
* @param pathPattern Pattern to match files (e.g., directory glob)
* @param filter Optional filter for matching files
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",81,91,"/**
* Initializes a Globber instance to recursively collect files matching 
* the specified pattern and filter.
* @param fs FileSystem instance
* @param pathPattern Path pattern to match against
* @param filter Optional PathFilter for additional filtering
* @param resolveSymlinks Whether to resolve symbolic links
*/","* Filesystem constructor for use by {@link GlobBuilder}.
   * @param fs filesystem
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.lang.String),73,75,"/**
 * Initializes a new MachineList instance from given host entries.
 * @param hostEntries string containing machine host entries
 */","* 
   * @param hostEntries comma separated ip/cidr/host addresses",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,<init>,org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String),52,65,"/**
* Initializes FileBasedIPList from a file.
* @param fileName path to IP list file
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfo.java,newInstance,org.apache.hadoop.util.SysInfo:newInstance(),36,44,"/**
* Returns platform-specific system information based on the operating system.
* @return SysInfo object for Linux or Windows, or throws exception otherwise
*/","* Return default OS instance.
   * @throws UnsupportedOperationException If cannot determine OS.
   * @return Default instance for the detected OS.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize(),594,600,"/**
* Calculates and returns the function mask value.
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize(),619,622,"/**
* Calculates the function mask value.
* @return Total free swap size in bytes plus result of m1()
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,<init>,org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream),58,64,"/**
* Constructs FSDataInputStream from a given InputStream, enforcing Seekable and PositionedReadable compliance.
* @param in input stream to be wrapped
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",196,212,"/**
* Fetches a M3-encoded data block from the input stream.
* @param bufferPool ByteBuffer pool to allocate buffers from
* @param maxLength maximum length of the encoded data block
* @param opts Read options for the encoded data block
* @return M3-encoded data block as a ByteBuffer or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,put,org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object),91,96,"/**
* Calls superclass's version of m1 with given element and returns result.
* @param element object to be processed by superclass
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet$SetIterator:remove(),346,357,"/**
* Removes the current element from the set, throwing an exception if none exists.
* @throws IllegalStateException if there is no current element to remove
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,remove,org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object),103,106,"/**
* Calls superclass's m1 method with given key.
* @param key unique identifier of type K
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evict,org.apache.hadoop.util.LightWeightCache:evict(),155,161,"/**
* Polls a value from the queue and removes it using FUNC_MASK operation.
* @return polled element of type E or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,transform,"org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)",79,95,"/**
* Applies XSLT transformation to XML data and writes result to output stream.
* @param styleSheet InputStream containing XSL stylesheet
* @param xml InputStream containing XML data
* @param out Writer for transformed XML output
*/","* Transform input xml given a stylesheet.
   * 
   * @param styleSheet the style-sheet
   * @param xml input xml data
   * @param out output
   * @throws TransformerConfigurationException synopsis signals a problem
   *         creating a transformer object.
   * @throws TransformerException this is used for throwing processor
   *          exceptions before the processing has started.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)",446,469,"/**
* Generates a string mask based on options and storage types.
* @param qOption query option
* @param hOption human-readable option
* @param tOption truncate option (if true, returns result of m1)
* @param xOption exclude snapshot values
* @param types list of storage types
*/","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   * if hOption is false, file sizes are returned in bytes
   * if hOption is true, file sizes are returned in human readable
   * if tOption is true, display the quota by storage types
   * if tOption is false, same logic with #toString(boolean,boolean)
   * if xOption is false, output includes the calculation from snapshots
   * if xOption is true, output excludes the calculation from snapshots
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @param types Storage types to display
   * @return the string representation of the object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toSnapshot,org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean),498,503,"/**
* Formats a string with snapshot details based on the given option.
* @param hOption boolean indicating whether to include human-readable format
*/","* Return the string representation of the snapshot counts in the output
   * format.
   * @param hOption flag indicating human readable or not
   * @return String representation of the snapshot counts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getQuotaUsage,org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean),329,346,"/**
* Formats disk usage quotas and remaning values.
* @param hOption boolean flag for high option
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getTypesQuotaUsage,"org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)",348,366,"/**
* Generates a formatted string summarizing storage types and quotas.
* @param hOption boolean flag for human-readable format
* @param types list of StorageType objects to process
* @return a formatted string or null if an error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)",374,377,"/**
* Calculates a value based on the given percentage and map name.
* @param percentage percentage value
* @param mapName map identifier
*/","* Let t = percentage of max memory.
   * Let e = round(log_2 t).
   * Then, we choose capacity = 2^e/(size of reference),
   * unless it is outside the close interval [1, 2^30].
   *
   * @param mapName mapName.
   * @param percentage percentage.
   * @return compute capacity.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,fill,org.apache.hadoop.fs.FSInputChecker:fill(),217,222,"/**
* Updates internal position and chunk count based on masking operation.
* @throws IOException if read operation fails
*/","* Fills the buffer with a chunk data. 
   * No mark is supported.
   * This method assumes that all data in the buffer has already been read in,
   * hence pos > count.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readAndDiscard,org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int),231,245,"/**
* Calculates the total length of data available in chunks.
* @return Total length
*/","* Like read(byte[], int, int), but does not provide a dest buffer,
   * so the read data is discarded.
   * @param      len maximum number of bytes to read.
   * @return     the number of bytes read.
   * @throws     IOException  if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toString,org.apache.hadoop.io.UTF8:toString(),163,175,"/**
* Processes input bytes and populates a StringBuilder.
* @return the populated StringBuilder as a String
*/",Convert to a String.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toStringChecked,org.apache.hadoop.io.UTF8:toStringChecked(),183,190,"/**
* Generates a function mask by synchronizing with the IBuffer and performing two operations.
*/","* Convert to a string, checking for valid UTF8.
   * @return the converted string
   * @throws UTFDataFormatException if the underlying bytes contain invalid
   * UTF8 data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,fromBytes,org.apache.hadoop.io.UTF8:fromBytes(byte[]),257,263,"/**
* Packs byte array into a human-readable mask string.
* @param bytes input byte array
*/","* @return Convert a UTF-8 encoded byte array back into a string.
   *
   * @param bytes input bytes.
   * @throws IOException if the byte array is invalid UTF8",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readString,org.apache.hadoop.io.UTF8:readString(java.io.DataInput),272,277,"/**
* Extracts and formats a string from DataInput stream.
* @param in input data stream
* @return formatted string or null on error
*/","* @return Read a UTF-8 encoded string.
   *
   * @see DataInput#readUTF()
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkResponse,org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto),252,267,"/**
* Verifies and extracts client ID from RpcResponseHeaderProto.
* @param header Response header to check
*/",Check the rpc response header.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,toString,org.apache.hadoop.ipc.Client:toString(),1353,1357,"/**
* Generates mask string by concatenating M1 and M2 values with client ID.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte),210,212,"/**
* Wraps single byte in an array for consistency with other methods.
* @param b single byte to be wrapped
*/","* Convert a byte to a hex string.
   * @see #byteToHexString(byte[])
   * @see #byteToHexString(byte[], int, int)
   * @param b byte
   * @return byte's hex value as a String",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,toString,org.apache.hadoop.ha.ActiveStandbyElector:toString(),1274,1280,"/**
* Generates a function mask string with elector ID, app data, and callback details.
*@return Function mask string in the format ""elector id=... appData=...""",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,uncaughtException,"org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)",779,783,"/**
* Handles uncaught exceptions by logging and exiting the thread.
* @param thread Thread object that encountered the exception
* @param exception Throwable instance of the caught exception
*/","* Handler for uncaught exceptions: terminate the service.
   * @param thread thread
   * @param exception exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithUsageMessage,org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage(),1033,1035,"/**
 * Displays exit usage message with error details.
 */","* Exit with the usage exit code {@link #EXIT_USAGE}
   * and message {@link #USAGE_MESSAGE}.
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,uncaughtException,"org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)",83,127,"/**
* Handles exceptions thrown by threads during shutdown.
* @param thread the throwing thread
* @param exception the thrown exception or error
*/","* Uncaught exception handler.
   * If an error is raised: shutdown
   * The state of the system is unknown at this point -attempting
   * a clean shutdown is dangerous. Instead: exit
   * @param thread thread that failed
   * @param exception the raised exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,"org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)",856,858,"/**
* Logs error with specified exit code and message.
* @param exitCode system exit code
* @param message error description
*/","* Exit the JVM.
   *
   * This is method can be overridden for testing, throwing an 
   * exception instead. Any subclassed method MUST raise an 
   * {@code ExitException} instance/subclass.
   * The service launcher code assumes that after this method is invoked,
   * no other code in the same method is called.
   * @param exitCode code to exit
   * @param message input message.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(int),368,370,"/**
* Throws exit exception with custom message.
* @param status integer status code
* @throws ExitException if exception occurs
*/","* Like {@link #terminate(int, Throwable)} without a message.
   *
   * @param status exit code
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,terminate,"org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)",121,124,"/**
* Logs and exits with specified status and message.
* @param status system exit code
* @param msg error message to be logged
*/","* Prints a message to stderr and exits with a status code.
   *
   * @param status exit code
   * @param msg message",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,interrupted,org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData),103,135,"/**
* Handles service interruption by ID, escalating to JVM halt on repeated interrupts.
* @param interruptData Interrupt data containing service ID
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(int),389,391,"/**
 * Handles halt exception with custom message.
 * @param status integer status code
 * @throws HaltException if an error occurs
 */","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",58,61,"/**
* Wraps calls to overloaded m1() variant with default value.
* @param s IndexedSortable object
* @param p int parameter
* @param r int parameter
*/","* Sort the given range of items using quick sort.
   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},
   * then switch to {@link HeapSort}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,<init>,"org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)",216,223,"/**
* Constructs an AclStatus object from given parameters.
* @param owner file owner
* @param group file group
* @param stickyBit whether sticky bit is set
* @param entries list of AclEntry objects
* @param permission FsPermission value for the file
*/","* Private constructor.
   *
   * @param file Path file associated to this ACL
   * @param owner String file owner
   * @param group String file group
   * @param stickyBit the sticky bit
   * @param entries the ACL entries
   * @param permission permission of the path",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseACLs,org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String),95,122,"/**
* Parses ACL string into list of ACL objects.
* @param aclString comma-separated ACLs in scheme:id:perm format
* @return List of ACL objects or empty list if input is null
*/","* Parse comma separated list of ACL entries to secure generated nodes, e.g.
   * <code>sasl:hdfs/host1@MY.DOMAIN:cdrwa,sasl:hdfs/host2@MY.DOMAIN:cdrwa</code>
   *
   * @param aclString aclString.
   * @return ACL list
   * @throws BadAclFormatException if an ACL is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseAuth,org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String),133,154,"/**
* Parses auth string into list of ZKAuthInfo objects.
* @param authString comma-separated auth strings in scheme:auth format
*/","* Parse a comma-separated list of authentication mechanisms. Each
   * such mechanism should be of the form 'scheme:auth' -- the same
   * syntax used for the 'addAuth' command in the ZK CLI.
   * 
   * @param authString the comma-separated auth mechanisms
   * @return a list of parsed authentications
   * @throws BadAuthFormatException if the auth format is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,preserveAttributes,"org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)",445,490,"/**
* Copies file metadata from source to target while preserving specified attributes.
* @param src PathData containing source file
* @param target PathData containing target file
* @param preserveRawXAttrs whether to preserve raw xattrs or only those specified in FileAttribute.XATTR
*/","* Preserve the attributes of the source to the target.
   * The method calls {@link #shouldPreserve(FileAttribute)} to check what
   * attribute to preserve.
   * @param src source to preserve
   * @param target where to preserve attributes
   * @param preserveRawXAttrs true if raw.* xattrs should be preserved
   * @throws IOException if fails to preserve attributes",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,add,org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object),132,146,"/**
* Adds an element to the list and potentially resizes chunks.
* @param e added element
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getMinimalAcl,org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission),99,116,"/**
* Transforms FsPermission into a list of AclEntries for ACCESS scope.
* @param perm FsPermission object to transform
* @return List of AclEntry objects representing permission types (USER, GROUP, OTHER)
*/","* Translates the given permission bits to the equivalent minimal ACL.
   *
   * @param perm FsPermission to translate
   * @return List&lt;AclEntry&gt; containing exactly 3 entries representing the
   *         owner, group and other permissions",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)",461,468,"/**
* Resolves duration tracker based on provided key and count.
* @param key unique identifier
* @param count long value for tracking purposes
*/","* If the store is tracking the given key, return the
   * duration tracker for it. If not tracked, return the
   * stub tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return a tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)",58,62,"/**
* Constructs StatisticDurationTracker with default duration count.
* @param iostats IO statistics store instance
* @param key statistic key identifier
*/","* Constructor -increments the counter by 1.
   * @param iostats statistics to update
   * @param key prefix of values.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])",57,59,"/**
* Constructs DurationInfo instance with logging.
* @param log Logger instance
* @param format duration format string
*/","* Create the duration text from a {@code String.format()} code call;
   * log output at info level.
   * @param log log to write to
   * @param format format string
   * @param args list of arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture),116,126,"/**
* Waits for the asynchronous operation to complete and handles exceptions.
*@param future CompletableFuture containing the result or exception
*/","* Wait for a single of future to complete, extracting IOEs afterwards.
   *
   * @param <T> Generics Type T.
   * @param future future to wait for.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletionIgnoringExceptions,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture),133,143,"/**
* Waits for and ignores the result of a CompletableFuture.
* @param future nullable CompletableFuture to be ignored
*/","* Wait for a single of future to complete, ignoring exceptions raised.
   * @param future future to wait for.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,toString,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString(),107,112,"/**
* Returns formatted string with duration and failure status.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,toString,org.apache.hadoop.util.DurationInfo:toString(),89,92,"/**
* Combines result of base class's m2() with this class's own m2().
* @return concatenated string or null if base class result is null
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newCrcComposer,"org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)",60,64,"/**
* Creates CRC composer with specified parameters.
* @param type checksum type
* @param bytesPerCrcHint hint for CRC computation rate
*/","* Returns a CrcComposer which will collapse all ingested CRCs into a single
   * value.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @throws IOException raised on errors performing I/O.
   * @return a CrcComposer which will collapse all ingested CRCs into a single value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(int,long)",167,192,"/**
* Updates the composite CRC and advances position in stripe.
* @param crcB new CRC byte
* @param bytesPerCrc CRC byte count per update
*/","* Updates with a single additional CRC which corresponds to an underlying
   * data size of {@code bytesPerCrc}.
   *
   * @param crcB crcB.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lock,org.apache.hadoop.util.InstrumentedLock:lock(),101,107,"/**
* Executes critical section of code with timing measurements.
* @param waitStart initial timer value
* @param currentTime current timer value
* @param isLast flag indicating last function call in sequence
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lockInterruptibly,org.apache.hadoop.util.InstrumentedLock:lockInterruptibly(),109,115,"/**
* Executes critical operations while holding the lock and measures execution time.
* @throws InterruptedException if interrupted while waiting
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,"org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)",126,136,"/**
* Acquires lock and performs action within specified time.
* @param time duration to wait for lock acquisition
* @param unit time unit (e.g. seconds)
* @return true if lock acquired and action performed, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,unlock,org.apache.hadoop.util.InstrumentedLock:unlock(),138,144,"/**
* Acquires and releases a lock while measuring execution time.
* @param acquireTime timestamp of lock acquisition
* @param releaseTime timestamp of lock release
* @param isSuccessful whether the operation was successful
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)",375,379,"/**
* Recursively formats and returns a timestamp string.
* @param dateFormat date format to apply
* @param finishTime end time in milliseconds
* @param startTime start time in milliseconds
*/","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   *
   * @param dateFormat date format to use
   * @param finishTime finish time
   * @param startTime  start time
   * @return formatted value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,org.apache.hadoop.util.StringUtils:escapeString(java.lang.String),665,667,"/**
* Escapes and joins input string with comma separator.
* @param str input string to process
*/","* Escape commas in the string using the default escape char
   * @param str a string
   * @return an escaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String),723,725,"/**
* Converts input string to CSV format with specified escape and separator characters.
* @param str input string
*/","* Unescape commas in the string using the default escape char
   * @param str a string
   * @return an unescaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,startupShutdownMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)",1010,1016,"/**
* Generates function mask by combining class name and arguments.
* @param classname Java class name
* @param args list of argument values
*/","* @return Build a log message for starting up and shutting down.
   * @param classname the class of the server
   * @param args arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext(),489,499,"/**
* Retrieves the function mask.
* @throws IOException if an I/O error occurs
*/","* Get the next source value.
     * This calls {@link #sourceHasNext()} first to verify
     * that there is data.
     * @return the next value
     * @throws IOException failure
     * @throws NoSuchElementException no more data",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext(),790,793,"/**
* Combines the results of continuing work and superclass operation.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future),65,69,"/**
* Wraps FutureIO's read operation on a future object.
* @param future future object to be read from
* @return the result of reading from the future or null if interrupted
* @throws InterruptedIOException if IO is interrupted
* @throws IOException if an I/O error occurs
* @throws RuntimeException if any other exception occurs
*/","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection),162,169,"/**
* Combines the results of asynchronous operations into a single list.
* @param collection collection of futures containing operation results
* @return List of T objects, or throws an exception if any operation fails","* Evaluates a collection of futures and returns their results as a list.
   * <p>
   * This method blocks until all futures in the collection have completed.
   * If any future throws an exception during its execution, this method
   * extracts and rethrows that exception.
   * </p>
   * @param collection collection of futures to be evaluated
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,"org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",86,93,"/**
* Wraps a {@link Future} in a deprecated interface for legacy compatibility.
* @param future the asynchronous result to retrieve
* @param timeout maximum time to wait for completion (in specified {@code unit})
* @param unit time unit for {@code timeout}
* @return the underlying result or throws an exception if not completed within the deadline
*/","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @param timeout timeout.
   * @param unit unit.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,"org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)",188,197,"/**
* Retrieves results from a collection of futures with timeout.
* @param collection Collection of asynchronous tasks
* @param duration Time to wait for each task to complete
* @return List of task results or throws an exception if timed out
*/","* Evaluates a collection of futures and returns their results as a list,
   * but only waits up to the specified timeout for each future to complete.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first. If any future throws an
   * exception during its execution, this method extracts and rethrows that exception.
   * @param collection collection of futures to be evaluated
   * @param duration timeout duration
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,cancelAllFuturesAndAwaitCompletion,"org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)",211,239,"/**
* Executes and gathers results from a collection of futures with timeout.
* @param collection Collection of asynchronous tasks
* @param interruptIfRunning If true, interrupts running futures on timeout
* @param duration Timeout duration in milliseconds
* @return List of task results or null if all tasks cancelled
*/","* Cancels a collection of futures and awaits the specified duration for their completion.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first.
   * All exceptions thrown by the futures are ignored. as is any TimeoutException.
   * @param collection collection of futures to be evaluated
   * @param interruptIfRunning should the cancel interrupt any active futures?
   * @param duration total timeout duration
   * @param <T> type of the result.
   * @return all futures which completed successfully.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])",138,161,"/**
* Instantiates class with given arguments using cached constructor.
* @param theClass Class to instantiate
* @param conf Configuration object
* @param argTypes Constructor parameter types
* @param values Arguments to pass to constructor
* @return Instantiated object of type T or null if failed
*/","Create an object for the given class and initialize it from conf
   *
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param argTypes the types of the arguments
   * @param values the values of the arguments
   * @param <T> Generics Type.
   * @return a new object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getKeyClass,org.apache.hadoop.io.SequenceFile$Reader:getKeyClass(),2196,2205,"/**
* Retrieves or initializes the key class based on configuration.
* @return The loaded Class object, or null if initialization fails
*/",@return Returns the class of keys in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getValueClass,org.apache.hadoop.io.SequenceFile$Reader:getValueClass(),2213,2222,"/**
* Retrieves and caches the class associated with a mask.
* @return The Class<?> object, or null if not initialized
*/",@return Returns the class of values in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,readFields,org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput),117,133,"/**
* Reads and initializes an EnumSet from the input stream.
* @param in DataInput stream containing EnumSet data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadClass,org.apache.hadoop.util.FindClass:loadClass(java.lang.String),244,259,"/**
* Loads and initializes a class by its name.
* @return int indicating success or error code
*/","* Loads the class of the given name
   * @param name classname
   * @return outcome code",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,createClassInstance,org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String),278,302,"/**
* Creates an instance of a class by its name and returns success code.
* @return int success or error code
*/","* Create an instance of a class
   * @param name classname
   * @return the outcome",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path),115,117,"/**
* Initializes an instance of FSBuilder with the specified file system path.
* @param path the file system path to initialize the builder with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle),119,121,"/**
 * Initializes the builder with a file system handle.
 * @param pathHandle unique identifier for the file system
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandler:<init>(),42,44,"/**
 * Initializes an FsUrlStreamHandler with default configuration. 
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createConfiguration,org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration(),399,401,"/**
* Creates a new instance of Configuration.","* Override point: create the base configuration for the service.
   *
   * Subclasses can override to create HDFS/YARN configurations etc.
   * @return the configuration to use as the service initializer.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,loadConf,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf(),437,449,"/**
* Creates a configuration object. If supplied, uses provided configuration; otherwise creates a new instance.
*/","* Return the supplied configuration for testing or otherwise load a new
   * configuration.
   *
   * @return the configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(),123,125,"/**
 * Initializes FindClass with default configuration.","* Empty constructor; passes a new Configuration
   * object instance to its superclass's constructor",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(),75,77,"/**
* Initializes ReconfigurableBase with default configuration.
*/",* Construct a ReconfigurableBase.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration),84,86,"/**
* Initializes ReconfigurableBase with a given configuration.
* @param conf configuration object or null to use default configuration
*/","* Construct a ReconfigurableBase with the {@link Configuration}
   * conf.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)",135,150,"/**
* Creates a data checksum object based on the specified type and byte count.
* @param type checksum type (NULL, CRC32, or CRC32C)
* @param bytesPerChecksum number of bytes to include in checksum
* @return DataChecksum object or null for invalid input
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinPath,org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String),701,704,"/**
* Generates a mask string from an executable file.
* @param executable path to executable file
*/","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException on path canonicalization failures",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,runCommand,org.apache.hadoop.util.Shell:runCommand(),967,1098,"/**
* Executes a shell command with optional redirection of output and error streams.
* @param dir working directory for the command
* @param redirectErrorStream whether to redirect error stream to output
*/","* Run the command.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(java.lang.String),61,65,"/**
* Creates and configures progress instance based on given status.
* @param status current progress status
*/","* Adds a named node to the tree.
   * @param status status.
   * @return Progress.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String),361,363,"/**
 * Calls m1 with provided path and no filter.
 * @param path directory to process
 */","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @throws Exception If it cannot create the file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invoke,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])",77,82,"/**
* Returns a masked result of type R, applying method m2 to the provided arguments.
* @param target ignored, but must be null (asserts if not)
* @param args variable number of arguments for method m2
* @return Object of type R or its supertype, or null if m2 returns null",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,pathCapabilities_hasPathCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",349,356,"/**
* Checks if the file system has a specific capability.
*@param fs the file system object
*@param path the directory path
*@param capability the capability to check for
*@return true if the capability is present, false otherwise
*/","* Does a path have a given capability?
   * Calls {@code PathCapabilities#hasPathCapability(Path, String)},
   * mapping IOExceptions to false.
   * @param fs filesystem
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return true if the capability is supported
   * under that part of the FS
   * false if the method is not loaded or the path lacks the capability.
   * @throws IllegalArgumentException invalid arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,streamCapabilities_hasCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",367,372,"/**
* Checks if an object has a specific capability.
* @param object Object to check
* @param capability Capability name to look for
* @return true if the object has the capability, false otherwise
*/","* Does an object implement {@code StreamCapabilities} and, if so,
   * what is the result of the probe for the capability?
   * Calls {@code StreamCapabilities#hasCapability(String)},
   * @param object object to probe
   * @param capability capability string
   * @return true iff the object implements StreamCapabilities and the capability is
   * declared available.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable),611,614,"/**
* Retrieves statistics counters map from IostatisticsCountersMethod.
* @param source input data (not used in this implementation)
*/","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable),621,625,"/**
* Converts input data to map of string-long pairs.
* @param source input data (type not specified in comment)
*/","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable),632,635,"/**
* Calculates minimum statistics mask from given data.
* @param source input data to process
*/","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable),642,645,"/**
* Returns map of statistics maximums from iostatisticsMaximumsMethod.
* @param source input data to process
*/","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable),654,657,"/**
* Computes mask statistics from given data.
* @param source input data to process
*/","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[]),219,221,"/**
* Invokes an instance method on a target object using varargs.
* @param args variable arguments to pass to the method
* @return result of the invoked method
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[]),106,109,"/**
* Invokes function with provided arguments and masking logic.
* @param args variable number of arguments to pass to the function
*/","* Invoke a static method.
     * @param args arguments.
     * @return result.
     * @param <R> type of result.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[]),202,204,"/**
* Invokes method m1 with receiver and variable arguments.
* @param args variable number of objects to be passed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])",308,311,"/**
* Adds class to be processed by builder.
* @param className class name
* @param varargs classes to process
*/","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",411,414,"/**
* Configures builder with class name and argument classes.
* @param className target class name
* @param argClasses variable number of argument classes
*/","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadStaticMethod,"org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",139,149,"/**
* Creates a dynamic unbound method instance.
* @param source class containing the method
* @param returnType expected return type of the method
* @param name method name to find and validate
* @param parameterTypes variable number of parameter types
* @return UnboundMethod object or null if not found/invalid
*/","* Load a static method from the source class, which will be a noop() if
   * the class is null or the method isn't found.
   * If the class and method are not found, then an {@code IllegalStateException}
   * is raised on the basis that this means that the binding class is broken,
   * rather than missing/out of date.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or a no-op.
   * @throws IllegalStateException if the method is not static.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSource,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object),394,397,"/**
* Checks if an object meets the FUNC_MASK condition.
* @param object Object to check
*/","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatistics,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object),405,408,"/**
* Checks if the given object matches the function mask criteria.
* @param object the object to check
* @return true if object meets the mask requirements, false otherwise
*/","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable),416,419,"/**
* Checks whether an object has a valid mask.
* @param object Serializable object to check
* @return true if the object has a valid mask, false otherwise
*/","* Probe for an object being an instance of {@code IOStatisticsSnapshot}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled(),427,430,"/**
* Evaluates condition based on m1 and context-enabled m2 result. 
* @return true if both conditions are met, false otherwise
*/","* Probe to check if the thread-level IO statistics enabled.
   * If the relevant classes and methods were not found, returns false
   * @return true if the IOStatisticsContext API was found
   * and is enabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,toString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString(),671,677,"/**
* Returns a string representation of the DynamicWrappedStatistics object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",263,268,"/**
* Calculates the mask value for a given file system and path.
* @param fileSystem the file system to operate on
* @param path the path within the file system
* @return the calculated mask value
*/","* Get the maximum number of objects/files to delete in a single request.
   * @param fileSystem filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws IOException problems resolving paths
   * @throws RuntimeException invocation failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",293,299,"/**
* Retrieves a list of file system entries for the given paths.
* @param fs FileSystem instance
* @param base Base path for filtering
* @param paths Collection of Path objects to process
* @return List of Map.Entry objects containing Path and String values
*/","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other than
   *          ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException if a path argument is invalid.
   * @throws IOException IO problems including networking, authentication and more.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",323,335,"/**
* Opens a file in the specified file system with the given policy and options.
* @param fs target file system
* @param path file location
* @param policy access control policy
* @param status optional file status (null if unknown)
* @param length optional file length (null if unknown)
* @param options additional file system options (null if none)","* OpenFile assistant, easy reflection-based access to
   * {@code FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",412,419,"/**
* Reads and processes input stream to fill buffer at specified position.
* @param in InputStream to read from
* @param position starting position for reading
* @param buf ByteBuffer to be filled with data
*/","* Delegate to {@code ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * @throws UnsupportedOperationException if the input doesn't implement
   * the interface or, if when invoked, it is raised.
   * Note: that is the default behaviour of {@code FSDataInputStream#readFully(long, ByteBuffer)}.
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable(),376,378,"/**
* Creates an iostatistics snapshot using the specified method. 
* @param iostatisticsSnapshotCreateMethod method to use for creating the snapshot
*/","* Require a IOStatistics to be available.
   * @throws UnsupportedOperationException if the method was not found.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable(),384,386,"/**
* Applies mask to iostatistics context. 
* @param iostatisticsContextEnabledMethod context to be masked
*/","* Require IOStatisticsContext methods to be available.
   * @throws UnsupportedOperationException if the classes/methods were not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,<init>,org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String),355,358,"/**
* Initializes a new ComparableVersion instance from a string representation.
* @param version string in ""major.minor.patch"" format
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)",112,118,"/**
* Initializes a LightWeightCache instance with specified parameters.
* @param recommendedLength recommended cache length
* @param sizeLimit maximum cache size
* @param creationExpirationPeriod period for item expiration after creation
* @param accessExpirationPeriod period for item expiration after last access
*/","* @param recommendedLength Recommended size of the internal array.
   * @param sizeLimit the limit of the size of the cache.
   *            The limit is disabled if it is &lt;= 0.
   * @param creationExpirationPeriod the time period C &gt; 0 in nanoseconds
   *            that the creation of an entry is expired if it is added to the
   *            cache longer than C.
   * @param accessExpirationPeriod the time period A &gt;= 0 in nanoseconds that
   *            the access of an entry is expired if it is not accessed
   *            longer than A.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object),250,254,"/**
* Delegates call to internal set's method 1.
* @param o arbitrary object to be passed through
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMap,"org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)",123,128,"/**
* Applies mask operation to file contents.
* @param type operation type
* @param filename input file path
* @param map configuration settings
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)",236,259,"/**
* Updates host details by reading and processing include/exclude lists from files.
* @param inFileInputStream InputStream for include list file (optional)
* @param exFileInputStream InputStream for exclude list file (optional)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,<init>,"org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)",102,107,"/**
* Initializes a Filter object with specified parameters.
* @param vectorSize size of the hashing vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function (implementation-specific)
*/","* Constructor.
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash functions to consider.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,readFields,org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput),204,218,"/**
* Initializes mask settings from input stream.
* @param in DataInput object containing mask configuration
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,delete,org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key),135,160,"/**
* Updates the bucket mask for a given key.
* @param key unique identifier
*/","* Removes a specified key from <i>this</i> counting Bloom filter.
   * <p>
   * <b>Invariant</b>: nothing happens if the specified key does not belong to <i>this</i> counter Bloom filter.
   * @param key The key to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),175,188,"/**
* Recursively searches a matrix of Key objects for the given key.
* @param key Key object to search for
* @return true if found, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection),156,164,"/**
* Recursively processes each Key in the given collection.
* @param coll Collection of Keys to process
*/","* Adds a collection of false positive information to <i>this</i> retouched Bloom filter.
   * @param coll The collection of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List),170,178,"/**
* Recursively processes each Key in the provided list.
* @param keys List of Key objects to process
*/","* Adds a list of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The list of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[]),184,192,"/**
* Recursively processes an array of Key objects.
* @param keys array of Key objects to process
*/","* Adds an array of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The array of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,clearBit,org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int),313,344,"/**
* Updates the key and fingerprint vectors at specified index.
* @param index position in vector to update
*/","* Clears a specified bit in the bit vector and keeps up-to-date the KeyList vectors.
   * @param index The position of the bit to clear.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,ratioRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[]),294,307,"/**
* Finds the index of the hash value with the minimum ratio.
* @param h array of hash values
*/","* Chooses the bit position that minimizes the number of false negative generated while maximizing.
   * the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated while maximizing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,driver,org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[]),155,159,"/**
* Executes function m1 and handles its result.
* @param argv array of command-line arguments
*/","* API compatible with Hadoop 1.x.
   *
   * @param argv argv.
   * @throws Throwable Anything thrown
   *                   by the example program's main",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String),130,132,"/**
* Creates a new Builder instance with the specified title.
* @param title document title
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)",134,136,"/**
 * Constructs a builder instance with the given title and justification.
 * @param title the title of the object being built
 * @param justification the reason for building the object
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)",138,140,"/**
* Creates a new builder instance with specified title and wrapping behavior.
* @param title document title
* @param wrap whether to enable text wrapping
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getCredentialEntry,"org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)",2439,2469,"/**
* Retrieves CredentialEntry by name, handling aliasing and deprecated keys.
* @param provider credential provider instance
* @param name unique credential name or alias
* @return CredentialEntry object or null if not found
*/","* Get the credential entry by name from a credential provider.
   *
   * Handle key deprecation.
   *
   * @param provider a credential provider
   * @param name alias of the credential
   * @return the credential entry or null if not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResource,"org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)",3102,3147,"/**
* Retrieves a Resource object for the given properties and wrapper.
* @param properties configuration data
* @param wrapper resource container
* @param quiet suppress errors if resource not found
* @return Resource object or null on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)",594,600,"/**
* Marks deprecated keys and updates custom message.
* @param key key to be deprecated
* @param newKeys replacement keys
* @param customMessage updated deprecation message
*/","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   *
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   * 
   * @param key to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @param customMessage depcrication message
   * @deprecated use {@link #addDeprecation(String key, String newKey,
      String customMessage)} instead",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parseNext,org.apache.hadoop.conf.Configuration$Parser:parseNext(),3448,3466,"/**
* Handles incoming XML events and performs corresponding actions.
* @throws IOException if an I/O error occurs
* @throws XMLStreamException if a parsing error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,openListeners,org.apache.hadoop.http.HttpServer2:openListeners(),1537,1552,"/**
* Iterates over listeners and triggers corresponding actions based on their status.
* @throws Exception if an error occurs during processing
*/","* Open the main listener for the server
   * @throws Exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,checkArgs,org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String),73,78,"/**
* Parses and validates fencing configuration string.
* @param argStr input configuration string
* @throws BadFencingConfigurationException on parsing error
*/","* Verify that the argument, if given, in the conf is parseable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])",505,507,"/**
* Creates a new Command Line instance with given command name and arguments.
* @param cmdName unique command identifier
* @param opts options to configure the command line
* @param argv array of argument values
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,clearParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode(),407,427,"/**
* Recursively deletes a ZooKeeper node and its children.
*/","* Clear all of the state held within the parent ZNode.
   * This recursively deletes everything within the znode as well as the
   * parent znode itself. It should only be used when it's certain that
   * no electors are currently participating in the election.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fenceOldActive,org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive(),1016,1047,"/**
* Fetches and processes ZK data for fencing an active session.
* @return Stat object or null if no old node found
*/","* If there is a breadcrumb node indicating that another node may need
   * fencing, try to fence that node.
   * @return the Stat of the breadcrumb node that was read, or null
   * if no breadcrumb node existed",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",1082,1091,"/**
* Executes M2 operation on ZooKeeper client with path and ACL.
* @param path ZK path to operate on
* @param data byte array for M2 operation
* @param acl list of access control entries (ACLs)
* @param mode create mode for the operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)",1093,1101,"/**
* Executes ZK action to mask a file at the specified path.
* @param path file system path
* @param watch flag to enable file watching
* @param stat file status object
* @return byte array result or throws exception on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)",1103,1111,"/**
* Performs theFUNC_MASK operation on a ZooKeeper node.
* @param path node path
* @param data byte array data (not used in this implementation)
* @param version node version (not used in this implementation)
* @return Stat object representing the resulting node state
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,deleteWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)",1113,1122,"/**
* Executes a ZooKeeper operation with the specified path and version.
* @param path path to operate on
* @param version version number
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readRangeFrom,"org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)",117,142,"/**
* Reads data from the specified stream into a ByteBuffer within the given range.
* @param stream input stream
* @param range file range to read from
* @param allocate callback for allocating buffer memory
* @return CompletableFuture containing the read ByteBuffer or null on failure
*/","* Synchronously reads a range from the stream dealing with the combinations
   * of ByteBuffers buffers and PositionedReadable streams.
   * @param stream the stream to read from
   * @param range the range to read
   * @param allocate the function to allocate ByteBuffers
   * @return the CompletableFuture that contains the read data or an exception.
   * @throws IllegalArgumentException the range is invalid other than by offset or being null.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData),435,482,"/**
* Processes BufferData by caching or executing operations.
* @param data input buffer data
*/","* Requests that the given block should be copied to the local cache.
   * The block must not be accessed by the caller after calling this method
   * because it will released asynchronously relative to the caller.
   *
   * @throws IllegalArgumentException if data is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setPrefetch,org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future),181,186,"/**
* Sets the action future and updates internal state.
* @param actionFuture Future object containing asynchronous operation
*/","* Indicates that a prefetch operation is in progress.
   *
   * @param actionFuture the {@code Future} of a prefetch action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setReady,org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),209,218,"/**
* Updates buffer and checksum based on expected current state.
* @param expectedCurrentState array of states to update to
*/","* Marks the completion of reading data into the buffer.
   * The buffer cannot be modified once in this state.
   *
   * @param expectedCurrentState the collection of states from which transition to READY is allowed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getSize,org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int),154,164,"/**
* Calculates the mask value for a given block number.
* @param blockNumber unique block identifier
*/","* Gets the size of the given block.
   * @param blockNumber the id of the desired block.
   * @return the size of the given block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getRelativeOffset,"org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)",194,198,"/**
* Calculates function mask value based on block number and offset.
* @param blockNumber unique block identifier
* @param offset memory address offset
* @return int function mask value
*/","* Gets the relative offset corresponding to the given block and the absolute offset.
   * @param blockNumber the id of the given block.
   * @param offset absolute offset in the file.
   * @return the relative offset corresponding to the given block and the absolute offset.
   * @throws IllegalArgumentException if either blockNumber or offset is invalid.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStateString,org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString(),225,241,"/**
* Generates a string representation of functional blocks.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)",75,95,"/**
* Initializes BlockData object with file size and block size parameters.
* @param fileSize total file size in bytes
* @param blockSize fixed block size in bytes
*/","* Constructs an instance of {@link BlockData}.
   * @param fileSize the size of a file.
   * @param blockSize the file is divided into blocks of this size.
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,blockNumber,org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber(),194,197,"/**
* Returns mask value based on buffer data.
* @return integer mask value
*/","* Gets the id of the current block.
   *
   * @return the id of the current block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run(),3834,3841,"/**
* Closes all cache files and handles any IO exceptions.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(),3790,3792,"/**
* Calls m1 with default flag value (false).
* @throws IOException if an I/O error occurs
*/","* Close all FileSystems in the cache, whether they are marked for
     * automatic closing or not.
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAllForUGI,org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation),653,657,"/**
* Closes all resources and caches user-specific information.
* @param ugi UserGroupInformation object to process
*/","* Close all cached FileSystem instances for a given UGI.
   * Be sure those filesystems are not used anymore.
   * @param ugi user group info to close
   * @throws IOException a problem arose closing one or more filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])",135,139,"/**
* Convenience wrapper to invoke m1 with default offset and length.
* @param position starting file position
* @param buffer data buffer
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)",120,123,"/**
* Delegates file reading to underlying FSInputStream.
* @param position current file position
* @param buffer data buffer
* @param offset buffer offset
* @param length number of bytes to read
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2063,2066,"/**
* Wraps the provided file context and path with a character set.
* @param fileContext underlying file system context
* @param path file or directory path
* @param charseq character encoding to use (default is UTF-8)
*/","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fileContext the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,createFile,org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path),1305,1308,"/**
* Wraps FSDataOutputStream creation with Fluent API.
* @param path file system path
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])",1851,1862,"/**
* Writes byte array to M5 file system, creating file if it doesn't exist.
* @param fs M5 file system instance
* @param path file path
* @param bytes data to write
* @return the same file system instance
*/","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1910,1928,"/**
* Writes a list of strings to the specified file system path.
* @param fs target file system
* @param path file path to write to
* @param lines iterable of strings to write
* @param cs character set for encoding
* @return the modified file system
*/","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",1983,1997,"/**
* Writes CharSequence to a file with specified charset and returns the FileSystem.
* @param fs FileSystem instance
* @param path Path to write to
* @param charseq CharSequence to write
* @param cs Charset for encoding
*/","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createFile,org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path),699,702,"/**
* Wraps the given file system operation with FSDataOutputStreamBuilder.
* @param path file system path to operate on
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,appendFile,org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path),1310,1313,"/**
* Wraps FSDataOutputStream creation with the given Path.
*@param path file system location to create output at 
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,appendFile,org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path),704,707,"/**
 * Wraps the file system's {@code m1} operation to create an output stream builder.
 * @param path file system path
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",150,153,"/**
* Constructs a new BlockLocation object.
* @param names array of block names
* @param hosts array of hostnames for the blocks
* @param topologyPaths array of topology paths (optional)
* @param offset block start offset in bytes
* @param length block size in bytes
* @param corrupt indicates if the block is corrupted
*/","* Constructor with host, name, network topology, offset, length 
   * and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(),60,64,"/**
* Retrieves FTP server defaults. 
* @return FtpConfigKeys object containing default values or null if unavailable
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path),66,69,"/**
* Returns FTP server defaults based on pre-configured values.
* @param f ignored (no effect in this implementation)
* @return FsServerDefaults object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path),66,70,"/**
* Returns FsServer Defaults based on configuration key.
* @return FsServerDefaults object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(),72,76,"/**
* Returns server defaults using deprecated configuration key.
* @deprecated use alternative methods instead
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(),1135,1139,"/**
 * Returns server defaults as per local configuration keys.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1141,1144,"/**
* Returns default FS server configuration.
* @return FsServerDefaults object with default settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(),292,296,"/**
* Retrieves FsServerDefaults instance from LocalConfigKeys.
* @throws IOException if configuration retrieval fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path),298,307,"/**
* Resolves file system server defaults for the given path.
* @param f Path to resolve
* @return FsServerDefaults object or default values if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)",93,95,"/**
* Wraps an integer value in a MutableCounter object.
* @param name counter description
* @param desc counter description details
* @param iVal initial integer value
* @return MutableCounterInt instance with wrapped value
*/","* Create a mutable integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)",117,119,"/**
* Wraps immutable counter with initial value into mutable counter.
* @param name and @param desc are used as key for interned string
* @param iVal is the initial value of the counter
*/","* Create a mutable long integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)",166,168,"/**
* Creates a new mutable gauge with initial value.
* @param name gauge name
* @param desc gauge description
* @param iVal initial gauge value
*/","* Create a mutable long integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)",190,192,"/**
* Creates and returns a MutableGaugeFloat instance with the given value.
* @param name gauge name
* @param desc gauge description
* @param iVal initial gauge value
*/","* Create a mutable float gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)",142,144,"/**
* Creates a mutable gauge with initial value and description.
* @param name unique identifier
* @param desc descriptive text
* @param iVal initial integer value
*/","* Create a mutable integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkCalls,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls(),126,143,"/**
* Calculates the minimum wait time for pending asynchronous calls.
* @return minimum wait time in milliseconds or -1 if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedStringArray,"org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])",149,158,"/**
* Writes an array of strings to the output stream using variable-length integer encoding.
* @param out DataOutput stream
* @param s array of string values
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,copy,org.apache.hadoop.fs.statistics.MeanStatistic:copy(),281,283,"/**
* Creates a MeanStatistic instance with this object as input.
*/","* Create a copy of this instance.
   * @return copy.
   *",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString(),297,302,"/**
* Returns function mask string based on input source.
* @return function mask or NULL_SOURCE if source is null.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)",227,238,"/**
* Logs a message with optional statistics.
* @param log Logger instance
* @param message Log message to display
* @param source Source object for logging context
*/","* Extract any statistics from the source and log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param log log to log to
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,toString,org.apache.hadoop.fs.FSInputStream:toString(),148,158,"/**
* Formats the superclass's output with additional statistics.
* @return Formatted string or null if not found
*/","* toString method returns the superclass toString, but if the subclass
   * implements {@link IOStatisticsSource} then those statistics are
   * extracted and included in the output.
   * That is: statistics of subclasses are automatically reported.
   * @return a string value.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object),324,328,"/**
* Converts IOStatistics to string representation.
* @param statistics nullable IOStatistics object
* @return string representation or empty string if null
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance; may be null
   * @return string value or the empty string if null",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,measureDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",484,507,"/**
* Calculates and returns a duration based on the provided factory, statistic, and IOE.
* @param factory DurationTrackerFactory instance
* @param statistic statistical string
* @param input InvocationRaisingIOE object
* @return calculated Duration object
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic,
   * returning the measured duration.
   *
   * {@link #trackDurationOfInvocation(DurationTrackerFactory, String, InvocationRaisingIOE)}
   * with the duration returned for logging etc.; added as a new
   * method to avoid linking problems with any code calling the existing
   * method.
   *
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @return the duration of the operation, as measured by the duration tracker.
   * @throws IOException IO failure.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfSupplier,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)",642,663,"/**
* Executes a supplier function with masked duration tracking.
* @param factory DurationTrackerFactory instance
* @param statistic tracked statistic name
* @return result of input supplier or null if not found
*/","* Given a Java supplier, evaluate it while
   * tracking the duration of the operation and success/failure.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the output of the supplier.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListAndEvictIfRequired,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),421,439,"/**
* Evicts oldest entry from cache when max block count is reached.
* @param entry Entry object to manage
*/","* Add the given entry to the head of the linked list and if the LRU cache size
   * exceeds the max limit, evict tail of the LRU linked list.
   *
   * @param entry Block entry to add.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)",92,140,"/**
* Initializes IO statistics store with dynamic counters, gauges,
* minimums, maximums and mean statistics.
* @param counters list of counter keys
* @param gauges list of gauge keys
* @param minimums list of minimum value keys
* @param maximums list of maximum value keys
* @param meanStatistics list of mean statistic function keys
*/","* Constructor invoked via the builder.
   * @param counters keys to use for the counter statistics.
   * @param gauges names of gauges
   * @param minimums names of minimums
   * @param maximums names of maximums
   * @param meanStatistics names of mean statistics.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)",162,220,"/**
* Reads and decrypts data from input buffer.
* @param b input byte array
* @param off starting offset in bytes
* @param len length of data to read
* @return number of bytes read or -1 if error
*/","* Decryption is buffer based.
   * If there is data in {@link #outBuffer}, then read it out of this buffer.
   * If there is no data in {@link #outBuffer}, then read more from the 
   * underlying stream and do the decryption.
   * @param b the buffer into which the decrypted data is read.
   * @param off the buffer offset.
   * @param len the maximum number of decrypted data bytes to read.
   * @return int the total number of decrypted data bytes read into the buffer.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)",395,425,"/**
* Decrypts and writes data from buffer to offset using provided decryptor.
* @param position current position in the stream
* @param buffer input data array
* @param offset starting write position
* @param length amount of data to process
*/","* Decrypt length bytes in buffer starting at offset. Output is also put 
   * into buffer starting at offset. It is thread-safe.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)",456,499,"/**
* Decrypts and writes encrypted data to the provided ByteBuffer.
* @param filePosition starting position of the data
* @param buf output buffer
* @param length total length of the data
* @param start offset in the output buffer
*/","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns. This method is thread-safe.
   *
   * <p>
   *   This method decrypts the input buf chunk-by-chunk and writes the
   *   decrypted output back into the input buf. It uses two local buffers
   *   taken from the {@link #bufferPool} to assist in this process: one is
   *   designated as the input buffer and it stores a single chunk of the
   *   given buf, the other is designated as the output buffer, which stores
   *   the output of decrypting the input buffer. Both buffers are of size
   *   {@link #bufferSize}.
   * </p>
   *
   * <p>
   *   Decryption is done by using a {@link Decryptor} and the
   *   {@link #decrypt(Decryptor, ByteBuffer, ByteBuffer, byte)} method. Once
   *   the decrypted data is written into the output buffer, is is copied back
   *   into buf. Both buffers are returned back into the pool once the entire
   *   buf is decrypted.
   * </p>
   *
   * @param filePosition the current position of the file being read
   * @param buf the {@link ByteBuffer} to decrypt
   * @param length the number of bytes in {@code buf} to decrypt
   * @param start the position in {@code buf} to start decrypting data from",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)",649,670,"/**
* Decrypts data from ByteBuffer using a decryptor.
* @param buf input ByteBuffer
* @param length total bytes to process
* @param start starting offset in ByteBuffer
*/","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns.
   *
   * @see #decrypt(long, ByteBuffer, int, int)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",124,140,"/**
* Initializes a CryptoInputStream with encryption/decryption settings.
* @param in input stream to encrypt/decrypt
* @param codec crypto codec to use
* @param bufferSize buffer size for read/write operations
* @param key encryption key
* @param iv initialization vector
* @param streamOffset starting offset of the encrypted stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seek,org.apache.hadoop.crypto.CryptoInputStream:seek(long),523,546,"/**
* Seeks to a specific position in the input stream.
* @param pos target position
* @throws IOException if seeking fails or is unsupported
*/",Seek to a position.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,skip,org.apache.hadoop.crypto.CryptoInputStream:skip(long),549,577,"/**
* Skips bytes from the input stream and updates output buffer.
* @param n number of bytes to skip
* @return actual number of bytes skipped
*/",Skip n bytes,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seekToNewSource,org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long),693,705,"/**
* Seeks to a specific position in the input stream.
* @param targetPos desired offset (must be non-negative)
* @return true if successful, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,org.apache.hadoop.crypto.CryptoOutputStream:write(int),271,275,"/**
* Invokes recursive processing with a single byte buffer.
* @param b the input byte value
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,close,org.apache.hadoop.crypto.CryptoOutputStream:close(),238,256,"/**
* Closes the output stream, invoking parent and codec close methods as needed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hflush,org.apache.hadoop.crypto.CryptoOutputStream:hflush(),294,300,"/**
* Calls m1 and recursively invokes m2 on syncable output streams.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hsync,org.apache.hadoop.crypto.CryptoOutputStream:hsync(),302,308,"/**
* Calls m1(), then invokes m2() on any attached Syncable output. 
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/GlobFilter.java,compile,org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String),36,39,"/**
* Returns a compiled glob pattern from the given string.
* @param s input string to compile into a glob pattern
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String),49,51,"/**
* Initializes GlobFilter with specified file pattern.
* @param filePattern glob pattern to filter files
*/","* Creates a glob filter with the specified file pattern.
   *
   * @param filePattern the file pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,"org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)",60,62,"/**
* Initializes a glob filter with the given pattern and path filter.
* @param filePattern glob pattern to match files
* @param filter path filter for further filtering
*/","* Creates a glob filter with the specified file pattern and an user filter.
   *
   * @param filePattern the file pattern.
   * @param filter user filter in addition to the glob pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)",985,1003,"/**
* Creates directory and extracts tarball archive to specified path.
* @param inputStream input stream containing tarball data
* @param untarDir target directory for extraction
* @param gzipped whether the tarball is gzipped or not
*/","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inputStream The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @param gzipped The input stream is gzipped
   *                TODO Use magic number and PusbackInputStream to identify
   * @throws IOException an exception occurred
   * @throws InterruptedException command interrupted
   * @throws ExecutionException task submit failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStatistics,org.apache.hadoop.fs.FileContext:getAllStatistics(),2420,2422,"/**
* Retrieves statistics by URI, delegating to AbstractFileSystem.
* @return map of URIs to corresponding Statistics objects
*/","* @return Map of uri and statistics for each filesystem instantiated. The uri
   *         consists of scheme and authority for the filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,clearStatistics,org.apache.hadoop.fs.FileContext:clearStatistics(),2404,2406,"/**
* Calls AbstractFileSystem's m1() method.
*/","* Clears all the statistics stored in AbstractFileSystem, for all the file
   * systems.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",551,559,"/**
* Delegates file creation to the underlying filesystem.
* @param f target file path
* @param ... see superclass method for parameter details
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)",977,988,"/**
* Initializes InternalDirOfViewFs object with provided parameters.
* @param dir inode directory of view file system
* @param cTime creation time
* @param ugi user group information
* @param uri URI of view file system
* @param fsState inode tree state of file system
* @param conf configuration settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,<init>,org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),63,66,"/**
* Initializes custom file system filter with provided AbstractFileSystem instance.
* @param fs AbstractFileSystem object to wrap
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBacksFromInput,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",64,120,"/**
* Generates parity units and piggyback data from input buffers.
* @param inputs input ByteBuffer arrays
* @param piggyBackIndex indices for generating piggyback data
* @param numParityUnits number of parity units to generate
* @param pgIndex index for generating piggyback data
* @param encoder RawErasureEncoder instance
* @return array of generated piggyback data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",146,150,"/**
* Recursively processes input and output chunks.
* @param inputs array of input chunks
* @param outputs array of output chunks
*/","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object),163,165,"/**
* Initializes internal state with given object value.
* @param value object to be stored internally
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquireHelper,"org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)",161,187,"/**
* Retrieves or creates BufferData for a given block number.
* @param blockNumber unique block identifier
* @param canBlock whether to allow blocking operations
* @return BufferData object or null if creation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable(),300,303,"/**
* Calls m1() and then delegates to pool's m2(), returning result.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newInstance,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)",122,148,"/**
* Creates a blocking thread pool with customizable parameters.
* @param activeTasks initial number of threads
* @param waitingTasks target number of idle threads
* @param keepAliveTime time to maintain threads for reuse
* @param unit time unit for keep alive time
* @param prefixName custom executor name prefix
* @return BlockingThreadPoolExecutorService instance
*/","* A thread pool that that blocks clients submitting additional tasks if
   * there are already {@code activeTasks} running threads and {@code
   * waitingTasks} tasks waiting in its queue.
   *
   * @param activeTasks maximum number of active tasks
   * @param waitingTasks maximum number of waiting tasks
   * @param keepAliveTime time until threads are cleaned up in {@code unit}
   * @param unit time unit
   * @param prefixName prefix of name for threads
   * @return BlockingThreadPoolExecutorService.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setData,"org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)",109,128,"/**
* Updates internal state with masked buffer data.
* @param bufferData BufferData object
* @param startOffset starting offset of mask operation
* @param readOffset current read offset
*/","* Associates a buffer with this file.
   *
   * @param bufferData the buffer associated with this file.
   * @param startOffset Start offset of the buffer relative to the start of a file.
   * @param readOffset Offset where reading starts relative to the start of a file.
   *
   * @throws IllegalArgumentException if bufferData is null.
   * @throws IllegalArgumentException if startOffset is negative.
   * @throws IllegalArgumentException if readOffset is negative.
   * @throws IllegalArgumentException if readOffset is outside the range [startOffset, buffer end].",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent(),261,263,"/**
* Returns a mask value using the m1 function.
* @return result of m1 function call
*/","* Get the context's {@link IOStatisticsContext} which
   * implements {@link IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@link IOStatisticsContext}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset(),287,289,"/**
* Calls m1() and subsequently calls its m2() method.","* Reset the context's IOStatistics.
   * {@link IOStatisticsContext#reset()}",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot(),296,298,"/**
 * Returns a serialized mask value by executing nested functions.
 */","* Take a snapshot of the context IOStatistics.
   * {@link IOStatisticsContext#snapshot()}
   * @return an instance of {@link IOStatisticsSnapshot}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),308,316,"/**
* Checks for valid data in the provided source object.
* @param source input object to process
* @return true if valid data found, false otherwise
*/","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,runParallel,org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task),385,519,"/**
* Executes a task for each item in the iterator, handling failures and retries.
* @throws E any exception thrown by the task
* @return true if all tasks succeeded, false otherwise
*/","* Parallel execution.
     * All tasks run within the same IOStatisticsContext as the
     * thread calling this method.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),270,273,"/**
* Sets IO statistics context using provided value.
* @param statisticsContext null or valid IOStatisticsContext object
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,setStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext(),524,528,"/**
* Updates IO statistics context using mask operation. 
* @param ioStatisticsContext Context to update, or null to skip
*/",* Set the statistics context for this thread.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,resetStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext(),535,539,"/**
 * Updates IO statistics context mask.
 * @param ioStatisticsContext non-null IOStatisticsContext instance
 */","* Reset the statistics context if it was set earlier.
     * This unbinds the current thread from any statistics
     * context.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processPath,org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData),394,401,"/**
* Skips processing PathData item based on condition m1.m2().
* If skipped, calls m3() to process the item.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,postProcessPath,org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData),403,410,"/**
* Filters out PathData items based on custom condition.
* @param item PathData object to filter
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList),174,178,"/**
* Calls the superclass's m2 method with the given arguments after modifying them by adding ""-r"".",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList),411,416,"/**
* Calls superclass's m2 with modified argument list by prepending ""-R"".
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList),236,240,"/**
* Calls superclass method with modified argument list.
* @param args collection of strings to modify and pass up
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception),474,493,"/**
* Handles and logs an exception, potentially re-throwing a command interrupt.
* @param e the Exception to handle
*/","* Display an exception prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param e exception to display",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getPathHandle,"org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1050,1057,"/**
* Returns a path handle with functional bits masked.
* @param stat file status
* @param opt optional handle options
* @return PathHandle object
*/","* Create a durable, serializable handle to the referent of the given
   * entity.
   * @param stat Referent in the target FileSystem
   * @param opt If absent, assume {@link HandleOpt#path()}.
   * @throws IllegalArgumentException If the FileStatus does not belong to
   *         this FileSystem
   * @throws UnsupportedOperationException If {@link #createPathHandle}
   *         not overridden by subclass.
   * @throws UnsupportedOperationException If this FileSystem cannot enforce
   *         the specified constraints.
   * @return path handle.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)",1227,1230,"/**
* Constructs a ShellCommandExecutor instance with specified execution string, directory, environment variables, and timeout.
* @param execString array of strings representing the command to execute
* @param dir directory from which to run the command
* @param env map of environment variables to set for the command
* @param timeout maximum time in milliseconds to wait for command completion
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,init,org.apache.hadoop.fs.CachingGetSpaceUsed:init(),90,102,"/**
* Resets function mask state and refreshes data as needed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addToken,"org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",121,137,"/**
* Updates tokens in the map based on the given alias and token.
* @param alias Text identifier for the update
* @param t Token to apply to matching aliases
*/","* Add a token in the storage (in memory).
   * @param alias the alias for the key
   * @param t the token object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",148,152,"/**
* Processes input and output buffers using m1 function.
* @param inBuffer input data buffer
* @param outBuffer output data buffer
*/","* AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to encrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",158,162,"/**
 * Processes input and output buffers using function M1.
 * @param inBuffer input data stream
 * @param outBuffer output data stream
 */","*  AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to decrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initialize,org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize(),86,96,"/**
* Initializes mount point and destination path mapping.
* @throws IOException if pattern syntax exception occurs
*/","* Initialize regex mount point.
   *
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",149,164,"/**
* Resolves a child path relative to the provided parent path.
* @param parent parent directory path
* @param child child file or directory path
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,org.apache.hadoop.fs.Path:<init>(java.lang.String),184,223,"/**
* Initializes a Path object from a string representation.
* @param pathString string to parse (e.g. ""/path/to/resource"")
*/","* Construct a path from a String.  Path strings are URIs, but with
   * unescaped elements and some additional normalization.
   *
   * @param pathString the path string",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)",241,256,"/**
* Initializes a new Path object from scheme, authority, and path components.
* @param scheme URI scheme (e.g., ""http"")
* @param authority host and port of the URI
* @param path URI path (may be modified to avoid scheme conflicts)
*/","* Construct a Path from components.
   *
   * @param scheme the scheme
   * @param authority the authority
   * @param path the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toString,org.apache.hadoop.fs.shell.PathData:toString(),461,464,"/**
* Returns the function mask based on URI and scheme.
* @return function mask string
*/","* Returns the printable version of the path that is either the path
   * as given on the commandline, or the full path
   * @return String of the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotRelative,org.apache.hadoop.fs.Path:checkNotRelative(),92,96,"/**
* Validates path flags and throws exception if invalid.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUriPath,org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path),423,431,"/**
* Extracts the filename mask from a Path object.
* @param p Path to extract filename mask from
* @return Filename mask string or throws exception if invalid
*/","* Get the path-part of a pathname. Checks that URI matches this file system
   * and that the path-part is a valid name.
   * 
   * @param p path
   * 
   * @return path-part of the Path p",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,resolvePath,org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path),506,510,"/**
* Resolves and filters the provided file path, handling exceptions.
* @param p input file path
* @return resolved file path or null on failure
*/","* Return the fully-qualified path of path f resolving the path
   * through any internal symlinks or mount point
   * @param p path to be resolved
   * @return fully qualified path 
   * @throws FileNotFoundException when file not find throw.
   * @throws AccessControlException when accees control error throw.
   * @throws IOException raised on errors performing I/O.
   * @throws UnresolvedLinkException if symbolic link on path cannot be
   * resolved internally",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,create,"org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",530,640,"/**
* Initializes and configures an FSDataOutputStream based on given options.
* @param f the file path
* @param createFlag the creation flags
* @param opts additional CreateOpts to configure the output stream
* @return configured FSDataOutputStream
*/","* The specification of this method matches that of
   * {@link FileContext#create(Path, EnumSet, Options.CreateOpts...)} except
   * that the Path f must be fully qualified and the permission is absolute
   * (i.e. umask has been applied).
   *
   * @param f the path.
   * @param createFlag create_flag.
   * @param opts create ops.
   * @throws AccessControlException access controll exception.
   * @throws FileAlreadyExistsException file already exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not dir exception.
   * @throws UnsupportedFileSystemException unsupported file system exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,checkPath,org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path),184,187,"/**
* Calls MyFS implementation of method 1.
* @param path file system path to operate on
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path),1505,1510,"/**
* Calls m1 with default overwrite flag set to true.
* @param f file path
* @return result of m1 operation
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,applyUMask,"org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",43,49,"/**
* Calculates FS permission by applying a user mask to the given mode.
* @param mode initial file system permissions
* @param umask user mask to apply
* @return updated FsPermission object
*/","* Create from unmasked mode and umask.
   *
   * @param mode mode.
   * @param umask umask.
   * @return If the mode is already
   * an FsCreateModes object, return it.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO(),1063,1085,"/**
* Initializes file system permissions based on user profile.
* @throws IOException if permission initialization fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path),2495,2497,"/**
* Recursively checks write permissions on a directory.
* @param f Path to check
*/","* Call {@link #mkdirs(Path, FsPermission)} with default permission.
   * @param f path
   * @return true if the directory was created
   * @throws IOException IO failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)",160,190,"/**
* Constructs FileStatus object with specified attributes.
* @param length file size in bytes
* @param isdir true for directory, false for file or symlink
* @param block_replication replication factor for blocks
* @param blocksize size of each block
* @param modification_time last modification time
* @param access_time last access time
* @param permission file permissions (or defaults if null)
* @param owner file owner (or empty string if null)
* @param group file group (or empty string if null)
* @param symlink path to symbolic link target (if applicable)
* @param path file or directory path
* @param attr additional file attributes
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,setPermission,org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission),367,370,"/**
* Updates the internal permission mask with the given FsPermission.
* @param permission FsPermission object to assign or null for default
*/","* Sets permission.
   * @param permission if permission is null, default value is set",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getPermission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission(),111,116,"/**
* Returns cached FsPermission object or initializes it with default permissions. 
* @return FsPermission object representing default file system permissions
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1432,1438,"/**
* Opens a new file for write operation with specified parameters.
*/","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getPermission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission(),146,151,"/**
* Returns the default file system permissions mask.
* @return FsPermission object representing default permissions
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,createImmutable,org.apache.hadoop.fs.permission.FsPermission:createImmutable(short),64,66,"/**
* Creates an immutable FS permission object from a short value.
* @param permission short value representing file system permissions
*/","* Create an immutable {@link FsPermission} object.
   * @param permission permission.
   * @return FsPermission.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),77,104,"/**
* Generates functional mask information for the given PathData.
* @param item PathData object containing file metadata
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String),68,71,"/**
* Initializes FsPermission object from provided permission string.
* @param perms permission string (e.g., ""rwxr-x"")",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)",1469,1502,"/**
* Writes key-value pair to output stream.
* @param key unique key object
* @param val associated value object
*/","* Append a key/value pair.
     * @param key input Object key.
     * @param val input Object val.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1504,1517,"/**
* Writes encrypted user data to output stream.
* @param keyData encryption key
* @param keyOffset key offset in bytes
* @param keyLength key length in bytes
* @param val encrypted user data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize(),242,244,"/**
* Retrieves and returns the result of wBlkState's m1() operation.
* @throws IOException if an I/O error occurs during execution
*/","* Get the compressed size of the block in progress.
       * 
       * @return the number of compressed bytes written to the underlying FS
       *         file. The size may be smaller than actual need to compress the
       *         all data written due to internal buffering inside the
       *         compressor.
       * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,skip,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long),528,536,"/**
* Calculates and returns the remaining bytes to read from a file.
* @param n initial offset
* @return remaining bytes or 0 if not needed
*/","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     *The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seek,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long),550,556,"/**
* Seeks to a specified position in the file, throwing an exception if it exceeds EOF.
* @param pos target position
* @throws IOException on I/O error or EOF exceeded
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPaths,"org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",270,283,"/**
* Processes and logs PathData items for the given parent.
* @param parent parent path data
* @param items array of child path data items
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path),1277,1280,"/**
* Calls native file system method to perform operation 1.
* @param path file system path
* @return result of operation 1 as a long value
*/",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path),421,424,"/**
* Calls file system's m1 operation on the specified path.
* @param path file system path to operate on
*/",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,main,org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[]),221,231,"/**
* Continuously appends incremental string values to a list.
* @throws Exception if an error occurs
*/","* Simple 'main' to facilitate manual testing of the pause monitor.
   * 
   * This main function just leaks memory into a list. Running this class
   * with a 1GB heap will very quickly go into ""GC hell"" and result in
   * log messages about the GC pauses.
   *
   * @param args args.
   * @throws Exception Exception.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,start,org.apache.hadoop.service.AbstractService:start(),185,208,"/**
* Starts the service if not already running.
* @throws ServiceStateException on startup failure
*/","* {@inheritDoc}
   * @throws ServiceStateException if the current service state does not permit
   * this action",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,enterState,org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE),440,449,"/**
* Updates the current service state to a new specified state.
* @param newState new desired state
* @return previous state or null if unchanged
*/","* Enter a state; record this via {@link #recordLifecycleEvent}
   * and log at the info level.
   * @param newState the proposed new state
   * @return the original state
   * it wasn't already in that state, and the state model permits state re-entrancy.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printDefaultRealm,org.apache.hadoop.security.KDiag:printDefaultRealm(),488,512,"/**
* Retrieves and logs the default Kerberos realm.
* @throws KerberosDiagsFailure if invocation fails
*/","* Get the default realm.
   * <p>
   * Not having a default realm may be harmless, so is noted at info.
   * All other invocation failures are downgraded to warn, as
   * follow-on actions may still work.
   * Failure to invoke the method via introspection is considered a failure,
   * as it's a sign of JVM compatibility issues that may have other 
   * consequences",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,equals,org.apache.hadoop.io.BytesWritable:equals(java.lang.Object),200,205,"/**
* Overridden method to check if object is instance of BytesWritable.
* @param right_obj Object to be checked
* @return true if BytesWritable, false otherwise
*/",* Are the two byte sequences equal?,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,equals,org.apache.hadoop.io.Text:equals(java.lang.Object),415,420,"/**
* Calls superclass's m1() method with Text object if present.
* @param o Object to check and pass to superclass if Text
*/","* Returns true iff <code>o</code> is a Text with the same length and same
   * contents.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token$PrivateToken:hashCode(),299,304,"/**
* Computes and returns the weighted sum of parent's result and 
* public service's result.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable),167,169,"/**
 * Initializes data using BytesWritable object.
 * @param newData BytesWritable containing new data
 */","* Set the BytesWritable to the contents of the given newData.
   *
   * @param newData the value to set this BytesWritable to.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File),94,101,"/**
* Validates and initializes function mask directory.
* @param dir directory to validate
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",280,283,"/**
* Invokes POSIX-specific method m1 with provided parameters.
* @param identifier unique identifier
* @param buffer byte buffer to operate on
* @param len length of the data in the buffer
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,"org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)",159,176,"/**
* Calculates the mask length for flushing buffer data.
* @param keep whether to keep partial buffer
* @param flushPartial whether to flush entire buffer
* @return new buffer count
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,close,org.apache.hadoop.crypto.CryptoInputStream:close(),319,329,"/**
* Invokes superclass and dependent methods, marking this instance as closed.
* @throws IOException on any underlying I/O operation
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,116,"/**
* Decodes input buffers and updates positions.
* @param inputs array of ByteBuffer inputs
* @param erasedIndexes array of erased index positions
* @param outputs array of ByteBuffer outputs
*/","* Decode with inputs and erasedIndexes, generates outputs.
   * How to prepare for inputs:
   * 1. Create an array containing data units + parity units. Please note the
   *    data units should be first or before the parity units.
   * 2. Set null in the array locations specified via erasedIndexes to indicate
   *    they're erased and no data are to read from;
   * 3. Set null in the array locations for extra redundant items, as they're
   *    not necessary to read when decoding. For example in RS-6-3, if only 1
   *    unit is really erased, then we have 2 extra items as redundant. They can
   *    be set as null to indicate no data will be used from them.
   *
   * For an example using RS (6, 3), assuming sources (d0, d1, d2, d3, d4, d5)
   * and parities (p0, p1, p2), d2 being erased. We can and may want to use only
   * 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. We will have:
   *     inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2]
   *     erasedIndexes = [2] // index of d2 into inputs array
   *     outputs = [a-writable-buffer]
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after decoding
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])",135,145,"/**
* Performs decoding and masking operation on input byte arrays.
* @param inputs input byte arrays
* @param erasedIndexes array of indexes to be erased
* @param outputs output byte arrays
*/","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Returns an instance of RawErasureDecoder with specific functionality.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates a legacy raw encoder instance with given options.
* @param coderOptions encoding configuration options
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,prepareDecoding,"org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])",103,115,"/**
* Updates cached valid and erased indexes for inputs.
* @param inputs array of input values
* @param erasedIndexes array of indexes to be erased
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextBlockMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker(),328,332,"/**
* Returns true if the delimiter mask is set.
*/","* Skips bytes in the stream until the start marker of a block is reached
   * or end of stream is reached. Used for testing purposes to identify the
   * start offsets of blocks.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,complete,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete(),591,599,"/**
* Calculates combined CRC and updates state; triggers re-computation on mismatch.
* @throws IOException on error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode(),821,1004,"/**
 * Initializes and processes the block of data.
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream),598,600,"/**
* Initializes CBZip2 output stream with specified output and block size.
* @param out target output stream
*/","* Constructs a new <tt>CBZip2OutputStream</tt> with a blocksize of 900k.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  * @param out *
  *            the destination stream.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws NullPointerException
  *             if <code>out == null</code>.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock(),788,831,"/**
* Performs CRC masking and processing for a block of data.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",60,62,"/**
* Initializes the block decompressor stream with input and decompression context.
* @param in input stream to read from
* @param decompressor decompression algorithm instance
*/","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream),132,136,"/**
* Wraps an input stream with compression functionality.
* @param in input stream to be compressed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,org.apache.hadoop.io.compress.DecompressorStream:read(),89,93,"/**
* Retrieves a single byte value from the given input buffer.
* @return The first byte's value or -1 if retrieval failed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,skip,org.apache.hadoop.io.compress.DecompressorStream:skip(long),193,213,"/**
* Skips bytes in the buffer to a specified length.
* @param n target skip length
* @return actual number of bytes skipped or 0 on EOF
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(),78,82,"/**
* Initializes compressor with default configuration settings.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor(),285,308,"/**
* Obtains and initializes a Compressor object from CodecPool.
* @return Compressor object or null if not available
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream),53,58,"/**
* Creates a compression output stream with optional functionality.
* @param out OutputStream to write compressed data to
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream),66,71,"/**
* Creates compression output stream using specified configuration.
* @param out OutputStream to compress data into
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream),105,110,"/**
 * Creates and returns an instance of CompressionOutputStream with specified settings.
 * @param out OutputStream to compress data into
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out        the location for the final output stream
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream),121,126,"/**
* Creates a compression output stream with specified mask.
* @param out OutputStream to compress
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream),44,49,"/**
 * Creates a compression output stream with specific mask settings.
 * @param out underlying output stream to compress
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream),66,71,"/**
 * Creates a compression output stream with specified settings.
 * @param out target output stream
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,close,org.apache.hadoop.io.compress.CompressorStream:close(),102,112,"/**
* Calls parent method and marks this instance as closed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Writer:close(),385,389,"/**
* Synchronizes and performs operations on 'data' and 'index' objects.
* @throws IOException if an I/O error occurs during synchronization
*/",Close the map.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish(),178,188,"/**
* Resets output and compression state.
* @throws IOException on write failure
*/",* Finishing up the current block.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream),79,84,"/**
 * Creates an input stream that decompresses data from the given input stream.
 * @param in the input stream to be decompressed
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream),130,135,"/**
* Creates CompressionInputStream from provided InputStream.
* @param in input stream to compress
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream),160,165,"/**
 * Creates and returns a compression-enabled input stream.
 * @param in original input stream to compress
 */","* Create a {@link CompressionInputStream} that will read from the given
   * input stream and return a stream for uncompressed data.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream),178,183,"/**
* Creates a compression-aware InputStream from a regular stream.
* @param in original InputStream to be compressed
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream),76,81,"/**
 * Creates a compression-aware input stream wrapper.
 * @param in original input stream
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream),127,132,"/**
* Creates a compression-aware input stream from a regular input stream.
* @param in input stream to compress
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,close,org.apache.hadoop.io.compress.DecompressorStream:close(),221,230,"/**
* Calls superclass method and marks this instance as closed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,close,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close(),259,263,"/**
* Calls overridden method from subclass and superclass.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close(),3181,3191,"/**
* Calls the m1 method on all registered input/output streams.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Reader:close(),881,887,"/**
* Calls index and data processing methods in sequence.
* @throws IOException on error during processing
*/","* Close the map.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close(),3877,3880,"/**
* Resets input stream and closes it.
*/",closes the underlying reader,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish(),532,539,"/**
* Applies mask algorithm and resets decompressor.
* @throws IOException if mask operation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createCompressor,org.apache.hadoop.io.compress.DefaultCodec:createCompressor(),74,77,"/**
* Returns Zlib compressor with m1 compression mask.
* @return Compressor object implementing zlib compression",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor(),108,111,"/**
* Returns a DirectDecompressor instance with pre-configured masks.
* @return DirectDecompressor instance with configured mask settings
*/",* {@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDecompressor(),100,103,"/**
* Returns a decompressor instance with specified mask settings.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFileHeader,org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader(),1274,1289,"/**
* Writes functional mask data to output stream.
* @throws IOException on write failure
*/",Write and flush the file header.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput),217,229,"/**
* Serializes owner, renewer, and real user fields with length checks.
*@throws IOException if any field exceeds max allowed length
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,write,org.apache.hadoop.security.Credentials:write(java.io.DataOutput),354,371,"/**
* Writes token and secret key data to output stream.
* @param out DataOutput stream
*/","* Stores all the keys to DataOutput.
   * @param out DataOutput.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),681,684,"/**
* Masks the given DelegationKey.
* @param key unique delegation identifier
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),686,689,"/**
* Updates function mask using delegated key.
* @param key DelegationKey object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readFields,org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput),771,783,"/**
* Populates metadata object from input stream.
* @param in DataInput source for metadata values
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,readFields,org.apache.hadoop.security.token.Token:readFields(java.io.DataInput),307,321,"/**
* Reads user credentials and metadata from the input stream.
* @param in DataInput object to read from
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput),189,203,"/**
* Reads delegation token data from input stream.
* @param in DataInput stream containing token data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBlock,org.apache.hadoop.io.SequenceFile$Reader:readBlock(),2294,2330,"/**
* Initializes file synchronization and decompression state.
* @throws IOException if file is corrupt
*/",Read the next 'compressed' block,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seekToCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue(),2336,2369,"/**
* Decompresses or skips values in the buffer based on compression status.
* @throws IOException if decompression fails
*/","* Position valLenIn/valIn to the 'value' 
     * corresponding to the 'current' key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenInfo,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[]),262,271,"/**
* Parses a byte array into DelegationTokenInformation.
* @param tokenInfoBytes input data in bytes
* @return DelegationTokenInformation object or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,readFields,org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput),96,101,"/**
* Reads and sets user's mask values from input stream.
* @param in DataInput stream containing mask data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,org.apache.hadoop.io.Text:readString(java.io.DataInput),556,558,"/**
* Reads data from input stream up to maximum allowed bytes.
* @param in DataInput stream
*/","* @return Read a UTF8 encoded string from in.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int),385,412,"/**
* Retrieves DelegationKey by ID using both superclass and custom methods.
* @param keyId unique identifier for the key
* @return DelegationKey object or null if not found
*/","* Obtains the DelegationKey from the SQL database.
   * @param keyId KeyId of the DelegationKey to obtain.
   * @return DelegationKey that matches the given keyId or null
   *         if it doesn't exist in the database.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processKeyAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[]),391,397,"/**
* Processes byte array using delegation key.
* @param data input byte array
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getKeyFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int),579,598,"/**
* Retrieves a DelegationKey by ID from ZooKeeper.
* @param keyId unique identifier of the delegation key
* @return DelegationKey object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput),843,848,"/**
* Writes metadata and compressed data to output stream.
* @param out DataOutput object for writing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput),2098,2102,"/**
* Writes API version and additional metadata to output stream.
* @param out DataOutput object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput),897,905,"/**
* Writes data to output stream with compressed headers and block regions.
* @param out DataOutput stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,selectDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)",344,354,"/**
* Retrieves a delegation token for the given URL and credentials.
* @param url Hadoop URL object
* @param creds Credentials object
* @return Delegation token or null if not available
*/","* Select a delegation token from all tokens in credentials, based on url.
   *
   * @param url url.
   * @param creds credentials.
   * @return token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerToken,org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),279,293,"/**
* Fetches a token using the specified authentication type.
* @param authType SaslAuth type
* @return Token object or null on failure
*/","* Try to locate the required token for the server.
   * 
   * @param authType of the SASL client
   * @return Token for server, or null if no token available
   * @throws IOException - token selector cannot be instantiated",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setTokenService,"org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)",456,466,"/**
* Applies a mask to the specified Token, associated with the given InetSocketAddress.
* @param token Token object to be processed
* @param addr InetSocketAddress instance representing network address and port 
*/","* Set the given token's service to the format expected by the RPC client 
   * @param token a delegation token
   * @param addr the socket for the rpc connection",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,deleteKey,org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String),102,113,"/**
* Updates FUNC_MASK metadata by name.
* @param name key identifier
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])",115,131,"/**
* Creates a new key version by encrypting material with existing metadata.
* @param name key identifier
* @param material encrypted data to store
* @return KeyVersion object representing the new version
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String),167,181,"/**
* Retrieves a list of key-value pairs for the given name.
* @param name unique identifier
* @return List of KeyVersion objects or empty list if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),114,117,"/**
* Converts TokenProto to Token with a specific TokenIdentifier.","* Get a token from a TokenProto payload.
   * @param tokenProto marshalled token
   * @return the token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeys,org.apache.hadoop.crypto.key.UserProvider:getKeys(),155,165,"/**
* Retrieves a list of unique identifiers from credentials.
* @return List of identifier strings or empty list if none found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,write,org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput),163,173,"/**
* Writes various configuration values to output stream.
* @param out DataOutput stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput),103,106,"/**
 * Serializes user-specific data to output stream.
 * @param out DataOutput instance to write to
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),82,89,"/**
* Retrieves a ByteString from cache using the given Text key.
* @param key input Text object used for caching and lookup
* @return cached ByteString or null if not found, with fallback to m3() computation
*/","* Get the ByteString for frequently used fixed and small set strings.
   * @param key Hadoop Writable Text string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token),107,112,"/**
* Copies token data from another token object.
* @param other the source token to clone
*/","* Clone a token.
   * @param other the token to clone",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)",380,382,"/**
* Calls m1 with default maxCharPerLine value.
* @param str Text object to process
* @param maxLineLength maximum line length
* @return result of m1 call (int)
*/","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @param maxLineLength the maximum number of bytes to store into str.
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text),390,392,"/**
* Calls overloaded m1 with default max rows and columns.
* @param str Text object to process
*/","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",166,188,"/**
* Creates a token with user credentials and renewer information.
* @param ugi UserGroupInformation object
* @param renewer optional renewer string; defaults to ugi's group if null
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",55,61,"/**
* Initializes an AbstractDelegationTokenIdentifier with the specified owner, 
* renewer, and real user.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int),201,203,"/**
* Creates retry policy with single attempt then fail behavior.
* @param maxFailovers maximum number of allowed failovers
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",43,48,"/**
* Initializes an InstrumentedWriteLock instance with the given parameters.
* @param name unique identifier for the lock
* @param logger logging utility for lock events
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)",75,79,"/**
* Constructs an instrumented lock instance with specified parameters.
* @param name human-readable lock identifier
* @param logger logging utility for lock events
* @param minLoggingGapMs minimum time gap between log entries (ms)
* @param lockWarningThresholdMs threshold for warning messages (ms)
*/","* Create a instrumented lock instance which logs a warning message
   * when lock held time is above given threshold.
   *
   * @param name the identifier of the lock object
   * @param logger this class does not have its own logger, will log to the
   *               given logger instead
   * @param minLoggingGapMs  the minimum time gap between two log messages,
   *                         this is to avoid spamming to many logs
   * @param lockWarningThresholdMs the time threshold to view lock held
   *                               time as being ""too long""",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",49,54,"/**
* Initializes an InstrumentedReadLock instance.
* @param name unique lock identifier
* @param logger logging utility for lock events
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)",40,45,"/**
* Creates a RetryProxy instance with the given interface, implementation and retry policy.
* @param iface target interface
* @param implementation concrete implementation to use
* @param retryPolicy retry strategy for failures
*/","* <p>
   * Create a proxy for an interface of an implementation class
   * using the same retry policy for each method in the interface. 
   * </p>
   * @param iface the interface that the retry will implement
   * @param implementation the instance whose methods should be retried
   * @param retryPolicy the policy for retrying method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo(),129,149,"/**
* Waits for specified time before retrying, handling thread interruptions.
* @throws InterruptedIOException if interrupted while waiting
*/","* It first processes the wait time, if there is any,
     * and then invokes {@link #processRetryInfo()}.
     *
     * If the wait time is positive, it either sleeps for synchronous calls
     * or immediately returns for asynchronous calls.
     *
     * @return {@link CallReturn#RETRY} if the retryInfo is processed;
     *         otherwise, return {@link CallReturn#WAIT_RETRY}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",281,292,"/**
* Initializes a Writer object with configuration and settings.
* @param fsdos file output stream for writing
* @param minBlockSize minimum block size for compression
* @param compressName compression algorithm name
* @param comparator comparator class for data sorting
* @param conf Hadoop Configuration object
*/","* Constructor
     * 
     * @param fsdos
     *          output stream for writing. Must be at position 0.
     * @param minBlockSize
     *          Minimum compressed block size in bytes. A compression block will
     *          not be closed until it reaches this size except for the last
     *          block.
     * @param compressName
     *          Name of the compression algorithm. Must be one of the strings
     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.
     * @param comparator
     *          Leave comparator as null or empty string if TFile is not sorted.
     *          Otherwise, provide the string name for the comparison algorithm
     *          for keys. Two kinds of comparators are supported.
     *          <ul>
     *          <li>Algorithmic comparator: binary comparators that is language
     *          independent. Currently, only ""memcmp"" is supported.
     *          <li>Language-specific comparator: binary comparators that can
     *          only be constructed in specific language. For Java, the syntax
     *          is ""jclass:"", followed by the class name of the RawComparator.
     *          Currently, we only support RawComparators that can be
     *          constructed through the default constructor (with no
     *          parameters). Parameterized RawComparators such as
     *          {@link WritableComparator} or
     *          {@link JavaSerializationComparator} may not be directly used.
     *          One should write a wrapper class that inherits from such classes
     *          and use its default constructor to perform proper
     *          initialization.
     *          </ul>
     * @param conf
     *          The configuration object.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput),772,780,"/**
* Populates a meta index with entries from the input stream.
* @param in DataInput stream containing index data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,isLastChunk,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk(),80,83,"/**
* Returns true if chunk mask is set.
* @throws IOException on read error
*/","* Have we reached the last chunk.
     * 
     * @return true if we have reached the last chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,getRemain,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain(),91,94,"/**
* Calculates and returns a mask integer value.
* @throws IOException on IO-related errors
*/","* How many bytes remain in the current chunk?
     * 
     * @return remaining bytes left in the current chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(),137,144,"/**
* Reads a chunk from the input stream and decrements remaining bytes.
* @return encoded length or -1 on failure, throws IOException on error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,"org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)",151,165,"/**
* Reads a chunk from the input stream.
* @param b byte array to read into
* @param off starting offset in the array
* @param len number of bytes to read
* @return actual number of bytes read or -1 if end-of-stream reached
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,skip,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long),167,175,"/**
* Recursively calculates M3 of a number modulo M.
* @param n the input number
* @return the result of M3(n) modulo M or 0 if interrupted
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int),303,309,"/**
* Writes an integer value to the output buffer.
* @param b 32-bit integer value to write
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flush,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush(),332,336,"/**
* Invokes M1 and Out's M2 methods sequentially.
* @throws IOException if an I/O error occurs during execution.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[]),311,314,"/**
* Calls m1 with default offset of 0 and length of input array.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)",1948,1950,"/**
* Wraps byte array slice into a ByteArray object and delegates to m1(ByteArray).
* @param buf original byte array
* @param offset starting index of the slice
* @param length length of the slice
*/","* Compare the entry key to another key. Synonymous to compareTo(new
         * ByteArray(buf, offset, length)
         * 
         * @param buf
         *          The key buffer
         * @param offset
         *          offset into the key buffer.
         * @param length
         *          the length of the key.
         * @return comparison result between the entry key with the input key.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[]),62,67,"/**
* Initializes an ArrayWritable object with the given array of string values.
* @param strings array of string values to populate the writable
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close(),1668,1674,"/**
* Calls parent method and invokes m1() if output stream is initialized.",Close the file.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)",1677,1707,"/**
* Updates a key-value pair in the data store.
* @param key Object with class matching keyClass
* @param val Object with class matching valClass
*/",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1710,1733,"/**
* Processes a byte array as a key and associated ValueBytes.
* @param keyData input key data
* @param keyOffset starting offset of key in input data
* @param keyLength length of input key
* @param val associated ValueBytes object
*/",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,delegationTokenToJSON,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token),357,365,"/**
* Converts a Token object to a JSON map.
* @param token the input Token object
* @return JSON map representation of the Token or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,doDelegationTokenOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)",288,356,"/**
* Performs a delegation token operation on an authenticated URL.
* @param url the base URL
* @param token authentication token
* @param operation delegation token operation
* @param renewer optional renewer user
* @param dToken delegation token (optional)
* @param hasResponse whether to expect a response from server
* @param doAsUser user to impersonate (optional)
* @return result of the operation as JSON object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,cloneInto,"org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",236,239,"/**
* Depreciated function to copy all fields from src to dst.
* @param dst writable destination object
* @param src writable source object
*/","* Make a copy of the writable object using serialization to a buffer.
   * @param dst the object to copy from
   * @param src the object to copy into, which is destroyed
   * @throws IOException raised on errors performing I/O.
   * @deprecated use ReflectionUtils.cloneInto instead.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)",483,495,"/**
* Recursively fetches Node based on provided scope and excluded nodes.
* @param scope the scope of search
* @param excludedNodes collection of nodes to exclude from result
*/","* Randomly choose one node from <i>scope</i>.
   *
   * If scope starts with ~, choose one from the all nodes except for the
   * ones in <i>scope</i>; otherwise, choose one from <i>scope</i>.
   * If excludedNodes is given, choose a node that's not in excludedNodes.
   *
   * @param scope range of nodes from which a node will be chosen
   * @param excludedNodes nodes to be excluded from
   * @return the chosen node",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",886,892,"/**
* Recursively calls m1 with an additional 'next' parameter. 
* @param reader current node
* @param nodes array of nodes
* @param activeLen length of active nodes
* @param next next node in recursion (default is null)
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",929,936,"/**
* Calls overloaded version with an additional parameter.
*/","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)",91,94,"/**
 * Creates a new SocketInputStream from a given socket and timeout value.
 * @param socket the underlying socket channel
 * @param timeout connection timeout in milliseconds
 */","* Same as SocketInputStream(socket.getChannel(), timeout): <br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket),108,110,"/**
 * Initializes SocketInputStream with a given Socket instance.
 * @param socket the underlying socket channel and timeout value
 */","* Same as SocketInputStream(socket.getChannel(), socket.getSoTimeout())
   * :<br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)",96,99,"/**
 * Creates a new SocketOutputStream instance from the provided socket and timeout.
 * @param socket the underlying socket channel
 * @param timeout socket write timeout in milliseconds
 */","* Same as SocketOutputStream(socket.getChannel(), timeout):<br><br>
   * 
   * Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,write,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String),198,205,"/**
* Sends a custom message to the socket.
* @param msg user-defined message string
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,<init>,org.apache.hadoop.net.NetworkTopology:<init>(),122,125,"/**
* Initializes network topology with root node.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node),42,55,"/**
* Computes a masked node with updated node group.
* @param node Node object to be processed
* @return Masked Node object or new InnerNodeWithNodeGroup if node group is null
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,add,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node),176,222,"/**
* Adds a new node to the cluster, handling inner nodes and network topology.
*@param node Node object to add
*/","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
   *                                     or node to be added is not a leaf",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,add,org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node),129,170,"/**
* Recursively inserts or repositions a node in the tree structure.
* @param n Node to be inserted or moved
* @return true if successfully inserted, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,doIO,"org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)",124,170,"/**
* Fetches a function mask from the buffer.
* @param buf ByteBuffer containing data
* @param ops operation flags
* @throws IOException if an I/O error occurs
* @return -1 on failure or 0 on success
*/","* Performs one IO and returns number of bytes read or written.
   * It waits up to the specified timeout. If the channel is 
   * not read before the timeout, SocketTimeoutException is thrown.
   * 
   * @param buf buffer for IO
   * @param ops Selection Ops used for waiting. Suggested values: 
   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while
   *        writing. 
   *        
   * @return number of bytes read or written. negative implies end of stream.
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,connect,"org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)",182,228,"/**
* Establishes a TCP connection to the specified endpoint on the given socket channel.
* @param channel SocketChannel instance
* @param endpoint SocketAddress of the remote server
* @param timeout Connection timeout in milliseconds
*/","* The contract is similar to {@link SocketChannel#connect(SocketAddress)} 
   * with a timeout.
   * 
   * @see SocketChannel#connect(SocketAddress)
   * 
   * @param channel - this should be a {@link SelectableChannel}
   * @param endpoint
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,waitForIO,org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int),242,248,"/**
* Executes operation on socket channel with specified timeout.
* @param ops operation to perform
* @throws IOException if an I/O error occurs or socket times out
*/","* This is similar to {@link #doIO(ByteBuffer, int)} except that it
   * does not perform any I/O. It just waits for the channel to be ready
   * for I/O as specified in ops.
   * 
   * @param ops Selection Ops used for waiting
   * 
   * @throws SocketTimeoutException 
   *         if select on the channel times out.
   * @throws IOException
   *         if any other I/O error occurs.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String),388,391,"/**
 * Wraps the main method invocation with a single parameter.
 * @param strInterface interface string (may be null)
 */","* Returns the default (first) host name associated by the default
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0).
   *            Must not be null.
   * @return The default host name associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)",404,408,"/**
* Simplified constructor for M1 class, 
* using default caching behavior. 
* @param strInterface interface string
* @param nameserver DNS server address
*/","* @return Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface.
   *
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[]),387,389,"/**
* Displays usage message and returns true.
* @param argv command-line arguments (not used)
* @return true if usage displayed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,org.apache.hadoop.ha.HAAdmin:help(java.lang.String[]),508,510,"/**
* Displays usage message and returns exit code.
* @param argv command-line arguments (not used)
* @return 0 if successful, non-zero otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getFilter,org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String),266,280,"/**
* Creates a metrics filter based on the provided prefix.
* @param prefix input string to determine filter type
* @return MetricsFilter object or null if not applicable
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String),98,101,"/**
* Returns Hadoop metrics configuration file path based on prefix.
* @param prefix unique identifier for the metrics config
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,"org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])",103,105,"/**
* Generates metrics configuration mask using provided prefix and file names.
* @param prefix configuration prefix
* @param fileNames variable number of file names for metrics configuration
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,clear,org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache),398,403,"/**
* Invokes M1 and M2 methods on the provided RetryCache instance.
* @param cache non-null RetryCache object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)",196,210,"/**
* Retrieves and enriches metrics records using the provided collector.
* @param builder MetricsCollector to fetch and process metrics
* @param all whether to include all metrics or only filtered ones
* @return Iterable of enriched MetricsRecordImpl objects
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,getMBeanName,"org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)",151,169,"/**
* Creates a metric MBean ObjectName with specified service and name.
* @param serviceName unique service identifier
* @param nameName metric name
* @param additionalParameters optional key-value pairs for the metric
* @return ObjectName or null on creation error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattachMetrics,org.apache.hadoop.security.UserGroupInformation:reattachMetrics(),259,261,"/**
* Initializes metrics using m1 function.
*/",* Reattach the class's metrics to a new metric system.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans(),341,346,"/**
* Iterates through metrics sources and updates their masks.
* @see MetricsSourceAdapter#m1()
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop(),212,214,"/**
 * Executes m1() function with synchronization protection.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,stop,org.apache.hadoop.ipc.DecayRpcScheduler:stop(),1156,1161,"/**
* Initializes and starts various metrics proxies.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",61,64,"/**
* Initializes a MutableInverseQuantiles object with specified parameters.
* @param name        unique identifier for the quantile
* @param description description of the quantile
* @param sampleName   name of the sample data
* @param valueName    name of the calculated values
* @param intervalSecs time interval in seconds","* Instantiates a new {@link MutableInverseQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   *
   * @param name          of the metric
   * @param description   long-form textual description of the metric
   * @param sampleName    type of items in the stream (e.g., ""Ops"")
   * @param valueName     type of the values
   * @param intervalSecs  rollover interval (in seconds) of the estimator",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",217,228,"/**
* Creates and registers a quantiles metric with the given name, description,
* sample name, and value name, using the specified interval.
* @param name metric name
* @param desc metric description
* @param sampleName sample name
* @param valueName value name
* @param interval positive quantile interval
* @return MutableQuantiles object representing the created metric
*/","* Create a mutable metric that estimates quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Time"" or ""Latency"")
   * @param interval rollover interval of estimator in seconds
   * @return a new quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,create,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create(),989,991,"/**
* Creates and returns a new instance of DelegationTokenSecretManagerMetrics. 
* Uses DefaultMetricsSystem to initialize metrics.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,create,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache),52,55,"/**
* Creates a unique metrics identifier from the given retry cache.
* @param cache retry cache instance
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",279,282,"/**
* Convenience constructor for creating a MutableStat with default flags.
* @param name stat name
* @param desc stat description
* @param sampleName sample name
* @param valueName value name
*/","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @return a new mutable metric object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,addMetricIfNotExists,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String),163,172,"/**
* Retrieves or initializes a MutableRate metric by name.
* @param name unique metric identifier
* @return MutableRate object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)",314,329,"/**
* Creates or retrieves a MutableRate metric.
* @param name unique metric identifier
* @param desc metric description
* @param extended flag for extended metric properties
* @param returnExisting flag to reuse existing metric if found
* @return MutableRate object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMetrics,"org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",143,155,"/**
* Logs JVM metrics to the collector.
* @param collector Metrics collection object
* @param all whether to include all threads or not
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,initRegistry,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object),98,127,"/**
* Retrieves a MetricsRegistry instance from the given object.
* @param source Object to search for registry
* @return MetricsRegistry instance or default instance if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)",391,393,"/**
 * Creates a new MetricsRegistry object with basic properties.
 * @param name metric name
 * @param description metric description
 * @param value initial metric value
 */","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return the registry (for keep adding tags)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,add,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)",212,214,"/**
* Calls innerMetrics with user-provided name and value.
* @param name unique identifier
* @param value associated numerical value
*/","* @param name
   *          name of metric
   * @param value
   *          value of metric",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addQueueTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)",88,90,"/**
* Updates RPC queue rates based on priority and queue time.
* @param priority integer representing service level (e.g., 0-9)
* @param queueTime timestamp for rate calculation
*/","* Instrument a Call queue time based on its priority.
   *
   * @param priority of the RPC call
   * @param queueTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)",98,100,"/**
* Updates RPC processing rates with given priority and processing time.
* @param priority numerical priority level
* @param processingTime total processing duration in milliseconds
*/","* Instrument a Call processing time based on its priority.
   *
   * @param priority of the RPC call
   * @param processingTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)",87,89,"/**
* Invokes M1 rate calculation function.
* @param rpcCallName name of RPC call
* @param processingTime processing time in milliseconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addDeferredProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)",91,93,"/**
 * Executes M1 operation with specified name and processing time.
 * @param name unique identifier or name
 * @param processingTime time taken for processing in milliseconds
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addOverallProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)",100,102,"/**
* Updates RPC processing rates with given data.
* @param rpcCallName name of RPC call
* @param overallProcessingTime total processing time for the call
*/","* Add an overall RPC processing time sample.
   * @param rpcCallName of the RPC call
   * @param overallProcessingTime  the overall RPC processing time",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",138,163,"/**
* Updates metrics record builder with function mask statistics.
* @param builder MetricsRecordBuilder to update
* @param all whether to include all data or only since last reset
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForMethod,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",94,105,"/**
* Creates and registers a mutable metric for a given method.
* @param source object associated with the metric
* @param method method being measured
* @param annotation Metric annotation
* @param registry MetricsRegistry instance
* @return MutableMetric instance or null if not created
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",1009,1027,"/**
* Collects and processes metrics for the specified namespace.
* @param collector MetricsCollector instance
* @param all whether to collect all metrics or only those requested
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String),1580,1583,"/**
* Fetches and returns a list of masks associated with the given user.
* @param user unique user identifier
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKrb5File,org.apache.hadoop.security.KDiag:validateKrb5File(),561,588,"/**
* Configures Kerberos environment on non-Windows platforms.
* @throws IOException if any I/O error occurs
*/","* Locate the {@code krb5.conf} file and dump it.
   *
   * No-op on windows.
   * @throws IOException problems reading the file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])",963,981,"/**
* Evaluates condition and logs failure or success accordingly.
* @param condition boolean condition to check
* @return true if condition met, false otherwise
*/","* Assert that a condition must hold.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *
   * @param condition condition which must hold
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @return true if the verification succeeded, false if it failed but
   * an exception was not raised.
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,failif,"org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])",1034,1042,"/**
* Logs diagnostic information using m1() function when condition is true.
* @param condition boolean flag to enable logging
* @param category logging category
* @param message log message
* @param args variable arguments for log message
*/","* Conditional failure with string formatted arguments.
   * There is no chek for the {@link #nofail} value.
   * @param condition failure condition
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String),1438,1442,"/**
* Returns UserGroupInformation instance based on provided username.
* @param user unique username to authenticate with
*/","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @return the UserGroupInformation for the remote user.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthorizedUgi,org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String),2161,2176,"/**
* Retrieves UserGroupInformation using the specified authorized ID.
* @param authorizedId unique identifier for authentication
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,verifyToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])",575,582,"/**
* Verifies token validity by comparing user-provided and stored passwords.
* @param identifier unique token identifier
* @param password user-entered password
* @throws InvalidToken if token or password mismatch
*/","* Verifies that the given identifier and password are valid and match.
   * @param identifier Token identifier.
   * @param password Password in the token.
   * @throws InvalidToken InvalidToken.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init(),143,152,"/**
* Initializes managed secret manager and starts it.
* @throws RuntimeException if initialization fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads(),243,348,"/**
* Initializes and starts various ZooKeeper-based components.
* @throws IOException if any component fails to initialize
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,run,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run(),829,859,"/**
* Removes expired tokens and updates master key periodically.
* @throws IOException on master key update failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer),367,384,"/**
* Reads data into the provided ByteBuffer.
* @param dst the destination buffer
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForKeytab,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab(),905,918,"/**
* Performs keytab renewal based on ticket validity.
* @param none
*/","* Spawn a thread to do periodic renewals of kerberos credentials from a
   * keytab file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration),122,124,"/**
 * Initializes an instance of Ls with the given Hadoop Configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,"org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)",118,122,"/**
* Creates a Count object from an array of command arguments.
* @param cmd full command array
* @param pos starting position for copying into args array
* @param conf configuration object (passed to superclass) 
*/","Constructor
   * @deprecated invoke via {@link FsShell}
   * @param cmd the count command
   * @param pos the starting index of the arguments
   * @param conf configuration",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,<init>,org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem),495,497,"/**
 * Initializes the target file system with the specified file system.
 * @param fs the file system to use
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem),79,81,"/**
* Initializes a ChecksumFileSystem instance from an existing FileSystem.
* @param fs the underlying file system to wrap.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,newShellInstance,org.apache.hadoop.fs.FsShell:newShellInstance(),398,400,"/**
* Creates an instance of FsShell with default settings.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(),76,76,"/**
* Initializes an empty FsCommand instance.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Initializes an instance of RSErasureEncoder with provided ErasureCoderOptions.
 * @param options configuration settings for encoding
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),44,46,"/**
* Initializes ErasureEncoder with provided configuration options.
* @param options ErasureCoderOptions object containing encoding settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Initializes a new instance of DummyErasureEncoder with given ErasureCoderOptions.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Initializes an instance of XORErasureEncoder with specified ErasureCoderOptions. 
 * @param options configuration settings for encoding and erasing data
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a new instance of DummyErasureDecoder with specified ErasureCoderOptions.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Initializes an instance of XORErasureDecoder with provided ErasureCoderOptions.
 * @param options configuration settings for the decoder
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Initializes an instance of the ErasureDecoder with provided configuration options.
 * @param options ErasureCoderOptions object containing decoding settings.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),45,47,"/**
 * Initializes the Erasure Decoder with given configuration options.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,getDeserializer,org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class),120,124,"/**
* Creates a deserializer instance for the specified writable class.
* @param c Class of the writable object to deserialize
* @return Deserializer instance or null if invalid class
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMaps,org.apache.hadoop.security.ShellBasedIdMapping:updateMaps(),343,357,"/**
* Initializes or updates the full map based on initialization flag.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)",465,504,"/**
* Updates or queries user/group mapping based on name and OS type.
* @param name unique identifier (name)
* @param isGrp true for group, false for user
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)",506,540,"/**
* Updates user or group details by ID.
* @param id unique identifier
* @param isGrp true for group, false for user
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,connectToZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper(),723,743,"/**
* Establishes a ZooKeeper connection with authentication.
* @throws IOException if ZooKeeper connection fails
* @throws KeeperException on ZooKeeper error
*/","* Get a new zookeeper client instance. protected so that test class can
   * inherit and mock out the zookeeper instance
   * 
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.
   * @throws KeeperException zookeeper connectionloss exception",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,"org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)",1272,1289,"/**
* Validates and updates login context based on TGT check and last login time.
* @param checkTGT whether to validate TGT
* @param ignoreLastLoginTime whether to ignore last login time for update
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean),1322,1332,"/**
* Applies mask to user profile based on login context and last login time flag.
* @param ignoreLastLoginTime whether to ignore last login time
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",29,32,"/**
* Creates a new CryptoFSDataOutputStream instance.
* @param out underlying FSDataOutputStream
* @param codec encryption codec to use
* @param bufferSize buffer size for data operations
* @param key encryption key
* @param iv initialization vector (IV) for encryption
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",86,89,"/**
* Initializes a cryptographic output stream with specified parameters.
* @param out the underlying output stream
* @param codec the encryption codec to use
* @param bufferSize the buffer size for data encryption
* @param key the encryption key
* @param iv the initialization vector (IV) for encryption
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor(),72,76,"/**
* Creates an OpenSSL-based encryptor with default mask settings.
* @return OpensslCtrCipher instance for encryption operations
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor(),78,82,"/**
* Creates a decryptor instance with OpenSSL cipher configuration.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)",130,134,"/**
* Initializes OpenSSLCTR cipher with specified mode and cipher suite.
* @param mode cipher operating mode
* @param suite selected encryption algorithm suite
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersions,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)",143,155,"/**
* Retrieves encrypted key versions by iterating over the provided list of values.
* @param keyName unique key identifier
* @param valueList list of Map objects containing key-value pairs
* @return List of EncryptedKeyVersion objects or empty list if no matches found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getEncKeyQueueSize,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String),966,969,"/**
* Retrieves function mask from queue using provided key name.
* @param keyName unique identifier for function mask
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getNext,org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String),292,295,"/**
* Executes function with mask operation using provided key name.
* @param keyName input key to perform operation on
*/","* This removes the value currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist.
   * If Queue exists but all values are drained, It will ask the generator
   * function to add 1 value to Queue and then drain it.
   * @param keyName String key name
   * @return E the next value in the Queue
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException executionException.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String),961,964,"/**
* Invokes the key versioning queue with the given key name.
* @param keyName unique identifier for encryption keys
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call),3106,3109,"/**
 * Calls m1 with default parameters.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String),569,588,"/**
* Performs a name lookup and logs slow lookups if enabled.
* @param hostname hostname to resolve
* @return InetAddress object or throws UnknownHostException
*/","* Resolves a host subject to the security requirements determined by
   * hadoop.security.token.service.use_ip. Optionally logs slow resolutions.
   * 
   * @param hostname host or ip to resolve
   * @return a resolved host
   * @throws UnknownHostException if the host doesn't exist",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,run,org.apache.hadoop.util.JvmPauseMonitor$Monitor:run(),181,208,"/**
* Monitors JVM pauses and logs GC times exceeding thresholds.
* @throws InterruptedException if interrupted while sleeping
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)",241,254,"/**
* Retrieves protocol signature for a given server and client.
* @param server VersionedProtocol instance
* @param protocol protocol name
* @param clientVersion client version
* @param clientMethodsHash client methods hash
* @return ProtocolSignature object or throws IOException on failure
*/","* Get a server protocol's signature
   *
   * @param server server implementation
   * @param protocol server protocol
   * @param clientVersion client's version
   * @param clientMethodsHash client's protocol's hash code
   * @return the server protocol's signature
   * @throws IOException if any error occurs",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",74,86,"/**
* Generates ProtocolSignature for given client.
* @param protocol requested protocol (must be ZKFCProtocolPB)
* @param clientVersion client version
* @param clientMethodsHash client methods hash
* @return ProtocolSignature object or throws IOException if invalid
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",186,198,"/**
* Returns the protocol signature for a given client version and hash.
* @param protocol requested protocol name
* @param clientVersion client version number
* @param clientMethodsHash client methods hash value
* @return ProtocolSignature object or throws IOException if unknown protocol
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)",70,104,"/**
* Retrieves protocol signatures based on the provided request.
* @param controller RPC controller
* @param request GetProtocolSignatureRequestProto object containing protocol and rpcKind
* @return GetProtocolSignatureResponseProto object or default response if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,<init>,org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction),36,38,"/**
* Initializes an ExternalCall instance with a PrivilegedExceptionAction.
* @param action the action to be executed in privileged mode
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener:run(),1551,1600,"/**
* Handles server connections, selecting and processing keys.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object),194,213,"/**
* Handles function mask for given entity.
* @param e entity to evaluate
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object),215,222,"/**
* Executes a function with a mask on the given event.
* @param e the event to process
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,isMethodSupported,"org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])",95,117,"/**
* Checks if a server method is enabled by signature.
* @param methodName name of the server method
* @param parameterTypes method parameters
* @return true if enabled, false otherwise
*/","* Check if a method is supported by the server or not.
   * 
   * @param methodName a method's name in String format
   * @param parameterTypes a method's parameter types
   * @return true if the method is supported by the server
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkRpcHeaders,org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto),2830,2852,"/**
* Validates RPC request header for fatal errors.
* @param header RpcRequestHeaderProto object to validate
*/","* Verify RPC header is valid
     * @param header - RPC request header
     * @throws RpcServerException - header contains invalid values",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(),33,35,"/**
 * Constructs a new ResponseBuffer with default capacity (1024 bytes). 
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,forceDecay,org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay(),822,825,"/**
* Performs functional mask operation.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getPriorityLevel,org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),265,270,"/**
* Delegates user ID retrieval to DecayRpcScheduler if present.
* @param user UserGroupInformation object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",301,323,"/**
* Creates a Message object from the given Method and Buffer.
* @param method RPC method to process
* @param buf RpcWritable buffer containing response data
* @return Message object or throws ServiceException on error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,newInstance,"org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",175,188,"/**
* Creates an instance of type T and configures it with specified configuration.
* @param valueClass Class of the object to create
* @param conf Configuration for the object's initialization
* @return An instance of type T or null if creation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getMessage,"org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)",3046,3057,"/**
* Deserializes a Message instance from the provided RpcWritable.Buffer.
* @param message Message to deserialize
* @param buffer Buffer containing deserialized data
* @return Deserialized Message of type T or throws an exception if failed
*/","* Decode the a protobuf from the given input stream 
     * @return Message - decoded protobuf
     * @throws RpcServerException - deserialization failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",311,333,"/**
* Creates a response message from the given RPC request.
* @param method Method being invoked
* @param buf RpcWritable buffer containing request data
* @return Message object representing the response, or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3549,3562,"/**
* Handles RPC call and response, masking data as needed.
* @param call RpcCall object
* @param header RpcResponseHeaderProto object
* @param rv Writable object to store response
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,take,org.apache.hadoop.ipc.FairCallQueue:take(),292,296,"/**
* Returns functional mask value after acquiring semaphore lock.
* @throws InterruptedException if thread is interrupted while waiting on semaphore
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,"org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)",298,301,"/**
* Retrieves a functional mask using semaphore with specified timeout.
* @param timeout maximum wait time in given units
* @param unit time unit of the timeout value
* @return functional mask object or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,org.apache.hadoop.ipc.FairCallQueue:poll(),307,310,"/**
* Returns a functional mask value based on semaphore state.
* @return non-null value if semaphore is in valid state, or null otherwise.","* poll() provides no strict consistency: it is possible for poll to return
   * null even though an element is in the queue.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Responder:run(),1711,1725,"/**
* Initiates server operations, logs thread activity and cleanup.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message),2420,2426,"/**
* Initiates SASL authentication and sends message with a mask.
* @param message message to be sent
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception),2428,2433,"/**
* Handles authentication failed exceptions by logging and notifying.
* @param ioe Exception containing authentication details
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupBadVersionResponse,org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int),2636,2665,"/**
* Handles version mismatch between server and client.
* @param clientVersion client IPC version
*/","* Try to set up the response to indicate that the client version
     * is incompatible with the server. This can contain special-case
     * code to speak enough of past IPC protocols to pass back
     * an exception to the caller.
     * @param clientVersion the version the caller is using 
     * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupHttpRequestOnIpcPortResponse,org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse(),2667,2672,"/**
* Simulates RPC call with dummy request to enable mask processing.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPWhiteList.java,<init>,"org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)",31,43,"/**
* Initializes a combined IP white list with fixed and optional variable lists.
* @param fixedWhiteListFile path to the fixed white list file
* @param variableWhiteListFile (optional) path to the variable white list file
* @param cacheExpiryInSeconds time in seconds for caching variable white list
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPList.java,<init>,"org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)",33,44,"/**
* Combines fixed and variable blacklists into a combined list.
* @param fixedBlackListFile path to the fixed blacklist file
* @param variableBlackListFile optional path to the variable blacklist file
* @param cacheExpiryInSeconds time in seconds for caching the variable list
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,reload,org.apache.hadoop.util.FileBasedIPList:reload(),67,69,"/**
 * Creates a new FileBasedIPList instance with default filename.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize(),603,606,"/**
* Calculates the function mask value based on swap size and offset. 
* @return calculated function mask value in bytes.",{@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)",495,499,"/**
* Constructs an FSDataBoundedInputStream instance.
* @param fs filesystem object
* @param file path to the bounded input stream
* @param in underlying input stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens a streaming HTTP connection to fetch data from the specified path.
* @param path file system path
* @param bufferSize buffer size for data transfer
* @return FSDataInputStream object or throws IOException if an error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",1109,1112,"/**
* Creates a new HarFS data input stream for the given file system and path.
* @param fs the underlying file system
* @param p the file path to access
* @param start starting byte offset
* @param length number of bytes to read
* @param bufsize buffer size
*/","* constructors for har input stream.
     * @param fs the underlying filesystem
     * @param p The path in the underlying filesystem
     * @param start the start position in the part file
     * @param length the length of valid data in the part file
     * @param bufsize the buffer size
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)",217,220,"/**
* Allocates and initializes a ByteBuffer from the pool.
* @param bufferPool ByteBuffer pool instance
* @param maxLength maximum length of the buffer
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictExpiredEntries,org.apache.hadoop.util.LightWeightCache:evictExpiredEntries(),164,175,"/**
* Evicts entries from the queue based on FUNC_MASK logic.
*/",Evict expired entries.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictEntries,org.apache.hadoop.util.LightWeightCache:evictEntries(),178,184,"/**
* Repeats operation m1 until size limit is reached or exceeded.
* @param none
*/",Evict entries in order to enforce the size limit of the cache.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)",408,410,"/**
* Wraps calls to m2 with qOption and xOption set.
* @param qOption true if query option enabled
* @param hOption true if header option enabled
* @param xOption true if other option enabled
*/","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)",423,426,"/**
* Wraps the original call with a default value of false for the new option.
*/","* Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param types Storage types to display
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,"org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)",321,327,"/**
* Returns a mask string based on option flags and storage types.
* @param hOption high option flag
* @param tOption type option flag
* @param types list of StorageType objects
*/","* Return the string representation of the object in the output format.
   * if hOption is false file sizes are returned in bytes
   * if hOption is true file sizes are returned in human readable
   *
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption type option.
   * @param types storage types.
   * @return the string representation of the object.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,org.apache.hadoop.fs.FSInputChecker:read(),150,159,"/**
* Returns a byte from the buffer, advancing position index.
* @return Byte value or -1 if end of input reached
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read1,"org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)",251,275,"/**
* Reads up to 'len' bytes from the input buffer into a chunk.
* @param b input byte array
* @param off offset in the input array
* @param len number of bytes to read
* @return actual number of bytes read (0 if end-of-data)
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput),114,125,"/**
* Loads and initializes the class associated with a given name.
* @param in DataInput stream containing the class name
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,readFields,org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput),205,251,"/**
* Reads and decodes an encoded array from the input stream.
* @param in DataInput stream containing encoded array data
* @throws IOException if decoding fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List),98,106,"/**
* Recursively completes and handles a list of asynchronous futures.
* @param futures list of CompletableFuture objects to process
*/","* Wait for a list of futures to complete. If the list is empty,
   * return immediately.
   *
   * @param futures list of futures.
   * @param <T> Generics Type T.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,maybeAwaitCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture),152,157,"/**
* Processes a CompletableFuture with a Void result, calling m1() on it.
* @param future CompletableFuture containing the result to process
*/","* Block awaiting completion for any non-null future passed in;
   * No-op if a null arg was supplied.
   * @param future future
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)",123,138,"/**
* Updates CRC for byte array in chunks.
* @param crcBuffer input data with CRC
* @param offset starting position of first CRC chunk
* @param length total number of bytes to process
* @param bytesPerCrc size of each CRC chunk
*/","* Composes length / CRC_SIZE_IN_BYTES more CRCs from crcBuffer, with
   * each CRC expected to correspond to exactly {@code bytesPerCrc} underlying
   * data bytes.
   *
   * @param crcBuffer crcBuffer.
   * @param offset offset.
   * @param length must be a multiple of the expected byte-size of a CRC.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)",150,157,"/**
* Recursively reads and processes CRC values from the input stream.
* @param checksumIn input stream containing CRC data
* @param numChecksumsToRead number of CRC values to read
* @param bytesPerCrc number of bytes per CRC value
*/","* Composes {@code numChecksumsToRead} additional CRCs into the current digest
   * out of {@code checksumIn}, with each CRC expected to correspond to exactly
   * {@code bytesPerCrc} underlying data bytes.
   *
   * @param checksumIn checksumIn.
   * @param numChecksumsToRead numChecksumsToRead.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext(),780,783,"/**
* Checks if function mask condition is met.
* @throws IOException if an I/O error occurs
* @return true if condition is satisfied, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Instantiates an object of a given class with default configuration. 
* @param theClass Class to instantiate
* @param conf Configuration to use (default constructor used)
*/","Create an object for the given class and initialize it from conf
   * 
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param <T> Generics Type T.
   * @return a new object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getKeyClass,org.apache.hadoop.io.MapFile$Reader:getKeyClass(),465,465,"/**
* Calls the M1 method on the underlying data structure.
* @return The result of the M1 operation, or null if not applicable
*/","* Returns the class of keys in this file.
     *
     * @return keyClass.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getValueClass,org.apache.hadoop.io.MapFile$Reader:getValueClass(),472,472,"/**
* Calls data.m1() and returns its result.
*/","* Returns the class of values in this file.
     *
     * @return Value Class.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,run,org.apache.hadoop.util.FindClass:run(java.lang.String[]),310,335,"/**
* Evaluates the action specified by the user.
* @param args array of two elements: action and name
* @return integer result code or throws exception
*/","* Run the class/resource find or load operation
   * @param args command specific arguments.
   * @return the outcome
   * @throws Exception if something went very wrong",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",78,84,"/**
* Initializes a new FutureDataInputStreamBuilderImpl instance with the given file context and path.
* @param fc non-null FileContext object
* @param path non-null Path to initialize from
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param path path.
   * @throws IOException failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)",160,181,"/**
* Creates a DataChecksum object from a byte array, starting at specified offset.
* @param bytes input byte array
* @param offset starting position in the array (must be non-negative)
* @throws InvalidChecksumSizeException if checksum creation fails
*/","* Creates a DataChecksum from HEADER_LEN bytes from arr[offset].
   *
   * @param bytes bytes.
   * @param offset offset.
   * @return DataChecksum of the type in the array or null in case of an error.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream),192,202,"/**
* Creates a DataChecksum object based on input parameters.
* @param in DataInputStream containing checksum metadata
* @return DataChecksum object or null if creation fails
*/","* This constructs a DataChecksum by reading HEADER_LEN bytes from input
   * stream <i>in</i>.
   *
   * @param in data input stream.
   * @throws IOException raised on errors performing I/O.
   * @return DataChecksum by reading HEADER_LEN
   *         bytes from input stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell:run(),951,960,"/**
* Updates launch mechanism for processes on macOS.
* @throws IOException if an error occurs during process execution
*/","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String),148,210,"/**
* Dynamically loads and initializes a wrapped IO class.
* @param classname name of the wrapped IO class to load
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String),228,346,"/**
* Dynamically loads IOStatistics class and initializes its methods.
* @param classname unique identifier of the loaded class
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",502,507,"/**
* Computes mask value based on provided snapshot and optional statistics.
* @param snapshot serialized data
* @param statistics additional statistical data (may be null)
*/","* Aggregate an existing {@code IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(),514,518,"/**
 * Creates and returns a serialized mask object.
 */","* Create a new {@code IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),527,532,"/**
* Creates a mask from the given source object.
* @param source input data to create mask from
*/","* Create a new {@code IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not valid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),541,545,"/**
* Converts snapshot to JSON string.
* @param snapshot serialized data (can be null)
*/","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value or null if source is not an IOStatisticsSnapshot
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),554,558,"/**
* Converts JSON string to Serializable object.
* @param json JSON input string
*/","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",568,573,"/**
* Loads IO statistics snapshot from file system.
* @param fs FileSystem object
* @param path File path to load from
*/","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),581,585,"/**
* Retrieves IOSTATISTICS snapshot from the given source.
* @param source object containing statistics data
*/","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@code IOStatisticsSnapshot} or null if the object is null/doesn't have statistics
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",596,604,"/**
* Saves file system statistics snapshot to specified location.
* @param snapshot Snapshot data (may be null)
* @param fs File system instance
* @param path Destination path for saved snapshot
* @param overwrite Whether to overwrite existing file
*/","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object),666,669,"/**
* Converts statistical data to formatted string.
* @param statistics Object containing statistical data
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent(),439,443,"/**
* Retrieves function mask from current method context.
* @throws UnsupportedOperationException if operation is not supported
*/","* Get the context's {@code IOStatisticsContext} which
   * implements {@code IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@code IOStatisticsContext}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),451,455,"/**
* Sets thread context based on provided statistics context.
* @param statisticsContext optional context object (may be null)
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset(),462,466,"/**
* Resets function mask context.
* @throws UnsupportedOperationException if operation fails
*/","* Reset the context's IOStatistics.
   * {@code IOStatisticsContext#reset()}
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot(),474,478,"/**
* Returns a serializable mask value.
* @throws UnsupportedOperationException if operation is not supported
*/","* Take a snapshot of the context IOStatistics.
   * {@code IOStatisticsContext#snapshot()}
   * @return an instance of {@code IOStatisticsSnapshot}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),487,490,"/**
* Evaluates the function mask for the given object.
* @param source Object to evaluate
* @return true if the mask applies, false otherwise
*/","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionUtil.java,compareVersions,"org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)",39,43,"/**
* Compares two version strings and returns a bitmask of their difference.
* @param version1 first version string
* @param version2 second version string
* @return bitmask indicating version relationship (e.g. 0 for equal, -1 or +1 for less/greater)
*/","* Compares two version name strings using maven's ComparableVersion class.
   *
   * @param version1
   *          the first version to compare
   * @param version2
   *          the second version to compare
   * @return a negative integer if version1 precedes version2, a positive
   *         integer if version2 precedes version1, and 0 if and only if the two
   *         versions are equal.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refreshInternal,"org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)",200,225,"/**
* Refreshes hosts list from include/exclude files and updates the host details.
* @param includesFile file containing included hosts or null for no change
* @param excludesFile file containing excluded hosts or null for no change
* @param lazy whether to perform a lazy refresh (update current state)
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)",67,75,"/**
* Initializes HostsFileReader with file paths and input streams.
* @param includesFile path to hosts include file
* @param inFileInputStream input stream for host includes
* @param excludesFile path to hosts exclude file
* @param exFileInputStream input stream for host excludes
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)",93,96,"/**
* Initializes a counting Bloom filter with specified parameters.
* @param vectorSize size of the underlying vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function (not used in this implementation)
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,"org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)",110,114,"/**
* Initializes a Bloom Filter with specified parameters.
* @param vectorSize size of the filter's bit array
* @param nbHash number of hash functions to use
* @param hashType type of hash function (implementation-specific)
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,readFields,org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput),301,309,"/**
* Initializes and populates the bucket array from input stream.
* @param in DataInput object containing data to initialize buckets
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,readFields,org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput),218,233,"/**
* Initializes object from data input stream.
* @param in DataInput stream containing object data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,probablyHasKey,org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable),264,272,"/**
* Checks membership in the Bloom filter using provided key.
* @param key WritableComparable object to check
* @return true if key is likely present, false otherwise
*/","* Checks if this MapFile has the indicated key. The membership test is
     * performed using a Bloom filter, so the result has always non-zero
     * probability of false positives.
     * @param key key to check
     * @return  false iff key doesn't exist, true if key probably exists.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,selectiveClearing,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)",199,235,"/**
* Applies a mask to the given Key based on the specified scheme.
* @param k Key object
* @param scheme Masking scheme (e.g. RANDOM, MINIMUM_FN, etc.)
*/","* Performs the selective clearing for a given key.
   * @param k The false positive key to remove from <i>this</i> retouched Bloom filter.
   * @param scheme The selective clearing scheme to apply.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,createOptionTableListing,org.apache.hadoop.fs.FsShell:createOptionTableListing(),292,295,"/**
* Creates a TableListing instance with default settings and mask values.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResources,"org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)",3082,3100,"/**
* Updates and processes resources from specified index onwards.
* @param properties configuration settings
* @param resources collection of resources to update
* @param startIdx starting index for processing
* @param fullReload flag for full reload operation
* @param quiet flag for suppressing output
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)",616,619,"/**
* Overloads m1 to accept either a single new key or an array of keys.
*/","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key to be deprecated
   * @param newKey key that take up the values of deprecated key
   * @param customMessage deprecation message",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])",640,643,"/**
 * Legacy wrapper function to call m1 with user ID and new keys.
 * @param key unique identifier of the user
 * @param newKeys array of updated keys
 */","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @deprecated use {@link #addDeprecation(String key, String newKey)} instead",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)",659,661,"/**
* Updates database key to a new value.","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKey key that takes up the value of deprecated key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,start,org.apache.hadoop.http.HttpServer2:start(),1382,1434,"/**
* Starts and initializes the web server.
* @throws IOException if an I/O error occurs
*/","* Start the server. Does not wait for the server to start.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,writeBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat),963,977,"/**
* Updates or creates a ZNode breadcrumb with the given application data.
* @param oldBreadcrumbStat previous breadcrumb stat (optional)
*/","* Write the ""ActiveBreadCrumb"" node, indicating that this node may need
   * to be fenced on failover.
   * @param oldBreadcrumbStat",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,tryDeleteOwnBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode(),985,1008,"/**
* Deletes the breadcrumb of an active node.
* @throws IllegalStateException if breadcrumb data does not match application data
*/","* Try to delete the ""ActiveBreadCrumb"" node when gracefully giving up
   * active status.
   * If this fails, it will simply warn, since the graceful release behavior
   * is only an optimization.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readVectored,"org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)",98,104,"/**
* Processes file ranges on the given stream.
* @param stream input stream
* @param ranges list of file ranges to process
* @param allocate callback for allocating buffer space
*/","* This is the default implementation which iterates through the ranges
   * to read each synchronously, but the intent is that subclasses
   * can make more efficient readers.
   * The data or exceptions are pushed into {@link FileRange#getData()}.
   * @param stream the stream to read the data from
   * @param ranges the byte ranges to read
   * @param allocate the byte buffer allocation
   * @throws IllegalArgumentException if there are overlapping ranges or a range is invalid
   * @throws EOFException the range offset is negative",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches(),294,306,"/**
* Executes operation and processes buffered data.
* Performs operation on prefetched or ready data blocks.
* @see #m1() for related operation execution
*/",* Requests cancellation of any previously issued prefetch requests.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int),77,86,"/**
* Retrieves data for the specified block number.
* @param blockNumber unique block identifier
* @return BufferData object containing block data or throws IOException on failure.","* Gets the block having the given {@code blockNumber}.
   *
   * The entire block is read into memory and returned as a {@code BufferData}.
   * The blocks are treated as a limited resource and must be released when
   * one is done reading them.
   *
   * @param blockNumber the number of the block to be read and returned.
   * @return {@code BufferData} having data from the given block.
   *
   * @throws IOException if there an error reading the given block.
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,readBlock,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",331,395,"/**
* Performs a read operation on the buffer, handling caching and prefetching.
* @param data BufferData object to operate on
* @param isPrefetch true for prefetching, false otherwise
* @param expectedState one or more states to set in the BufferData after operation
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,<init>,"org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)",83,95,"/**
* Initializes FilePosition object with fileSize and blockSize parameters.
* @param fileSize total size of the file
* @param blockSize size of each block in the file
*/","* Constructs an instance of {@link FilePosition}.
   *
   * @param fileSize size of the associated file.
   * @param blockSize size of each block within the file.
   *
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock(),204,206,"/**
 * Calls block data's m2 method with result of m1 as argument.
 */","* Determines whether the current block is the last block in this file.
   *
   * @return true if the current block is the last block in this file, false otherwise.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,toString,org.apache.hadoop.fs.impl.prefetch.FilePosition:toString(),275,296,"/**
* Generates a debug string describing the current buffer state.
* @return formatted debug string
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem:closeAll(),642,645,"/**
* Calls cache cleanup and initializes it.
* @throws IOException on cache operation failure
*/","* Close all cached FileSystem instances. After this operation, they
   * may not be used in any operations.
   *
   * @throws IOException a problem arose closing one or more filesystem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])",125,128,"/**
* Calls FSInputStream's m1() method to read data from input stream.
* @param position offset to start reading from
* @param buffer output buffer for read data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2044,2047,"/**
* Wraps existing m1() call with UTF-8 encoding. 
* @param fs file system instance
* @param path file path to operate on
* @param charseq character sequence for operation
*/","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fs the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)",122,125,"/**
* Creates a new BlockLocation object with provided parameters.
* @param names array of file names
* @param hosts array of hostnames
* @param offset block start position
* @param length block size in bytes
* @param corrupt flag indicating corrupted data
*/","* Constructor with host, name, offset, length and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)",135,138,"/**
* Constructs a BlockLocation object with specified parameters.
* @param names array of block identifiers
* @param hosts array of host names
* @param topologyPaths array of topology paths
* @param offset starting position in bytes
* @param length block length in bytes
*/","* Constructor with host, name, network topology, offset and length.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clone,org.apache.hadoop.fs.statistics.MeanStatistic:clone(),271,274,"/**
* Returns Mean statistic with mask.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,<init>,org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>(),58,59,"/**
* Initializes dynamic I/O statistics.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,aggregateMeanStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)",312,317,"/**
* Combines two MeanStatistics into one.
* @param l left MeanStatistic
* @param r right MeanStatistic
* @return combined MeanStatistic
*/","* Aggregate the mean statistics.
   * This returns a new instance.
   * @param l left value
   * @param r right value
   * @return aggregate value",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,snapshot,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics),160,168,"/**
* Synchronizes and updates counters, gauges, minimums, maximums, and mean statistics.
* @param source IOStatistics object containing data to update from
*/","* Take a snapshot.
   *
   * This completely overwrites the map data with the statistics
   * from the source.
   * @param source statistics source.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)",250,254,"/**
* Wraps call to main logging method with default logger.
* @param message log message
* @param source optional source object
*/","* Extract any statistics from the source and log to
   * this class's log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtLevel,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)",263,281,"/**
* Logs IO statistics at specified level.
* @param log Logger instance
* @param level logging level (INFO, ERROR, WARN)
* @param source Object to be logged
*/","* A method to log IOStatistics from a source at different levels.
   *
   * @param log    Logger for logging.
   * @param level  LOG level.
   * @param source Source to LOG.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,cleanupRemoteIterator,org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator),297,304,"/**
* Closes and logs statistics for a RemoteIterator.
* @param source Iterator to close and log
*/","* Clean up after an iteration.
   * If the log is at debug, calculate and log the IOStatistics.
   * If the iterator is closeable, cast and then cleanup the iterator
   * @param source iterator source
   * @param <T> type of source",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",460,466,"/**
* Calls m1 function with given parameters.
* @param factory DurationTrackerFactory instance
* @param statistic specific statistic name
* @param input InvocationRaisingIOE object for I/O operations
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @throws IOException IO failure.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,build,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build(),107,111,"/**
* Creates and returns an instance of IOStatisticsStore.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(),779,782,"/**
* Retrieves single byte value from oneByteBuf.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)",332,348,"/**
* Reads from underlying input source to buffer at specified position and offset.
* @param position current read position
* @param buffer data buffer
* @param offset starting offset in buffer
* @param length number of bytes to read
* @return actual number of bytes read or -1 on error
*/",Positioned read. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)",502,515,"/**
* Reads data from a positioned source.
* @param position current read position
* @param buffer target byte array
* @param offset initial buffer offset
* @param length number of bytes to read
*/",Positioned read fully. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)",353,369,"/**
* Reads data from the input stream to the byte buffer.
* @param position current stream position
* @param buf target byte buffer
* @return number of bytes read or -1 on error
*/",* Positioned read using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)",374,389,"/**
* Performs a positioned read operation using a ByteBuffer.
* @param position current file position
* @param buf ByteBuffer to read from
*/",* Positioned readFully using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer),588,639,"/**
* Reads data from ByteBuffer or ReadableByteChannel and updates internal buffers.
* @param buf input buffer
* @return number of bytes read or -1 on error
*/",ByteBuffer read.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",707,736,"/**
* Fetches enhanced byte buffer from input source.
* @param bufferPool ByteBuffer pool for allocation
* @param maxLength maximum length of the buffer
* @param opts read options (EnumSet<ReadOption>)
* @return enhanced ByteBuffer object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",118,122,"/**
* Initializes a crypto stream input from the given input stream.
* @param in input stream to encrypt
* @param codec encryption codec to use
* @param bufferSize buffer size for encryption
* @param key encryption key
* @param iv initialization vector
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,doEncode,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])",101,118,"/**
* Performs error correction using the raw encoder and mask functions.
* @param inputs input buffers for each parity unit
* @param outputs output buffers for each parity unit
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",50,54,"/**
* Encodes input chunks using M1 algorithm and writes to output chunks.
* @param inputChunks array of input chunks to encode
* @param outputChunks array of output chunks for encoded data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",165,234,"/**
* Writes Java object to output stream.
* @param out DataOutput stream
* @param instance Object to be written
* @param declaredClass Class of the object
* @param conf Configuration object (not used)
* @param allowCompactArrays Flag for compact arrays (not used)
*/","* Write a {@link Writable}, {@link String}, primitive type, or an array of
     * the preceding.  
     * 
     * @param allowCompactArrays - set true for RPC and internal or intra-cluster
     * usages.  Set false for inter-cluster, File, and other persisted output 
     * usages, to preserve the ability to interchange files with other clusters 
     * that may not be running the same version of software.  Sometime in ~2013 
     * we can consider removing this parameter and always using the compact format.
     *
     * @param conf configuration.
     * @param out dataoutput.
     * @param declaredClass declaredClass.
     * @param instance instance.
     * @throws IOException raised on errors performing I/O.
     *",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int),157,159,"/**
* Fetches buffer data by block number with mask functionality.
* @param blockNumber unique block identifier
*/","* Acquires a buffer if one is immediately available. Otherwise returns null.
   * @param blockNumber the id of the block to try acquire.
   * @return the acquired block's {@code BufferData} or null.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable(),600,602,"/**
 * Delegates call to underlying buffer pool's m1 method.
 */","* Number of ByteBuffers available to be acquired.
   *
   * @return the number of available buffers.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,run,org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task),268,282,"/**
* Determines whether to mask a task based on its status and service availability.
* @param task Task object containing I/O operations
*/","* Execute the task across the data.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",342,351,"/**
* Processes multiple path data items in bulk, handling exceptions individually.
* @param parent PathData object representing the parent context
* @param items  variable number of PathData objects to process
*/","*  Iterates over the given expanded paths and invokes
   *  {@link #processPath(PathData)} on each element.  If ""recursive"" is true,
   *  will do a post-visit DFS on directories.
   *  @param parent if called via a recurse, will be the parent dir, else null
   *  @param items a list of {@link PathData} objects to process
   *  @throws IOException if anything goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,getPathHandle,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path),163,166,"/**
* Retrieves and handles file path using Hadoop FS.
* @param filePath input file path
* @return PathHandle object or throws IOException if failed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,resolve,"org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])",359,362,"/**
* Returns a function to map FileStatus objects to PathHandles.
* @param fs FileSystem instance
* @param opt Handle options (variable arguments)
*/","* Utility function for mapping {@link FileSystem#getPathHandle} to a
     * fixed set of handle options.
     * @param fs Target filesystem
     * @param opt Options to bind in partially evaluated function
     * @return Function reference with options fixed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createPathHandle,"org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",177,180,"/**
* Returns a path handle based on file status and options.
* @param stat FileStatus object
* @param opts HandleOpt array (optional)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String),132,135,"/**
 * Creates a ShellCommandExecutor instance with specified parameters.
 * @param userName username to be used in execution
 */","* Create a ShellCommandExecutor object using the user's name.
   *
   * @param userName user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupIDExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String),153,156,"/**
* Creates ShellCommandExecutor instance with custom mask function.
* @param userName user name to be masked
*/","* Create a ShellCommandExecutor object for fetch a user's group id list.
   *
   * @param userName the user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)",1222,1225,"/**
* Constructs a new ShellCommandExecutor instance.
* @param execString array of shell command strings
* @param dir working directory for execution
* @param env environment variables to pass to the process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)",1373,1379,"/**
* Executes shell command with masking functionality.
* @param env environment variables
* @param cmd command to execute
* @param timeout execution time limit in milliseconds
* @return output of the executed command or null on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readProto,org.apache.hadoop.security.Credentials:readProto(java.io.DataInput),403,413,"/**
* Processes credentials data from input stream.
* @throws IOException if input operation fails
*/","* Populates keys/values from proto buffer storage.
   * @param in - stream ready to read a serialized proto buffer message",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,"org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)",463,476,"/**
 * Updates the local secret keys and tokens with values from another credentials object.
 * @param other the source Credentials object
 * @param overwrite whether to override existing key/token pairs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,collectDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)",98,138,"/**
* Recursively fetches and adds tokens from the given issuer hierarchy.
* @param issuer DelegationTokenIssuer to fetch tokens from
* @param renewer string used for token renewal
* @param credentials Credentials object containing existing tokens
* @param tokens list of fetched Token objects
*/","* NEVER call this method directly.
   *
   * @param issuer issuer.
   * @param renewer renewer.
   * @param credentials cache in which to add new delegation tokens.
   * @param tokens list of new delegation tokens.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,"org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",1712,1717,"/**
* Synchronizes and delegates to m1() with given alias and token.
* @param alias Text alias
* @param token Token with identifier
* @return True if operation successful
*/","* Add a named token to this UGI
   * 
   * @param alias Name of the token
   * @param token Token to be added
   * @return true on successful add of new token",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addRegexMountEntry,org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry),807,816,"/**
* Adds a regex mount point to the list.
* @param le LinkEntry object containing mount point details
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",176,181,"/**
* Resolves absolute or relative path within the given working directory.
* @param workDir base directory
* @param path input path to resolve
* @return resolved Path object
*/","* Resolve against given working directory.
   *
   * @param workDir
   * @param path
   * @return absolute path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",269,274,"/**
* Resolves a relative path to an absolute path by prepending the work directory.
* @param workDir base directory path
* @param path relative path to resolve
* @return absolute path or original path if already absolute
*/","* Resolve against given working directory. *
   * 
   * @param workDir
   * @param path
   * @return",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,makeAbsolute,org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),105,111,"/**
* Resolves file path by applying function mask.
* @param f input file path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,pathToFile,org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),119,125,"/**
* Creates a File object from the given Path, applying transformations as needed.
* @param path input Path to process
*/","* Convert a path to a File.
   *
   * @param path the path.
   * @return file.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fixRelativePart,org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path),2884,2890,"/**
* Recursively applies mask function to path.
* @param p input path
*/","* See {@link FileContext#fixRelativePart}.
   * @param p the path.
   * @return relative part.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,makeAbsolute,org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),269,271,"/**
* Returns a Path instance with working directory prepended if the original path is invalid.
* @param f original Path to check and possibly augment
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),188,191,"/**
* Updates the working directory to the provided path.
* If the provided path is a subdirectory of the current working directory,
* it will be returned as an absolute path; otherwise, it will be relative to the current working directory. 
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,"org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)",562,598,"/**
* Constructs a URI for the FUNC_MASK file based on the working directory and default URI.
* @param defaultUri default URI to use when constructing the new URI
* @param workingDir working directory to use as the base path
* @return new Path object representing the constructed URI
*/","* Returns a qualified path object.
   *
   * @param defaultUri if this path is missing the scheme or authority
   * components, borrow them from this URI
   * @param workingDir if this path isn't absolute, treat it as relative to this
   * working directory
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,fixRelativePart,org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path),284,291,"/**
* Returns a path with working directory prepended if it's not absolute.
* @param p input path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory(),273,276,"/**
* Returns a Path instance representing the m1 function URI. 
* @return Path object for m1 function endpoint",* return the top level archive.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.HarFileSystem:getHomeDirectory(),809,812,"/**
* Returns a path representing the FUNC_MASK URI.
*/",* return the top level archive path.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp),680,686,"/**
* Returns SFTP function mask path or null on error.
* @param channel SFTP connection channel
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),58,63,"/**
* Initializes a ChecksumFs with an underlying filesystem.
* @param theFs AbstractFileSystem instance
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory(),842,845,"/**
* Returns a path to function directory.
* @return Path object representing function directory location
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory(),861,864,"/**
* Returns system path to user's home directory. 
* @return system path as Path object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,abort,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)",245,261,"/**
* Performs file system operations for the given upload ID and file path.
* @param uploadId unique identifier for the upload
* @param filePath path to the file in the file system
* @return a CompletableFuture indicating completion of the operation
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,lookupStat,"org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)",175,186,"/**
* Retrieves file status for a given path, handling FileNotFoundExceptions.
* @param fs FileSystem instance
* @param pathString file system path as string
* @param ignoreFNF whether to ignore or rethrow FileNotFoundExceptions
* @return FileStatus object or null if not found
*/","* Get the FileStatus info
   * @param ignoreFNF if true, stat will be null if the path doesn't exist
   * @return FileStatus for the given path
   * @throws IOException if anything goes wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getPath,org.apache.hadoop.fs.PathIOException:getPath(),107,107,"/**
 * Returns a Path representing the function's mask. 
 * @return Path object representing the function's mask
 */",@return Path that generated the exception,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getTargetPath,org.apache.hadoop.fs.PathIOException:getTargetPath(),110,112,"/**
* Returns a path to function mask based on target path.
* @return Path object or null if targetPath is null
*/","@return Path if the operation involved copying or moving, else null",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(),2719,2722,"/**
* Returns the total size of the file system rooted at the specified path.
* @param path root directory of the file system
*/","* Return the total size of all files in the filesystem.
   * @throws IOException IO failure
   * @return the number of path used.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory(),422,434,"/**
* Returns the user's home directory path.
* @return Path object representing the user's home directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints(),1060,1070,"/**
* Retrieves an array of mounted filesystems.
* @return Array of MountPoint objects representing filesystem mounts
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)",105,116,"/**
* Initializes a ChRootedFileSystem with the given file system and URI.
* @param fs underlying file system
* @param uri URI of the chroot directory
*/","* Constructor
   * @param fs base file system
   * @param uri base uri
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),177,181,"/**
* Constructs a new path by appending the file root part to the input path.
* @param f input path to be modified
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory(),314,326,"/**
* Returns the user's home directory path.
* @return Path object representing the user's home directory or null if not set
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints(),720,730,"/**
* Retrieves an array of MountPoint objects from the file system state.
* @return array of MountPoint objects
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getRemainingPathStr,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)",207,215,"/**
* Constructs a new Path object from the given source and resolved paths.
* @param srcPath source path string
* @param resolvedPathStr resolved path string
* @return Path object or null if invalid
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,createLink,"org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",423,505,"/**
* Creates a symbolic link to the target resource at the specified path.
* @param src source path
* @param target target URI or directory name
* @param linkType type of link (SINGLE, MERGE, etc.)
* @param settings additional settings for the link
* @param aUgi user/group information for access control
* @throws URISyntaxException if the target URI is invalid
* @throws IOException if an I/O error occurs
* @throws FileAlreadyExistsException if the target path already exists",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRemainingPath,"org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)",996,1008,"/**
* Constructs a partial path from the provided array, starting at the specified index.
* @param path array of path elements
* @param startIndex index to start constructing the path from
* @return Path object representing the constructed partial path or SlashPath if out-of-bounds","* Return remaining path from specified index to the end of the path array.
   * @param path An array of path components split by slash
   * @param startIndex the specified start index of the path array
   * @return remaining path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getTargetLink,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink(),370,377,"/**
* Concatenates directory links into a single path string.
* @return concatenated path or null if no links are provided
*/","* Get the target of the link. If a merge link then it returned
     * as "","" separated URI list.
     *
     * @return the path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)",119,121,"/**
* Constructs a Path instance by combining two existing Paths.
* @param parent parent directory path
* @param child child file or directory path
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)",129,131,"/**
* Creates a new path by appending a child element to the specified parent.
* @param parent the parent directory or file
* @param child the name of the child element to append
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)",139,141,"/**
 * Initializes a new Path instance by delegating to its constructor.
 * @param parent directory path of the parent
 * @param child child path object
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,rename,"org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)",898,905,"/**
* Renames a file or directory in the given filesystem.
* @param fs FileSystem instance
* @param oldName original path name
* @param newName new path name
*/","* Renames an existing map directory.
   * @param fs fs.
   * @param oldName oldName.
   * @param newName newName.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,insecureCreateForWrite,"org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)",243,262,"/**
* Creates a FileOutputStream for the specified file, applying permissions.
* @param f File object to create output stream for
* @param permissions int value representing file permissions
* @return FileOutputStream instance or null on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,fileToPath,org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File),93,95,"/**
* Creates a path from file metadata.
* @param f File object containing m1 and m2 data
*/",Add the service prefix for a local filesystem.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,unnestUri,org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI),80,101,"/**
* Constructs a Path object from the given URI.
* @param nestedUri input URI to process
* @return constructed Path object
*/","* Convert a nested URI to decode the underlying path. The translation takes
   * the authority and parses it into the underlying scheme and authority.
   * For example, ""myscheme://hdfs@nn/my/path"" is converted to
   * ""hdfs://nn/my/path"".
   * @param nestedUri the URI from the nested URI
   * @return the unnested path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructNewPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path),300,302,"/**
* Creates a new path by appending '_NEW' to the original path.
* @param path original path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructOldPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path),304,306,"/**
* Prefixes input path with ""_OLD"" to create a mask.
* @param path original file or directory path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,stringToPath,org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[]),273,282,"/**
* Creates an array of Path objects from the given string array.
* @param str array of strings to convert
* @return array of Path objects or null if input is null
*/","* stringToPath.
   * @param str str.
   * @return path array.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeQualified,org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path),400,412,"/**
* Resolves file system path for function mask processing.
* @param path input path to resolve
* @return resolved Path object or null if failed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getPathWithoutSchemeAndAuthority,org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path),104,111,"/**
* Applies mask operation to the given file system path.
* @param path input file system path
*/","* Return a version of the given Path without the scheme information.
   *
   * @param path the source Path
   * @return a copy of this Path without the scheme information",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,mergePaths,"org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,286,"/**
* Combines two paths using a custom mask operation.
* @param path1 primary path
* @return combined Path object
*/","* Merge 2 paths such that the second path is appended relative to the first.
   * The returned path has the scheme and authority of the first path.  On
   * Windows, the drive specification in the second path is discarded.
   * 
   * @param path1 the first path
   * @param path2 the second path, to be appended relative to path1
   * @return the merged path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParentUtil,org.apache.hadoop.fs.Path:getParentUtil(),444,459,"/**
* Constructs a file system path for function masks.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,apply,"org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)",59,63,"/**
* Processes PathData with mask function at specified depth.
* @param item PathData object to process
* @param depth level of processing (int)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData),197,214,"/**
* Calculates and displays the checksum of a PathData item.
* @param item PathData object to process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,checkIfExists,org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement),220,233,"/**
* Validates file type requirement for the existing path.
* @param typeRequirement file type to validate (directory or not)
*/","* Ensure that the file exists and if it is or is not a directory
   * @param typeRequirement Set it to the desired requirement.
   * @throws PathIOException if file doesn't exist or the type does not match
   * what was specified in typeRequirement.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getStringForChildPath,org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path),319,328,"/**
* Generates a function mask string from the given child path.
* @param childPath input path
*/","* Given a child of this directory, use the directory's path and the child's
   * basename to construct the string to the child.  This preserves relative
   * paths since Path will fully qualify.
   * @param childPath a path contained within this directory
   * @return String of the path relative to this directory",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPath,org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData),285,321,"/**
* Formats and writes path data to output stream.
* @param item PathData object containing file status and other metadata
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,rename,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",546,563,"/**
* Renames or deletes source path data and updates target path data.
* @param src original path data
* @param target destination path data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData),205,217,"/**
* Validates and possibly creates a directory mask for the given item.
* @param item PathData object to process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processPath,org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData),58,67,"/**
* Validates file system path existence and type.
* @param item PathData object containing file statistics
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),143,148,"/**
* Verifies directory existence by checking its mask.
* @param item PathData object to validate
* @throws IOException if directory check fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processNonexistentPath,org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),330,332,"/**
* Throws a PathNotFoundException for the given PathData item.
* @param item PathData object to check for existence
*/","*  Provides a hook for handling paths that don't exist.  By default it
   *  will throw an exception.  Primarily overriden by commands that create
   *  paths such as mkdir or touch.
   *  @param item the {@link PathData} that doesn't exist
   *  @throws FileNotFoundException if arg is a path and it doesn't exist
   *  @throws IOException if anything else goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),57,62,"/**
* Verifies that the given path is a directory.
* @param item PathData object to check
* @throws IOException if verification fails or an exception occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",111,128,"/**
* Validates path transformation by checking URI and existence.
* @param src source PathData object
* @param target target PathData object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),102,107,"/**
* Validates directory existence.
* @param item PathData object containing path and status information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processPath,org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData),75,97,"/**
* Truncates file by ID, throwing exceptions or waiting on block recovery as needed.
* @param item PathData object containing file information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processPath,org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData),81,103,"/**
* Handles symbolic links and sets file replication.
* @param item PathData object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",61,68,"/**
* Verifies and enforces path uniqueness before copying.
* @param src source data
* @param target target data to copy into
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,postProcessPath,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData),70,78,"/**
* Removes file mask from the given path.
* @param src PathData object containing source information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFSofPath,org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path),325,337,"/**
* Performs file system operations on the given path.
* @param absOrFqPath absolute or fully qualified path to operate on
*/","* Get the file system of supplied path.
   * 
   * @param absOrFqPath - absolute or fully qualified path
   * @return the file system of the path
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>absOrFqPath</code> is not supported.
   * @throws IOException If the file system for <code>absOrFqPath</code> could
   *         not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",102,122,"/**
* Initializes a ChRootedFs instance with the specified filesystem and root path.
* @param fs AbstractFileSystem object
* @param theRoot Path to the chrooted directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUriPath,org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path),189,192,"/**
* Wraps file system operation with MyFs instance.
* @param p file path to operate on (must not be null)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path),328,338,"/**
* Resolves a file path using the inode tree.
* @param f input file path
* @return the resolved path, or the original path if unresolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,resolvePath,org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path),168,172,"/**
* Wraps FS operation with error handling.
* @param p file path to access
* @return resulting path or throws exception if failed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createInternal,"org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",88,97,"/**
* Wraps native file system operation for FSDataOutputStream creation.
* @param f path to the file
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link is encountered
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,delete,"org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)",99,104,"/**
* Calls M1 and delegates recursive file system traversal to MyFS.
* @param f Path object representing the file or directory
* @param recursive whether to traverse subdirectories
* @return true if successful, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",106,111,"/**
* Calls M1 and delegates to FS for block locations.
* @param f file path
* @param start starting offset in bytes
* @param len length of data to retrieve
* @return array of BlockLocation objects or null if failed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileChecksum,org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path),113,118,"/**
* Calls m1 and then delegates to myFs's m2 to compute file checksum.
* @param f Path to the file
* @return FileChecksum object or throws exception on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileStatus,org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path),120,125,"/**
* Calls underlying file system to fetch status of a given path.
* @param f Path object representing the file or directory
* @return FileStatus object describing the file's metadata
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path),139,144,"/**
* Calls underlying file system to fetch status of a given path.
* @param f the path to query
* @return FileStatus object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listStatus,org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path),194,199,"/**
* Calls M1 and delegates to MyFS's M2.
* @param f input file path
* @return array of file statuses or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listLocatedStatus,org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path),201,207,"/**
* Wraps existing file system operation to return a remote iterator.
* @param f path to the file or directory being iterated over
* @return RemoteIterator of LocatedFileStatus objects
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,mkdir,"org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",215,221,"/**
* Calls m1 and then delegates to myFs.m2 for directory operations.
* @param dir Path to the directory
* @param permission FsPermission for the directory
* @param createParent whether to create parent directories if needed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path),223,228,"/**
* Calls internal file system operations and returns data input stream.
* @param f Path to the file
* @return FSDataInputStream object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,"org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)",230,235,"/**
* Wraps m1() call and delegates to myFs.m2() with provided buffer size.
* @param f file path
* @param bufferSize memory buffer size for reading
* @return FSDataInputStream object or throws IOException/UnresolvedLinkException",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,truncate,"org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)",237,243,"/**
* Updates file metadata to specified length.
* @param f file Path object
* @param newLength desired file length in bytes
* @throws various exceptions on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setOwner,"org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",261,267,"/**
* Calls m1 and then delegates file operations to myFs.m2.
* @param f the Path object
* @param username user name for access control
* @param groupname group name for access control
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setPermission,"org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",269,274,"/**
* Invokes File System operations on the specified file.
* @param f Path to the file
* @param permission File system permissions
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setReplication,"org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)",276,281,"/**
* Calls m1() and delegates to myFs.m2() with provided parameters.
* @param f file path
* @param replication replication factor
* @return true if successful (m1()'s result is ignored), false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setTimes,"org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)",283,288,"/**
* Calls internal file system operation and updates metadata.
* @param f Path to the file
* @param mtime last modified timestamp
* @param atime last accessed timestamp
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,"org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",764,771,"/**
* Updates file system permissions and returns true if directory exists.
* @param fs FileSystem object
* @param dir Path to the directory
* @param permission FsPermission value to set
* @return true if directory exists; false otherwise
*/","* Create a directory with the provided permission.
   * The permission of the directory is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * @see #create(FileSystem, Path, FsPermission)
   *
   * @param fs FileSystem handle
   * @param dir the name of the directory to be created
   * @param permission the permission of the directory
   * @return true if the directory creation succeeds; false otherwise
   * @throws IOException A problem creating the directories.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,mkdirs,org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path),986,989,"/**
* Delegate file system operation to underlying FS implementation.
* @param f Path object representing the file or directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,mkdirs,org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path),339,342,"/**
 * Delegates file system operation to underlying FS implementation. 
 * @param f input Path object 
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto),49,106,"/**
* Creates a FileStatus object from the given proto.
* @param proto FileStatusProto object to parse
* @return FileStatus object or throws exception if invalid
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)",151,158,"/**
* Initializes a FileStatus object with given parameters.
* @param length file size
* @param isdir directory flag
* @param block_replication replication factor
* @param blocksize block size
* @param modification_time last modified time
* @param access_time last accessed time
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param symlink symbolic link path
* @param path file path
* @param hasAcl ACL present flag
* @param isEncrypted encrypted flag
* @param isErasureCoded erasure coded flag",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])",142,149,"/**
* Initializes a LocatedFileStatus object with file metadata.
* @param locations array of block locations for the file
*/","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param attr Attribute flags (See {@link FileStatus.AttrFlags}).
   * @param locations a file's block locations",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getPermission,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission(),59,62,"/**
* Calls superclass to retrieve file system permissions.
* @return FsPermission object representing file system access rights
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",1458,1461,"/**
* Convenience wrapper to call m1 with Object parameters.
*/","* Append a key/value pair.
     * @param key input Writable key.
     * @param val input Writable val.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFile,"org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)",3431,3438,"/**
* Writes masked key-value pairs to the output stream.
* @param records iterator over raw key-value data
* @param writer output writer for masked data
*/","* Writes records from RawKeyValueIterator into a file represented by the 
     * passed writer.
     * @param records the RawKeyValueIterator
     * @param writer the Writer created earlier 
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,init,org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration),152,178,"/**
* Initializes service state based on configuration.
* @param conf service configuration
*/","* {@inheritDoc}
   * This invokes {@link #serviceInit}
   * @param conf the configuration of the service. This must not be null
   * @throws ServiceStateException if the configuration was null,
   * the state change not permitted, or something else went wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,stop,org.apache.hadoop.service.AbstractService:stop(),213,240,"/**
* Stops the service, invoking necessary cleanup and notifications.
*/",* {@inheritDoc},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata),797,820,"/**
* Compares two Metadata objects for equality.
* @param other the other metadata object to compare
* @return true if equal, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,handleKind,org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text),528,531,"/**
* Evaluates mask condition based on text type.
* @param kind Text type to evaluate
* @return true if match, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.java,selectToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)",46,60,"/**
* Finds a token with matching kind and service properties.
* @param service Text service object
* @param tokens Collection of tokens to search
* @return Matching Token<TokenIdent> or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isPrivateCloneOf,org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text),279,282,"/**
* Checks if text matches public service mask criteria.
* @param thePublicService Text object to evaluate
*/","* Whether this is a private clone of a public token.
     * @param thePublicService the public service name
     * @return true when the public service is the same as specified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token:equals(java.lang.Object),386,400,"/**
* Compares this user profile with another object for equality.
* @param right the object to compare with
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchAlias,"org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",73,75,"/**
 * Checks if given token matches specified text alias.
 * @param token Token object to check
 * @param alias Text alias for matching
 */",Match token service field to alias text.  True if alias is null.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchService,"org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)",78,83,"/**
* Checks if a URL matches the specified service.
* @param fetcher DtFetcher instance
* @param service Text object representing the service
* @param url Text object containing the URL to check
*/",Match fetcher's service name to the service text and/or url prefix.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)",1008,1018,"/**
* Retrieves a suitable authentication token based on credentials and service.
* @param creds Credentials object
* @param service Text representation of the service
* @return Token object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,handleKind,org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text),179,182,"/**
* Checks if text matches specific token type.
* @param kind Text object to check
* @return true if match found, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,org.apache.hadoop.util.DiskChecker:checkDir(java.io.File),76,78,"/**
 * Applies file system mask to specified directory.
 * @param dir directory path to apply mask to
 */","* Create the directory if it doesn't exist and check that dir is readable,
   * writable and executable
   *  
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File),88,92,"/**
 * Applies disk error handling and masking operations on the specified directory.
 * @param dir directory to process
 */","* Create the directory if it doesn't exist and check that dir is
   * readable, writable and executable. Perform some disk IO to
   * ensure that the disk is usable for writes.
   *
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,org.apache.hadoop.fs.FSOutputSummer:flushBuffer(),145,147,"/**
 * Initializes and starts main processing flow.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flush,org.apache.hadoop.fs.FSOutputSummer:flush(),183,185,"/**
* Disables all function masks.
*/","* Checksums all complete data chunks and flushes them to the underlying
   * stream. If there is a trailing partial chunk, it is not flushed and is
   * maintained in the buffer.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeSingle,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)",123,222,"/**
* Performs error correction and masking on input data.
* @param inputs input data buffers
* @param outputs output data buffers
* @param erasedLocationToFix location to fix in the parity units
* @param bufSize buffer size
* @param isDirect whether to perform direct or indirect operations
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeMultiAndParity,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)",265,351,"/**
* Corrects errors in Reed-Solomon encoded data by masking parity units.
* @param inputs input buffers for data and parity
* @param outputs output buffers for corrected data and parity
* @param erasedLocationToFix locations to fix in the output buffers
* @param bufSize buffer size used in the correction process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",73,128,"/**
* Updates and decodes input buffers while removing specified indexes.
* @param inputs original input buffers
* @param erasedIndexes indices of removed elements
* @param outputs updated output buffers
*/","* Validate outputs decoded from inputs, by decoding an input back from
   * the outputs and comparing it with the original one.
   *
   * For instance, in RS (6, 3), let (d0, d1, d2, d3, d4, d5) be sources
   * and (p0, p1, p2) be parities, and assume
   *  inputs = [d0, null (d1), d2, d3, d4, d5, null (p0), p1, null (p2)];
   *  erasedIndexes = [1, 6];
   *  outputs = [d1, p0].
   * Then
   *  1. Create new inputs, erasedIndexes and outputs for validation so that
   *     the inputs could contain the decoded outputs, and decode them:
   *      newInputs = [d1, d2, d3, d4, d5, p0]
   *      newErasedIndexes = [0]
   *      newOutputs = [d0']
   *  2. Compare d0 and d0'. The comparison will fail with high probability
   *     when the initial outputs are wrong.
   *
   * Note that the input buffers' positions must be the ones where data are
   * read: If the input buffers have been processed by a decoder, the buffers'
   * positions must be reset before being passed into this method.
   *
   * This method does not change outputs and erasedIndexes.
   *
   * @param inputs input buffers used for decoding. The buffers' position
   *               are moved to the end after this method.
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers, which are ready to be read after
   *                the call
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",168,173,"/**
* Recursively processes input and output ECChunks.
* @param inputs encoded chunks to process
* @param erasedIndexes indexes of chunks to erase
* @param outputs buffer for processed output chunks
*/","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * Note, for both input and output ECChunks, no mixing of on-heap buffers and
   * direct buffers are allowed.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",55,68,"/**
* Calls the superclass method with transformed input/output arrays.
* @param inputs input buffers (modified by subclass)
* @param erasedIndexes array of indexes to be removed
* @param outputs output buffers (modified by subclass)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])",70,83,"/**
* Calls helper method to process inputs and calls superclass method with results.
* @param inputs input byte arrays
* @param erasedIndexes index arrays for erasures
* @param outputs output byte arrays
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),73,84,"/**
* Encodes inputs using FUNC_MASK algorithm.
* @param decodingState state object containing input/output buffers and indices
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,101,"/**
* Performs byte array decoding with masking.
* @param decodingState state object containing input/output data and offsets
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock(),511,570,"/**
* Processes block header, either by-block or normal mode.
* @throws IOException if invalid block header encountered
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset(),274,280,"/**
* Resets and initializes output stream with CBZip2 compression.
* @throws IOException on write error
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,writeRun,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun(),654,706,"/**
* Updates data block with character and run length information.
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Merger:close(),1148,1157,"/**
* Closes all input readers and output writer resources.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close(),256,271,"/**
* Closes the current block and updates state.
* @throws IOException on failure
*/","* Signaling the end of write to the block. The block register will be
       * called for registering the finished block.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup(),3887,3892,"/**
* Clears and initializes mask segment.
* @throws IOException on I/O error
*/","* The default cleanup. Subclasses can override this with a custom
       * cleanup.
       * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close(),557,569,"/**
* Closes and flushes block state.
* @throws IOException on write error
*/",* Finishing reading the block. Release all resources.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeWritableOutputStream,org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream),321,326,"/**
* Writes function metadata to output stream.
* @param os DataOutputStream instance
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readFields,org.apache.hadoop.security.Credentials:readFields(java.io.DataInput),420,443,"/**
* Reads and populates token and secret key maps from input stream.
* @param in DataInput stream containing tokens and secret keys
*/","* Loads all the keys.
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenIdent,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[]),253,260,"/**
* Deserializes a byte array into a TokenIdent object.
* @param tokenIdentBytes input bytes to deserialize
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[]),411,427,"/**
* Retrieves and processes a token from the given byte array data.
* @param data input byte array
* @return TokenIdent object or null on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenRemoved,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData),429,435,"/**
* Processes child data and updates tokens.
* @param data ChildData object containing binary data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)",652,679,"/**
* Retrieves DelegationTokenInformation from ZooKeeper.
* @param nodePath unique token node path
* @param quiet suppresses error logging if true
* @return DelegationTokenInformation object or null on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer),2662,2697,"/**
* Fetches and processes compressed or uncompressed key from underlying buffer.
* @param key DataOutputBuffer to read key from
*/","* Read 'raw' keys.
     * @param key - The buffer into which the key is read
     * @return Returns the key length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable),2376,2408,"/**
* Processes and writes configurable data to output stream.
* @param val Writable object containing configuration data
*/","* Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object),2415,2448,"/**
* Processes and returns the given object, applying configurable settings and handling compressed block reads.
* @param val the object to process
* @return the processed object or null if an exception occurs
*/","* @return Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRaw,"org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)",2603,2654,"/**
* Computes the length of a compressed or uncompressed key-value pair.
* @param key DataOutputBuffer for key data
* @param val ValueBytes object for value data
* @return total length of key and value, or -1 on error
*/","* Read 'raw' records.
     * @param key - The buffer into which the key is read
     * @param val - The 'raw' value
     * @return Returns the total record length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),2765,2790,"/**
* Decodes compressed or decompressed user value bytes.
* @param val ValueBytes object to process
* @return length of decoded value
*/","* Read 'raw' values.
     * @param val - The 'raw' value
     * @return Returns the value length
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getTokenInfoFromSQL,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),238,251,"/**
* Retrieves DelegationTokenInformation for the given TokenIdent.
* @param ident unique token identifier
* @return DelegationTokenInformation object or throws exception if not found
*/","* Obtains the DelegationTokenInformation associated with the given
   * TokenIdentifier in the SQL database.
   * @param ident Existing TokenIdentifier in the SQL database.
   * @return DelegationTokenInformation that matches the given TokenIdentifier or
   *         null if it doesn't exist in the database.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,read,org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput),114,118,"/**
* Parses DataInput to create a PermissionStatus object.
* @param in input stream containing permission data
*/","* Create and initialize a {@link PermissionStatus} from {@link DataInput}.
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return PermissionStatus.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readEnum,"org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)",422,425,"/**
* Reads an enum value from input stream and returns it.
* @param in DataInput stream containing the enum value
* @param enumType class of the enum to read
* @return Enum instance or null if not found
*/","* Read an Enum value from DataInput, Enums are read and written 
   * using String values. 
   * @param <T> Enum type
   * @param in DataInput to read from 
   * @param enumType Class type of Enum
   * @return Enum represented by String read from DataInput
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,readFields,org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput),326,330,"/**
* Reads and processes ACL string from input stream.
* @param in DataInput stream containing ACL data
*/",* Deserializes the AccessControlList object,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int),561,577,"/**
* Retrieves DelegationKey by ID with fallback to ZooKeeper.
* @param keyId unique key identifier
* @return DelegationKey object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput),790,796,"/**
* Writes metadata indices to output stream.
* @param out DataOutput stream
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),85,87,"/**
* Retrieves a ByteString from Protobuf helper.
* @param key Text object used as input
*/","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),142,149,"/**
* Builds a TokenProto object from the given token.
* @param tok Token object to extract data from
*/","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,copyToken,org.apache.hadoop.security.token.Token:copyToken(),114,116,"/**
* Creates a token with this instance as its value.
* @return a Token object representing this instance
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",160,164,"/**
* Wraps the main token retrieval call with a null argument.
* @param ugi UserGroupInformation instance
* @param renewer string identifier for the renewer
* @return Token object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",49,53,"/**
* Constructs a DelegationTokenIdentifier with specified attributes.
* @param kind token type
* @param owner token owner
* @param renewer token renewer
* @param realUser real user associated with the token
*/","* Create a new delegation token identifier
   *
   * @param kind token kind
   * @param owner the effective username of the token owner
   * @param renewer the username of the renewer
   * @param realUser the real username of the token owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(),51,53,"/**
 * Initializes an empty token identifier with default values.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)",40,47,"/**
* Initializes an instrumented read-write lock with customizable logging and thresholds.
* @param fair whether the lock is fair
* @param name identifier for log messages
* @param logger logging instance
* @param minLoggingGapMs minimum gap between log entries (ms)
* @param lockWarningThresholdMs threshold for warning about long locks (ms)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeOnce,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce(),89,117,"/**
* Executes method with retries and failovers.
* @throws Exception on failure or retry policy exceeded
* @return result of successful execution or error object
*/",Invoke the call once without retrying.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey(),1596,1611,"/**
* Fetches key-value pair from input stream.
* @throws IOException on read error
*/","* check whether we have already successfully obtained the key. It also
       * initializes the valueInputStream.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable),1706,1720,"/**
* Reads data from input stream and returns a FUNC_MASK long value.
* @param value BytesWritable object to read into
*/","* Copy the value into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual value size. The implementation
         * directly uses the buffer inside BytesWritable for storing the value.
         * The call does not require the value length to be known.
         * 
         * @param value value.
         * @throws IOException raised on errors performing I/O.
         * @return long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,writeValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream),1747,1763,"/**
* Transfers data to OutputStream and returns total size transferred.
* @param out target output stream
* @return total size in bytes or -1 on error
*/","* Writing the value to the output stream. This method avoids copying
         * value data from Scanner into user buffer, then writing to the output
         * stream. It does not require the value length to be known.
         * 
         * @param out
         *          The output stream
         * @return the length of the value
         * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[]),146,149,"/**
* Invokes m1 with default offset and length.
* @param b byte array to process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close(),186,197,"/**
* Closes the resource by repeatedly processing data until completion.
* @throws IOException on I/O errors
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[]),1932,1934,"/**
* Returns the result of m1 with default offset and length.
* @param buf input byte array
*/","* Compare the entry key to another key. Synonymous to compareTo(key, 0,
         * key.length).
         * 
         * @param buf
         *          The key buffer.
         * @return comparison result between the entry key with the input key.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,equals,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object),1966,1971,"/**
* Compares this entry with another object for equality.
* @param other Object to compare with
* @return true if equal, false otherwise
*/",* Compare whether this and other points to the same key value.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)",188,203,"/**
* Obtains a Delegation Token by URL and authentication details.
* @param url the URL to fetch the token from
* @param token authenticated token for the request
* @param renewer user or service requesting the token
* @param doAsUser user on behalf of whom the token is requested
* @return DelegationToken object or null if not obtained
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",237,245,"/**
* Retrieves a long value from the JSON response of renewing a delegation token.
* @param url URL to access
* @param token authenticated token
* @param dToken delegation token
* @param doAsUser user to perform operation as
* @return long value or throws exception if failed","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",275,286,"/**
* Cancels a delegation token using the provided credentials.
* @param url URL to access the service
* @param token Authentication token for the operation
* @param dToken Token to cancel
* @param doAsUser User to perform the cancellation as
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String),468,470,"/**
* Returns a Node instance using the provided scope.
* @param scope unique identifier for the node scope
*/","* Randomly choose a node.
   *
   * @param scope range of nodes from which a node will be chosen
   * @return the chosen node
   *
   * @see #chooseRandom(String, Collection)",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,sortByDistance,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",285,300,"/**
* Processes a Node and its children recursively.
* @param reader Node to process
* @param nodes array of child Nodes
* @param activeLen length of active data
*/","* Sort nodes array by their distances to <i>reader</i>.
   * <p>
   * This is the same as {@link NetworkTopology#sortByDistance(Node, Node[],
   * int)} except with a four-level network topology which contains the
   * additional network distance of a ""node group"" which is between local and
   * same rack.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,"org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)",496,503,"/**
* Creates a SocketInputWrapper instance with the given timeout.
* @param socket network socket
* @param timeout connection timeout in milliseconds
* @return SocketInputWrapper object or throws IOException if an error occurs
*/","* Return a {@link SocketInputWrapper} for the socket and set the given
   * timeout. If the socket does not have an associated channel, then its socket
   * timeout will be set to the specified value. Otherwise, a
   * {@link SocketInputStream} will be created which reads with the configured
   * timeout.
   * 
   * Any socket created using socket factories returned by {@link #NetUtils},
   * must use this interface instead of {@link Socket#getInputStream()}.
   * 
   * In general, this should be called only once on each socket: see the note
   * in {@link SocketInputWrapper#setTimeout(long)} for more information.
   *
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. zero for waiting as
   *                long as necessary.
   * @return SocketInputWrapper for reading from the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,"org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)",550,554,"/**
* Returns an OutputStream instance based on socket status.
* @param socket active socket connection
* @param timeout connection timeout in milliseconds
* @return SocketOutputStream or existing OutputStream if available
*/","* Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. This may not always apply. zero
   *        for waiting as long as necessary.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,writeMetric,org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String),150,157,"/**
* Sends a metric to StatsD with the given line.
* @param line metric data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>(),38,40,"/**
 * Initializes the network topology with a root node group.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)",590,636,"/**
* Establishes a connection to a remote endpoint.
* @param socket active socket object
* @param endpoint destination address and port
* @param localAddr source address and port (optional)
* @param timeout connection timeout in milliseconds
*/","* Like {@link NetUtils#connect(Socket, SocketAddress, int)} but
   * also takes a local address and port to bind the socket to. 
   * 
   * @param socket socket.
   * @param endpoint the remote address
   * @param localAddr the local address to bind the socket to
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)",90,98,"/**
* Initializes MetricsSourceAdapter with provided parameters and configuration.
* @param prefix        metric prefix
* @param name           metric name
* @param description    metric description
* @param source         underlying metrics source
* @param injectedTags   additional tags to inject into metrics
* @param period         sampling period
* @param conf            metrics configuration",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,snapshotMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)",420,427,"/**
* Snapshots metrics from the given adapter.
* @param sa MetricsSourceAdapter instance
* @param bufferBuilder MetricsBufferBuilder object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateJmxCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache(),160,194,"/**
* Updates JMX cache and clears metrics records.
* @return none
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)",89,114,"/**
* Registers an MBean with the given properties.
* @param serviceName JMX service name
* @param nameName ObjectName name
* @param properties bean registration properties
* @return registered ObjectName or null on failure
*/","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param properties - Key value pairs to define additional JMX ObjectName
   *                     properties.
   * @param theMbean    - the MBean to register
   * @return the named used to register the MBean",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,unregisterSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String),245,258,"/**
* Updates metrics and callbacks for the specified name.
* @param name unique identifier for the update operation
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources(),460,469,"/**
* Stops and releases all metrics sources.
* @param none
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newInverseQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",240,251,"/**
* Creates and initializes a MutableQuantiles object.
* @param name unique quantile metric identifier
* @param desc metric description
* @param sampleName sampling information
* @param valueName value associated with the quantile
* @param interval positive interval value for quantile calculation
* @return initialized MutableQuantiles object or null on error
*/","* Create a mutable inverse metric that estimates inverse quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Rate"")
   * @param interval rollover interval of estimator in seconds
   * @return a new inverse quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,<init>,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>(),53,71,"/**
* Initializes disk validator metrics with quantile intervals.
* @param quantileIntervals array of interval values
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)",196,204,"/**
* Initializes a retry cache with specified name, percentage, and expiration time.
* @param cacheName unique cache identifier
* @param percentage percentage of capacity to compute
* @param expirationTime time-to-live in milliseconds
*/","* Constructor
   * @param cacheName name to identify the cache by
   * @param percentage percentage of total java heap space used by this cache
   * @param expirationTime time for an entry to expire in nanoseconds",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class),71,81,"/**
* Processes a given protocol class, logging and performing actions based on its methods.
* @param protocol the Class<?> to process
*/","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[]),89,93,"/**
* Applies mask operation to each name in the provided array.
* @param names array of strings to be processed
*/","* Initialize the registry with all rate names passed in.
   * This is an alternative to the above init function since this metric
   * can be used more than just for rpc name.
   * @param names the array of all rate names",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,aggregateLocalStatesToGlobalMetrics,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap),149,157,"/**
* Updates sample statistics in the local map with corresponding global metrics.
* @param localStats concurrent map of user IDs to ThreadSafeSampleStat objects
*/","* Aggregates the thread's local samples into the global metrics. The caller
   * should ensure its thread safety.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)",310,312,"/**
 * Creates a new mutable rate with default enabled state.
 * @param name rate name
 * @param desc rate description
 * @param extended whether the rate is extended
 */","* Create a mutable rate metric (for throughput measurement).
   * @param name  of the metric
   * @param desc  description
   * @param extended  produce extended stat (stdev/min/max etc.) if true
   * @return a new mutable rate metric object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,init,org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class),58,69,"/**
* Registers protocol methods with the registry and logs their names.
* @param protocol Class<?> to register
*/","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String),53,58,"/**
* Registers detailed metrics for RPC scheduler decay.
* @param ns namespace string to identify metrics
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int),57,62,"/**
* Initializes RPC detailed metrics with specified port.
* @param port the RPC port number
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",747,770,"/**
* Updates response time metrics and schedules RPCs based on provided details.
* @param callName name of the function being called
* @param schedulable Schedulable object containing relevant data
* @param details ProcessingDetails object with timing information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateDeferredMetrics,"org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)",670,673,"/**
* Logs RPC metrics and detailed information.
* @param name RPC operation name
* @param processingTime time taken to process the request
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateMetrics,"org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)",617,668,"/**
* Updates metrics for a completed RPC call.
* @param call the RPC call
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)",162,170,"/**
* Processes metric annotations on a given method.
* @param source the object being processed
* @param method the method containing the annotations
*/",Add {@link MutableMetric} for a method annotated with {@link Metric},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",963,969,"/**
* Calls M2 on the RPC scheduler, if available.
* @param collector Metrics collection object
* @param all Whether to collect metrics for all entities
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKeyLength,org.apache.hadoop.security.KDiag:validateKeyLength(),442,450,"/**
* Configures AES encryption maximum key length.
* @throws NoSuchAlgorithmException if Java Cryptography Extensions are not installed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateUGI,"org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",697,706,"/**
* Validates and logs user authentication status.
* @param messagePrefix prefix for error messages
* @param user UserGroupInformation object
*/","* Validate the UGI: verify it is kerberized.
   * @param messagePrefix message in exceptions
   * @param user user to validate",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verifyFileIsValid,"org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)",795,805,"/**
* Validates a file based on various criteria.
* @param file File object to check
* @param category Category for error messages (e.g. log level)
* @param text Placeholder text for error messages
* @return true if all checks pass, false otherwise
*/","* Verify that a file is valid: it is a file, non-empty and readable.
   * @param file file
   * @param category category for exceptions
   * @param text text message
   * @return true if the validation held; false if it did not <i>and</i>
   * {@link #nofail} has disabled raising exceptions.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateShortName,org.apache.hadoop.security.KDiag:validateShortName(),459,476,"/**
* Validates and extracts Kerberos short name from a given principal.
* @param principal user principal to process
*/","* Verify whether auth_to_local rules transform a principal name
   * <p>
   * Having a local user name ""bar@foo.com"" may be harmless, so it is noted at
   * info. However if what was intended is a transformation to ""bar""
   * it can be difficult to debug, hence this check.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,getUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser(),71,87,"/**
* Computes and returns the function mask for the current user.
* @return null if owner is null or has no permissions, otherwise a UserGroupInformation object
*/","* Get the username encoded in the token identifier
   * 
   * @return the username or owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto),133,150,"/**
* Creates a UserGroupInformation object based on the provided user info.
* @param userInfo Proto object containing effective and real user information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem),70,72,"/**
* Initializes a LocalFileSystem instance from an underlying raw file system.
* @param rawLocalFileSystem underlying file system to be wrapped
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,<init>,org.apache.hadoop.fs.shell.find.Find:<init>(),162,164,"/**
 * Enables recursive search in the find operation.",Default constructor for the Find command.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(),120,120,"/**
* Creates an instance of Ls. 
* This is a protected constructor, suggesting it's intended to be used internally by subclasses only. 
* The exact purpose and behavior depend on the context in which this class is used. 
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,org.apache.hadoop.fs.shell.Count:<init>(),110,110,"/**
* Initializes a new instance of the Count class.",Constructor,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder(),38,41,"/**
* Returns an encoder for function masks.
* @return RSErasureEncoder instance to encode function mask data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder(),38,41,"/**
 * Returns an encoder for bit-wise operations on function masks.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder(),36,39,"/**
* Returns an encoder with function mask data.
* @return Encoder instance with function mask information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder(),39,42,"/**
* Returns an XOR-based erasure encoder instance.
* @return XORErasureEncoder object for encoding function mask values.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder(),41,44,"/**
* Creates and returns a dummy erasure decoder instance.
* @return DummyErasureDecoder object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder(),44,47,"/**
* Returns an XOR-based erasure decoder instance.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder(),43,46,"/**
* Returns an ErasureDecoder instance with specified configuration.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder(),43,46,"/**
* Returns an ErasureDecoder instance for function mask decoding. 
* @return ErasureDecoder object configured for function mask erasure.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,checkAndUpdateMaps,org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps(),166,176,"/**
* Updates cache if conditions are met, logging and handling potential IOException.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createConnection,org.apache.hadoop.ha.ActiveStandbyElector:createConnection(),894,909,"/**
* Closes and re-establishes ZooKeeper client connection.
* @throws IOException if interrupted or connection failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab(),1262,1266,"/**
* Initializes mask functionality.
*/","* Force re-Login a user in from a keytab file irrespective of the last login
   * time. Loads a user identity from a keytab file and logs them in. They
   * become the currently logged-in user. This method assumes that
   * {@link #loginUserFromKeytab(String, String)} had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean),1268,1270,"/**
* Convenience method to call m1 with TGT check enabled/disabled.
* @param checkTGT true to enable TGT checking, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache(),1302,1306,"/**
 * Initializes mask functionality by calling underlying m1 method with default parameters.
 */","* Force re-Login a user in from the ticket cache irrespective of the last
   * login time. This method assumes that login had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException
   *           raised on errors performing I/O.
   * @throws KerberosAuthException
   *           on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(),1316,1320,"/**
* Performs a default invocation of m1 with isInit=false.","* Re-Login a user in from the ticket cache.  This
   * method assumes that login had happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor(),58,62,"/**
* Creates an OpenSSL-based encryptor instance in function mask mode.
* @throws GeneralSecurityException if encryption setup fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor(),64,68,"/**
* Creates an OpenSSL-based decryptor instance.
* @throws GeneralSecurityException if decryption fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,fillQueueForKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)",145,161,"/**
* Generates and retrieves a specified number of encrypted key versions.
* @param keyName unique key identifier
* @param keyQueue queue to store the generated key versions
* @param numEKVs number of key versions to generate
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String),788,799,"/**
* Retrieves an encrypted key version by name, handling potential socket timeouts.
* @param encryptionKeyName unique identifier for the key
* @return EncryptedKeyVersion object or throws exception if not found or timed out
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String),311,316,"/**
* Calls M1 method on each KMS client provider.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,queueCall,org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call),3097,3104,"/**
* Wraps and handles RPC server exceptions in the m2() method.
* @param call Call object
* @throws IOException if an exception occurs
* @throws InterruptedException if interrupted while waiting
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrForHost,"org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)",308,325,"/**
* Resolves hostname to IP address and returns InetSocketAddress.
* @param host hostname or IP address
* @param port TCP port number
* @return InetSocketAddress object
*/","* Create a socket address with the given host and port.  The hostname
   * might be replaced with another host that was set via
   * {@link #addStaticResolution(String, String)}.  The value of
   * hadoop.security.token.service.use_ip will determine whether the
   * standard java host resolver is used, or if the fully qualified resolver
   * is used.
   * @param host the hostname or IP use to instantiate the object
   * @param port the port number
   * @return InetSocketAddress",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,canonicalizeHost,org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String),362,377,"/**
* Canonicalizes a host name to FQDN and caches the result.
* @param host input host name
* @return fully qualified domain name or original host if unknown
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getLocalInetAddress,org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String),798,811,"/**
* Resolves host name to InetAddress.
* @param host hostname or IP address
* @return InetAddress object or null if not found
*/","* Checks if {@code host} is a local host name and return {@link InetAddress}
   * corresponding to that address.
   * 
   * @param host the specified host
   * @return a valid local {@link InetAddress} or null
   * @throws SocketException if an I/O error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,sendSaslMessage,"org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",457,469,"/**
* Sends SASL protocol message to OutputStream.
* @param out OutputStream to send response to
* @param message SaslProto message to be sent
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,writeConnectionContext,"org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1007,1028,"/**
* Sends a FUNC_MASK request to the server with provided remote ID and authentication method.
* @param remoteId unique identifier of the connection
* @param authMethod method used for authentication
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendRpcRequest,org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call),1159,1191,"/**
* Continues RPC call with final packet.
* @param call ongoing RPC operation
*/","Initiates a rpc call by sending the rpc request to the remote server.
     * Note: this is not called from the current thread, but by another
     * thread, so that if the current thread is interrupted that the socket
     * state isn't corrupted with a partially written message.
     * @param call - the rpc request",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),727,730,"/**
* Calls m1 on the underlying queue with given UserGroupInformation.
* @param ugi UserGroupInformation instance to pass to m1
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,receiveRpcResponse,org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse(),1196,1249,"/**
* Processes RPC response and updates the corresponding Call object.
* @throws RpcClientException on response length mismatch
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2869,2970,"/**
* Processes RPC request from client.
* @param header RpcRequestHeaderProto object
* @param buffer RpcWritable.Buffer object
*/","* Process an RPC Request 
     *   - the connection headers and context must have been already read.
     *   - Based on the rpcKind, decode the rpcRequest.
     *   - A successfully decoded RpcCall will be deposited in RPC-Q and
     *     its response will be sent later when the request is processed.
     * @param header - RPC request header
     * @param buffer - stream to request payload
     * @throws RpcServerException - generally due to fatal rpc layer issues
     *   such as invalid header or deserialization error.  The call queue
     *   may also throw a fatal or non-fatal exception on overflow.
     * @throws IOException - fatal internal error that should/could not
     *   be sent to client.
     * @throws InterruptedException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3507,3547,"/**
* Handles RPC call response based on status.
* @param call RpcCall object
* @param status RpcStatusProto enum value
* @param erCode RpcErrorCodeProto enum value
* @param rv Writable object (may be null)
* @param errorClass String representing error class
* @param error String containing error message
*/","* Setup response for the IPC Call.
   * 
   * @param call {@link Call} to which we are setting up the response
   * @param status of the IPC call
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,wrapWithSasl,org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall),3641,3661,"/**
* Wraps RPC call with SASL authentication token.
* @param call RPC call to modify
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,initializeAuthContext,org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int),2567,2593,"/**
* Retrieves AuthProtocol based on input type, handling unknown types and SIMPLE authentication enablement.
* @param authType unique protocol identifier
* @return AuthProtocol object or null if unknown; throws IOException on error
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,reset,org.apache.hadoop.util.CacheableIPList:reset(),42,45,"/**
* Applies mask functions to IP list and performs additional processing.
* @param none
* @return none",* Reloads the ip list,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,main,org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[]),705,734,"/**
* Prints system info to console.
* Fetches and displays various system metrics.
*/","* Test the {@link SysInfoLinux}.
   *
   * @param args - arguments to this calculator test",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Retrieves a file stream from a remote location using the provided buffer size.
* @param path URI of the remote resource
* @param bufferSize size of the input buffer
* @return FSDataInputStream object or throws IOException if an error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens an HTTP connection to the specified file system path and returns a data input stream.
* @param path file system path
* @param bufferSize buffer size for reading from the stream
* @return FSDataInputStream object or throws IOException if network error occurs 
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,remove,org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object),223,232,"/**
* Removes and returns user profile by ID, also updating the queue.
* @param key unique user identifier
* @return UserProfile object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,put,org.apache.hadoop.util.LightWeightCache:put(java.lang.Object),201,221,"/**
* Updates or inserts user profile by ID.
* @param entry UserProfile object to update or insert
* @return existing UserProfile object or null if new
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)",394,396,"/**
* Calls M1 with default values for 'useSystemFont' and 'fontSize'. 
* @param qOption query option flag
* @param hOption header option flag
*/","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   * 
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @return the string representation of the object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processPath,org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData),195,217,"/**
* Processes PathData to generate output string.
* @param src input data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(boolean),307,309,"/**
 * Calls the overloaded version of m1 with default values.
 * @param hOption true to enable option, false otherwise
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,"org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)",191,209,"/**
* Reads data from byte array using FUNC_MASK algorithm.
* @param b input byte array
* @param off offset within the array to start reading
* @return number of bytes read or 0 for EOF, -1 on error
*/","* Read checksum verified bytes from this byte-input stream into 
   * the specified byte array, starting at the given offset.
   *
   * <p> This method implements the general contract of the corresponding
   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
   * the <code>{@link InputStream}</code> class.  As an additional
   * convenience, it attempts to read as many bytes as possible by repeatedly
   * invoking the <code>read</code> method of the underlying stream.  This
   * iterated <code>read</code> continues until one of the following
   * conditions becomes true: <ul>
   *
   *   <li> The specified number of bytes have been read,
   *
   *   <li> The <code>read</code> method of the underlying stream returns
   *   <code>-1</code>, indicating end-of-file.
   *
   * </ul> If the first <code>read</code> on the underlying stream returns
   * <code>-1</code> to indicate end-of-file then this method returns
   * <code>-1</code>.  Otherwise this method returns the number of bytes
   * actually read.
   *
   * @param      b     destination buffer.
   * @param      off   offset at which to start storing bytes.
   * @param      len   maximum number of bytes to read.
   * @return     the number of bytes read, or <code>-1</code> if the end of
   *             the stream has been reached.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if any checksum error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)",127,134,"/**
* Creates an instance of the specified expression class from configuration.
* @param expressionClass Class of the expression to create
* @param conf Configuration object for instantiation
* @return Instance of the expression class or null if creation fails
*/","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClass
   *          {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,"org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)",118,131,"/**
* Retrieves a command instance by name from the configuration.
* @param cmdName unique command identifier
* @param conf application configuration
* @return Command object or null if not found
*/","* Get an instance of the requested command
   * @param cmdName name of the command to lookup
   * @param conf the hadoop configuration
   * @return the {@link Command} or null if the command is unknown",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,newKey,org.apache.hadoop.io.WritableComparator:newKey(),166,168,"/**
* Returns a WritableComparable instance based on key class and configuration.
* @return instance of WritableComparable or null
*/","* Construct a new {@link WritableComparable} instance.
   * @return WritableComparable.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,readFields,org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput),156,180,"/**
* Processes data entries from input stream.
* @param in DataInput stream containing serialized data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/GenericWritable.java,readFields,org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput),124,135,"/**
* Initializes an instance of a writable class from byte stream.
* @param in DataInput object containing serialized data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,add,"org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)",69,79,"/**
* Loads and initializes a custom serialization class from the configuration.
* @param conf Hadoop Configuration object
* @param serializationName name of the serialization class to load
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,deserialize,org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable),62,73,"/**
* Creates or retrieves a FUNC_MASK Writable object.
* @param w optional existing Writable object to reuse
* @return the created or reused Writable object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,readFields,org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput),165,191,"/**
* Processes input to populate instance data.
* @param in DataInput stream for reading entries
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,"org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",63,74,"/**
* Creates a Writable instance using the given class and configuration.
* @param c Class of the Writable to create
* @param conf Configuration for initializing Configurable instances
*/","* Create a new instance of a class with a defined factory.
   *
   * @param c input c.
   * @param conf input configuration.
   * @return a new instance of a class with a defined factory.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactoryFromProperty,"org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)",142,152,"/**
* Creates SocketFactory instance based on configuration and property value.
* @param conf Configuration object
* @param propValue Property value to resolve factory class
* @return SocketFactory instance or throws exception if class not found
*/","* Get the socket factory corresponding to the given proxy URI. If the
   * given proxy URI corresponds to an absence of configuration parameter,
   * returns null. If the URI is malformed raises an exception.
   *
   * @param conf configuration.
   * @param propValue the property which is the class name of the
   *        SocketFactory to instantiate; assumed non null and non empty.
   * @return a socket factory as defined in the property value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeIdentifier,org.apache.hadoop.security.token.Token:decodeIdentifier(),164,176,"/**
* Retrieves a masked token identifier based on the current state.
* @return T object representing the token identifier or null if not found
*/","* Get the token identifier object, or null if it could not be constructed
   * (because the class could not be loaded, for example).
   * @return the token identifier, or null if there was no class found for it
   * @throws IOException failure to unmarshall the data
   * @throws RuntimeException if the token class could not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class),45,62,"/**
* Retrieves a valid instance of DiskValidator based on the provided class.
* @param clazz Class<? extends DiskValidator> type to validate
*/","* Returns a {@link DiskValidator} instance corresponding to the passed clazz.
   * @param clazz a class extends {@link DiskValidator}
   * @return disk validator.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,createFenceMethod,"org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",167,195,"/**
* Retrieves and configures a fencing method with an argument.
* @param conf configuration object
* @param clazzName name of the class containing the fencing method
* @param arg the argument to pass to the fencing method
* @return FenceMethodWithArg instance or throws BadFencingConfigurationException if failed",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(),144,146,"/**
* Constructs a new instance of DynamicWrappedIO with default class name.
* @param wrappedIoClassName default class name for wrapped IO operation
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(),224,226,"/**
* Initializes DynamicWrappedStatistics with default class name.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)",190,193,"/**
 * Applies mask rules from the specified include and exclude files.
 * @param includesFile path to file containing include masks
 * @param excludesFile path to file containing exclude masks
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,lazyRefresh,"org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)",195,198,"/**
 * Initializes function mask configuration from specified include and exclude files.
 * @param includesFile path to file containing included functions
 * @param excludesFile path to file containing excluded functions
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)",126,134,"/**
* Initializes a DynamicBloomFilter with specified parameters.
* @param vectorSize size of the filter vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function (e.g. murmur3)
* @param nr reserved capacity for records
*/","* Constructor.
   * <p>
   * Builds an empty Dynamic Bloom filter.
   * @param vectorSize The number of bits in the vector.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).
   * @param nr The threshold for the maximum number of keys to record in a
   * dynamic Bloom filter row.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,addRow,org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow(),275,285,"/**
* Initializes the last row of the bloom filter matrix.
*/",* Adds a new row to <i>this</i> dynamic Bloom filter.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)",111,116,"/**
* Initializes a new RetouchedBloomFilter instance with specified parameters.
* @param vectorSize size of the filter's underlying vector
* @param nbHash number of hash functions to use
* @param hashType type of hashing algorithm to employ
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,readFields,org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput),259,270,"/**
* Initializes the object by reading from input stream.
* @param in DataInput stream containing initialization data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,readFields,org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput),429,454,"/**
* Processes input data and initializes FP, key, and ratio vectors.
* @param in DataInput object containing processed data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceHelp,"org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",253,288,"/**
* Prints command-specific mask data to the output stream.
* @param out output stream
* @param instance Command object containing mask data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProps,"org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)",2961,2981,"/**
* Updates resource masks for the given Properties instance.
* @param props Properties object to update
* @param startIdx starting index for mask updates
* @param fullReload whether to fully reload resources or not
*/","* Loads the resource at a given index into the properties.
   * @param props the object containing the loaded properties.
   * @param startIdx the index where the new resource has been added.
   * @param fullReload flag whether we do complete reload of the conf instead
   *                   of just loading the new resource.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,becomeActive,org.apache.hadoop.ha.ActiveStandbyElector:becomeActive(),936,956,"/**
* Tries to become active in election by executing necessary steps.
* @return true if successful, false on exception or failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,quitElection,org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean),443,452,"/**
* Executes synchronization logic for election.
* @param needFence whether a fence is needed
*/","* Any service instance can drop out of the election by calling quitElection. 
   * <br>
   * This will lose any leader status, if held, and stop monitoring of the lock
   * node. <br>
   * If the instance wants to participate in election again, then it needs to
   * call joinElection(). <br>
   * This allows service instances to take themselves out of rotation for known
   * impending unavailable states (e.g. long GC pause or software upgrade).
   * 
   * @param needFence true if the underlying daemon may need to be fenced
   * if a failover occurs due to dropping out of the election.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PositionedReadable.java,readVectored,"org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)",132,135,"/**
* Allocates memory and reads data into ByteBuffer objects using Vectored Read.
* @param ranges collection of file ranges to read
* @param allocate function to allocate memory for each range
*/","* Read fully a list of file ranges asynchronously from this file.
   * The default iterates through the ranges to read each synchronously, but
   * the intent is that FSDataInputStream subclasses can make more efficient
   * readers.
   * As a result of the call, each range will have FileRange.setData(CompletableFuture)
   * called with a future that when complete will have a ByteBuffer with the
   * data from the file's range.
   * <p>
   *   The position returned by getPos() after readVectored() is undefined.
   * </p>
   * <p>
   *   If a file is changed while the readVectored() operation is in progress, the output is
   *   undefined. Some ranges may have old data, some may have new and some may have both.
   * </p>
   * <p>
   *   While a readVectored() operation is in progress, normal read api calls may block.
   * </p>
   * @param ranges the byte ranges to read
   * @param allocate the function to allocate ByteBuffer
   * @throws IOException any IOE.
   * @throws IllegalArgumentException if the any of ranges are invalid, or they overlap.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,close,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close(),228,248,"/**
* Closes the operation pipeline and cleans up resources.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,read,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData),308,317,"/**
* Synchronizes and processes the given buffer data.
* @param data BufferData object to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,prefetch,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)",319,329,"/**
 * Updates prefetching statistics and marks data for caching.
 * @param data BufferData object to update
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)",109,112,"/**
* Constructs a new BlockLocation instance with the given parameters.
* @param names array of block names
* @param hosts array of host identifiers
* @param offset starting offset in bytes
* @param length total length of the block in bytes
*/","* Constructor with host, name, offset and length.
   * @param names names array.
   * @param hosts host array.
   * @param offset offset.
   * @param length length.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,<init>,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)",71,83,"/**
* Creates a DurationStatisticSummary object with specified metrics.
* @param key unique identifier for the statistic
* @param success indicates whether the duration is considered successful
* @param count total number of samples
* @param max maximum value
* @param min minimum value
* @param mean optional mean value, cloned if provided
*/","* Constructor.
   * @param key Statistic key.
   * @param success Are these success or failure statistics.
   * @param count Count of operation invocations.
   * @param max Max duration; -1 if unknown.
   * @param min Min duration; -1 if unknown.
   * @param mean Mean duration -may be null. (will be cloned)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,aggregate,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),178,199,"/**
* Aggregates statistics from the given source into local counters.
* @param source IOStatistics object to aggregate
* @return true if aggregation successful, false otherwise
*/","* Aggregate the current statistics with the
   * source reference passed in.
   *
   * The operation is synchronized.
   * @param source source; may be null
   * @return true if a merge took place.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics),123,129,"/**
* Initializes IO statistics snapshot from given source or creates default maps.
* @param source optional IOStatistics instance to snapshot
*/","* Construct, taking a snapshot of the source statistics data
   * if the source is non-null.
   * If the source is null, the empty maps are created
   * @param source statistics source. Nullable.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,foreach,"org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",273,288,"/**
* Counts elements in the remote iterator and applies a consumer to each element.
* @param source RemoteIterator containing data
* @param consumer Consumer function to process each element
* @return Number of processed elements or -1 on error
*/","* Apply an operation to all values of a RemoteIterator.
   *
   * If the iterator is an IOStatisticsSource returning a non-null
   * set of statistics, <i>and</i> this classes log is set to DEBUG,
   * then the statistics of the operation are evaluated and logged at
   * debug.
   * <p>
   * The number of entries processed is returned, as it is useful to
   * know this, especially during tests or when reporting values
   * to users.
   * </p>
   * This does not close the iterator afterwards.
   * @param source iterator source
   * @param consumer consumer of the values.
   * @return the number of elements processed
   * @param <T> type of source
   * @throws IOException if the source RemoteIterator or the consumer raise one.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackInvocation,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)",1014,1024,"/**
* Updates a rate metric with statistics for an invocation.
* @param invocation IOE object containing invocation data
* @param statistic name of the statistic to update
* @param metric MutableRate object to be updated
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])",517,520,"/**
* Wraps call to overloaded m1 with default offset and length.
* @param position current file pointer position
* @param buffer input/output data buffer
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",28,31,"/**
* Constructs a CryptoFSDataInputStream with the given parameters.
* @param in input stream to encrypt
* @param codec encryption algorithm to use
* @param bufferSize buffer size for encryption
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",67,99,"/**
* Processes input and output data packets for Reed-Solomon encoding.
* @param inputs array of input ByteBuffer objects
* @param outputs array of output ByteBuffer objects
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)",142,146,"/**
* Invokes serialization of an object to the given output stream.
* @param out output stream for serialized data
* @param instance object being serialized
* @param declaredClass class declaring the serialization method
* @param conf configuration parameters (not used in this implementation)
*/","* Write a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param out DataOutput.
   * @param instance instance.
   * @param conf Configuration.
   * @param declaredClass declaredClass.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,write,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput),166,179,"/**
* Serializes method invocation data to output stream.
* @param out DataOutput stream
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int),256,289,"/**
* Processes block operations based on the provided block number.
* @param blockNumber unique block identifier
*/","* Requests optional prefetching of the given block.
   * The block is prefetched only if we can acquire a free buffer.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int),631,633,"/**
* Retrieves buffered data from pool based on block number.
* @param blockNumber unique identifier for data block
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int),125,150,"/**
* Acquires BufferData for the given block number with retries.
* @param blockNumber unique block identifier
* @return BufferData object or null if acquisition fails
*/","* Acquires a {@code ByteBuffer}; blocking if necessary until one becomes available.
   * @param blockNumber the id of the block to acquire.
   * @return the acquired block's {@code BufferData}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathArgument,org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData),315,320,"/**
* Initializes functional mask by traversing PathData structure.
* @param item PathData object to process
*/","*  This is the last chance to modify an argument before going into the
   *  (possibly) recursive {@link #processPaths(PathData, PathData...)}
   *  {@literal ->} {@link #processPath(PathData)} loop.  Ex.  ls and du use
   *  this to expand out directories.
   *  @param item a {@link PathData} representing a path which exists
   *  @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)",361,380,"/**
* Recursively groups and processes remote path data into chunks of specified size.
* @param parent the parent directory
* @param itemsIterator iterator over remote path data
*/","* Iterates over the given expanded paths and invokes
   * {@link #processPath(PathData)} on each element. If ""recursive"" is true,
   * will do a post-visit DFS on directories.
   * @param parent if called via a recurse, will be the parent dir, else null
   * @param itemsIterator a iterator of {@link PathData} objects to process
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,resolvePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)",280,319,"/**
* Resolves partial group names for a given user.
* @param userName The name of the user
* @param errMessage Error message to be used in exceptions
* @return Set of group names or null if not found
* @throws PartialGroupNameException If resolution fails
*/","* Attempt to partially resolve group names.
   *
   * @param userName the user's name
   * @param errMessage error message from the shell command
   * @param groupNames the incomplete list of group names
   * @return a set of resolved group names
   * @throws PartialGroupNameException if the resolution fails or times out",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)",1218,1220,"/**
* Constructs a new ShellCommandExecutor instance with default settings.
* @param execString command to execute
* @param dir working directory
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,org.apache.hadoop.util.Shell:execCommand(java.lang.String[]),1358,1360,"/**
* Executes command with optional arguments and timeout.
* @param cmd vararg command to execute
*/","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])",1390,1393,"/**
* Executes shell command in environment with default timeout.
* @param env execution environment
* @param cmd command to execute
*/","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param env the map of environment key=value
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException on any problem.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials),450,452,"/**
 * Calls m1 with default configuration (sync=true).
 * @param other Credentials object to process
 */","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,mergeAll,org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials),459,461,"/**
* Masks credentials by calling underlying function.
* @param other Credentials to mask
*/","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are not overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,addDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)",79,87,"/**
* Retrieves an array of tokens with mask functionality.
* @param renewer unique identifier for token renewal
* @param credentials authentication data (may be null)
*/","* Given a renewer, add delegation tokens for issuer and it's child issuers
   * to the <code>Credentials</code> object if it is not already present.
   *<p>
   * Note: This method is not intended to be overridden.  Issuers should
   * implement getCanonicalService and getDelegationToken to ensure
   * consistent token acquisition behavior.
   *
   * @param renewer the user allowed to renew the delegation tokens
   * @param credentials cache in which to add new delegation tokens
   * @return list of new delegation tokens
   * @throws IOException thrown if IOException if an IO error occurs.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token),1701,1703,"/**
* Recursively checks token validity.
* @param token input token to validate
* @return true if valid, false otherwise
*/","* Add a token to this UGI
   * 
   * @param token Token to be added
   * @return true on successful add of new token",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),850,854,"/**
 * Updates and processes the current working directory. 
 * @param newDir Path to the new directory.",* Set the working directory to the given directory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,exists,org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path),768,771,"/**
* Calls m1() to process file and returns its result.
* @param f Path object representing the file
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getStatus,org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path),866,874,"/**
* Retrieves file system status for the given path.
* @param p file system path
* @return FsStatus object containing file metadata or null on failure
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setTimes,"org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",1129,1140,"/**
* Updates file metadata with modified and accessed times.
* @param p Path to the file
* @param mtime Modified time in milliseconds, or -1 for no change
* @param atime Accessed time in milliseconds, or -1 for no change
*/","* Sets the {@link Path}'s last modified time and last access time to
   * the given valid times.
   *
   * @param mtime the modification time to set (only if no less than zero).
   * @param atime the access time to set (only if no less than zero).
   * @throws IOException if setting the times fails.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,pathToFile,org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),79,81,"/**
 * Calls RawLocalFileSystem's m1 method to perform an operation on the given file path.
 * @param path Path object representing the file location
 */","* Convert a path to a File.
   * @param path the path.
   * @return file.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUriPath,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path),264,267,"/**
* Generates functional mask by applying multiple operations to the input path.
* @param p input file path
*/","* Make the path Absolute and get the path-part of a pathname.
   * Checks that URI matches this file system
   * and that the path-part is a valid name.
   *
   * @param p path
   * @return path-part of the Path p",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),854,859,"/**
* Recursively invokes fs.m1() on each NflyNode's file system. 
* @param newDir the directory to invoke m1() on in each node's FS
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,<init>,"org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)",50,68,"/**
* Initializes Stat object with file system and block size, 
* qualifying the original path and stripping URI fragments.
* @param path original file path
* @param blockSize block size in bytes
* @param deref whether to dereference symlinks
* @param fs file system instance
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem),547,550,"/**
* Calls m3 with file system's m1 and m2 results.
* @param fs FileSystem object
*/","* Returns a qualified path object for the {@link FileSystem}'s working
   * directory.
   *  
   * @param fs the target FileSystem
   * @return a qualified path object for the FileSystem's working directory
   * @deprecated use {@link #makeQualified(URI, Path)}",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,makeQualified,org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path),629,631,"/**
* Applies M3 operation to the given file system path.
* @param path input file system path
*/","* Make the path fully qualified if it is isn't. 
   * A Fully-qualified path has scheme and authority specified and an absolute
   * path.
   * Use the default file system and working dir in this FileContext to qualify.
   * @param path the path.
   * @return qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,makeQualified,org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path),438,441,"/**
* Computes and appends M3 value to the given path.
* @param path input path object
*/","* Make the path fully qualified to this file system
   * @param path the path.
   * @return the qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path),1926,1937,"/**
* Resolves file status for the given path.
* @param f input path
*/","* List the statuses of the files/directories in the given path 
     * if the path is a directory.
     * 
     * @param f is the path
     *
     * @return an array that contains statuses of the files/directories 
     *         in the given path
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,fixRelativePart,org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path),138,144,"/**
* Resolves file system path using File System or File Client.
* @param path the input path to resolve
* @return resolved Path object or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,delete,"org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)",842,853,"/**
* Resolves symbolic links for specified file path and returns true if successful.
* @param f the original file path
* @param recursive whether to resolve links recursively
* @return true if resolution was successful, false otherwise
*/","* Delete a file.
   * @param f the path to delete.
   * @param recursive if path is a directory and set to 
   * true, the directory is deleted else throws an exception. In
   * case of a file the recursive can be set to either true or false.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid
   *
   * @return if delete success true, not false.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path),873,883,"/**
* Resolves and returns a file stream for the given path.
* @param f file system path
*/","* Opens an FSDataInputStream at the indicated Path using
   * default buffersize.
   * @param f the file name to open
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return input stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,"org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)",904,915,"/**
* Resolves symbolic link to FSDataInputStream.
* @param f symbolic link path
* @param bufferSize buffer size for input stream
* @return FSDataInputStream object or throws exception if failed
*/","* Opens an FSDataInputStream at the indicated Path.
   * 
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return output stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,truncate,"org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)",947,958,"/**
* Resizes a file to the specified length.
* @param f file path
* @param newLength desired file size in bytes
* @return true if resize was successful, false otherwise
*/","* Truncate the file in the indicated path to the indicated size.
   * <ul>
   * <li>Fails if path is a directory.
   * <li>Fails if path does not exist.
   * <li>Fails if path is not closed.
   * <li>Fails if new size is greater than current size.
   * </ul>
   * @param f The path to the file to be truncated
   * @param newLength The size the file is to be truncated to
   *
   * @return <code>true</code> if the file has been truncated to the desired
   * <code>newLength</code> and is immediately available to be reused for
   * write operations such as <code>append</code>, or
   * <code>false</code> if a background process of adjusting the length of
   * the last block has been started, and clients should wait for it to
   * complete before proceeding with further file updates.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setReplication,"org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)",978,989,"/**
* Resolves file system links to fetch data with specified replication.
* @param f original file path
* @param replication desired replication level
* @return true if successful, false otherwise
*/","* Set replication for an existing file.
   * 
   * @param f file name
   * @param replication new replication
   *
   * @return true if successful
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setPermission,"org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1079,1091,"/**
* Updates file permissions and metadata.
* @param f target file path
* @param permission new file permissions
*/","* Set permission of a path.
   * @param f the path.
   * @param permission - the new absolute permission (umask is not applied)
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setOwner,"org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1117,1134,"/**
* Resolves file system links and applies access control for the given path.
* @param f input file path
* @param username user identifier (optional)
* @param groupname group identifier (optional)
*/","* Set owner of a path (i.e. a file or a directory). The parameters username
   * and groupname cannot both be null.
   * 
   * @param f The path
   * @param username If it is null, the original username remains unchanged.
   * @param groupname If it is null, the original groupname remains unchanged.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws HadoopIllegalArgumentException If <code>username</code> or
   *           <code>groupname</code> is invalid.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setTimes,"org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)",1158,1170,"/**
* Updates file metadata (mtime and atime).
* @param f file path
* @param mtime last modified time
* @param atime last accessed time
*/","* Set access time of a file.
   * @param f The path
   * @param mtime Set the modification time of this file.
   *        The number of milliseconds since epoch (Jan 1, 1970). 
   *        A value of -1 means that this call should not set modification time.
   * @param atime Set the access time of this file.
   *        The number of milliseconds since Jan 1, 1970. 
   *        A value of -1 means that this call should not set access time.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileChecksum,org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path),1191,1202,"/**
* Resolves file checksum for a given path.
* @param f path to resolve
* @return FileChecksum object or null if not found
*/","* Get the checksum of a file.
   *
   * @param f file path
   *
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileStatus,org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path),1248,1258,"/**
* Resolves M2 status for a given file path.
* @param f input file path
*/","* Return a file status object that represents the path.
   * @param f The path we want information from
   *
   * @return a FileStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,access,"org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1305,1318,"/**
* Resolves file system links and applies access control.
* @param path file system path
* @param mode FsAction to apply to the path
*/","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation of this method calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   * Note that the getFileStatus call will be subject to authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws UnsupportedFileSystemException if file system for <code>path</code>
   *   is not supported
   * @throws IOException see specific implementation
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileLinkStatus,org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path),1334,1350,"/**
* Resolves file status for the given path, handling symbolic links.
* @param f input path
*/","* Return a file status object that represents the path. If the path 
   * refers to a symlink then the FileStatus of the symlink is returned.
   * The behavior is equivalent to #getFileStatus() if the underlying
   * file system does not support symbolic links.
   * @param  f The path we want information from.
   * @return A FileStatus object
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLinkTarget,org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path),1367,1378,"/**
* Resolves absolute file path and returns its masked equivalent.
* @param f input file path
*/","* Returns the target of the given symbolic link as it was specified
   * when the link was created.  Links in the path leading up to the
   * final path component are resolved transparently.
   *
   * @param f the path to return the target of
   * @return The un-interpreted target of the symbolic link.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If path <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If the given path does not refer to a symlink
   *           or an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileBlockLocations,"org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1437,1450,"/**
* Resolves block locations for a file path.
* @param f file path
* @param start start offset
* @param len length
* @return array of BlockLocation objects or null if not found
*/","* Return blockLocation of the given file for the given offset and len.
   *  For a nonexistent file or regions, null will be returned.
   *
   * This call is most helpful with DFS, where it returns 
   * hostnames of machines that contain the given file.
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param f - get blocklocations of this file
   * @param start position (byte offset)
   * @param len (in bytes)
   *
   * @return block locations for given file at specified offset of len
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFsStatus,org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path),1476,1489,"/**
* Resolves file system status for a given path.
* @param f the input file path
* @return FsStatus object or default FS if null input
*/","* Returns a status object describing the use and capacity of the
   * file system denoted by the Parh argument p.
   * If the file system has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * 
   * @param f Path for which status should be obtained. null means the
   * root partition of the default file system. 
   *
   * @return a FsStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSymlink,"org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1570,1588,"/**
* Resolves and creates a symbolic link to the target file system.
* @param target destination file path
* @param link symbolic link to create
* @param createParent whether to create parent directories if needed
*/","* Creates a symbolic link to an existing file. An exception is thrown if 
   * the symlink exits, the user does not have permission to create symlink,
   * or the underlying file system does not support symlinks.
   * 
   * Symlink permissions are ignored, access to a symlink is determined by
   * the permissions of the symlink target.
   * 
   * Symlinks in paths leading up to the final path component are resolved 
   * transparently. If the final path component refers to a symlink some 
   * functions operate on the symlink itself, these are:
   * - delete(f) and deleteOnExit(f) - Deletes the symlink.
   * - rename(src, dst) - If src refers to a symlink, the symlink is 
   *   renamed. If dst refers to a symlink, the symlink is over-written.
   * - getLinkTarget(f) - Returns the target of the symlink. 
   * - getFileLinkStatus(f) - Returns a FileStatus object describing
   *   the symlink.
   * Some functions, create() and mkdir(), expect the final path component
   * does not exist. If they are given a path that refers to a symlink that 
   * does exist they behave as if the path referred to an existing file or 
   * directory. All other functions fully resolve, ie follow, the symlink. 
   * These are: open, setReplication, setOwner, setTimes, setWorkingDirectory,
   * setPermission, getFileChecksum, setVerifyChecksum, getFileBlockLocations,
   * getFsStatus, getFileStatus, exists, and listStatus.
   * 
   * Symlink targets are stored as given to createSymlink, assuming the 
   * underlying file system is capable of storing a fully qualified URI.
   * Dangling symlinks are permitted. FileContext supports four types of 
   * symlink targets, and resolves them as follows
   * <pre>
   * Given a path referring to a symlink of form:
   * 
   *   {@literal <---}X{@literal --->}
   *   fs://host/A/B/link 
   *   {@literal <-----}Y{@literal ----->}
   * 
   * In this path X is the scheme and authority that identify the file system,
   * and Y is the path leading up to the final path component ""link"". If Y is
   * a symlink  itself then let Y' be the target of Y and X' be the scheme and
   * authority of Y'. Symlink targets may:
   * 
   * 1. Fully qualified URIs
   * 
   * fs://hostX/A/B/file  Resolved according to the target file system.
   * 
   * 2. Partially qualified URIs (eg scheme but no host)
   * 
   * fs:///A/B/file  Resolved according to the target file system. Eg resolving
   *                 a symlink to hdfs:///A results in an exception because
   *                 HDFS URIs must be fully qualified, while a symlink to 
   *                 file:///A will not since Hadoop's local file systems 
   *                 require partially qualified URIs.
   * 
   * 3. Relative paths
   * 
   * path  Resolves to [Y'][path]. Eg if Y resolves to hdfs://host/A and path 
   *       is ""../B/file"" then [Y'][path] is hdfs://host/B/file
   * 
   * 4. Absolute paths
   * 
   * path  Resolves to [X'][path]. Eg if Y resolves hdfs://host/A/B and path
   *       is ""/file"" then [X][path] is hdfs://host/file
   * </pre>
   * 
   * @param target the target of the symbolic link
   * @param link the path to be created that points to target
   * @param createParent if true then missing parent dirs are created if 
   *                     false then parent must exist
   *
   *
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>link</code> already exists
   * @throws FileNotFoundException If <code>target</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>link</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for 
   *           <code>target</code> or <code>link</code> is not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path),1611,1623,"/**
* Resolves file status iterator for a given path.
* @param f input file path
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * 
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listCorruptFileBlocks,org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path),1633,1644,"/**
* Resolves remote file system links for the given path.
* @param path input path to resolve
*/","* List CorruptFile Blocks.
   *
   * @param path the path.
   * @return an iterator over the corrupt files under the given path
   * (may contain duplicates if a file has more than one corrupt block)
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listLocatedStatus,org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path),1673,1686,"/**
* Resolves file status for the given path.
* @param f the input file path
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory. 
   * Return the file's status and block locations If the path is a file.
   * 
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   * If any IO exception (for example the input directory gets deleted while
   * listing is being executed), next() or hasNext() of the returned iterator
   * may throw a RuntimeException with the io exception as the cause.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveAbstractFileSystems,org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path),2372,2386,"/**
* Resolves file system links and returns a set of matching AbstractFileSystem instances.
* @param f input path to resolve
* @return Set<AbstractFileSystem> or throws IOException if an error occurs
*/","* Returns the list of AbstractFileSystems accessed in the path. The list may
   * contain more than one AbstractFileSystems objects in case of symlinks.
   * 
   * @param f
   *          Path which needs to be resolved
   * @return List of AbstractFileSystems accessed in the path
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,modifyAclEntries,"org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2456,2467,"/**
* Resolves ACL specifications for the given file path.
* @param path absolute file path
* @param aclSpec list of access control entries to apply
*/","* Modifies ACL entries of files and directories.  This method can add new ACL
   * entries or modify the permissions on existing ACL entries.  All existing
   * ACL entries that are not specified in this call are retained without
   * changes.  (Modifications are merged into the current ACL.)
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAclEntries,"org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2478,2489,"/**
* Resolves file system links and applies ACL specifications.
* @param path input path
* @param aclSpec list of access control entries
*/","* Removes ACL entries from files and directories.  Other ACL entries are
   * retained.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing entries
   * to remove
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeDefaultAcl,org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path),2497,2508,"/**
* Resolves file system link for the given path.
* @param path input file path to resolve
*/","* Removes all default ACL entries from files and directories.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAcl,org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path),2518,2528,"/**
* Resolves file system link for the given path.
* @param path input file path
*/","* Removes all but the base ACL entries of files and directories.  The entries
   * for user, group, and others are retained for compatibility with permission
   * bits.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be removed",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setAcl,"org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)",2540,2551,"/**
* Resolves ACL for a given file path.
* @param path the file path
* @param aclSpec list of access control entries
*/","* Fully replaces ACL of files and directories, discarding all existing
   * entries.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications, must include entries for user, group, and others for
   * compatibility with permission bits.
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAclStatus,org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path),2561,2570,"/**
* Resolves ACL status for the given path.
* @param path input file system path
* @return AclStatus object or null if not found
*/","* Gets the ACLs of files and directories.
   *
   * @param path Path to get
   * @return RemoteIterator{@literal <}AclStatus{@literal >} which returns
   *         each AclStatus
   * @throws IOException if an ACL could not be read",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",2603,2614,"/**
* Resolves and sets file system attribute for the given path.
* @param path input path
* @param name attribute name
* @param value attribute value
* @param flag flags for setting attributes
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @param flag xattr set flag
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttr,"org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2628,2637,"/**
* Resolves file system link and fetches data by name.
* @param path input path
* @param name target file or directory name
* @return byte array containing the resolved data
*/","* Get an xattr for a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attribute
   * @param name xattr name.
   * @return byte[] xattr value.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path),2651,2660,"/**
* Resolves file system links and retrieves metadata for the given path.
* @param path input file path
* @return a map of metadata (key-value pairs)
*/","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,"org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",2675,2685,"/**
* Resolves file system links to fetch key-value pairs.
* @param path input path
* @param names list of key names
* @return Map of key-value pairs or null if not found
*/","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @param names XAttr names.
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeXAttr,"org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2698,2708,"/**
* Resolves file system link for the given path and name.
* @param path input file path
* @param name associated file name
*/","* Remove an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to remove extended attribute
   * @param name xattr name
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listXAttrs,org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path),2722,2731,"/**
* Resolves file system links to fetch a list of strings from the specified path.
* @param path input path to resolve
* @return list of strings or null if an error occurs
*/","* Get all of the xattr names for a file or directory.
   * Only those xattr names which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return List{@literal <}String{@literal >} of the XAttr names of the
   * file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,"org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2766,2777,"/**
* Resolves a file system link for the given path and snapshot name.
* @param path input file system path
* @param snapshotName linked snapshot identifier
* @return resolved Path object or throws IOException if failed
*/","* Create a snapshot.
   *
   * @param path The directory where snapshots will be taken.
   * @param snapshotName The name of the snapshot
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,renameSnapshot,"org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",2794,2805,"/**
* Resolves file system link for the given path and renames snapshots.
* @param path input directory path
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/","* Rename a snapshot.
   *
   * @param path The directory path where the snapshot was taken
   * @param snapshotOldName Old name of the snapshot
   * @param snapshotNewName New name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteSnapshot,"org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2821,2832,"/**
* Resolves file system link for the given path and snapshot name.
* @param path absolute file system path
* @param snapshotName name of the snapshot to resolve
*/","* Delete a snapshot of a directory.
   *
   * @param path The directory that the to-be-deleted snapshot belongs to
   * @param snapshotName The name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,satisfyStoragePolicy,org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path),2839,2850,"/**
* Resolves file system link for the given path.
* @param path input file system path
*/","* Set the source path to satisfy storage policy.
   * @param path The source path referring to either a directory or a file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setStoragePolicy,"org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",2861,2872,"/**
* Resolves file system link for given path and policy name.
* @param path input path
* @param policyName file system access policy
*/","* Set the storage policy for a given file or directory.
   *
   * @param path file or directory path.
   * @param policyName the name of the target storage policy. The list
   *                   of supported Storage policies can be retrieved
   *                   via {@link #getAllStoragePolicies}.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,unsetStoragePolicy,org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path),2879,2889,"/**
* Resolves file system links for the given source path.
* @param src input path to resolve
* @throws IOException if an I/O error occurs
*/","* Unset the storage policy set for a given file or directory.
   * @param src file or directory path.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStoragePolicy,org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path),2898,2908,"/**
* Resolves BlockStoragePolicySpi for the given path.
* @param path input path to resolve
* @return BlockStoragePolicySpi instance or null if not found
*/","* Query the effective storage policy ID for the given file or directory.
   *
   * @param path file or directory path.
   * @return storage policy for give file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,hasPathCapability,"org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3001,3007,"/**
* Resolves file system link for the given path and capability.
* @param path directory path
* @param capability required capability
* @return true if resolved, false otherwise
*/","* Return the path capabilities of the bonded {@code AbstractFileSystem}.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return true iff the capability is supported under that FS.
   * @throws IOException path resolution or other IO failure
   * @throws IllegalArgumentException invalid arguments",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getServerDefaults,org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path),3015,3020,"/**
* Resolves server defaults for the given file system path.
* @param path file system path to resolve
*/","* Return a set of server default configuration values based on path.
   * @param path path to fetch server defaults
   * @return server default configuration values for path
   * @throws IOException an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createMultipartUploader,org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path),3029,3035,"/**
* Creates an MultipartUploaderBuilder instance with the given base path.
* @param basePath file system path to resolve links from
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory(),278,281,"/**
* Returns a path representing function mask.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp),652,655,"/**
* Returns SFTP channel path mask.
* @param client SFTP client instance
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,org.apache.hadoop.fs.RawLocalFileSystem:<init>(),101,103,"/**
* Initializes local file system with default working directory.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,refreshStatus,org.apache.hadoop.fs.shell.PathData:refreshStatus(),198,208,"/**
* Retrieves file status based on a function mask.
* @return FileStatus object or null if failed
*/","* Updates the paths's file status
   * @return the updated FileStatus
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(),1271,1274,"/**
 * Calls file system's m1 method to retrieve data.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(),415,418,"/**
 * Calls underlying file system's implementation of m1.
 */",Return the total size of all files in the filesystem.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,resolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)",168,205,"/**
* Resolves a file system path using interceptors and regex patterns.
* @param srcPath original file system path
* @param resolveLastComponent whether to resolve the last component of the path
* @return ResolveResult object or null if no mapping occurred
*/","* Get resolved path from regex mount points.
   *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   * @param srcPath - the src path to resolve
   * @param resolveLastComponent - whether resolve the path after last `/`
   * @return mapped path of the mount point.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDest,"org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",607,630,"/**
* Copies a file to the specified destination.
* @param srcName source file name or null for directory copy
* @param dstFS destination file system
* @param dst target path
* @param overwrite whether to overwrite existing files
* @return destination path
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,advance,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance(),561,569,"/**
* Iterates through directory paths, checking for a matching file using the m1() method.
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",615,636,"/**
* Checks if a file exists in the local filesystem.
* @param pathStr relative or absolute file path
* @param conf configuration object
* @return true if file found, false otherwise
*/","We search through all the configured dirs for the file's existence
     *  and return true when we find one",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,delete,"org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",913,921,"/**
* Recursively cleans files and directories in the specified FS.
* @param fs FileSystem instance
* @param name root directory name
*/","* Deletes the named map file.
   * @param fs input fs.
   * @param name input name.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,delete,"org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",59,69,"/**
* Initializes a Map directory on the file system.
* @param fs FileSystem object
* @param name unique map directory identifier
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,findCurrentDirectory,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date),551,558,"/**
* Calculates the file system path for a specific date.
* @param now current time
*/","* Use the given time to determine the current directory. The current
   * directory will be based on the {@link #rollIntervalMinutes}.
   *
   * @param now the current time
   * @return the current directory",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,createForWrite,"org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)",273,280,"/**
* Returns a FileOutputStream with specified permissions.
* @param f file to open
* @param permissions access rights for the stream
*/","* Open the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred
   * @return createForWrite FileOutputStream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI),165,171,"/**
* Initializes backing JKS path from provided URI.
* @param keystoreUri URI of the key store
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,extractKMSPath,org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI),443,445,"/**
 * Resolves URI to a file path with a specific format.
 * @param uri input URI
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,checkPathsForReservedRaw,"org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",385,406,"/**
* Determines whether to preserve raw xattributes during copy operation.
* @param src source file path
* @param target destination file path
* @return true if raw xattributes should be preserved, false otherwise
*/","* Check the source and target paths to ensure that they are either both in
   * /.reserved/raw or neither in /.reserved/raw. If neither src nor target are
   * in /.reserved/raw, then return false, indicating not to preserve raw.*
   * xattrs. If both src/target are in /.reserved/raw, then return true,
   * indicating raw.* xattrs should be preserved. If only one of src/target is
   * in /.reserved/raw then throw an exception.
   *
   * @param src The source path to check. This should be a fully-qualified
   *            path, not relative.
   * @param target The target path to check. This should be a fully-qualified
   *               path, not relative.
   * @return true if raw.* xattrs should be preserved.
   * @throws PathOperationException is only one of src/target are in
   * /.reserved/raw.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",997,1042,"/**
 * Creates a new file on the file system with specified permissions and settings.
 * @param f path to the file
 * @param flag creation flags (e.g. CreateFlag)
 * @param absolutePermission file permissions
 * @param bufferSize buffer size for writing data
 * @param replication replication factor for data redundancy
 * @param blockSize block size for data storage
 * @param progress progress monitor for write operation
 * @param checksumOpt checksum options for data verification
 * @param createParent whether to create parent directory if it does not exist
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1051,1071,"/**
* Resolves file block locations for the given path and offset range.
* @param f Path to resolve
* @param start Start of offset range
* @param len Length of offset range
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1267,1299,"/**
* Creates a directory with specified permissions and creates parent if needed.
* @param dir directory path to create
* @param permission file system permissions for the new directory
* @param createParent whether to automatically create the parent directory if it doesn't exist
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1458,1496,"/**
* Creates a new FSDataOutputStream for the specified Path.
* @param f Path of the file to create
* @return FSDataOutputStream object
* @throws IOException if file creation fails
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink(),1638,1657,"/**
* Retrieves file statuses from the configured fallback root.
* @return array of FileStatus objects or empty if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1696,1730,"/**
* Tries to create a directory with the given permission.
* @param dir path to create
* @param permission file system permissions
* @return true if creation was successful, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,makeTrashRelativePath,"org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",120,122,"/**
* Computes file mask path by combining base and RM file paths.
* @param basePath base directory path
* @param rmFilePath relative path to RM file
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerPutPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",124,153,"/**
* Creates a PartHandle for the specified part number.
* @param filePath directory path
* @param inputStream input stream to write from
* @param partNumber unique part identifier
* @param uploadId unique upload ID
* @param lengthInBytes total bytes written
* @return BBPartHandle object
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParent,org.apache.hadoop.fs.Path:getParent(),429,431,"/**
* Returns the path to mask file.
* @return Path to mask file or null if not available
*/","* Returns the parent of a path or null if at root. Better alternative is
   * {@link #getOptionalParentPath()} to handle nullable value for root path.
   *
   * @return the parent of a path or null if at root",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getOptionalParentPath,org.apache.hadoop.fs.Path:getOptionalParentPath(),440,442,"/**
* Generates the function mask path.
* @return Optional Path object representing the function mask or empty if not found
*/","* Returns the parent of a path as {@link Optional} or
   * {@link Optional#empty()} i.e an empty Optional if at root.
   *
   * @return Parent of path wrappen in {@link Optional}.
   * {@link Optional#empty()} i.e an empty Optional if at root.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContentsIterator,org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator(),293,299,"/**
* Fetches path data iterator for the given directory.
* @return iterator of PathData objects or null if not found
*/","* Returns a RemoteIterator for PathData objects of the items contained in the
   * given directory.
   * @return remote iterator of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,schemeFromPath,org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path),171,182,"/**
* Extracts file system scheme from the given path.
* @param path Path object to fetch scheme from
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,authorityFromPath,org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path),184,195,"/**
* Extracts file system authority from a given path.
* @param path input path
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",111,122,"/**
* Configures FSDataOutputStreamBuilder with file context and path.
* @param fc FileContext object
* @param p Path to the file
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setVerifyChecksum,"org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)",1223,1228,"/**
* Validates and processes a file on the specified filesystem.
* @param verifyChecksum whether to check the file's checksum
* @param f file path
*/","* Set the verify checksum flag for the  file system denoted by the path.
   * This is only applicable if the 
   * corresponding FileSystem supports checksum. By default doesn't do anything.
   * @param verifyChecksum verify check sum.
   * @param f set the verifyChecksum for the Filesystem containing this path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,readFields,org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput),496,522,"/**
* Reads and parses FileStatusProto from input stream.
* @param in DataInput stream containing file status data
*/","* Read instance encoded as protobuf from stream.
   * @param in Input stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",140,149,"/**
* Constructs a FileStatus object with specified attributes.
* @param length total file size
* @param isdir true if directory, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])",113,123,"/**
* Initializes LocatedFileStatus object with given parameters.
* @param length file length
* @param isdir true if directory
*/","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param hasAcl entity has associated ACLs
   * @param isEncrypted entity is encrypted
   * @param isErasureCoded entity is erasure coded
   * @param locations a file's block locations",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)",87,96,"/**
* Initializes a new instance of the uploader with the given parameters.
* @param builder configuration builder for the uploader
* @param fs file system to use for uploads",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,append,"org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",399,416,"/**
* Updates and indexes data with new key-value pair.
* @param key unique identifier
* @param val associated value
*/","* Append a key/value pair to the map.  The key must be greater or equal
     * to the previous key added to the map.
     *
     * @param key key.
     * @param val value.
     * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,close,org.apache.hadoop.service.AbstractService:close(),246,249,"/**
* Performs functional mask operation.
*/","* Relay to {@link #stop()}
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object),785,795,"/**
* Recursively compares two Metadata objects for equality.
* @param other metadata object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,equals,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object),100,108,"/**
* Compares this RenewAction instance with another object.
* @param that Object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object),284,297,"/**
* Compares this object with another for equality.
* @param o the other object to compare
* @return true if equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),998,1006,"/**
* Resolves token using first available service.
* @param creds credentials object
* @return Token object or null if resolution fails
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BasicDiskValidator.java,checkStatus,org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File),30,33,"/**
 * Checks disk space usage and performs mask operation on specified directory.
 * @param dir directory to check and process
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,org.apache.hadoop.fs.FSOutputSummer:write(int),76,82,"/**
* Writes an integer to the buffer and triggers overflow handling when full.
* @param b the integer value to write
*/",Write one byte,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write1,"org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)",120,140,"/**
* Copies and masks specified bytes into a buffer.
* @param b input byte array
* @param off starting offset in the input array
* @param len number of bytes to process
* @return number of bytes successfully copied
*/","* Write a portion of an array, flushing to the underlying
   * stream at most once if necessary.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",79,121,"/**
* Reorganizes input and output buffers for Reed-Solomon decoding.
* @param inputs array of ByteBuffer objects
* @param outputs array of ByteBuffer objects
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",138,143,"/**
* Recursively re-encodes input/output arrays with erasures.
* @param inputs array of ECChunks to encode
* @param erasedIndexes indexes of elements to erase in the encoding process
* @param outputs array of encoded ECChunks
*/","*  Validate outputs decoded from inputs, by decoding an input back from
   *  those outputs and comparing it with the original one.
   * @param inputs input buffers used for decoding
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",54,58,"/**
* Processes EC chunks using M1 algorithm.
* @param inputChunks input chunk array
* @param outputChunks output chunk array
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA(),1077,1104,"/**
* Updates random bit mask and state based on input data.
* @throws IOException if I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA(),1106,1126,"/**
* Updates state and fetches next character, handling continuous and by-block read modes.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finish,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish(),718,732,"/**
* Closes output stream and performs cleanup operations.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write0,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int),881,900,"/**
* Updates current character and run length based on input byte.
* @param b input byte to mask
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,finishDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean),653,671,"/**
* Forces or schedules completion of block appender.
* @param bForceFinish true to immediately finish, false to schedule for next attempt
*/","* Close the current data block if necessary.
     * 
     * @param bForceFinish
     *          Force the closure regardless of the block size.
     * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close(),3549,3556,"/**
* Iterates through segments and clears mask. 
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,parkCursorAtEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd(),1561,1571,"/**
* Resets block reader and updates location to end.",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageStream,org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream),273,295,"/**
* Reads and processes token storage header and data.
* @param in input stream containing token storage
*/","* Convenience method for reading a token from a DataInputStream.
   *
   * @param in DataInputStream.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getCandidateTokensForCleanup,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup(),176,197,"/**
* Retrieves a map of tokens and their corresponding information for cleanup.
* @return A map of TokenIdent to DelegationTokenInformation, or an empty map if failed
*/","* Obtain a list of tokens that will be considered for cleanup, based on the last
   * time the token was updated in SQL. This list may include tokens that are not
   * expired and should not be deleted (e.g. if the token was last renewed using a
   * higher renewal interval).
   * The number of results is limited to reduce performance impact. Some level of
   * contention is expected when multiple routers run cleanup simultaneously.
   * @return Map of tokens that have not been updated in SQL after the token renewal
   *         period.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",644,650,"/**
* Retrieves delegation token information for the specified TokenIdent.
* @param ident unique token identifier
* @param quiet whether to suppress output if not found
* @return DelegationTokenInformation object or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),3866,3869,"/**
* Retrieves the length of a value from the input stream.
* @param rawValue Value bytes to process
* @return Length of the processed value
*/","* Fills up the passed rawValue with the value corresponding to the key
       * read earlier.
       * @param rawValue input ValueBytes rawValue.
       * @return the length of the value
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),213,229,"/**
* Removes and logs a token if it's outdated or already deleted.
* @param ident TokenIdent object to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,readFields,org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput),175,185,"/**
* Reads and initializes various configuration fields from input stream.
* @param in input data to populate field values
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProto,org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput),378,397,"/**
* Serializes token and secret key data to output stream.
* @throws IOException if serialization fails
*/","* Write contents of this instance as CredentialsProto message to DataOutput.
   * @param out
   * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),128,130,"/**
* Wraps token in Proto wrapper.
* @param tok input token object
* @return wrapped TokenProto object
*/","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text),37,39,"/**
* Creates a new DelegationTokenIdentifier with specified token type. 
* @param kind unique identifier of the token type",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",358,374,"/**
* Invokes method on remote object, handling RPC and async calls.
* @param proxy remote object instance
* @return result of method invocation or null for async call
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,entry,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry(),1619,1622,"/**
* Creates a new Entry instance with default properties.","* Get an entry to access the key and value.
       * 
       * @return The Entry object to access the key and value.
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareCursorKeyTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable),1642,1646,"/**
* Calculates and returns a bitmask based on the provided RawComparable object.
* @param other RawComparable object containing relevant data
*/","* Internal API. Comparing the key at cursor to user-specified key.
       * 
       * @param other
       *          user-specified key.
       * @return negative if key at cursor is smaller than user key; 0 if equal;
       *         and positive if key at cursor greater than user key.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,get,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)",1675,1679,"/**
 * Applies mask functions to key and value BytesWritable objects.
 * @param key input key object
 * @param value input value object
 */","* Copy the key and value in one shot into BytesWritables. This is
         * equivalent to getKey(key); getValue(value);
         * 
         * @param key
         *          BytesWritable to hold key.
         * @param value
         *          BytesWritable to hold value
         * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long),1986,1995,"/**
* Iterates and processes 'n' data blocks.
* @param n number of blocks to process
*/","* Advance cursor by n positions within the block.
       * 
       * @param n
       *          Number of key-value pairs to skip in block.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)",168,172,"/**
* Retrieves a delegationToken using provided parameters.
* @param url URL for authentication
* @param token AuthenticatedToken object
* @param renewer Renewer string (optional)
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",217,222,"/**
* Constructs an authenticated URL using a token and delegated token.
* @param url target URL
* @param token authentication token
* @param dToken delegated token identifier
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",257,262,"/**
* Convenience wrapper for fetching user profile with provided authentication tokens.
* @param url URL of the service
* @param token authenticated URL token
* @param dToken delegation token
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket),470,473,"/**
* Wraps the given socket with an input wrapper.
* @param socket underlying socket to wrap
*/","* Same as <code>getInputStream(socket, socket.getSoTimeout()).</code>
   *
   * @param socket socket.
   * @throws IOException raised on errors performing I/O.
   * @return SocketInputWrapper for reading from the socket.
   * @see #getInputStream(Socket, long)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket),526,529,"/**
* Wraps Socket's output stream with a default buffer size of 0.
* @param socket network endpoint
*/","* Same as getOutputStream(socket, 0). Timeout of zero implies write will
   * wait until data is available.<br><br>
   * 
   * From documentation for {@link #getOutputStream(Socket, long)} : <br>
   * Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see #getOutputStream(Socket, long)
   * 
   * @param socket socket.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,putMetrics,org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),97,148,"/**
* Processes metrics record and builds masked output string.
* @param record MetricsRecord object to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)",574,578,"/**
* Overloaded variant of m1() with specified connect timeout.
* @param socket client socket to use
* @param address server endpoint to connect to
* @param timeout connection timeout in milliseconds (optional)
*/","* This is a drop-in replacement for 
   * {@link Socket#connect(SocketAddress, int)}.
   * In the case of normal sockets that don't have associated channels, this 
   * just invokes <code>socket.connect(endpoint, timeout)</code>. If 
   * <code>socket.getChannel()</code> returns a non-null channel,
   * connect is implemented using Hadoop's selectors. This is done mainly
   * to avoid Sun's connect implementation from creating thread-local 
   * selectors, since Hadoop does not have control on when these are closed
   * and could end up taking all the available file descriptors.
   * 
   * @see java.net.Socket#connect(java.net.SocketAddress, int)
   * 
   * @param socket socket.
   * @param address the remote address
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,sampleMetrics,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics(),403,418,"/**
* Creates a metrics buffer by aggregating sources and filtering based on the source filter.
* @return MetricsBuffer object
*/","* Sample all the sources for a snapshot of metrics/tags
   * @return  the metrics buffer containing the snapshot",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttribute,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String),104,118,"/**
* Retrieves attribute value by name, caching and logging the result.
* @param attribute attribute name
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttributes,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[]),127,141,"/**
* Returns a list of attributes for the given names.
* @param attributes array of attribute names
* @return AttributeList object or null if an error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMBeanInfo,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo(),154,158,"/**
* Returns cached MBean information.
* @return MBeanInfo object
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)",71,74,"/**
* Calls overloaded m2 with default parameters.
* @param serviceName service identifier
* @param nameName name identifier
* @param theMbean MBean object to access
*/","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param theMbean - the MBean to register
   * @return the named used to register the MBean",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stop,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop(),196,219,"/**
* Stops the metrics system, executing callback methods and logging events.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,getMetric,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String),86,103,"/**
* Retrieves ReadWriteDiskValidatorMetrics instance for the specified directory.
* @param dirName unique directory name
* @return Metrics object or a new instance if not found
*/","* Get a metric by given directory name.
   *
   * @param dirName directory name
   * @return the metric",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)",190,193,"/**
* Recursively configures the type prefix based on the given protocol.
* @param protocol Class<?> representing the protocol to configure
* @param prefix String prefix value for configuration
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int),70,80,"/**
* Initializes RPC stats for the given number of priority levels.
* @param numLevels the number of priority levels to initialize
*/","* Initialize the metrics for JMX with priority levels.
   * @param numLevels input numLevels.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",116,132,"/**
* Processes weak references of sample stats and updates global metrics.
* @param rb MetricsRecordBuilder to update
* @param all Whether to include all data
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates(),137,143,"/**
* Synchronizes and updates mask-specific statistics.
*/","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)",62,94,"/**
* Creates a metrics sink adapter with specified configuration.
* @param name unique identifier for this sink
* @param description human-readable description of the sink
* @param sink underlying metrics sink object
* @param context additional context information
* @param sourceFilter filter for incoming metric sources
* @param recordFilter filter for individual metric records
* @param metricFilter filter for specific metrics
* @param periodMs interval between periodic publishes
* @param queueCapacity maximum size of the internal queue
* @param retryDelay initial delay before retrying a failed publish
* @param retryBackoff factor by which to increase the retry delay
* @param retryCount maximum number of retries before giving up
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String),289,291,"/**
 * Creates a new mutable rate with specified name.
 * @param name name of the rate
 */","* Create a mutable rate metric
   * @param name  of the metric
   * @return a new mutable metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)",299,301,"/**
* Constructs a new MutableRate instance with basic details.
* @param name rate name
* @param description rate description
*/","* Create a mutable rate metric
   * @param name  of the metric
   * @param description of the metric
   * @return a new mutable rate metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String),60,64,"/**
* Creates and returns DecayRpcSchedulerDetailedMetrics object for the given namespace.
* @param ns namespace string
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int),66,69,"/**
* Creates and returns RpcDetailedMetrics object with specified port.
* @param port network port number
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Handler:run(),3151,3230,"/**
* Processes incoming calls from the call queue.
* @throws InterruptedException if interrupted
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpKeytab,org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File),596,620,"/**
* Inspects and extracts keytab entries from the provided file.
* @param keytabFile path to the keytab file
*/","* Dump a keytab: list all principals.
   *
   * @param keytabFile the keytab file
   * @throws IOException IO problems",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateJAAS,org.apache.hadoop.security.KDiag:validateJAAS(boolean),755,771,"/**
* Validates and prints JAAS configuration details if required.
* @param jaasRequired true to enable JAAS validation
*/","* Validate any JAAS entry referenced in the {@link #SUN_SECURITY_JAAS_FILE}
   * property.
   * @param jaasRequired is JAAS required",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateNTPConf,org.apache.hadoop.security.KDiag:validateNTPConf(),773,784,"/**
* Configures NTP settings on non-Windows platforms.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenRealOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),907,917,"/**
* Resolves the user identifier to a string representation.
* @param id TokenIdent object containing user information
*/","* Return the real owner for a token. If this is a token from a proxy user,
   * the real/effective user will be returned.
   *
   * @param id
   * @return real owner",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto),124,131,"/**
* Fetches user group information from IPC connection context.
* @param context IpcConnectionContextProto object
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUid,org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String),632,644,"/**
* Retrieves the unique ID from a given username.
* @param user input username
* @return unique ID or throws IOException on failure
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGid,org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String),646,658,"/**
* Retrieves the unique ID for a specified user group.
* @param group name of the user group
* @return Integer ID or throws IOException if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUserName,"org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)",660,676,"/**
* Fetches and returns the username associated with a given UID.
* @param uid unique identifier
* @param unknown default username to use if lookup fails
* @return Username or null if not found and no alternative provided
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGroupName,"org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)",678,694,"/**
* Retrieves or creates a group name from its ID.
* @param gid unique group identifier
* @param unknown default group name to use if not found
* @return Group name as a string
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,ensureParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode(),360,396,"/**
* Ensures existence of the working directory ZNode and its parents.
* @throws IOException if creation fails
*/","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.
   * @throws KeeperException other zookeeper operation errors.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getActiveData,org.apache.hadoop.ha.ActiveStandbyElector:getActiveData(),474,491,"/**
* Fetches and returns the function mask from ZooKeeper.
* @throws ActiveNotFoundException if the function is not found
* @throws KeeperException if a ZooKeeper exception occurs
* @throws InterruptedException if interrupted while waiting for ZooKeeper response
* @throws IOException if an I/O error occurs","* get data set by the active leader
   * 
   * @return data set by the active instance
   * @throws ActiveNotFoundException
   *           when there is no active leader
   * @throws KeeperException
   *           other zookeeper operation errors
   * @throws InterruptedException
   *           interrupted exception.
   * @throws IOException
   *           when ZooKeeper connection could not be established",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reEstablishSession,org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession(),872,892,"/**
* Establishes and verifies zookeeper connection with retry.
* @return true if connected, false on max retry failure
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,checkTGTAndReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab(),1197,1199,"/**
 * Applies mask to input data (m1) with all bits set.","* Re-login a user from keytab if TGT is expired or is close to expiry.
   * 
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(),1245,1249,"/**
* Invokes m1 with default parameter value (false).
* @throws IOException if an I/O error occurs
*/","* Re-Login a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user. This
   * method assumes that {@link #loginUserFromKeytab(String, String)} had
   * happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)",229,261,"/**
* Resolves and creates an InetSocketAddress from the provided target address.
* @param target input address string
* @param defaultPort default port to use when not specified in the address
* @return resolved InetSocketAddress or null if invalid
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress),450,460,"/**
* Updates InetSocketAddress with a default host if necessary.
* @param addr existing address object
* @return updated or original address object
*/","* Returns an InetSocketAddress that a client can use to connect to the
   * given listening address.
   * 
   * @param addr of a listener
   * @return socket address that a client can use to connect to the server.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,updateAddress,org.apache.hadoop.ipc.Client$Connection:updateAddress(),588,606,"/**
* Detects and updates the IPC client address if it has changed.
* @return true if a change was detected, false otherwise
*/","* Update the server address if the address corresponding to the host
     * name has changed.
     *
     * @return true if an addr change was detected.
     * @throws IOException when the hostname cannot be resolved.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getCanonicalUri,"org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)",333,354,"/**
* Constructs or modifies a URI with a host and optional port.
* @param uri original URI
* @param defaultPort default port to use if missing in the input URI
* @return modified URI with a fully qualified host and port (if any)
*/","* Resolve the uri's hostname and add the default port if not in the uri
   * @param uri to resolve
   * @param defaultPort if none is given
   * @return URI",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1467,1531,"/**
* Fetches and processes RPC response with fallback to simple authentication.
* @param rpcKind the type of RPC request
* @param rpcRequest the RPC request object
* @param remoteId unique server identifier
* @param serviceClass the service class for the connection
* @param fallbackToSimpleAuth flag to enable simple authentication fallback
* @param alignmentContext the alignment context for the call
* @return the processed response or null if async
*/","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc response.
   *
   * @param rpcKind
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param serviceClass - service class for RPC
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @param alignmentContext - state alignment context
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection:run(),1082,1109,"/**
* Executes a function with multiple stages and error handling.
*@throws IOException on unexpected errors
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,isIn,org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String),62,75,"/**
* Checks and updates IP address cache.
* @param ipAddress IP address to check
* @return true if updated, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry),259,300,"/**
* Retrieves a cached entry and updates its state according to the function's result.
* @param newEntry the new entry to cache
* @return the updated cache entry or null if not found
*/","* This method handles the following conditions:
   * <ul>
   * <li>If retry is not to be processed, return null</li>
   * <li>If there is no cache entry, add a new entry {@code newEntry} and return
   * it.</li>
   * <li>If there is an existing entry, wait for its completion. If the
   * completion state is {@link CacheEntry#FAILED}, the expectation is that the
   * thread that waited for completion, retries the request. the
   * {@link CacheEntry} state is set to {@link CacheEntry#INPROGRESS} again.
   * <li>If the completion state is {@link CacheEntry#SUCCESS}, the entry is
   * returned so that the thread that waits for it can can return previous
   * response.</li>
   * <ul>
   * 
   * @return {@link CacheEntry}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntry,"org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)",309,319,"/**
* Updates cache entry for client ID with specified call ID.
* @param clientId unique client identifier
* @param callId call identifier
*/","* Add a new cache entry into the retry cache. The cache entry consists of 
   * clientId and callId extracted from editlog.
   *
   * @param clientId input clientId.
   * @param callId input callId.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntryWithPayload,"org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)",321,333,"/**
* Inserts a new cache entry with payload.
* @param clientId unique client identifier
* @param callId call identifier
* @param payload associated data
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(boolean),381,384,"/**
 * Simplifies the call to m1() with default values.
 * @param qOption flag to enable additional processing (default: false)
 */","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @return the string representation of the object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(),302,305,"/**
 * Calls m1 with default parameter value (false) and returns result. 
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,getExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",108,116,"/**
* Creates an instance of a named expression based on the provided configuration.
* @param expressionName name of the expression to instantiate
* @param conf Hadoop Configuration object
* @return Expression instance or null if instantiation fails
*/","* Get an instance of the requested expression
   *
   * @param expressionName
   *          name of the command to lookup
   * @param conf
   *          the Hadoop configuration
   * @return the {@link Expression} or null if the expression is unknown",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",145,155,"/**
* Creates an instance of the specified Expression class.
* @param expressionClassname fully qualified name of the Expression class
* @param conf configuration object
* @return Expression object or null if creation fails
*/","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClassname
   *          name of the {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,buildDescription,org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory),109,159,"/**
* Generates a mask string describing recognised expressions and operators.
* @param factory ExpressionFactory instance
* @return A formatted string containing information about primary expressions and operators
*/",Build the description used by the help command.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class),438,442,"/**
* Creates an instance of the specified expression class with masked parameters.
* @param expressionClass class of the expression to create
*/",Gets an instance of an expression from the factory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String),108,110,"/**
* Calls m2 with given command and result of m1.
* @param cmd command string
*/","* Returns an instance of the class implementing the given command.  The
   * class must have been registered via
   * {@link #addClass(Class, String...)}
   * @param cmd name of the command
   * @return instance of the requested command",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",141,154,"/**
* Initializes a WritableComparator instance with the specified class, configuration,
* and instance creation flag. If instances are created, initializes two keys and a data input buffer.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readObject,"org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)",261,340,"/**
* Deserializes and instantiates an object from a DataInput stream.
* @param in input data
* @param objectWritable optional ObjectWritable to populate with the result
* @param conf configuration for deserialization (e.g. schema)
* @return instantiated object or null if not found
*/","* Read a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param in DataInput.
   * @param objectWritable objectWritable.
   * @param conf configuration.
   * @return Object.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class),81,83,"/**
* Creates an instance of the given writable class with default parameters. 
* @param c The class to instantiate
*/","* Create a new instance of a class with a defined factory.
   * @param c input c.
   * @return a new instance of a class with a defined factory.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,decodeTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token),870,872,"/**
* Extracts and returns the mask value from the provided token.
* @param token input token containing the mask value
* @return TokenIdent representing the extracted mask value
*/","* Decode the token identifier. The subclass can customize the way to decode
   * the token identifier.
   * 
   * @param token the token where to extract the identifier
   * @return the delegation token identifier
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,identifierToString,org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder),422,435,"/**
* Adds function mask to StringBuilder.
* @param buffer StringBuilder instance
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printCredentials,"org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)",137,163,"/**
* Prints tokens from Credentials to the specified PrintStream.
* @param creds Credentials object containing tokens
* @param alias Text alias for token kind
* @param out Output stream for formatted token data
*/","Print out a Credentials object.
   *  @param creds the Credentials object to be printed out.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param out print to this stream.
   *  @throws IOException failure to unmarshall a token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String),72,92,"/**
* Instantiates and returns a DiskValidator instance based on the provided name.
* @param diskValidator name of the validator to instantiate
* @return DiskValidator object or throws exception if invalid name
*/","* Returns {@link DiskValidator} instance corresponding to its name.
   * The diskValidator parameter can be ""basic"" for {@link BasicDiskValidator}
   * or ""read-write"" for {@link ReadWriteDiskValidator}.
   * @param diskValidator canonical class name, for example, ""basic""
   * @throws DiskErrorException if the class cannot be located
   * @return disk validator.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethod,"org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)",151,165,"/**
* Parses a configuration line and returns the corresponding FenceMethodWithArg.
* @param conf Configuration object
* @param line configuration line to parse
* @throws BadFencingConfigurationException if parsing fails
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)",58,65,"/**
* Initializes hosts file reader with input and excluded files.
* @param inFile path to host file
* @param exFile path to excluded hosts file
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,org.apache.hadoop.util.HostsFileReader:refresh(),118,121,"/**
* Invokes recursive file inclusion/exclusion logic.
* @param includesFile path to include file
* @param excludesFile path to exclude file
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,add,org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key),136,153,"/**
* Processes key in Bloom filter, potentially refreshing it.
* @param key unique identifier to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResourceObject,org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource),1034,1038,"/**
* Applies mask operation to the given Resource instance.
* @param resource Resource object to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getProps,org.apache.hadoop.conf.Configuration:getProps(),2946,2952,"/**
* Returns cached properties object.
* @return Properties object initialized with default settings
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readVectored,"org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)",179,183,"/**
* Delegates file reading to PositionedReadable.
* @param ranges collection of file ranges to read
* @param allocate function to allocate ByteBuffer for reading
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,readVectored,"org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)",304,308,"/**
* Calls PositionedReadable's m1 method to process file ranges.
* @param ranges list of file ranges
* @param allocate callback for allocating memory buffers
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getInternal,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData),177,208,"/**
* Performs a function mask operation on the provided BufferData.
* @param data BufferData object to operate on
* @return true if successful, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,org.apache.hadoop.fs.BlockLocation:<init>(),82,84,"/**
* Initializes BlockLocation with default values.
*/",* Default Constructor.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",883,901,"/**
* Calculates block locations for a given file status.
* @param file FileStatus object
* @param start starting offset
* @param len length of data
* @return array of BlockLocation objects or null if not applicable
*/","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For nonexistent
   * file or regions, {@code null} is returned.
   *
   * <pre>
   *   if f == null :
   *     result = null
   *   elif f.getLen() {@literal <=} start:
   *     result = []
   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]
   * </pre>
   * This call is most helpful with and distributed filesystem
   * where the hostnames of machines that contain blocks of the given file
   * can be determined.
   *
   * The default implementation returns an array containing one element:
   * <pre>
   * BlockLocation( { ""localhost:9866"" },  { ""localhost"" }, 0, file.getLen())
   * </pre>
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param file FilesStatus to get data from
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws IOException IO failure
   * @return block location array.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchDurationSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)",129,140,"/**
* Calculates duration statistic summary for a given key and success flag.
* @param source IOStatistics object
* @param key unique identifier
* @param success true if successful, false otherwise
*/","* Fetch the duration timing summary of success or failure operations
   * from an IO Statistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @param success fetch success statistics, or if false, failure stats.
   * @return a summary of the statistics.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot(),92,96,"/**
* Creates an IO statistics snapshot.
* @return IOStatisticsSnapshot object representing current IO usage.","* Returns a snapshot of the current thread's IOStatistics.
   *
   * @return IOStatisticsSnapshot of the context.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics),46,50,"/**
* Creates an IOStatisticsSnapshot instance from provided statistics.
* @param statistics input IOStatistics object
*/","* Take a snapshot of the current statistics state.
   * <p>
   * This is not an atomic option.
   * <p>
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @param statistics statistics
   * @return a snapshot of the current values.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),123,125,"/**
* Creates an IOStatisticsSnapshot from the provided source.
* @param source input data (nullable)
* @return serialized snapshot or null if source is null
*/","* Create a new {@link IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not null and not an IOStatistics instance",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toList,org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator),232,237,"/**
* Extracts elements from a RemoteIterator and returns them as a list.
* @param source iterator of elements to be extracted
*/","* Build a list from a RemoteIterator.
   * @param source source iterator
   * @param <T> type
   * @return a list of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackStoreToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1002,1004,"/**
* Handles invocation-related I/O exception and masks it.
* @param invocation InvocationRaisingIOE object containing exception details
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackUpdateToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1006,1008,"/**
* Updates token statistics based on invocation.
* @param invocation InvocationRaisingIOE object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackRemoveToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1010,1012,"/**
* Sets mask on InvocationRaisingIOE object.
* @param invocation InvocationRaisingIOE object to modify
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",59,65,"/**
* Calls optimized chunk processing method for input and output buffers.
* @param inputChunks array of input chunks
* @param outputChunks array of output chunks
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,write,org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput),135,154,"/**
* Writes EnumSet value to output stream.
* @param out DataOutput stream
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput),89,92,"/**
 * Writes mask data to output stream.
 * @param out DataOutput stream
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArgument,org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData),299,305,"/**
* Conditionally calls either m1() or m2() based on the existence of the PathData item. 
* @param item PathData object to evaluate
*/","* Processes a {@link PathData} item, calling
   * {@link #processPathArgument(PathData)} or
   * {@link #processNonexistentPath(PathData)} on each item.
   * @param item {@link PathData} item to process
   * @throws IOException if anything goes wrong...",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getUnixGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String),203,231,"/**
* Retrieves a set of user group masks by executing shell commands.
* @param user unique user identifier
* @return Set of String group names or EMPTY_GROUPS_SET if failed
*/","* Get the current user's group list from Unix by running the command 'groups'
   * NOTE. For non-existing user it will return EMPTY list.
   *
   * @param user get groups for this user
   * @return the groups list that the <code>user</code> belongs to. The primary
   *         group is returned first.
   * @throws IOException if encounter any error when running the command",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,runResolveCommand,"org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)",222,262,"/**
* Executes shell commands with arguments and returns output as a string.
* @param args list of command-line arguments
* @return concatenated output or null on failure
*/","* Build and execute the resolution command. The command is
     * executed in the directory specified by the system property
     * ""user.dir"" if set; otherwise the current working directory is used.
     * @param args a list of arguments
     * @param commandScriptName input commandScriptName.
     * @return null if the number of arguments is out of range,
     * or the output of the command.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[]),1214,1216,"/**
* Creates new instance of ShellCommandExecutor with given command string.
* @param execString array of strings representing shell command
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,readLink,org.apache.hadoop.fs.FileUtil:readLink(java.io.File),213,230,"/**
* Extracts the function mask from a file.
* @param f File object containing symbolic link
* @return Function mask as string, or empty if failed
*/","* Returns the target of the given symlink. Returns the empty string if
   * the given path does not refer to a symlink or there is an error
   * accessing the symlink.
   * @param f File representing the symbolic link.
   * @return The target of the symbolic link, empty string on error or if not
   *         a symlink.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execCommand,"org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])",1531,1537,"/**
* Executes command with file input and returns output.
* @param f File object to use as input
* @param cmd Command line arguments to execute
* @return Output string or null on error
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setPermission,"org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1108,1119,"/**
* Sets file system permissions for a given path.
* @param p the Path to set permissions on
* @param permission FsPermission values to apply
*/",* Use the command chmod to set permission.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,execShellGetUserForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String),134,146,"/**
* Generates a mask for the given network group.
* @param netgroup unique network group identifier
*/","* Calls shell to get users for a netgroup by calling getent
   * netgroup, this is a low level function that just returns string
   * that 
   *
   * @param netgroup get users for this netgroup
   * @return string of users for a given netgroup in getent netgroups format
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin(),1073,1078,"/**
* Renew Kerberos ticket and perform subsequent actions.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions(),92,114,"/**
* Retrieves file permissions based on the platform.
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,<init>,org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials),103,105,"/**
* Copies all credentials from the given source object.
* @param credentials source credentials to copy from
*/","* Create a copy of the given credentials.
   * @param credentials to copy",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addCredentials,org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials),1753,1757,"/**
* Applies mask to subject based on provided Credentials.
* @param credentials user authentication information
*/","* Add the given Credentials to this user.
   * @param credentials of tokens and secrets",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",580,590,"/**
* Creates a file output stream with specified permissions and buffering.
* @param f file path
* @param permission file permissions
* @param flags create flags (e.g. OVERWRITE)
* @param bufferSize buffer size for write operations
* @param replication replication factor for data storage
* @param blockSize block size for write operations
* @param progress progress callback for write operations
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toFile,org.apache.hadoop.fs.shell.PathData:toFile(),494,499,"/**
* Retrieves file mask from local file system.
* @return File object or throws exception on non-local path
*/","* Get the path to a local file
   * @return File representing the local path
   * @throws IllegalArgumentException if this.fs is not the LocalFileSystem",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,mkdirsWithExistsAndPermissionCheck,"org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",224,235,"/**
* Applies a file system permission mask to the specified directory.
* @param dir target directory path
* @param expected desired permissions
*/","* Create the directory or check permissions if it already exists.
   *
   * The semantics of mkdirsWithExistsAndPermissionCheck method is different
   * from the mkdirs method provided in the Sun's java.io.File class in the
   * following way:
   * While creating the non-existent parent directories, this method checks for
   * the existence of those directories if the mkdir fails at any point (since
   * that directory might have just been created by some other process).
   * If both mkdir() and the exists() check fails for any seemingly
   * non-existent directory, then we signal an error; Sun's mkdir would signal
   * an error (return false) if a directory it is attempting to create already
   * exists or the mkdir fails.
   *
   * @param localFS local filesystem
   * @param dir directory to be created or checked
   * @param expected expected permission
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),441,445,"/**
* Sets the working directory and executes mask-related operations.
* @param new_dir new working directory path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getNativeFileLinkStatus,"org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)",1300,1306,"/**
* Retrieves file status for the given path.
* @param f the file path
* @param dereference whether to follow symbolic links
* @return FileStatus object or null if an error occurs
*/","* Calls out to platform's native stat(1) implementation to get file metadata
   * (permissions, user, group, atime, mtime, etc). This works around the lack
   * of lstat(2) in Java 6.
   * 
   *  Currently, the {@link Stat} class used to do this only supports Linux
   *  and FreeBSD, so the old {@link #deprecatedGetFileLinkStatusInternal(Path)}
   *  implementation (deprecated) remains further OS support is added.
   *
   * @param f File to stat
   * @param dereference whether to dereference symlinks
   * @return FileStatus of f
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),167,171,"/**
* Generates a masked path by combining the function root with the given file.
* @param f input file path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1614,1625,"/**
* Checks if a file system supports symlinks based on the provided path and capability.
* @param path Path to check
* @param capability Capability to verify against
* @return true if FS_SYMLINKS is supported, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1652,1657,"/**
* Returns Path representing function mask directory.
* @param path input file system path
* @return Path to function mask directory or null if not created
*/","* Return path of the enclosing root for a given path
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,makeQualified,org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path),73,76,"/**
* Calls underlying file system to perform operation 'm1' on the provided path.
* @param path input file path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1892,1903,"/**
* Filters file status by applying the given filter to each entry.
* @param results collection of filtered FileStatus objects
* @param f directory path to list files from
* @param filter PathFilter instance for filtering
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,listStatus,org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path),125,136,"/**
* Fetches file statuses for the given path.
* @param path file system path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,processDeleteOnExit,org.apache.hadoop.fs.FileContext:processDeleteOnExit(),296,312,"/**
* Deletes files marked for deletion on exit.
* @param none
*/",* Delete all the paths that were marked as delete-on-exit.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,exists,org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path),1757,1766,"/**
* Checks if a file system path exists and is accessible.
* @param f the Path to check
* @return true if path exists, false otherwise
*/","* Does the file exist?
     * Note: Avoid using this method if you already have FileStatus in hand.
     * Instead reuse the FileStatus 
     * @param f the  file or dir to be checked
     *
     * @throws AccessControlException If access is denied
     * @throws IOException If an I/O error occurred
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * @return if f exists true, not false.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,getFileStatus,org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path),112,123,"/**
* Retrieves FileStatus for the given path.
* @param path file system path
* @return FileStatus object or null on failure
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setWorkingDirectory,org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path),542,554,"/**
* Sets the working directory to the specified path.
* @param newWDir the desired working directory
*/","* Set the working directory for wd-relative names (such a ""foo/bar""). Working
   * directory feature is provided by simply prefixing relative names with the
   * working dir. Note this is different from Unix where the wd is actually set
   * to the inode. Hence setWorkingDir does not follow symlinks etc. This works
   * better in a distributed environment that has multiple independent roots.
   * {@link #getWorkingDirectory()} should return what setWorkingDir() set.
   * 
   * @param newWDir new working directory
   * @throws IOException 
   * <br>
   *           NewWdir can be one of:
   *           <ul>
   *           <li>relative path: ""foo/bar"";</li>
   *           <li>absolute without scheme: ""/foo/bar""</li>
   *           <li>fully qualified with scheme: ""xx://auth/foo/bar""</li>
   *           </ul>
   * <br>
   *           Illegal WDs:
   *           <ul>
   *           <li>relative with scheme: ""xx:foo/bar""</li>
   *           <li>non existent directory</li>
   *           </ul>",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDest,"org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)",2261,2278,"/**
* Recursively copies or moves a file to the specified destination.
* @param srcName source file name
* @param dst target directory path
* @param overwrite true to overwrite existing files, false otherwise
*/","* Check if copying srcName to dst would overwrite an existing 
   * file or directory.
   * @param srcName File or directory to be copied.
   * @param dst Destination to copy srcName to.
   * @param overwrite Whether it's ok to overwrite an existing file. 
   * @throws AccessControlException If access is denied.
   * @throws IOException If dst is an existing directory, or dst is an 
   * existing file and the overwrite option is not passed.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getContentSummary,org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path),1786,1812,"/**
* Calculates content summary for a given file path.
* @param f the input file path
* @return ContentSummary object representing the file's size, blocks, and replicas
*/","* Return the {@link ContentSummary} of path f.
     * @param f path
     *
     * @return the {@link ContentSummary} of path f.
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>f</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getDelegationTokens,"org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)",2432,2443,"/**
* Collects tokens from all file systems associated with the given path.
* @param p Path to collect tokens for
* @param renewer Token renewer identifier
* @return List of tokens collected from all file systems or null on failure
*/","* Get delegation tokens for the file systems accessed for a given
   * path.
   * @param p Path for which delegations tokens are requested.
   * @param renewer the account name that is allowed to renew the token.
   * @return List of delegation tokens.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",2584,2588,"/**
* Creates or updates extended attributes on a file.
* @param path the file to modify
* @param name the attribute name
* @param value the attribute value data
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path),2747,2749,"/**
 * Wraps call to overloaded m1 method with a null context.
 */","* Create a snapshot with a default name.
   *
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",77,87,"/**
* Initializes MultipartUploaderBuilderImpl with file context and path.
* @param fc FileContext instance
* @param p Path to the file
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(),40,42,"/**
* Initializes a new instance of LocalFileSystem with default configuration.
* @param underlyingFileSystem default file system implementation
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,hasMoreThanOneSourcePaths,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList),105,118,"/**
* Evaluates a FUNC_MASK operation on a list of PathData objects.
* @param args list of PathData objects
* @return true if successful, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,waitForRecovery,org.apache.hadoop.fs.shell.Truncate:waitForRecovery(),102,116,"/**
* Truncates wait list items to specified length.
* @throws IOException if an I/O error occurs
*/",* Wait for all files in waitList to have length equal to newLength.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,tryResolveInRegexMountpoint,"org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)",1022,1032,"/**
* Resolves the path to a mount point using regex patterns and returns the result.
* @param srcPath path to be resolved
* @return ResolveResult object or null if not found
*/","* Walk through all regex mount points to see
   * whether the path match any regex expressions.
   *  E.g. link: ^/user/(?&lt;username&gt;\\w+) =&gt; s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   *
   * @param srcPath srcPath.
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])",548,554,"/**
* Initializes a new iterator for traversing the file system.
* @param fs the file system to iterate over
* @param pathStr the starting directory path
* @param rootDirs the potential root directories
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,next,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next(),571,583,"/**
* Returns the resolved path, invoking a method to check its existence.
* @return Path object or throws exception if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",248,251,"/**
* Executes allocator operation on specified path.
* @param pathStr path to operate on
* @param conf configuration for the operation
* @return true if successful, false otherwise
*/","*  We search through all the configured dirs for the file's existence
   *  and return true when we find.
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return true if files exist. false otherwise",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI),116,141,"/**
* Initializes and checks local file for reading and writing.
* @param uri URI object containing path to initialize
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,archivePath,org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path),200,211,"/**
* Finds the first path with a .har extension in a directory tree.
* @param p starting directory path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getPathInHar,org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path),356,373,"/**
* Constructs a path relative to the archive root by resolving symbolic links.
* @param path input path to resolve
* @return resolved path or separator path if failed
*/","* this method returns the path 
   * inside the har filesystem.
   * this is relative path inside 
   * the har filesystem.
   * @param path the fully qualified path in the har filesystem.
   * @return relative path in the filesystem.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeRelative,"org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)",379,393,"/**
* Constructs a relative path from an absolute or relative input path.
* @param initial base directory for the resulting path
* @param p input path to transform
* @return transformed relative path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path),88,90,"/**
* Returns a path to a CRC file based on the input file's name and extension.
* @param file input file path
*/","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,createCollectorPath,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path),155,161,"/**
* Generates a function mask path by concatenating the file extension and UUID.
* @param filePath input file path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,parentExists,org.apache.hadoop.fs.shell.PathData:parentExists(),250,253,"/**
* Returns true if mask is applied; otherwise fetches and applies the mask from file. 
* @throws IOException if an I/O error occurs during mask fetching
*/","* Test if the parent directory exists
   * @return boolean indicating parent exists
   * @throws IOException upon unexpected error",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processNonexistentPath,org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),69,90,"/**
* Validates item path and ensures parent directories exist.
* @param item PathData object to validate
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1392,1415,"/**
* Creates a directory with specified permissions and parent handling.
* @param f the file path to create
* @param absolutePermission permissions for the directory
* @param createParent whether to create parent directories if missing
*/","* This version of the mkdirs method assumes that the permission is absolute.
   * It has been added to support the FileContext that processes the permission
   * with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f the path.
   * @param absolutePermission permission.
   * @param createParent create parent.
   * @throws IOException IO failure.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,rename,"org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1669,1726,"/**
* Renames a file or directory.
* @param src source path
* @param dst destination path
* @param options optional rename options (e.g. OVERWRITE)
*/","* Renames Path src to Path dst
   * <ul>
   *   <li>Fails if src is a file and dst is a directory.</li>
   *   <li>Fails if src is a directory and dst is a file.</li>
   *   <li>Fails if the parent of dst does not exist or is a file.</li>
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails
   * if the dst already exists.
   * </p>
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites
   * the dst if it is a file or an empty directory. Rename fails if dst is
   * a non-empty directory.
   * </p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for
   * details. This default implementation is non atomic.
   * <p>
   * This method is deprecated since it is a temporary method added to
   * support the transition from FileSystem to FileContext for user
   * applications.
   * </p>
   *
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * @throws FileNotFoundException src path does not exist, or the parent
   * path of dst does not exist.
   * @throws FileAlreadyExistsException dest path exists and is a file
   * @throws ParentNotDirectoryException if the parent path of dest is not
   * a directory
   * @throws IOException on failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getNflyTmpPath,org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path),447,449,"/**
* Creates a new Path instance with modified function and file components.
* @param f existing Path object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path),120,122,"/**
* Creates a path for a function mask by appending '.crc' to the file's directory and name. 
* @param file input file path
*/","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return name of the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validatePathIsUnderParent,"org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",56,64,"/**
* Checks if a file path matches a base path.
* @param p the file path to check
* @param basePath the base path to match
* @return true if the file path matches, false otherwise
*/","* Check if a given path is the base path or under the base path.
   * @param p path to check.
   * @param basePath base path.
   * @return true if the given path is the base path or under the base path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isRoot,org.apache.hadoop.fs.Path:isRoot(),408,410,"/**
* Checks if mask is empty (null). 
*/","* Returns true if and only if this path represents the root of a file system.
   *
   * @return true if and only if this path represents the root of a file system",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,suffix,org.apache.hadoop.fs.Path:suffix(java.lang.String),467,474,"/**
* Constructs a Path object with the specified suffix appended to the parent directory.
* @param suffix file name suffix
*/","* Adds a suffix to the final name in the path.
   *
   * @param suffix the suffix to add
   * @return a new path with the suffix added",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,qualifySymlinkTarget,"org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",46,55,"/**
* Resolves a function mask by combining the provided paths and URI.
* @param pathURI base URI
* @param pathWithLink link to be masked
* @param target target path for masking
* @return Path object with resolved function mask or original target if not applicable
*/","* Return a fully-qualified version of the given symlink target if it
   * has no scheme and authority. Partially and fully-qualified paths
   * are returned unmodified.
   * @param pathURI URI of the filesystem of pathWithLink
   * @param pathWithLink Path that contains the symlink
   * @param target The symlink's absolute target
   * @return Fully qualified version of the target.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,renameInternal,"org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",848,897,"/**
* Renames a file or directory from one location to another.
* @param src source Path
* @param dst destination Path
* @param overwrite whether to allow overwriting an existing destination
*/","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param overwrite overwrite flag.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createPath,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)",362,377,"/**
* Verifies write access to a file and returns its path.
* @param dir directory path
* @param path file name
* @param checkWrite true if write access is required
* @return Path object or null on disk error
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createInternal,"org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",79,109,"/**
* Creates or appends to an FSDataOutputStream on the specified file.
* @param f Path object representing file
* @param flag EnumSet of CreateFlag values
* @return FSDataOutputStream object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",716,721,"/**
* Constructs an FCDataOutputStreamBuilder instance with a FileContext and Path.
* @param fc the FileContext
* @param p the output path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",131,138,"/**
* Constructs a FileStatus object with the given parameters.
* @param length file size in bytes
* @param isdir true if directory, false otherwise
* @param block_replication block replication factor
* @param blocksize block size in bytes
* @param modification_time last modified time
* @param access_time last accessed time
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param path file path
*/","* Constructor for file systems on which symbolic links are not supported
   *
   * @param length length.
   * @param isdir isdir.
   * @param block_replication block replication.
   * @param blocksize block size.
   * @param modification_time modification time.
   * @param access_time access_time.
   * @param permission permission.
   * @param owner owner.
   * @param group group.
   * @param path the path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus),198,206,"/**
* Copies a FileStatus object, preserving its attributes.
* @param other the original FileStatus to copy
*/","* Copy constructor.
   *
   * @param other FileStatus to copy
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,parseExecResult,org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader),110,170,"/**
* Parses and processes the output of a stat command.
* @param lines BufferedReader containing the stat output
* @throws IOException if an I/O error occurs while processing the output
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cloneStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus(),172,182,"/**
* Creates a FileStatus object based on the provided mask values.
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])",49,62,"/**
* Converts FileStatus to LocatedFileStatus with BlockLocation array.
* @param locations array of block locations
* @throws RuntimeException on unexpected IOException
*/","* Constructor 
   * @param stat a file status
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])",80,92,"/**
* Constructs a LocatedFileStatus object from given parameters.
* @param length file length
* @param isdir true if directory, false otherwise
* @param block_replication replication factor for blocks
* @param blocksize size of each block
* @param modification_time last modified time
* @param access_time last accessed time
* @param permission FsPermission object (optional)
* @param owner and group file ownership information
* @param symlink and path file location information
* @param locations array of BlockLocation objects
*/","* Constructor
   * 
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,build,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build(),48,52,"/**
* Creates and returns a file system multipart uploader instance.
* @throws IllegalArgumentException if an error occurs
* @throws IOException on I/O errors
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,append,org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable),84,87,"/**
* Increments user counter and updates database.
* @param value Writable object to be persisted
*/","* Append a value to the file.
     * @param value value.
     * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),147,165,"/**
* Retrieves a secure token using various KMS client providers.
* @param creds Credentials object
* @return Token object or null on failure
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,"org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)",102,114,"/**
* Masks a portion of the input byte array.
* @param b input byte array
* @param off starting offset
* @param len length to mask
* @throws IOException if an I/O error occurs
*/","* Writes <code>len</code> bytes from the specified byte array 
   * starting at offset <code>off</code> and generate a checksum for
   * each data chunk.
   *
   * <p> This method stores bytes from the given array into this
   * stream's buffer before it gets checksumed. The buffer gets checksumed 
   * and flushed to the underlying output stream when all data 
   * in a checksum chunk are in the buffer.  If the buffer is empty and
   * requested length is at least as large as the size of next checksum chunk
   * size, this method will checksum and write the chunk directly 
   * to the underlying output stream.  Thus it avoids unnecessary data copy.
   *
   * @param      b     the data.
   * @param      off   the start offset in the data.
   * @param      len   the number of bytes to write.
   * @exception  IOException  if an I/O error occurs.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",67,77,"/**
* Recursively processes chunk buffers with optimized memory usage.
* @param inputChunks array of input chunks
* @param outputChunks array of output chunks
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC(),1156,1167,"/**
* Advances to next character in SU sequence or switches to RAND_PART_A state.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock(),1039,1075,"/**
* Updates stream data structures for random access.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC(),1183,1195,"/**
* Advances state machine to NO_RAND_PART_C_STATE or executes M1 function if su_j2 exceeds su_z.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finalize,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize(),711,715,"/**
* Calls parent class's m2() and invokes child-specific logic in m1().
* @throws Throwable if any exception occurs during execution.",* Overriden to close the stream.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,close,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close(),734,746,"/**
* Calls m2() and delegates output to original OutputStream.
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,finish,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish(),263,272,"/**
* Executes output methods and resets state.
* @throws IOException on I/O errors
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int),645,652,"/**
 * Writes a bit mask to the output stream.
 * @param b the bitmask value
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)",859,879,"/**
* Processes a byte array segment using the provided function.
* @param buf input byte array
* @param offs starting offset in bytes
* @param len length of segment in bytes
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close(),492,510,"/**
* Executes M1 operation, incrementing error count and updating metadata.
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekToEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd(),1447,1449,"/**
* Performs function mask operation. 
* Calls m1() method.
*/","* Seek to the end of the scanner. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close(),1578,1581,"/**
* Applies mask to functional units.
*/","* Close the scanner. Release all resources. The behavior of using the
       * scanner after calling close is not defined. The entry returned by the
       * previous entry() call will be invalid.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)",250,265,"/**
* Reads and returns user credentials from a file.
* @param filename path to the file containing credentials
* @param conf configuration object
* @return Credentials object or throws IOException if failed
*/","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return Token.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),639,642,"/**
 * Retrieves DelegationTokenInformation using the given TokenIdent.
 * @param ident unique identifier of the token
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",791,831,"/**
* Removes a stored token from ZooKeeper.
* @param ident unique token identifier
* @param checkAgainstZkBeforeDeletion whether to verify with ZooKeeper before deletion
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProtobufOutputStream,org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream),328,333,"/**
* Writes serialization magic and format to output stream.
* @param os DataOutputStream instance
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,decodeToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",223,232,"/**
* Retrieves DelegationTokenIdentifier from the given token.
* @param token Token containing the identifier
* @param tokenKind Type of token (e.g. primary, secondary)
* @return DelegationTokenIdentifier object or null on error
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier(),80,83,"/**
* Returns a DelegationTokenIdentifier with the specified token kind.
* @return DelegationTokenIdentifier object with tokenKind set to tokenKind
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier(),103,106,"/**
* Returns a DelegationTokenIdentifier instance with token kind set.
* @return DelegationTokenIdentifier object with specified token kind.",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java,<init>,org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>(),43,45,"/**
* Constructs a new KMSDelegationTokenIdentifier instance.
* @param TOKEN_KIND predefined token kind identifier
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",41,46,"/**
* Calls superclass hook with default retry count. 
* @param proxy A dynamic proxy class that intercepts calls to the wrapped object.
* @param method The method being invoked on the proxy instance.
* @return Result of invoking the superclass hook.",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",2008,2028,"/**
* Searches for a key within a block.
* @param key RawComparable object to search for
* @param greater whether to search for keys greater than the given key
* @return true if key is found, false otherwise
* @throws IOException on input stream errors
*/","* Advance cursor in block until we find a key that is greater than or
       * equal to the input key.
       * 
       * @param key
       *          Key to compare.
       * @param greater
       *          advance until we find a key greater than the input key.
       * @return true if we find a equal key.
       * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)",1897,1903,"/**
* Initializes IPC streams from a Socket.
* @param socket underlying network connection
* @param maxResponseLength maximum allowed response length
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,onTimerEvent,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent(),382,387,"/**
* Updates logical time and triggers M3 function based on sink status.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetricsNow,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow(),392,397,"/**
* Triggers M3 function when sink condition is met.
*@param sinks current state of sink conditions
*/",* Requests an immediate publish of all metrics from sources to sinks.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initSystemMBean,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean(),583,588,"/**
* Initializes or updates MBean name.
* @param prefix prefix for MBean name
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,startMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans(),216,224,"/**
* Registers an MBean for the current source.
* @return null (method is void)
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",859,867,"/**
* Initializes MetricsProxy with namespace and scheduling settings.
* @param namespace unique metric name space
* @param numLevels number of levels in metrics calculations
* @param drs DecayRpcScheduler instance for scheduling
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String),403,408,"/**
* Initializes MetricsProxy with a given namespace and registers it as an MBean.
* @param namespace unique identifier for the metrics proxy
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,shutdown,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown(),590,614,"/**
* Shuts down the metrics system and releases resources.
* @return true if successful, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidator.java,checkStatus,org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File),42,94,"/**
* Validates disk integrity by writing and reading a test file.
* @param dir directory to be validated
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class),75,79,"/**
* Executes RPC rate calculations for a given protocol.
* @param protocol Class of the protocol to process
*/","* Initialize the metrics for JMX with protocol methods
   * @param protocol the protocol class",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,run,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run(),223,240,"/**
* Updates metrics mask by synchronizing on the parent object.
* @param none
* @return none
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates(),202,204,"/**
* Calls inner metrics method m1.
* @see innerMetrics
*/","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)",521,532,"/**
* Creates a MetricsSinkAdapter instance with customizable parameters.
* @param name adapter name
* @param desc adapter description
* @param sink underlying metrics sink
* @return configured MetricsSinkAdapter object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)",358,373,"/**
* Updates a mutable metric by adding a value.
* @param name unique metric identifier
* @param value new value to add to the metric
*/","* Add sample to a stat metric by name.
   * @param name  of the metric
   * @param value of the snapshot to add",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),924,928,"/**
* Updates token owner statistics by incrementing count for the given token ID.
* @param id unique token identifier
*/","* Add token stats to the owner to token count mapping.
   *
   * @param id token id.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),935,945,"/**
* Updates token owner statistics based on given identifier.
* @param id TokenIdent object
*/","* Remove token stats to the owner to token count mapping.
   *
   * @param id",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String),697,707,"/**
* Retrieves a unique identifier for the given user.
* @param user input user object
* @return integer mask or user's string hashcode if mapping fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String),710,720,"/**
* Generates a unique mask value for the specified group.
* @param group group name or identifier
* @return integer mask value or hashcode if mapping fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getCurrentActive,org.apache.hadoop.ha.ZKFailoverController:getCurrentActive(),784,802,"/**
* Fetches the current HAServiceTarget instance based on active node data.
* @return HAServiceTarget object or null if not found
*/","* @return an {@link HAServiceTarget} for the current active node
   * in the cluster, or null if no node is active.
   * @throws IOException if a ZK-related issue occurs
   * @throws InterruptedException if thread is interrupted",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",273,299,"/**
* Initializes an ActiveStandbyElector instance with ZooKeeper configuration and callbacks.
* @param zookeeperHostPorts ZooKeeper host:port string
* @param zookeeperSessionTimeout session timeout in milliseconds
* @param parentZnodeName parent ZNode name for working directory
* @param acl ACL list for ZooKeeper access control
* @param authInfo authentication info for ZooKeeper connections
* @param app callback for ActiveStandbyElector events
* @param maxRetryNum maximum retry count for failed operations
* @param failFast whether to fail fast or re-establish session on failure
* @param truststoreKeystore Truststore and Keystore configuration
*/","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param failFast
   *          whether need to add the retry when establishing ZK connection.
   * @param maxRetryNum max Retry Num
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException
   *          raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *          if valid data is not supplied.
   * @throws KeeperException
   *          other zookeeper operation errors.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElectionInternal,org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal(),783,796,"/**
* Initializes election membership by ensuring app data and ZooKeeper connection are valid.
* @param appData application data object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin(),1094,1097,"/**
* Applies mask to system functionality.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrUnresolved,org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String),166,168,"/**
* Generates an InetSocketAddress with default values based on input target.
* @param target target string used to create address
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)",222,227,"/**
* Creates an InetSocketAddress instance with default port.
* @param target host or IP address
* @param defaultPort default network port number
* @param configName configuration name (not used in this method)
* @param useCacheIfPresent whether to use cache if present (not used in this method)
* @return InetSocketAddress object","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @param useCacheIfPresent Whether use cache when create URI
   * @return  socket addr",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server),439,441,"/**
* Returns an InetSocketAddress instance based on the server's M1 address.
* @param server server object containing M1 address information
*/","* Returns InetSocketAddress that a client can use to 
   * connect to the server. Server.getListenerAddress() is not correct when
   * the server binds to ""0.0.0.0"". This returns ""hostname:port"" of the server,
   * or ""127.0.0.1:port"" when the getListenerAddress() returns ""0.0.0.0:port"".
   * 
   * @param server server.
   * @return socket address that a client can use to connect to the server.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupConnection,org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation),608,695,"/**
* Establishes a connection to the server using the provided socket factory.
* @param ticket UserGroupInformation object
* @throws IOException if connection fails or times out
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)",1415,1420,"/**
* Wraps existing implementation to provide a simplified interface.
* @param rpcKind type of request
* @param rpcRequest request data
* @param remoteId ID of the remote connection
* @param fallbackToSimpleAuth flag for simple auth fallback
*/","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc respond.
   *
   * @param rpcKind - input rpcKind.
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1422,1428,"/**
* Wraps the actual RPC call with default service class.
* @param rpcKind type of RPC operation
* @param rpcRequest request data
* @param remoteId ID of remote connection
* @param fallbackToSimpleAuth flag for authentication fallback
* @param alignmentContext context for alignment purposes
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)",1444,1450,"/**
* Wraps a single-parameter overload with an additional null parameter.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)",354,362,"/**
* Retrieves CacheEntry from the RetryCache instance.
* @param cache RetryCache instance to fetch from
* @param clientId unique client identifier
* @param callId specific call identifier
* @return CacheEntry object or null if not found
*/","* Static method that provides null check for retryCache.
   * @param cache input Cache.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntry.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)",372,380,"/**
* Retrieves a CacheEntryWithPayload from the retry cache.
* @param cache RetryCache instance
* @param payload associated data
* @param clientId unique client identifier
* @param callId call identifier
* @return CacheEntryWithPayload or null if not found
*/","* Static method that provides null check for retryCache.
   * @param cache input cache.
   * @param payload input payload.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntryWithPayload.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(),369,372,"/**
* Calls m1 with default flag value (true).",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String),432,435,"/**
* Creates an Expression instance using factory and cached value.
* @param expressionName name of the expression to create
*/",Gets a named expression from the factory.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInfo,"org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)",212,247,"/**
* Prints command mask to output stream.
* @param out PrintStream to write to
* @param cmd optional command name (null for all commands)
* @param showHelp whether to display detailed help
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,displayError,"org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)",353,365,"/**
* Logs error messages and suggests alternative commands.
* @param cmd command name
* @param message error message
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,"org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)",65,81,"/**
* Returns a WritableComparator instance for the given class.
* If not found in cache, creates one and initializes it with configuration.
* @param c Class of the WritableComparable to compare
* @param conf Configuration object used for initialization
*/","* Get a comparator for a {@link WritableComparable} implementation.
   * @param c class.
   * @param conf configuration.
   * @return WritableComparator.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class),132,134,"/**
 * Constructor for custom WritableComparator with specified key class.
 * @param keyClass class of keys to be compared
 */","* Construct for a {@link WritableComparable} implementation.
   * @param keyClass WritableComparable Class.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)",136,139,"/**
* Constructs a writable comparator with the specified class and instance creation flag.
* @param keyClass Class of the WritableComparable instances to compare
* @param createInstances Flag indicating whether to create instances for comparison
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput),84,87,"/**
 * Reads and processes data from input stream to configure function mask.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,readFields,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput),148,164,"/**
* Parses and populates method metadata from input stream.
* @param in DataInput stream containing method data
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,readFields,org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput),93,101,"/**
* Initializes an array of writable objects from the input stream.
* @param in DataInput stream containing object metadata
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,verifyToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token),208,215,"/**
* Retrieves user group information using the provided delegationToken.
* @param token Delegation token with identifier and other details
* @return UserGroupInformation object or throws IOException if failed
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,toString,org.apache.hadoop.security.token.Token:toString(),437,447,"/**
* Constructs and formats a string with user details (kind, service, and ident).
* @return Formatted string or null if an error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String),85,93,"/**
* Initializes LocalDirAllocator with context configuration item name.
* @param contextCfgItemName unique identifier for local directory allocation
*/","* Create an allocator object.
   * @param contextCfgItemName contextCfgItemName.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethods,"org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)",134,149,"/**
* Extracts and initializes fence methods from configuration specification.
* @param conf Configuration object
* @param spec Specification string
* @return List of initialized FenceMethodWithArg objects
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,append,"org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",182,190,"/**
* Updates the Bloom filter and associated data structures with the given key-value pair.
* @param key unique identifier for the stored value
* @param val associated data to be stored
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)",927,929,"/**
* Initializes a new resource with the given name and parser restriction.
* @param name unique resource identifier
* @param restrictedParser flag to restrict parser access
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)",945,947,"/**
* Initializes mask functionality with given URL and parser restriction.
* @param url URL to process
* @param restrictedParser whether parser is restricted or not
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)",963,965,"/**
* Processes file with optional restricted parsing.
* @param file Path to file to process
* @param restrictedParser flag for restricted parser mode
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)",984,986,"/**
* Processes input stream using masked parser.
* @param in InputStream to process
* @param restrictedParser flag indicating parser restrictions
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)",1002,1005,"/**
* Calls m1 with a new Resource instance based on InputStream and parser settings.
* @param in input stream to process
* @param name resource name
* @param restrictedParser flag for restricted parsing mode
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDeprecatedProperties,org.apache.hadoop.conf.Configuration:setDeprecatedProperties(),688,706,"/**
* Merges deprecated keys with new values from an overlay.
* @param deprecations context of deprecated keys and info
* @param props properties to update with merged values
* @param overlay overlay with new key-value pairs
*/","* Sets all deprecated properties that are not currently set but have a
   * corresponding new property that is set. Useful for iterating the
   * properties when all deprecated properties for currently set properties
   * need to be present.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updatePropertiesWithDeprecatedKeys,"org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])",764,775,"/**
* Updates deprecated function names with new aliases.
* @param deprecations DeprecationContext object
* @param newNames array of new function names to update
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration),843,875,"/**
* Copies another Configuration object into this one.
* @param other the configuration to copy from
*/","* A new configuration with the same settings cloned from another.
   * 
   * @param other the configuration from which to clone settings.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration),1015,1017,"/**
* Masks system properties using configuration.
* @param conf Configuration object containing system property restrictions
*/","* Add a configuration resource.
   *
   * The properties of this resource will override properties of previously
   * added resources, unless they were marked <a href=""#Final"">final</a>.
   *
   * @param conf Configuration object from which to load properties",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAlternativeNames,org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String),1372,1393,"/**
* Retrieves alternative names for a given function name from deprecation context.
* @param name the function name to look up
* @return array of alternative names or null if not found
*/","* Returns alternative names (non-deprecated keys or previously-set deprecated keys)
   * for a given non-deprecated key.
   * If the given key is deprecated, return null.
   *
   * @param name property name.
   * @return alternative names.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropertySources,org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String),2109,2129,"/**
* Retrieves a mask array by name.
* @param name the name of the mask to fetch
* @return an array of mask values or null if not found
*/","* Gets information about why a property was set.  Typically this is the 
   * path to the resource objects (file, URL, etc.) the property came from, but
   * it can also indicate that it was set programmatically, or because of the
   * command line.
   *
   * @param name - The property name to get the source of.
   * @return null - If the property or its source wasn't found. Otherwise, 
   * returns a list of the sources of the resource.  The older sources are
   * the first ones in the list.  So for example if a configuration is set from
   * the command line, and then written out to a file that is read back in the
   * first entry would indicate that it was set from the command line, while
   * the second one would indicate the file that the new configuration was read
   * in from.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,size,org.apache.hadoop.conf.Configuration:size(),2988,2990,"/**
* Calls m1 to retrieve an object and then calls its m2 method.
* @return result of object's m2 method
*/","* Return the number of keys in the configuration.
   *
   * @return number of keys in the configuration.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,clear,org.apache.hadoop.conf.Configuration:clear(),2995,2998,"/**
* Invokes m2() on objects retrieved from m1() and m3().
*/",* Clears all keys from the configuration.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration:iterator(),3006,3022,"/**
* Iterates over properties, filtering and mapping string key-value pairs.
* @return Iterator of Map.Entry objects containing filtered data
*/","* Get an {@link Iterator} to go through the list of <code>String</code> 
   * key-value pairs in the configuration.
   * 
   * @return an iterator over the entries.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,write,org.apache.hadoop.conf.Configuration:write(java.io.DataOutput),3965,3975,"/**
* Writes FUNC_MASK data to output stream.
* @param out DataOutput stream
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getValByRegex,org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String),3982,4001,"/**
* Compiles a regex pattern and maps matching strings to their replacement values.
* @param regex regular expression string
* @return Map of matched strings to replacement values
*/","* get keys matching the the regex.
   * @param regex the regex to match against.
   * @return {@literal Map<String,String>} with matching keys",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readVectored,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)",438,482,"/**
* Processes a list of file ranges and allocates buffers for checksums and data.
* @param ranges List of file ranges to process
* @param allocate Function to allocate buffers of specified type
*/","* Vectored read.
     * If the file has no checksums: delegate to the underlying stream.
     * If the file is checksummed: calculate the checksum ranges as
     * well as the data ranges, read both, and validate the checksums
     * as well as returning the data.
     * @param ranges the byte ranges to read
     * @param allocate the function to allocate ByteBuffer
     * @throws IOException",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int),144,175,"/**
* Retrieves BufferData by block number with retries and error handling.
* @param blockNumber the unique identifier of the block to fetch
* @return BufferData object or throws an exception on failure
*/","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,waitForReplication,org.apache.hadoop.fs.shell.SetReplication:waitForReplication(),108,141,"/**
* Polls wait list items, monitoring replication status and warning about potential long waits. 
* @throws IOException if an I/O error occurs
*/",* Wait for all files in waitList to have replication number equal to rep.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",924,931,"/**
* Retrieves block locations for a given path and length.
* @param p the file path
* @param start starting offset in bytes
* @param len requested number of blocks
* @return array of BlockLocation objects or throws IOException if an error occurs
*/","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For a nonexistent
   * file or regions, {@code null} is returned.
   *
   * This call is most helpful with location-aware distributed
   * filesystems, where it returns hostnames of machines that
   * contain the given file.
   *
   * A FileSystem will normally return the equivalent result
   * of passing the {@code FileStatus} of the path to
   * {@link #getFileBlockLocations(FileStatus, long, long)}
   *
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException IO failure
   * @return block location array.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",151,155,"/**
* Delegates block location retrieval to underlying FS.
* @param file FileStatus object representing the file
* @param start starting offset in bytes
* @param len length of data in bytes
* @return array of BlockLocation objects or throws IOException if failed.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchSuccessSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)",149,153,"/**
* Retrieves duration statistic summary with mask enabled.
* @param source IO statistics object
* @param key statistic key
*/","* Fetch the duration timing summary from an IOStatistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @return a summary of the statistics.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(),113,115,"/**
* Returns a default instance of Serializable. 
* This is an overloaded method with no parameters and no user-provided data.
* @return A generic Serializable object
*/","* Create a new {@link IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),146,152,"/**
* Computes and returns a serializable mask from the given source object.
* @param source input data to process
* @return Serializable mask or null on failure
*/","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@link IOStatisticsSnapshot} or null if the object is null/doesn't have statistics",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toArray,"org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])",248,252,"/**
* Merges items from a remote iterator into an array.
* @param source RemoteIterator to fetch items from
* @param a Target array to merge into
* @return The merged array, or null if source is empty
*/","* Build an array from a RemoteIterator.
   * @param source source iterator
   * @param a destination array; if too small a new array
   * of the same type is created
   * @param <T> type
   * @return an array of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createPassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),493,515,"/**
* Generates a password for the given identifier.
* @param identifier unique token identifier
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,renewToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",592,643,"/**
* Renews a delegationToken using the provided token and renewer.
* @param token delegationToken to be renewed
* @param renewer identity of the entity requesting renewal
* @return timestamp for the new expiration date or -1 on failure
*/","* Renew a delegation token.
   * @param token the token to renew
   * @param renewer the full principal name of the user doing the renewal
   * @return the new expiration time
   * @throws InvalidToken if the token is invalid
   * @throws AccessControlException if the user can't renew token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",654,685,"/**
* Cancels a token by ID, validating authorization and token existence.
* @param token TokenIdent object to cancel
* @param canceller principal requesting cancellation
* @return cancelled TokenIdent object or null if not found
*/","* Cancel a token by removing it from cache.
   *
   * @param token token.
   * @param canceller canceller.
   * @return Identifier of the canceled token
   * @throws InvalidToken for invalid token
   * @throws AccessControlException if the user isn't allowed to cancel",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArguments,org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList),281,290,"/**
* Processes a linked list of PathData objects and calls m2() on each, 
* catching any IOExceptions to be handled by m1().
*/","*  Processes the command's list of expanded arguments.
   *  {@link #processArgument(PathData)} will be invoked with each item
   *  in the list.  The loop catches IOExceptions, increments the error
   *  count, and displays the exception.
   *  @param args a list of {@link PathData} to process
   *  @throws IOException if anything goes wrong...",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String),98,101,"/**
* Retrieves functional masks for a given user name.
* @param userName unique user identifier
* @return list of functional mask strings or empty list if not found
*/","* Returns list of groups for a user
   *
   * @param userName get groups for this user
   * @return list of groups for a given user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String),121,124,"/**
* Retrieves and returns a set of function masks associated with the given user name.
* @param userName unique identifier of the user
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,resolve,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List),174,211,"/**
* Processes a list of names and returns a filtered list with switch info.
* @param names input list of names
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,getLinkCount,org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File),214,255,"/**
* Retrieves the mask value of a file.
* @param fileName path to the file
* @return int value of the file's mask or throws IOException/FNF if not found
*/","* Retrieves the number of links to the specified file.
    *
    * @param fileName file name.
    * @throws IOException raised on errors performing I/O.
    * @return link count.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)",1053,1084,"/**
* Untars a gzipped tarball to the specified directory.
* @param inFile input file to untar
* @param untarDir destination directory for untarred files
* @param gzipped true if file is compressed with gzip, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,symLink,"org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)",1226,1279,"/**
* Creates a symbolic link from the target file to the link file.
* @param target path to the target file
* @param linkname name of the symbolic link
* @return 0 on success, non-zero exit code on failure
*/","* Create a soft link between a src and destination
   * only on a local disk. HDFS does not support this.
   * On Windows, when symlink creation fails due to security
   * setting, we will log a warning. The return code in this
   * case is 2.
   *
   * @param target the target for symlink
   * @param linkname the symlink
   * @return 0 on success
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)",1303,1319,"/**
* Executes shell command to change file permissions.
* @param filename target file path
* @param perm desired permissions
* @param recursive whether to apply recursively
* @return exit status code of the command execution
*/","* Change the permissions on a file / directory, recursively, if
   * needed.
   * @param filename name of the file whose permissions are to change
   * @param perm permission string
   * @param recursive true, if permissions should be changed recursively
   * @return the exit code from the command.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getConf,org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String),158,170,"/**
* Retrieves Linux system configuration value by attribute name.
* @param attr configuration attribute to fetch
* @return system configuration value or -1 on failure
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getSystemInfoInfoFromShell,org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell(),81,92,"/**
* Executes systeminfo command to retrieve function mask.
* @return Function mask string or null on failure
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkIsBashSupported,org.apache.hadoop.util.Shell:checkIsBashSupported(),810,834,"/**
* Checks if bash shell is supported on the current OS.
* @return true if bash is supported, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,isSetsidSupported,org.apache.hadoop.util.Shell:isSetsidSupported(),845,879,"/**
* Checks if setsid command is available and supported.
* @return true if setsid is available, false otherwise
*/","* Look for <code>setsid</code>.
   * @return true if <code>setsid</code> was present",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNonNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO(),998,1045,"/**
* Fetches file permissions using a shell command.
* @throws RuntimeException on execution error
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setOwner,"org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)",1329,1338,"/**
* Executes a command to mask the specified file with the given username and/or group name.
* @param file File object to be masked
* @param username user identifier (optional)
* @param groupname group identifier (optional)
*/","* Set the ownership on a file / directory. User name and group name
   * cannot both be null.
   * @param file the file to change
   * @param username the new user owner name
   * @param groupname the new group owner name
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execSetPermission,"org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1520,1529,"/**
* Applies file permissions to the specified file.
* @param f the file to modify
* @param permission the desired file permissions
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getUsersForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String),97,123,"/**
* Extracts a list of usernames from the raw output of m1(netgroup).
* @param netgroup network group identifier
* @return List<String> of usernames, or empty if none found
*/","* Gets users for a netgroup
   *
   * @param netgroup return users for this netgroup
   * @return list of users for a given netgroup
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentials,org.apache.hadoop.security.UserGroupInformation:getCredentials(),1736,1747,"/**
* Returns a Credentials object with filtered tokens.
*/","* Obtain the tokens in credentials form associated with this user.
   * 
   * @return Credentials of tokens associated with this user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,flush,org.apache.hadoop.security.alias.UserProvider:flush(),94,97,"/**
* Applies function mask to user credentials.
* @param user current user instance
* @param credentials user credentials object
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,flush,org.apache.hadoop.crypto.key.UserProvider:flush(),138,141,"/**
* Updates user function mask with provided credentials.
* @param credentials user access rights and permissions data
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,"org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",138,143,"/**
* Validates file system permissions on a directory.
* @param localFS Local file system context
* @param dir Directory path to check
* @param expected Expected permission mask
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,hasPathCapability,"org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",451,455,"/**
* Calls underlying file system to check capability on given path.
* @param path filesystem path
* @param capability capability string
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path),463,466,"/**
* Delegates file system operation to underlying implementation.
* @param path file or directory path to operate on
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1850,1856,"/**
* Retrieves file statuses for a given path and optional filter.
* @param f the path to query
* @param filter optional filter to apply (null for all files)
* @return array of FileStatus objects or null if an error occurs
*/","* Filter files/directories in the given path using the user-supplied path
     * filter.
     * 
     * @param f is the path name
     * @param filter is the user-supplied path filter
     *
     * @return an array of FileStatus objects for the files under the given path
     *         after applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",1879,1886,"/**
* Retrieves status of multiple files by applying a filter.
* @param files array of file paths
* @param filter filter to apply on each file
* @return array of FileStatus objects or null if an error occurs
*/","* Filter files/directories in the given list of paths using user-supplied
     * path filter.
     * 
     * @param files is a list of paths
     * @param filter is the filter
     *
     * @return a list of statuses for the files under the given paths after
     *         applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If a file in <code>files</code> does not 
     *           exist
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,run,org.apache.hadoop.fs.FileContext$FileContextFinalizer:run(),2318,2321,"/**
* Executes function mask operation. 
* The operation is performed in a thread-safe manner due to synchronization.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,isMultiThreadNecessary,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList),98,102,"/**
* Determines if function mask condition is met.
* @param args list of path data
* @return true if thread count is greater than 1 and m1() returns true, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processArguments,org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList),68,73,"/**
* Invokes superclass method and optionally triggers additional processing based on wait option.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,resolve,"org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)",893,988,"/**
* Resolves a path to an object of type T.
* @param p the path to resolve
* @param resolveLastComponent whether to resolve last component as external directory
* @return ResolveResult object or null if not found
*/","* Resolve the pathname p relative to root InodeDir.
   * @param p - input path
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult which allows further resolution of the remaining path
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileHarStatus,org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path),646,659,"/**
* Retrieves the status of a file based on its path.
* @param f Path to the file
* @return HarStatus object or null if invalid
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)",153,178,"/**
* Validates checksums for a given file.
* @param fs ChecksumFS instance
* @param file Path to the file being checked
* @param bufferSize Buffer size for reading data and checksum files
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,setReplication,"org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)",463,475,"/**
* Recursively updates file metadata with specified replication factor.
* @param src the source file path
* @param replication the new replication value to apply
*/","* Set replication for an existing file.
   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>
   * @param src file name
   * @param replication new replication
   * @throws IOException if an I/O error occurs.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,delete,"org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)",529,550,"/**
* Recursively checks file existence and updates metadata.
* @param f Path to the file
* @param recursive whether to check subdirectories
*/","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",363,389,"/**
* Creates a ChecksumFSOutputSummer instance for the given file.
* @param fs underlying file system
* @param file Path to the file being checksummed
* @param createFlag flags controlling file creation
* @param absolutePermission permissions for the checksum file
* @param bufferSize buffer size for writing data
* @param replication replication factor for the checksum file
* @param blockSize block size for checksum calculations
* @param progress progress monitor for the write operation
* @param checksumOpt options for checksum calculation
* @param createParent whether to create parent directories if needed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processArguments,org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList),223,245,"/**
* Validates and processes destination path for export operation.
* @param args list of PathData objects
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,mkdir,"org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",185,192,"/**
* Initializes file system permissions and structure in the specified directory.
* @param dir Path to the directory
* @param permission File system permissions to apply
* @param createParent Whether to create parent directories if needed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,rename,"org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",2068,2082,"/**
* Wraps the rename operation on a distributed file system.
* @param srcFs source file system
* @param src source path
* @param dst destination path
* @param options rename options
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,renameInternal,"org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",206,212,"/**
* Copies file contents from source to destination with mask operation.
* @param src source file path
* @param dst destination file path 
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,rename,"org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",254,258,"/**
* Renames file from source to destination using filesystem service.
* @param src source file path
* @param dst destination file path
* @param options rename operation options
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InternalOperations.java,rename,"org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",35,39,"/**
* Wraps and delegates FS rename operation to a utility function. 
* @param fs FileSystem instance
* @param src Source file path
* @param dst Destination file path
* @param options Rename option(s)",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,run,org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path),771,780,"/**
* Checks file existence and updates mask if required.
* @param p the path to check
* @return true if update occurred, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)",187,214,"/**
* Initializes ChecksumFSInputChecker with file and fs parameters.
* @param fs ChecksumFileSystem instance
* @param file Path to the file for verification
* @param bufferSize buffer size for reading data and checksums
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,rename,"org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",890,915,"/**
* Copies file from src to dst, handling intermediate directories.
* @param src source path
* @param dst destination path
* @return true if copy successful, false otherwise
*/",* Rename files/dirs,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,delete,"org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",921,941,"/**
* Recursively processes a file path.
* @param f the file path to process
* @param recursive whether to recursively traverse subdirectories
* @return true if processing is successful, false otherwise
*/","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",621,642,"/**
* Creates a checksum FS output stream for the given file.
* @param fs ChecksumFileSystem instance
* @param file Path to the file
* @param overwrite Whether to overwrite existing files
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isAncestor,"org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",335,343,"/**
* Checks if a path in the source data tree matches a target path.
* @param source PathData object containing source paths
* @param target PathData object containing target path to match
* @return true if the source path matches the target, false otherwise
*/",Returns true if the target is an ancestor of the source.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path),91,95,"/**
* Returns a modified Path object based on the provided input.
* @param path original Path object to be transformed
*/","* 
   * @param path
   * @return return full path including the chroot",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path),137,148,"/**
* Masks a file path based on its contents and the given URI.
* @param p Path object to process
* @return modified string representation of the path or an empty string if it matches a specific pattern
*/","*  
   * Strip out the root from the path.
   * 
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path),153,163,"/**
* Extracts and formats the functional mask from a given Path.
* @param p input Path object
* @return formatted String or empty string if matching a specific pattern
*/","* Strip out the root from the path.
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /
   * @throws IOException if the p is not prefixed with root",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)",335,359,"/**
* Creates a trash checkpoint by renaming the current directory.
* @param trashRoot root path of the trash area
* @param date timestamp for checkpoint naming
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,resolve,"org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",79,112,"/**
* Resolves a file system path to its target object.
* @param fc FileContext instance
* @param path Path to resolve
* @return Target object of the path or null if not found
*/","* Performs the operation specified by the next function, calling it
   * repeatedly until all symlinks in the given path are resolved.
   * @param fc FileContext used to access file systems.
   * @param path The path to resolve symlinks on.
   * @return Generic type determined by the implementation of next.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,rename,"org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",795,808,"/**
* Renames a file by updating the source and destination paths.
* @param src original file path
* @param dst new file path
* @param options optional rename options (e.g. OVERWRITE)
*/","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param options options.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",253,259,"/**
* Delegates file operation to underlying filesystem.
* @param src source path
* @param dst destination path
* @param overwrite whether to overwrite existing files
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,toFileStatus,org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus),530,553,"/**
* Computes FileStatus for masked file.
* @param h HarStatus object containing file metadata
* @return FileStatus object or throws IOException if error occurs
*/","* Combine the status stored in the index and the underlying status. 
   * @param h status stored in the index
   * @return the combined file status
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,"org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)",942,949,"/**
* Constructs a RawLocalFileStatus object for the specified file.
* @param f the File to construct the status for
* @param defaultBlockSize the default block size (deprecated)
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(),107,107,"/**
* Initializes a new FileStatus object with default values.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)",110,115,"/**
* Initializes FileStatus object with basic properties.
* @param length total file size
* @param isdir true if directory, false otherwise
* @param block_replication replication factor for blocks
* @param blocksize block size in bytes
* @param modification_time last modified timestamp
* @param path file system path",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)",557,572,"/**
* Creates a FileStatus object from an FTPFile.
* @param ftpFile FTP file object
* @param parentPath path to the file's parent directory
* @return FileStatus object or null if not found
*/","* Convert the file information in FTPFile to a {@link FileStatus} object. *
   * 
   * @param ftpFile
   * @param parentPath
   * @return FileStatus",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB(),1128,1154,"/**
* Updates state machine based on current and previous SU values.
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,changeStateToProcessABlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock(),355,362,"/**
* Handles mask functionality based on skip result flag.
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init(),492,509,"/**
* Validates and initializes BZip2 stream parameters.
*@throws IOException if stream is not BZip2 formatted
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB(),1169,1181,"/**
* Updates function mask state based on input data.
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close(),302,308,"/**
* Calls superclass's m1 and outputs result using output.m1.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int),288,293,"/**
* Resets system state and outputs result of operation on 'b' to output stream.
* @param b operand value
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)",295,300,"/**
* Writes data to output stream, potentially resetting and rewriting.
* @param b the byte array to write
* @param off starting offset in the byte array
* @param len number of bytes to write
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",990,1003,"/**
* Checks token file integrity using M3 validation.
* @param tokenFile File containing authentication data
* @param conf Configuration object for validation
* @return True if validation passes, False otherwise
*/","* Verify that tokenFile contains valid Credentials.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printTokenFile,"org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)",123,129,"/**
* Processes a file with credentials and writes output to a stream.
* @param tokenFile input file containing credentials
* @param alias Text object for aliasing output
* @param conf Configuration settings for processing
* @param out PrintStream destination for output results
*/","Print out a Credentials file from the local filesystem.
   *  @param tokenFile a local File object.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param conf Configuration object passed along.
   *  @param out print to this stream.
   *  @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfo,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),600,617,"/**
* Retrieves DelegationTokenInformation for a given TokenIdent.
* @param ident unique token identifier
* @return DelegationTokenInformation object or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,syncLocalCacheWithZk,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),625,637,"/**
* Updates token information for the given identifier.
* @param ident TokenIdent object
*/","* This method synchronizes the state of a delegation token information in
   * local cache with its actual value in Zookeeper.
   *
   * @param ident Identifier of the token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),785,789,"/**
* Calls the overloaded version of m1 with isDefinition set to false.
* @param ident TokenIdent object containing identifier information
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,"org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)",306,319,"/**
* Writes mask data to output stream based on specified serialized format.
* @param os DataOutputStream instance
* @param format SerializedFormat enum value (WRITABLE or PROTOBUF)
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String),118,134,"/**
* Initializes MetricsSystem instance with the given prefix.
* @param prefix unique identifier for MBean registration
*/","* Construct the metrics system
   * @param prefix  for the system",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,startMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans(),334,339,"/**
* Iterates over all metrics source adapters and invokes their m1 method.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,start,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start(),100,102,"/**
* Initializes MBean masks based on start flag.
* @param startMBeans whether to initialize masks
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getInstance,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",869,881,"/**
* Retrieves or initializes a MetricsProxy instance for the specified namespace.
* @param namespace unique identifier
* @param numLevels number of levels to fetch
* @param drs DecayRpcScheduler object
* @return initialized MetricsProxy instance
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getInstance,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String),410,418,"/**
* Retrieves a MetricsProxy instance for the specified namespace.
* If no instance exists, creates and caches one.
* @param namespace identifier for metrics proxy
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,call,"org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",546,642,"/**
* Handles writable RPC call, validating client version and protocol.
* @param server the RPC server
* @param protocolName the name of the protocol
* @param rpcRequest the RPC request
* @return an ObjectWritable containing the result or null if failed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,processCall,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",463,504,"/**
* Executes a method on the server, handling exceptions and metrics.
* @param server RPC server instance
* @param connectionProtocolName protocol name for logging
* @param request RPC request message
* @param methodName method to invoke
* @param protocolImpl protocol implementation instance
* @return result of the invoked method or null if cancelled
*/","* This implementation is same as
     * ProtobufRpcEngine2.Server.ProtobufInvoker#call(..)
     * except this implementation uses non-shaded protobuf classes from legacy
     * protobuf version (default 2.5.0).",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",599,641,"/**
* Handles RPC call with detailed metrics and callback support.
* @param server RPC server instance
* @param connectionProtocolName protocol name for logging
* @param request RPC request message
* @param methodName method to invoke on the service
* @param protocolImpl protocol implementation object
* @return RpcWritable response or null on callback invocation
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages),219,221,"/**
* Initializes rates roller with a reference to its parent rolling averages.
* @param parent MutableRollingAverages instance that owns this rates roller
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",296,306,"/**
* Registers a new MetricsSink instance with the specified name and description.
* @param name unique sink identifier
* @param desc sink description
* @param sink MetricsSink object to register
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)",534,537,"/**
* Creates a MetricsSinkAdapter instance with given name and description.
* @param name adapter name
* @param desc adapter description
* @param conf configuration object containing sink and other metrics settings
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,add,"org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)",76,78,"/**
* Registers event with given name and elapsed time.
* @param name unique event identifier
* @param elapsed time elapsed since event occurrence in milliseconds
*/","* Add a rate sample for a rate metric
   * @param name of the rate metric
   * @param elapsed time",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addPersistedDelegationToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",404,433,"/**
* Adds a persisted delegation token with specified renew date.
* @param identifier TokenIdent object
* @param renewDate long value representing the renewal date
*/","* This method is intended to be used for recovering persisted delegation
   * tokens. Tokens that have an unknown <code>DelegationKey</code> are
   * marked as expired and automatically cleaned up.
   * This method must be called before this secret manager is activated (before
   * startThreads() is called)
   * @param identifier identifier read from persistent storage
   * @param renewDate token renew time
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,syncTokenOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats(),952,957,"/**
* Updates token owner stats and processes tokens.
*/","* This method syncs token information from currentTokens to tokenOwnerStats.
   * It is used when the currentTokens is initialized or refreshed. This is
   * called from a single thread thus no synchronization is needed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken(),762,780,"/**
* Removes and logs expired tokens, updating the database.
* @throws IOException on database access errors
*/",Remove expired delegation tokens from cache,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",226,233,"/**
* Initializes ActiveStandbyElector instance with ZooKeeper connection details.
* @param zookeeperHostPorts host and port of ZooKeeper ensemble
* @param zookeeperSessionTimeout timeout for ZooKeeper session
*/","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param maxRetryNum maxRetryNum.
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *         if valid data is not supplied.
   * @throws KeeperException
   *         other zookeeper operation errors.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElection,org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]),315,334,"/**
* Processes application data and initiates active election.
* @param data byte array containing application data
*/","* To participate in election, the app will call joinElection. The result will
   * be notified by a callback on either the becomeActive or becomeStandby app
   * interfaces. <br>
   * After this the elector will automatically monitor the leader status and
   * perform re-election if necessary<br>
   * The app could potentially start off in standby mode and ignore the
   * becomeStandby call.
   * 
   * @param data
   *          to be set by the app. non-null data must be set.
   * @throws HadoopIllegalArgumentException
   *           if valid data is not supplied",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElection,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int),798,823,"/**
* Re-establishes ZK session and joins election if app data is available.
* @param sleepTime time to wait before re-establishing session
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)",200,204,"/**
* Returns an InetSocketAddress instance with the given target and default port.
* @param target IP address or hostname
* @param defaultPort default TCP port number
*/","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @return socket addr.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,invoke,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",232,261,"/**
* Invokes client RPC with writable response.
* @param proxy unused
* @param method method to invoke
* @return ObjectWritable response or null on failure
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",212,294,"/**
* Handles RPC calls by converting the message into a buffer and sending it to the client.
* @param proxy unused parameter
* @param method RPC method being called
* @param args array containing the message to be sent (args[1])
* @return Message object or null if asynchronous call is used
*/","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     * 
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are 
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     * 
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",220,304,"/**
* Invokes a remote RPC method with the given arguments.
* @param proxy RPC client proxy
* @param method RPC method to invoke
* @param args RPC parameters (must be 2)
* @return invoked message or null on async call
*/","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     *
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     *
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,parseExpression,org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque),272,332,"/**
* Parses an expression from a deque of tokens.
* @param args deque of input tokens
* @return parsed Expression object or null if invalid
*/","* Parse a list of arguments to to extract the {@link Expression} elements.
   * The input Deque will be modified to remove the used elements.
   * 
   * @param args arguments to be parsed
   * @return list of {@link Expression} elements applicable to this command
   * @throws IOException if list can not be parsed",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream),193,195,"/**
* Prints function mask to output stream.
* @param out PrintStream object for output",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,"org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)",198,200,"/**
 * Prints a function mask to the specified output stream.
 * @param out PrintStream to write to
 * @param cmd command string
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream),203,205,"/**
* Prints function mask to output stream.
* @param out output stream to write to
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,"org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)",208,210,"/**
* Prints function mask command to output stream.
* @param out PrintStream object
* @param cmd command string
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,org.apache.hadoop.io.WritableComparator:get(java.lang.Class),55,57,"/**
* Returns a comparator instance for the given WritableComparable class.
* @param c Class of the WritableComparable to create a comparator for
*/","* For backwards compatibility.
   *
   * @param c WritableComparable Type.
   * @return WritableComparator.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable$Comparator:<init>(),88,90,"/**
 * Initializes comparator with ByteWritable as target class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable$Comparator:<init>(),90,92,"/**
* Initializes comparator with IntWritable class.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(),124,126,"/**
* Constructs a new instance with no prefix.
* @param prefix ignored (null) in this constructor
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text$Comparator:<init>(),429,431,"/**
 * Initializes text-based comparator. 
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/NullWritable.java,<init>,org.apache.hadoop.io.NullWritable$Comparator:<init>(),62,64,"/**
 * Initializes a new instance of the Comparator with NullWritable as its key type. 
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable$Comparator:<init>(),90,92,"/**
* Initializes comparator with LongWritable class.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable$Comparator:<init>(),88,90,"/**
* Initializes comparator with DoubleWritable class.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash$Comparator:<init>(),249,251,"/**
* Initializes comparator with MD5 hash class.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable$Comparator:<init>(),98,100,"/**
 * Initializes comparator with ShortWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable$Comparator:<init>(),85,87,"/**
* Initializes comparator with FloatWritable class.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable$Comparator:<init>(),224,226,"/**
* Initializes comparator with BytesWritable class as its key.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable$Comparator:<init>(),111,113,"/**
* Initializes comparator with writable boolean type.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8$Comparator:<init>(),212,214,"/**
* Initializes a new instance of the Comparator with a custom key class (UTF8).",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",380,410,"/**
* Authenticates using delegated token if provided; otherwise, falls back to handler.
* @param request HTTP servlet request
*/","* Authenticates a request looking for the <code>delegation</code>
   * query-string parameter and verifying it is a valid token. If there is not
   * <code>delegation</code> query-string parameter, it delegates the
   * authentication to the {@link KerberosAuthenticationHandler} unless it is
   * disabled.
   *
   * @param request the HTTP client request.
   * @param response the HTTP client response.
   * @return the authentication token for the authenticated request.
   * @throws IOException thrown if an IO error occurred.
   * @throws AuthenticationException thrown if the authentication failed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,<init>,"org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)",78,81,"/**
* Initializes NodeFencer with configuration and fencing specification.
* @param conf NodeFencer configuration
* @param spec Fencing specification string
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,"org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)",724,762,"/**
* Returns an array of names possibly transformed by deprecation rules.
* @param deprecations DeprecationContext object
* @param name Original name to transform
* @return Array of transformed names or overlayed values if applicable
*/","* Checks for the presence of the property <code>name</code> in the
   * deprecation map. Returns the first of the list of new keys if present
   * in the deprecation map or the <code>name</code> itself. If the property
   * is not presently set but the property map contains an entry for the
   * deprecated key, the value of the deprecated key is set as the value for
   * the provided property name.
   *
   * Also updates properties and overlays with deprecated keys, if the new
   * key does not already exist.
   *
   * @param deprecations deprecation context
   * @param name the property name
   * @return the first property in the list of properties mapping
   *         the <code>name</code> or the <code>name</code> itself.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,setConfAsEnvVars,org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map),207,211,"/**
* Replaces '.' with '_' in key names and updates environment map.
*/","* Set the environment of the subprocess to be the Configuration,
   * with '.'s replaced by '_'s.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setHeaders,org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration),1958,1970,"/**
* Builds a map of HTTP request headers based on configuration.
* @param conf Configuration object
* @return Map of header parameters
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getFilterParameters,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)",54,65,"/**
* Extracts and filters configuration parameters by prefix.
* @param conf Configuration object
* @param prefix Filter prefix to apply
* @return Map of filtered parameter names to values or empty map if none found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processArguments,org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList),74,79,"/**
* Calls parent method and optional wait operation.
* @param args list of PathData objects
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",117,122,"/**
* Retrieves block locations for a given file path and length range.
* @param f Path to the file
* @param start Starting offset in bytes
* @param len Length of the range in bytes
* @return Array of BlockLocation objects or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",1512,1535,"/**
* Resolves BlockLocation array for given FileStatus and range.
* @param fs FileStatus object
* @param start start offset in bytes
* @param len length of block range in bytes
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,renewToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",190,196,"/**
* Renews a delegationToken.
* @param token Token to be renewed
* @param renewer Identity of the entity performing renewal
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",198,206,"/**
* Cancels a delegationToken.
* @param token Token to cancel
* @param canceler Optional canceler or default from token context
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",154,164,"/**
* Customizes and returns a TokenIdent using the m2 and m3 methods.
* @param token input token with TokenIdent
* @param canceller string parameter for customization
* @return customized TokenIdent or throws IOException if error occurs
*/","* Cancels a token by removing it from the SQL database. This will
   * call the corresponding method in {@link AbstractDelegationTokenSecretManager}
   * to perform validation and remove the token from the cache.
   * @return Identifier of the canceled token",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),52,58,"/**
* Fetches and caches group affiliations for a given user.
* @param user unique user identifier
* @return list of group names or empty list if not found
*/","* Get unix groups (parent) and netgroups for given user
   *
   * @param user get groups and netgroups for this user
   * @return groups and netgroups for user",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)",1015,1033,"/**
* Creates a directory and extracts tar file contents.
* @param inFile input file to extract
* @param untarDir target directory for extraction
*/","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inFile The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @throws IOException an exception occurred.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)",1289,1292,"/**
* Invokes core operation with default flag value.
* @param filename input file name
* @param perm permission string
* @return result code (int)
*/","* Change the permissions on a filename.
   * @param filename the name of the file to change
   * @param perm the permission string
   * @return the exit code from the command
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException command interrupted.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setReadable,"org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)",1347,1359,"/**
* Sets file permissions on Windows.
* @param f the file to modify
* @param readable whether to grant or revoke read permission
* @return true if successful, false otherwise
*/","* Platform independent implementation for {@link File#setReadable(boolean)}
   * File#setReadable does not work as expected on Windows.
   * @param f input file
   * @param readable readable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setWritable,"org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)",1368,1380,"/**
* Sets file permissions on Windows.
* @param f target file
* @param writable whether to grant or revoke write permission
* @return true if successful, false otherwise
*/","* Platform independent implementation for {@link File#setWritable(boolean)}
   * File#setWritable does not work as expected on Windows.
   * @param f input file
   * @param writable writable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setExecutable,"org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)",1392,1404,"/**
* Sets file execute bit based on platform.
* @param f the file to modify
* @param executable whether to add or remove execute permission
* @return true if successful, false otherwise
*/","* Platform independent implementation for {@link File#setExecutable(boolean)}
   * File#setExecutable does not work as expected on Windows.
   * Note: revoking execute permission on folders does not have the same
   * behavior on Windows as on Unix platforms. Creating, deleting or renaming
   * a file within that folder will still succeed on Windows.
   * @param f input file
   * @param executable executable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,refreshIfNeeded,org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded(),94,141,"/**
* Refreshes system information and updates internal state.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfo,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo(),983,995,"/**
* Executes native functions based on system conditions.
* @throws IOException if native call fails
*/","* Load file permission information (UNIX symbol rwxrwxrwx, sticky bit info).
     *
     * To improve peformance, give priority to native stat() call. First try get
     * permission information by using native JNI call then fall back to use non
     * native (ProcessBuilder) call in case native lib is not loaded or native
     * call is not successful",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setOwner,"org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1099,1103,"/**
* Invokes M2 operation on file system data and passes results to FileUtil.
* @param p Path object
* @param username user identifier
* @param groupname group name
*/",* Use the command chown to set owner.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setPermission,"org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1470,1508,"/**
* Applies file permissions to the specified file.
* @param f File object
* @param permission FsPermission object containing user, group and other actions
*/","* Set permissions to the required value. Uses the java primitives instead
   * of forking if group == other.
   * @param f the file to change
   * @param permission the new permissions
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),75,88,"/**
* Updates net group cache based on provided groups.
* @param groups list of group identifiers
*/","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpTokens,org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation),811,819,"/**
* Logs user group information and associated tokens.
* @param ugi UserGroupInformation object containing tokens
*/","* Dump all tokens of a UGI.
   * @param ugi UGI to examine",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logUserInfo,"org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1980,1991,"/**
* Logs user group information with token details.
* @param log logging instance
* @param caption descriptive text
* @param ugi user and group information to be logged
*/","* Log current UGI and token information into specified log.
   * @param ugi - UGI
   * @param log log.
   * @param caption caption.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,containsKmsDt,org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation),1155,1165,"/**
* Checks if a user has a valid FUNC_MASK credential.
* @param ugi UserGroupInformation object containing user credentials
* @return true if credential is valid, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,"org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",113,117,"/**
* Validates file system permissions for a directory.
* @param localFS Local file system instance
* @param dir Directory path to validate
* @param expected Expected permission value
*/","* Create the local directory if necessary, check permissions and also ensure
   * it can be read from and written into.
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,"org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",131,136,"/**
* Validates file system permissions and disk integrity.
* @param localFS Local file system instance
* @param dir directory path to check
* @param expected expected permissions
*/","* Create the local directory if necessary, also ensure permissions
   * allow it to be read from and written into. Perform some diskIO
   * to ensure that the disk is usable for writes. 
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[]),1823,1826,"/**
* Returns status of multiple files.
* @param files array of file paths
*/","* See {@link #listStatus(Path[], PathFilter)}
     *
     * @param files files.
     * @throws AccessControlException If access is denied.
     * @throws FileNotFoundException If <code>files</code> does not exist.
     * @throws IOException If an I/O error occurred.
     * @return file status array.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1481,1496,"/**
* Resolves the path and returns the most enclosed path.
* @param path input path
* @return most enclosing path or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",447,453,"/**
* Resolves and delegates file system operations to a target file system.
* @param f the file path to resolve
* @param bufferSize buffer size for I/O operations
* @param progress progressable callback for reporting progress
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",455,468,"/**
* Creates or updates an FSDataOutputStream on the given path.
* @param f Path to create output stream for
* @param permission File permissions
* @param flags Create flags
* @param bufferSize Buffer size
* @param replication Replication factor
* @param blockSize Block size
* @param progress Progress tracker
* @return FSDataOutputStream instance
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",470,483,"/**
* Creates an FSDataOutputStream for a file with specified permissions and block size.
* @param f the Path to create the stream for
* @param permission the FsPermission for the new file
* @param overwrite whether to overwrite existing files
* @param bufferSize the buffer size for writing
* @param replication the replication factor for the file
* @param blockSize the block size for the file
* @param progress a Progressable object for reporting progress
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",486,496,"/**
* Recursively deletes a directory or file.
* @param f Path to delete
* @param recursive true for recursive deletion
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),514,521,"/**
* Resolves file checksum using inode tree and delegated file system.
* @param f Path to resolve
* @return FileChecksum object or throws exception if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",523,530,"/**
* Computes file checksum for a given path and length.
* @param f path to the file
* @param length expected file length
* @return FileChecksum object or throws an exception if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",630,655,"/**
* Resolves file system for given path and filter.
* @param f Path to resolve
* @param filter Optional filter to apply
* @return Iterator of LocatedFileStatus or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path),670,675,"/**
* Recursively traverses directory and resolves target file system.
* @param dir path to start resolving from
* @return true if resolution was successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",677,683,"/**
* Resolves file system for the given directory and permissions.
* @param dir directory path
* @param permission desired file system permissions
* @return true if resolved successfully, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)",685,691,"/**
* Resolves user access and reads file stream.
* @param f path to file
* @param bufferSize buffer size for streaming
* @return FSDataInputStream object or throws exception if failed.",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)",800,806,"/**
* Resizes a file to the specified length.
* @param f Path to the file
* @param newLength desired file size in bytes
* @return true if resize is successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",808,815,"/**
* Resolves file system and performs m3 operation on the resolved path.
* @param f Path to resolve
* @param username User identifier for access control
* @param groupname Group identifier for access control
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",817,823,"/**
* Resolves and applies permissions to a file path.
* @param f the file path
* @param permission the desired file system permissions
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",825,831,"/**
* Resolves file system for given path and replication level.
* @param f the input file path
* @param replication data replication level (short)
* @return true if resolved successfully, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",833,839,"/**
* Updates file timestamps in the target file system.
* @param f Path to the file
* @param mtime Last modified time
* @param atime Last accessed time
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",841,847,"/**
* Resolves and applies ACL specifications to a file system path.
* @param path the target file system path
* @param aclSpec list of ACL entries to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",849,855,"/**
* Resolves and applies ACL specifications to a file system.
* @param path file system path
* @param aclSpec list of access control entries
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),857,863,"/**
* Resolves inode tree and delegates remaining path to target file system.
* @param path input path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path),865,871,"/**
* Resolves inode tree and recursively calls m3 on the target file system.
* @param path input directory or file path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",873,878,"/**
* Resolves file system and applies ACL specification to a given path.
* @param path the target file system path
* @param aclSpec list of access control entries to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path),880,885,"/**
* Resolves ACL status for a given file system path.
* @param path the file system path to resolve
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",887,893,"/**
* Resolves inode tree and delegates write operation to target file system.
* @param path path to resolve
* @param name attribute name
* @param value binary attribute value
* @param flag flags for the write operation
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",895,900,"/**
* Resolves and executes M3 operation on the target file system.
* @param path input file path
* @param name output file name
* @return file contents as byte array or throws IOException if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path),902,907,"/**
* Resolves file system for given path and returns a map of metadata.
* @param path the file system path to resolve
* @return Map containing file system metadata, or null if not resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",909,915,"/**
* Resolves file system tree and fetches data for given path and names.
* @param path file system path
* @param names list of name keys to fetch
* @return Map of key-value pairs or throws IOException if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path),917,922,"/**
* Resolves file system path and delegates to target file system's M3 operation.
* @param path file system path
* @return list of strings representing resolved path contents
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",924,929,"/**
* Resolves and processes the inode tree for a given file system.
* @param path directory path
* @param name new file name
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),990,1002,"/**
* Resolves file system for given path.
* @param f Path to resolve
* @return short value or throws exception if resolution fails
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path),1015,1020,"/**
* Resolves and fetches content summary for a given file.
* @param f Path to the file
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1022,1027,"/**
* Resolves quota usage for a given file path.
* @param f the file path to resolve
* @return QuotaUsage object or throws IOException if error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1072,1078,"/**
* Resolves file system path using inode tree and fetches M3 data.
* @param path input file system path
* @param snapshotName name of the snapshot to fetch
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1080,1087,"/**
* Resolves file system path and performs snapshot renaming.
* @param path path to resolve
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1089,1095,"/**
* Resolves inode tree and triggers M3 operation on target file system.
* @param path inode tree input path
* @param snapshotName name of the snapshot to create
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1097,1102,"/**
* Resolves inode tree and delegates path resolution to target file system.
* @param src input path
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1104,1109,"/**
* Resolves file system path and applies access control policy.
* @param src source file system path
* @param policyName name of the access control policy to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),1111,1116,"/**
* Resolves file system path and delegates processing to target FS.
* @param src input path to resolve
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),1118,1123,"/**
* Resolves Block Storage Policy SPI for the given file system path.
* @param src file system path to resolve
* @return BlockStoragePolicySpi instance or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path),1304,1312,"/**
* Resolves file system status for a given path.
* @param p Path to resolve
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUsed,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed(),1321,1330,"/**
* Resolves and delegates to target file system's used space calculation.
* @throws IOException if an I/O error occurs
*/","* Return the total size of all files under ""/"", if {@link
   * Constants#CONFIG_VIEWFS_LINK_MERGE_SLASH} is supported and is a valid
   * mount point. Else, throw NotInMountpointException.
   *
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1332,1341,"/**
* Resolves file system path recursively using inode tree.
* @param path input file system path
* @return Path object or throws exception if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",468,481,"/**
* Retrieves block locations for a file.
* @param file FileStatus object
* @param start starting offset (in bytes)
* @param len length of the range to fetch (in bytes)
* @return array of BlockLocation objects or null if not found
*/","* Get block locations from the underlying fs and fix their
   * offsets and lengths.
   * @param file the input file status to get block locations
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @return block locations for this segment of file
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,open,"org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)",679,690,"/**
* Opens a file stream for reading from the archive.
* @param f path to the file
* @param bufferSize buffer size for I/O operations
* @return FSDataInputStream object or throws IOException if not found
*/","* Returns a har input stream which fakes end of 
   * file. It reads the index files to get the part 
   * file name and the size and start of the file.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)",148,151,"/**
* Constructs a Checker instance with default buffer size.
* @param fs File system object
* @param file Path to the file being checked
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,open,"org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)",334,339,"/**
* Creates a buffered input stream for file contents.
* @param f the path to the file
* @param bufferSize the size of the buffer
* @return an FSDataInputStream object or null if fails
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,createInternal,"org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",418,428,"/**
* Creates a file output stream with specified parameters.
* @param f the file path
* @param createFlag creation flags
* @param absolutePermission permissions
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor
* @param checksumOpt checksum option
* @param createParent flag to create parent directories
* @return FSDataOutputStream object
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,processArguments,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList),81,94,"/**
* Calls child methods and superclass method in a specific order.
* @param args list of PathData objects
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,open,"org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)",566,578,"/**
* Opens an input stream to the specified file with optional checksum verification.
* @param f Path to the file
* @param bufferSize Buffer size for reading from the file
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException if an I/O error occurs.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)",704,734,"/**
* Creates a new FSDataOutputStream for the given file path.
* @param f file path to create output stream for
* @param permission permissions for the output stream
* @param overwrite whether to overwrite existing files
* @param createParent whether to create parent directories if necessary
* @param bufferSize buffer size for writing data
* @param replication replication factor for the output stream
* @param blockSize block size for writing data
* @param progress progress callback object
* @return FSDataOutputStream instance or null on failure",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,isValidName,org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String),97,100,"/**
* Executes M4 preprocessor on the given source file.
* @param src path to the source file
* @return true if processing is successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",173,182,"/**
* Delegates file system operation to underlying FS implementation.
* @param f Path to the file
* @param flag File creation flags
* @return FSDataOutputStream instance or null if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)",184,188,"/**
* Recursively checks file existence and resolves symbolic links.
* @param f     file path to check
* @param recursive whether to traverse subdirectories
* @return true if file exists or false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",190,194,"/**
* Retrieves block locations from file system.
* @param f the input path
* @param start starting offset
* @param len length to fetch
* @return array of BlockLocation objects or throws exception if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path),196,200,"/**
* Computes file checksum using MyFS.
* @param f file path to compute checksum for
* @return FileChecksum object or null if error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path),202,206,"/**
* Recursively fetches file status by resolving symbolic links.
* @param f input path to resolve
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path),213,217,"/**
* Resolves file status using custom file system and path resolver.
* @param f input file path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path),230,233,"/**
* Calls M2 on MyFS with result of M1 on given file path.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path),240,244,"/**
* Resolves file status for given path using custom implementation.
* @param f input path to resolve
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path),246,250,"/**
* Wraps file status iterator with custom FS operations.
* @param f input path to iterate over
* @return iterator of FileStatus objects or null if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path),252,256,"/**
* Wraps file status iterator with resolved links.
* @param f input path to resolve
* @return iterator of LocatedFileStatus objects or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",258,263,"/**
* Invokes M2 operation on the file system with a transformed directory path.
* @param dir target directory
* @param permission desired permissions
* @param createParent whether to create parent directories if needed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)",265,269,"/**
* Wraps file read operation with resolved path and buffer size.
* @param f file path to resolve
* @param bufferSize read buffer capacity
* @return FSDataInputStream for the resolved file
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)",271,275,"/**
* Updates file length in the underlying file system.
* @param f file path to update
* @param newLength desired file size
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,283,"/**
* Copies file metadata from source to destination.
* @param src source directory path
* @param dst destination directory path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",285,292,"/**
* Copies data from source to destination, handling unresolved links.
* @param src source file path
* @param dst destination file path
* @param overwrite whether to overwrite existing files
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",294,300,"/**
* Calls the underlying file system to perform operation m2.
* @param f path to the relevant file
* @param username user identifier for access control
* @param groupname group identifier for access control
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",302,306,"/**
* Invokes M2 operation on the FS with transformed path and permissions.
* @param f file system path
* @param permission file system permissions
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)",308,312,"/**
* Performs file operation m2 on the given path with specified replication.
* @param f the file path to operate on
* @param replication replication factor for the operation
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)",314,318,"/**
* Invokes native file system operation on a path with specified modification and access times.
* @param f the file path
* @param mtime the new modification time (in milliseconds)
* @param atime the new access time (in milliseconds)
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",320,324,"/**
* Invokes m2 operation on file system, passing result of m1 and ACL spec.
* @param path file system path
* @param aclSpec access control list entries
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",326,330,"/**
* Calls M2 operation on file system with transformed input.
* @param path Path to file or directory
* @param aclSpec Access control list specifications
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path),332,335,"/**
* Invokes m2 operation on file system using result of m1 operation.
* @param path Path object representing file or directory to operate on 
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path),337,340,"/**
* Invokes operation 'm2' on the file system with result of 'm1' as input.
* @param path Path object representing the file or directory to process
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",342,345,"/**
* Invokes secondary operation on file system using result of primary operation.
* @param path file system path
* @param aclSpec access control list specifications
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path),347,350,"/**
* Computes ACL status using the file system and cached ACL result. 
* @param path file system path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",352,356,"/**
* Calls underlying file system's m2 method with transformed input.
* @param path the file path
* @param name the attribute name
* @param value the attribute value
* @param flag the attribute flags
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",358,361,"/**
* Calls M2 operation on file system with pre-processed input.
* @param path Path to file
* @param name File name
* @return Resulting byte array or null if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path),363,366,"/**
* Processes file system data using M2 algorithm.
* @param path filesystem path to process
* @return map of binary data or null on failure
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",368,372,"/**
* Invokes M2 operation on the file system, passing results of M1 and names.
* @param path file path
* @param names list of strings
* @return Map of string to bytes or throws IOException if an error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
* Executes an M2 operation on the file system using the result of M1.
* @param path file system path
* @return list of strings or null if an error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",379,382,"/**
* Invokes file operation m2 on MyFS instance with result of m1 and given name.
* @param path directory/file path
* @param name string identifier
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",384,387,"/**
* Calls M2 operation on file system with result of M1 operation on provided path.
* @param path input file path
* @param name additional parameter for M2 operation
* @return Path object representing the result of M2 operation
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",389,393,"/**
* Executes M2 operation on the specified file system.
* @param path file system path
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",395,399,"/**
* Calls underlying file system operation with modified snapshot directory. 
* @param snapshotDir Path to snapshot directory
* @param snapshotName name of the snapshot
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",406,410,"/**
* Invokes M2 operation on file system with transformed path and specified policy.
* @param path file system path to be processed
* @param policyName name of policy to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),412,416,"/**
* Executes M2 operation on the file system using result of M1 operation. 
* @param src source file path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",441,451,"/**
* Recursively links the directory tree from 'target' to 'link'.
* @param target source directory path
* @param link destination path
* @param createParent whether to create parent directories if missing
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path),453,456,"/**
* Applies operation 'm2' to the result of 'm1' on input file 'f'. 
* @param f input file path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",568,636,"/**
* Renames a file/directory from one location to another.
* @param src source Path
* @param dst destination Path
* @param overwrite whether to allow overwriting existing files
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,next,org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next(),945,952,"/**
* Fetches and processes user profile by ID.
* @return UserProfile object or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChrootedPath,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",657,668,"/**
* Generates a path with function mask by resolving file system and status.
* @param res ResolveResult containing target file system
* @param status FileStatus to determine function mask
* @return Path with updated function mask or original path if invalid
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",480,497,"/**
* Updates file mask for source and target paths.
* @param src source path
* @param dst target path
*/",* Rename files/dirs.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",499,523,"/**
* Renames a file from source to destination with optional overwrite.
* @param src original file path
* @param dst new file path
* @param overwrite whether to overwrite existing file at destination
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,rename,"org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1031,1060,"/**
* Renames a file from src to dst across possibly different filesystems.
* @param src source path
* @param dst destination path
* @param options rename options
* @throws various exceptions on failure or unsupported operation
*/","* Renames Path src to Path dst
   * <ul>
   * <li>Fails if src is a file and dst is a directory.
   * <li>Fails if src is a directory and dst is a file.
   * <li>Fails if the parent of dst does not exist or is a file.
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails if the dst
   * already exists.
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites the dst if
   * it is a file or an empty directory. Rename fails if dst is a non-empty
   * directory.
   * <p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for details
   * <p>
   * 
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If <code>dst</code> already exists and
   *           <code>options</code> has {@link Options.Rename#OVERWRITE}
   *           option false.
   * @throws FileNotFoundException If <code>src</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>dst</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>src</code>
   *           and <code>dst</code> is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",245,251,"/**
* Renames files and directories while preserving source file masks.
* @param src original directory path
* @param dst new directory path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fileStatusesInIndex,"org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)",511,522,"/**
* Recursively fetches file status metadata for child files under a given directory.
* @param parent Directory parent object
* @param statuses List to store FileStatus objects
*/","* Get filestatuses of all the children of a given directory. This just reads
   * through index file and reads line by line to get all statuses for children
   * of a directory. Its a brute force way of getting all such filestatuses
   * 
   * @param parent
   *          the parent path directory
   * @param statuses
   *          the list to add the children filestatuses to",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileStatus,org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path),640,644,"/**
* Calculates file status based on path.
* @param f file path to query
* @return FileStatus object or null if error occurs
*/","* return the filestatus of files in har archive.
   * The permission returned are that of the archive
   * index files. The permissions are not persisted 
   * while creating a hadoop archive.
   * @param f the path in har filesystem
   * @return filestatus.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path),910,919,"/**
* Retrieves file status for a given path.
* @param f Path to query
* @return FileStatus object or throws FileNotFoundException if the file does not exist
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,org.apache.hadoop.fs.LocatedFileStatus:<init>(),40,42,"/**
* Constructs an empty LocatedFileStatus object.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)",464,468,"/**
* Constructs an instance of NflyStatus from a file system and status.
* @param realFs the underlying file system
* @param realStatus the original file status
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",35,38,"/**
 * Updates the file system context and path for fetching file status.
 * @param fs FileStatus object representing the current file state
 * @param newPath new Path to fetch file status for
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",205,250,"/**
* Retrieves FileStatus for the given Path.
* @param file Path to fetch status for
* @return FileStatus object or null if not found
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Computes file status using specified path.
* @param path Path object to compute status for
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,doGlob,org.apache.hadoop.fs.Globber:doGlob(),208,396,"/**
* Fetches file statuses for a given glob pattern.
* @param pathPattern glob pattern to match
* @return array of matching file statuses or null if no matches found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,notFoundStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path),623,625,"/**
* Creates a FileStatus object with default values.
* @param f Path to the file
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",516,548,"/**
* Retrieves FileStatus for the given local file, using FTP metadata.
* @param client FTPClient instance
* @param file Path to the local file
* @return FileStatus object or throws FileNotFoundException if not found
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)",299,321,"/**
* Initializes a CBZip2InputStream with the given input stream and read mode.
* @param in InputStream to be wrapped
* @param readMode Mode of operation (CONTINUOUS or BYBLOCK)
* @param skipDecompression Whether to skip decompression steps
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0(),450,490,"/**
* Returns a function mask based on the current state.
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Print:execute(),193,198,"/**
* Performs masked operations on token files.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",833,843,"/**
* Processes TokenIdent using custom logic and delegates to superclass if necessary.
* @param token input data
* @param canceller cancellation reason (not used in this implementation)
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream),300,304,"/**
* Writes data to output stream using WRITABLE format.
* @param os DataOutputStream instance
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(),139,141,"/**
 * Initializes MetricsSystem instance with default configuration.
 */",* Construct the system but not initializing (read config etc.) it.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSource,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",260,270,"/**
* Registers a metrics source with the given name and description.
* @param name unique identifier for the source
* @param desc human-readable description of the source
* @param source MetricsSource object containing configuration and data
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)",577,597,"/**
* Handles RPC call to a remote method.
* @param server RPC server instance
* @param connectionProtocolName name of the connection protocol
* @param request RPC request data
* @param receiveTime timestamp of the request
* @param methodName name of the remote method being called
* @param declaringClassProtoName name of the protocol class implementing the method
* @return Writable response object or null if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String),144,155,"/**
* Initializes rolling averages with the specified metric value name.
* @param metricValueName unique identifier for the rolling average
*/","* Constructor for {@link MutableRollingAverages}.
   * @param metricValueName input metricValueName.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,replaceScheduledTask,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)",160,167,"/**
* Schedules a task to roll rates based on the specified window and interval.
* @param windows number of windows
* @param interval time interval between rate rolls
* @param timeUnit unit of time for the interval (e.g. seconds, minutes)
*/",* This method is for testing only to replace the scheduledTask.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",272,294,"/**
* Registers a metrics sink with the specified name and description.
* @param name unique identifier for the sink
* @param description human-readable description of the sink
* @param sink MetricsSink object to register
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks(),492,519,"/**
* Initializes and configures metrics sinks based on configuration.
* @param config metrics configuration object
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,recheckElectability,org.apache.hadoop.ha.ZKFailoverController:recheckElectability(),808,860,"/**
* Handles node's election participation based on its health state.
* @param lastHealthState current service health state
*/","* Check the current state of the service, and join the election
   * if it should be in the election.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElectionAfterFailureToBecomeActive,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive(),627,629,"/**
* Initiates mask functionality with sleep after failure to become active. 
* @param SLEEP_AFTER_FAILURE_TO_BECOME_ACTIVE sleep duration in milliseconds
*/","* We failed to become active. Re-join the election, but
   * sleep for a few seconds after terminating our existing
   * session, so that other nodes have a chance to become active.
   * The failure to become active is already logged inside
   * becomeActive().",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processWatchEvent,"org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)",635,713,"/**
* Handles ZooKeeper watched events.
* @param zk ZooKeeper connection
* @param event WatchedEvent object containing type and state information
*/","* interface implementation of Zookeeper watch events (connection and node),
   * proxied by {@link WatcherWithClientRef}.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)",180,183,"/**
* Creates an InetSocketAddress with default port.","* Util method to build socket addr from either.
   *   {@literal <host>}
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @param defaultPort default port.
   * @return socket addr.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processOptions,org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList),166,212,"/**
* Processes FUNC_MASK command with linked list of arguments.
* @param args input command arguments
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",154,182,"/**
* Handles delegated authentication by checking the authorization header.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
* @return AuthenticationToken or null on error
*/","* This method is overridden to restrict HTTP authentication schemes
   * available for delegation token management functionality. The
   * authentication schemes to be used for delegation token management are
   * configured using {@link DELEGATION_TOKEN_SCHEMES_PROPERTY}
   *
   * The basic logic here is to check if the current request is for delegation
   * token management. If yes then check if the request contains an
   * ""Authorization"" header. If it is missing, then return the HTTP 401
   * response with WWW-Authenticate header for each scheme configured for
   * delegation token management.
   *
   * It is also possible for a client to preemptively send Authorization header
   * for a scheme not configured for delegation token management. We detect
   * this case and return the HTTP 401 response with WWW-Authenticate header
   * for each scheme configured for delegation token management.
   *
   * If a client has sent a request with ""Authorization"" header for a scheme
   * configured for delegation token management, then it is forwarded to
   * underlying {@link MultiSchemeAuthenticationHandler} for actual
   * authentication.
   *
   * Finally all other requests (excluding delegation token management) are
   * forwarded to underlying {@link MultiSchemeAuthenticationHandler} for
   * actual authentication.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,org.apache.hadoop.conf.Configuration:handleDeprecation(),777,786,"/**
* Handles property deprecation in config by iterating over deprecated properties.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,onlyKeyExists,org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String),1295,1305,"/**
* Checks if name matches a func mask pattern.
* @param name input string to check
*/","* Return existence of the <code>name</code> property, but only for
   * names which have no valid value, usually non-existent or commented
   * out in XML.
   *
   * @param name the property name
   * @return true if the property <code>name</code> exists without value",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRaw,org.apache.hadoop.conf.Configuration:getRaw(java.lang.String),1355,1362,"/**
* Resolves and formats the input name using M2 and M3/M4 methods.
* @param name user-provided name to be processed
*/","* Get the value of the <code>name</code> property, without doing
   * <a href=""#VariableExpansion"">variable expansion</a>.If the key is 
   * deprecated, it returns the value of the first key which replaces 
   * the deprecated key and is not null.
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> property or 
   *         its replacing property and null if no such property exists.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)",1420,1458,"/**
* Sets or updates a property with deprecation handling.
* @param name property name
* @param value property value
* @param source optional source of the change (e.g. ""user"" or null for programmatically)","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated, it also sets the <code>value</code> to
   * the keys that replace the deprecated key. Name will be trimmed before put
   * into configuration.
   *
   * @param name property name.
   * @param value property value.
   * @param source the place that this configuration value came from 
   * (For debugging).
   * @throws IllegalArgumentException when the value or name is null.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,unset,org.apache.hadoop.conf.Configuration:unset(java.lang.String),1476,1492,"/**
* Updates mask data for a given user name.
* @param name unique user identifier
*/","* Unset a previously set property.
   * @param name the property name",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,tryFence,"org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",81,135,"/**
* Executes a shell command on the target system.
* @param target HAServiceTarget object
* @param args additional command arguments
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,grantPermissions,org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File),235,239,"/**
* Applies masks to specified file using custom utilities.
* @param f target file for masking operations
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize(),144,148,"/**
* Calculates and returns the function mask value.
* @return Function mask value in bytes.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize(),151,155,"/**
* Calculates and returns the function mask value.
* @return 64-bit unsigned integer representing the function mask.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize(),158,162,"/**
* Returns the functional mask value based on available memory.
* @return functional mask as a 64-bit integer value.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize(),165,169,"/**
* Returns system memory mask based on available memory.
* @return unique identifier reflecting current memory availability
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumProcessors,org.apache.hadoop.util.SysInfoWindows:getNumProcessors(),172,176,"/**
* Returns the number of processors available to this JVM instance.
* @return The number of processors as an integer.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuFrequency,org.apache.hadoop.util.SysInfoWindows:getCpuFrequency(),185,189,"/**
* Returns a function mask value based on CPU frequency.
* @return Function mask as a long integer value
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime(),192,196,"/**
* Returns function execution mask in milliseconds.
* @return Cumulative CPU time in milliseconds since last reset
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage(),199,207,"/**
* Calculates CPU usage as a percentage of total processors.
* @return normalized CPU usage or -1.0f if unavailable
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed(),210,218,"/**
* Calculates CPU usage as a percentage.
* @return CPU usage value or -1.0 if unavailable
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead(),221,225,"/**
* Returns a mask value based on network bytes read.
* @return unique identifier as a long integer
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten(),228,232,"/**
* Returns the function mask value based on network bytes written.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead(),234,238,"/**
* Returns the function mask value.
* Calculates based on storage bytes read.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten(),240,244,"/**
* Returns the function mask.
* Calculates and returns the number of bytes written to storage.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getPermission,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission(),951,957,"/**
* Calls superclass method to retrieve file permissions.
* If custom checks fail, performs additional setup.",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getOwner,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner(),959,965,"/**
* Calls superclass method m3() only if conditions in m1() are met.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getGroup,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup(),967,973,"/**
 * Calls superclass M3 while optionally invoking M2 based on M1's result. 
 * @return string result from superclass M3, or after M2 if M1 fails.",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput),1087,1093,"/**
* Calls superclass write method after conditional logic execution.
* @throws IOException if an I/O error occurs during writing.",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,flush,org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush(),143,160,"/**
* Resets file permissions on all platforms.
* @throws IOException if an I/O error occurs
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),63,68,"/**
* Executes mask operation on network groups.
*/",* Refresh the netgroup cache,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path),498,503,"/**
* Invokes m1 with default parameters.
* @param path file path to access
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(),1299,1302,"/**
* Default implementation of fs status retrieval.
* @throws IOException on error
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,updateMountPointFsStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)",169,175,"/**
* Updates file system status for a given mount point.
* @param viewFileSystem View of the file system
* @param mountPointMap Map of mount points to their statuses
* @param mountPoint Mount point to update
* @param path File system path to query
*/","* Update FsStatus for the given the mount point.
   *
   * @param viewFileSystem
   * @param mountPointMap
   * @param mountPoint
   * @param path",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,read,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)",194,210,"/**
* Reads data from the file system and performs checksum verification.
* @param position current position in the file
* @param b buffer to read into
* @param off offset into buffer
* @param len number of bytes to read
* @return actual number of bytes read or 0 on failure
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList),306,315,"/**
* Handles special case for input from standard in.
* @param args list of PathData objects
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",696,702,"/**
* Wraps the main file creation method with default buffer size.
* @param f target file path
* @param permission desired permissions
* @param overwrite whether to overwrite existing files
* @param progress reporting progress callback
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",736,742,"/**
* Creates a new output stream with specified settings and parameters.
* @param f Path to the file
* @param permission File permissions
* @param overwrite Whether to overwrite existing files
* @param bufferSize Buffer size for writes
* @param replication Replication factor for storage
* @param blockSize Block size for writes
* @param progress Progress tracker for write operations
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",757,768,"/**
* Creates a new FSDataOutputStream with specified parameters.
* @param f file path
* @param permission file permissions
* @param flags create flags (OVERWRITE)
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",638,644,"/**
* Copies a file from source to destination path.
* @param src source file path
* @param dst destination file path
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,listStatus,org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path),784,804,"/**
* Retrieves file status array for the given path.
* @param f Path to query
* @return Array of FileStatus objects or null if not found
*/","* liststatus returns the children of a directory 
   * after looking up the index files.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatusInternal,"org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)",1233,1242,"/**
* Resolves file status with optional dereferencing.
* @param f path to the file
* @param dereference whether to follow symbolic links
*/","* Public {@link FileStatus} methods delegate to this function, which in turn
   * either call the new {@link Stat} based implementation or the deprecated
   * methods based on platform support.
   * 
   * @param f Path to stat
   * @param dereference whether to dereference the final path component if a
   *          symlink
   * @return FileStatus of f
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)",32,35,"/**
* Initializes file status with provided file system and path.
* @param locatedFileStatus file system instance
* @param path file location
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",505,512,"/**
* Resolves file system and fetches block locations.
* @param fs FileStatus object
* @param start start offset
* @param len length of data to retrieve
* @return array of BlockLocation objects or null if not found
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path),407,426,"/**
* Resolves file status for a given path in the view FS.
* @param f input file path
* @return FileStatus object or null if not found
*/","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path),518,538,"/**
* Retrieves file statuses for a given path.
* @param f the input path
* @return array of FileStatus objects or null if not found
*/","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFs#getFileStatus(Path f)}
   *
   * Note: In ViewFs, by default the mount links are represented as symlinks.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,exists,"org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",189,198,"/**
* Checks if a file exists on the SFTP server.
* @param channel SFTP connection
* @param file local file path
* @return true if file found, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)",260,297,"/**
* Retrieves SFTP file status for a given file.
* @param channel SFTP channel
* @param sftpFile SFTP file entry
* @param parentPath Parent path of the file
* @return FileStatus object containing file metadata or null if not found
*/","* Convert the file information in LsEntry to a {@link FileStatus} object. *
   *
   * @param sftpFile
   * @param parentPath
   * @return file status
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,isFile,"org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",355,363,"/**
* Checks if a SFTP channel has read access to a specified file.
* @param channel SFTP channel object
* @param file local file path
* @return true if access is granted, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Creates a file status object based on the given path.
* @param path directory or file path to query
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Creates a FileStatus object based on the provided Path.
* @param path input file path
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,glob,org.apache.hadoop.fs.Globber:glob(),197,206,"/**
* Fetches file statuses for the globbed pattern.
* @return array of FileStatus objects
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,exists,"org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",391,398,"/**
* Checks if FTP client has write access to a file.
* @param client FTP client object
* @param file Path to the file
* @return True if accessible, False otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException on IO problems other than FileNotFoundException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",484,498,"/**
* Retrieves the status of one or more files on an FTP server.
* @param client FTPClient instance
* @param file Path to the file(s) being queried
* @return Array of FileStatus objects representing the file(s)
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isFile,"org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",617,625,"/**
* Checks if a local file exists on the remote server.
* @param client connected FTPClient instance
* @param file Path to the local file
* @return true if file is found, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",294,297,"/**
* Initializes CBZip2InputStream with input stream and read mode.
* @param in input stream to decompress
* @param readMode compression mode (see READ_MODE constants) 
*/","* Constructs a new CBZip2InputStream which decompresses bytes read from the
  * specified stream.
  *
  * <p>
  * Although BZip2 headers are marked with the magic <tt>""Bz""</tt> this
  * constructor expects the next byte in the stream to be the first one after
  * the magic. Thus callers have to skip the first two bytes. Otherwise this
  * constructor will throw an exception.
  * </p>
  * @param in in.
  * @param readMode READ_MODE.
  * @throws IOException
  *             if the stream content is malformed or an I/O error occurs.
  * @throws NullPointerException
  *             if <tt>in == null</tt>",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,numberOfBytesTillNextMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream),346,349,"/**
* Extracts and returns the CRC32 mask from a CBZip2 archive.
* @param in input stream to the archive
*/","* Returns the number of bytes between the current stream position
   * and the immediate next BZip2 block marker.
   *
   * @param in
   *             The InputStream
   *
   * @return long Number of bytes between current stream position and the
   * next BZip2 block start marker.
 * @throws IOException raised on errors performing I/O.
   *",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)",400,448,"/**
* Decompresses data into the given byte array, handling edge cases and lazy initialization.
*@param dest destination byte array
*@param offs offset within the array to start decompression
*@param len length of data to decompress
*@return number of bytes written to the array or an error value if failed
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",529,575,"/**
* Invokes M5 method on the server using RPC protocol.
* @param server RPC server instance
* @param connectionProtocolName name of connection protocol
* @param writableRequest protobuf request object
* @param receiveTime timestamp of request reception
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newMutableRollingAverages,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)",339,346,"/**
* Creates and registers a mutable rolling averages metric with the given name.
* @param name unique identifier for the metric
* @param valueName name of the values to be averaged
* @return MutableRollingAverages instance or null if registration fails
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,verifyChangedServiceState,org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState),884,922,"/**
* Handles HAServiceState changes, synchronizing with elector and updating local state.
* @param changedState new HAServiceState value
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)",496,551,"/**
* Handles create node result from Zookeeper.
* @param rc create result code
* @param path path to create node
* @param ctx context object
* @param name name of operation
*/",* interface implementation of Zookeeper callback for create,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)",556,613,"/**
* Handles StatNode result, performs actions based on the code and state.
* @param rc result code
* @param path file system path
* @param ctx context object
* @param stat StatNode object
*/",* interface implementation of Zookeeper callback for monitor (exists),,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,process,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent),1233,1247,"/**
* Processes watched event and updates ZooKeeper connection.
* @param event WatchedEvent object
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String),162,164,"/**
 * Returns an InetSocketAddress with default port (-1).
 * @param target hostname or IP address
 */","* Util method to build socket addr from either.
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @return socket addr.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Servers.java,parse,"org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)",50,62,"/**
* Parses specification string into a list of InetSocketAddress objects.
* @param specs comma-separated host:port strings or null to use localhost:defaultPort
* @param defaultPort default port number to use if not specified in specs
* @return List of InetSocketAddress objects or empty list if specs is null
*/","* Parses a space and/or comma separated sequence of server specifications
   * of the form <i>hostname</i> or <i>hostname:port</i>.  If
   * the specs string is null, defaults to localhost:defaultPort.
   *
   * @param specs   server specs (see description)
   * @param defaultPort the default port if not specified
   * @return a list of InetSocketAddress objects.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildDTServiceName,"org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)",338,345,"/**
* Constructs a function mask from the given URI and default port.
* @param uri input URI
* @param defPort default port number
* @return function mask string or null if invalid URI","* create the service name for a Delegation token
   * @param uri of the service
   * @param defPort is used if the uri lacks a port
   * @return the token service, or null if no authority
   * @see #buildTokenService(InetSocketAddress)",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,asXmlDocument,"org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3650,3685,"/**
* Generates a configuration document with masked values.
* @param propertyName property name to mask (null for all)
* @param redactor config redactor instance
* @return Document object containing the generated configuration
*/",* Return the XML DOM corresponding to this Configuration.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteVars,org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String),1150,1218,"/**
* Evaluates a string expression by substituting variables with their values.
* @param expr input string expression
* @return the evaluated string or null if failed
*/","* Attempts to repeatedly expand the value {@code expr} by replacing the
   * left-most substring of the form ""${var}"" in the following precedence order
   * <ol>
   *   <li>by the value of the environment variable ""var"" if defined</li>
   *   <li>by the value of the Java system property ""var"" if defined</li>
   *   <li>by the value of the configuration key ""var"" if defined</li>
   * </ol>
   *
   * If var is unbounded the current state of expansion ""prefix${var}suffix"" is
   * returned.
   * <p>
   * This function also detects self-referential substitutions, i.e.
   * <pre>
   *   {@code
   *   foo.bar = ${foo.bar}
   *   }
   * </pre>
   * If a cycle is detected then the original expr is returned. Loops
   * involving multiple substitutions are not detected.
   *
   * In order not to introduce breaking changes (as Oozie for example contains a method with the
   * same name and same signature) do not make this method public, use substituteCommonVariables
   * in this case.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,applyChanges,"org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)",140,197,"/**
* Updates reconfiguration based on provided parameters.
* @param out PrintWriter for output
* @param reconf Reconfigurable object
* @param req HttpServletRequest containing parameter values
*/",* Apply configuratio changes after admin has approved them.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)",1404,1406,"/**
* Calls overloaded variant with optional callback. 
* @param name key string
* @param value associated value string
* @param callback (optional) callback function reference
*/","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated or there is a deprecated name associated to it,
   * it sets the value to both names. Name will be trimmed before put into
   * configuration.
   * 
   * @param name property name.
   * @param value property value.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,set,"org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)",107,112,"/**
* Sets user preference with logging and parent class notification.
* @param name preference name
* @param value new value for the preference
* @param source optional source of the change (e.g. user or system)","* See {@link Configuration#set(String, String, String)}.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,"org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)",187,203,"/**
* Verifies directory permissions and grants them if requested.
* @param dir File object to verify
* @param tryGrantPermissions whether to attempt permission grant
* @return true if valid directory, false otherwise
*/","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir the file or directory to be deleted
   * @param tryGrantPermissions true if permissions should be modified to delete a file.
   * @return true on success false on failure.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumCores,org.apache.hadoop.util.SysInfoWindows:getNumCores(),179,182,"/**
* Calculates and returns the functional mask value.
* @return The computed functional mask integer value.",{@inheritDoc},,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,getStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",106,159,"/**
* Retrieves file system status for each mount point that the given path leads to.
* @param fileSystem ViewFileSystem instance
* @param path URI path to check
* @return Map of MountPoints to FsStatus objects or throws exception if not found
*/","* Get FsStatus for all ViewFsMountPoints matching path for the given
   * ViewFileSystem.
   *
   * Say ViewFileSystem has following mount points configured
   *  (1) hdfs://NN0_host:port/sales mounted on /dept/sales
   *  (2) hdfs://NN1_host:port/marketing mounted on /dept/marketing
   *  (3) hdfs://NN2_host:port/eng_usa mounted on /dept/eng/usa
   *  (4) hdfs://NN3_host:port/eng_asia mounted on /dept/eng/asia
   *
   * For the above config, here is a sample list of paths and their matching
   * mount points while getting FsStatus
   *
   *  Path                  Description                      Matching MountPoint
   *
   *  ""/""                   Root ViewFileSystem lists all    (1), (2), (3), (4)
   *                         mount points.
   *
   *  ""/dept""               Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/dept/sales""         Matches a mount point            (1)
   *
   *  ""/dept/sales/india""   Path is over a valid mount point (1)
   *                         and resolved down to
   *                         ""/dept/sales""
   *
   *  ""/dept/eng""           Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/erp""                Doesn't match or leads to or
   *                         over any valid mount points     None
   *
   *
   * @param fileSystem - ViewFileSystem on which mount point exists
   * @param path - URI for which FsStatus is requested
   * @return Map of ViewFsMountPoint and FsStatus
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",744,755,"/**
* Wraps the m2 call with default overwrite flag. 
* @param f output file path
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path),905,908,"/**
* Retrieves file status using mask 1.
* @param f Path object representing the file
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),1209,1220,"/**
* Resolves file status for the given path.
* @param f input path to resolve
* @return FileStatus object or null if not found
*/","* Return a FileStatus representing the given path. If the path refers
   * to a symlink return a FileStatus representing the link rather than
   * the object the link refers to.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1308,1313,"/**
* Processes file status with mask function.
* @param f input file path
* @return processed file status object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,wrapLocalFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",552,557,"/**
* Returns a FileStatus instance with modified functionality based on the input type.
* @param orig original FileStatus object
* @param qualified qualified path for the file status
* @return ViewFsLocatedFileStatus or ViewFsFileStatus instance
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",464,491,"/**
* Renames a file on SFTP server.
* @param channel active SFTP channel
* @param src source file path
* @param dst destination file path
* @return true if rename was successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   *
   * @param channel
   * @param src
   * @param dst
   * @return rename successful?
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",421,451,"/**
* Retrieves SFTP directory contents and converts to FileStatus array.
* @param client ChannelSftp object for SFTP connection
* @return FileStatus[] of files in the directory, or null if empty
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",314,347,"/**
* Creates or updates directory for file upload.
* @param client SFTP client
* @param file Path to file being uploaded
* @param permission File system permissions
* @return true if directory creation was successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path),2122,2126,"/**
* Returns an array of file statuses matching the given pattern.
* @param pathPattern globbing pattern to match files against
*/","* <p>Return all the files that match filePattern and are not checksum
     * files. Results are sorted by their names.
     * 
     * <p>
     * A filename pattern is composed of <i>regular</i> characters and
     * <i>special pattern matching</i> characters, which are:
     *
     * <dl>
     *  <dd>
     *   <dl>
     *    <dt> <tt> ? </tt>
     *    <dd> Matches any single character.
     *
     *    <dt> <tt> * </tt>
     *    <dd> Matches zero or more characters.
     *
     *    <dt> <tt> [<i>abc</i>] </tt>
     *    <dd> Matches a single character from character set
     *     <tt>{<i>a,b,c</i>}</tt>.
     *
     *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
     *    <dd> Matches a single character from the character range
     *     <tt>{<i>a...b</i>}</tt>. Note: character <tt><i>a</i></tt> must be
     *     lexicographically less than or equal to character <tt><i>b</i></tt>.
     *
     *    <dt> <tt> [^<i>a</i>] </tt>
     *    <dd> Matches a single char that is not from character set or range
     *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
     *     immediately to the right of the opening bracket.
     *
     *    <dt> <tt> \<i>c</i> </tt>
     *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
     *
     *    <dt> <tt> {ab,cd} </tt>
     *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
     *
     *    <dt> <tt> {ab,c{de,fh}} </tt>
     *    <dd> Matches a string from string set <tt>{<i>ab, cde, cfh</i>}</tt>
     *
     *   </dl>
     *  </dd>
     * </dl>
     *
     * @param pathPattern a glob specifying a path pattern
     *
     * @return an array of paths that match the path pattern
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,"org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2151,2155,"/**
* Retrieves file statuses matching a glob pattern and filter.
* @param pathPattern glob pattern to match
* @param filter optional filter for results
* @return array of FileStatus objects or null if empty
*/","* Return an array of FileStatus objects whose path names match pathPattern
     * and is accepted by the user-supplied path filter. Results are sorted by
     * their path names.
     * Return null if pathPattern has no glob and the path does not exist.
     * Return an empty array if pathPattern has a glob and no path matches it. 
     * 
     * @param pathPattern glob specifying the path pattern
     * @param filter user-supplied path filter
     *
     * @return an array of FileStatus objects
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path),2219,2226,"/**
* Returns an array of file statuses matching the specified glob pattern.
* @param pathPattern glob pattern to match files against
*/","* <p>Return all the files that match filePattern and are not checksum
   * files. Results are sorted by their names.
   *
   * <p>
   * A filename pattern is composed of <i>regular</i> characters and
   * <i>special pattern matching</i> characters, which are:
   *
   * <dl>
   *  <dd>
   *   <dl>
   *    <dt> <tt> ? </tt>
   *    <dd> Matches any single character.
   *
   *    <dt> <tt> * </tt>
   *    <dd> Matches zero or more characters.
   *
   *    <dt> <tt> [<i>abc</i>] </tt>
   *    <dd> Matches a single character from character set
   *     <tt>{<i>a,b,c</i>}</tt>.
   *
   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
   *    <dd> Matches a single character from the character range
   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be
   *     lexicographically less than or equal to character <tt><i>b</i></tt>.
   *
   *    <dt> <tt> [^<i>a</i>] </tt>
   *    <dd> Matches a single character that is not from character set or range
   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
   *     immediately to the right of the opening bracket.
   *
   *    <dt> <tt> \<i>c</i> </tt>
   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
   *
   *    <dt> <tt> {ab,cd} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
   *
   *    <dt> <tt> {ab,c{de,fh}} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>
   *
   *   </dl>
   *  </dd>
   * </dl>
   *
   * @param pathPattern a glob specifying a path pattern

   * @return an array of paths that match the path pattern
   * @throws IOException IO failure",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,"org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2241,2244,"/**
* Returns an array of file status objects matching the given pattern and filter.
* @param pathPattern globbed path pattern to match
* @param filter filtering criteria for matched files
*/","* Return an array of {@link FileStatus} objects whose path names match
   * {@code pathPattern} and is accepted by the user-supplied path filter.
   * Results are sorted by their path names.
   *
   * @param pathPattern a glob specifying the path pattern
   * @param filter a user-supplied path filter
   * @return null if {@code pathPattern} has no glob and the path does not exist
   *         an empty array if {@code pathPattern} has a glob and no path
   *         matches it else an array of {@link FileStatus} objects matching the
   *         pattern
   * @throws IOException if any I/O error occurs when fetching file status",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",670,705,"/**
* Renames a file on an FTP server.
* @param client active FTPClient session
* @param src source file path
* @param dst destination file path
* @return true if the rename was successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * 
   * @param client
   * @param src
   * @param dst
   * @return
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)",416,438,"/**
* Recursively deletes a file or directory on an FTP server.
* @param client FTPClient instance
* @param file Path to the file/directory to delete
* @param recursive true for recursive deletion, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",590,610,"/**
* Creates directory recursively on FTP server.
* @param client FTPClient instance
* @param file Path to create directory at
* @param permission FsPermission object for directory permissions
* @return true if directory creation was successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",358,407,"/**
* Initializes BZip2CompressionInputStream with input stream, start and end positions,
* and read mode. Strips header and updates reported byte count if necessary.
* @param in input stream
* @param start starting position
* @param end end position
* @param readMode read mode (BYBLOCK or BYREAD)
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset(),530,537,"/**
* Resets and initializes the input stream from a ZIP file.
* @throws IOException if an I/O error occurs
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream),351,353,"/**
 * Initializes a new CBZip2InputStream instance from the given input stream.
 * @param in input stream to read from
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(),365,376,"/**
* Returns a byte value from the input stream or throws an exception if closed.
* @throws IOException if stream is closed
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForField,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",40,92,"/**
* Creates a MutableMetric instance based on the provided field and Metric.
* @param field field to create metric for
* @param annotation metric configuration
* @param registry metrics registry to register the metric with
* @return created MutableMetric or null if not supported
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,reportServiceStatus,org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus),999,1002,"/**
* Updates mask based on HA service status.
* @param status HA service status object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeIP2HostName,org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String),731,738,"/**
* Validates and extracts mask from IP:port string.
* @param ipPort input string in format ""IP:port""
* @return formatted mask string or original input if invalid
*/","* Attempt to normalize the given string to ""host:port""
   * if it like ""ip:port"".
   *
   * @param ipPort maybe lik ip:port or host:port.
   * @return host:port",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getTokenServiceAddr,org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token),447,449,"/**
* Generates an InetSocketAddress based on token data.
* @param token input Token object
*/","* Decode the given token's service field into an InetAddress
   * @param token from which to obtain the service
   * @return InetAddress for the service",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI),495,497,"/**
* Resolves URI to Text representation using NetUtils.
* @param uri input URI object
*/","* Construct the service key for a token
   * @param uri of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,init,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration),120,172,"/**
* Initializes GangliaSink with configuration from SubsetConfiguration.
* @param conf SubsetConfiguration object containing ganglia metrics settings
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.FileSystem:getCanonicalServiceName(),453,460,"/**
* Calculates function mask based on internal conditions.
* @return Function mask string or null if not applicable.","* Get a canonical service name for this FileSystem.
   * The token cache is the only user of the canonical service name,
   * and uses it to lookup this FileSystem's service tokens.
   * If the file system provides a token of its own then it must have a
   * canonical name, otherwise the canonical name can be null.
   *
   * Default implementation: If the FileSystem has child file systems
   * (such as an embedded file system) then it is assumed that the FS has no
   * tokens of its own and hence returns a null name; otherwise a service
   * name is built using Uri and port.
   *
   * @return a service string that uniquely identifies this file system, null
   *         if the filesystem does not implement tokens
   * @see SecurityUtil#buildDTServiceName(URI, int)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName(),1243,1245,"/**
 * Generates a function mask by combining results from m1 and m2 functions.
 */","* Get a canonical name for this file system.
   * @return a URI string that uniquely identifies this file system",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteCommonVariables,org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String),1115,1117,"/**
* Applies the mask function to the given expression.
* @param expr input string to be masked
*/","* Provides a public wrapper over substituteVars in order to avoid compatibility issues.
   * See HADOOP-18021 for further details.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,org.apache.hadoop.conf.Configuration:get(java.lang.String),1263,1270,"/**
* Retrieves and processes user data based on the provided name.
* @param name User identifier
*/","* Get the value of the <code>name</code> property, <code>null</code> if
   * no such property exists. If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null.
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned.
   *
   * As a side effect get loads the properties from the sources if called for
   * the first time as a lazy init.
   * 
   * @param name the property name, will be trimmed before get value.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,"org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)",1524,1531,"/**
* Retrieves a value from the deprecation context or returns the default value.
* @param name unique identifier
* @param defaultValue fallback value to return if not found
* @return retrieved value or default value
*/","* Get the value of the <code>name</code>. If the key is deprecated,
   * it returns the value of the first key which replaces the deprecated key
   * and is not null.
   * If no such property exists,
   * then <code>defaultValue</code> is returned.
   * 
   * @param name property name, will be trimmed before get value.
   * @param defaultValue default value.
   * @return property value, or <code>defaultValue</code> if the property 
   *         doesn't exist.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doPost,"org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",214,236,"/**
* Handles FUNC_MASK POST request and renders response.
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)",169,175,"/**
* Performs a series of operations based on the provided key and value.
* Validates keys against mandatory and optional sets, then processes with options.
* @param key unique identifier for operation
* @param value associated data for processing
*/",* Set optional Builder parameter.,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)",256,261,"/**
* Validates and processes key-value pair.
* @param key unique identifier for the key
* @param value associated value for the key
*/","* Set mandatory option to the Builder.
   *
   * If the option is not supported or unavailable on the {@link FileSystem},
   * the client should expect {@link #build()} throws IllegalArgumentException.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)",311,313,"/**
* Sets the default file system name from the given URI.
* @param conf configuration object
* @param uri URI containing the file system information
*/","* Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)",55,59,"/**
* Configures Hadoop job with viewfs link and source data.
* @param conf Hadoop configuration
* @param mountTableName table name for mounting viewfs
* @param src source URI
* @param target target URI
*/","* Add a link to the config for the specified mount table
   * @param conf - add the link to this conf
   * @param mountTableName mountTable.
   * @param src - the src path name
   * @param target - the target URI link",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",79,83,"/**
* Configures M3 with a viewfs link merge path.
* @param conf Configuration object
* @param mountTableName table name to be mounted
* @param target URI to be merged
*/","* Add a LinkMergeSlash to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target target.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",102,106,"/**
* Configures viewfs link fallback for given mount table name.
* @param conf Hadoop configuration
* @param mountTableName unique mount table identifier
* @param target URI object
*/","* Add a LinkFallback to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",125,129,"/**
* Configures viewfs link merge for the given table and target URIs.
* @param conf Configuration object
* @param mountTableName name of the mounted table
* @param targets array of URI targets
*/","* Add a LinkMerge to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",150,156,"/**
* Configures viewfs link with specified settings and target directories.
* @param conf Configuration object
* @param mountTableName table name for mounting
* @param src source directory path
* @param settings configuration settings string
* @param targets target directory paths
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkRegex,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",191,202,"/**
* Configures the function mask for a given mount table.
* @param conf Configuration object
* @param mountTableName unique mount table identifier
* @param srcRegex source regex pattern
* @param targetStr target string value
* @param interceptorSettings optional interceptor settings
*/","* Add a LinkRegex to the config for the specified mount table.
   * @param conf - get mountable config from this conf
   * @param mountTableName - the mountable name of the regex config item
   * @param srcRegex - the src path regex expression that applies to this config
   * @param targetStr - the string of target path
   * @param interceptorSettings - the serialized interceptor string to be
   *                            applied while resolving the mapping",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",220,228,"/**
* Configures the viewfs home directory for a mount table.
* @param conf Configuration object
* @param mountTableName name of the mount table
* @param homedir path to the viewfs home directory
*/","* Add config variable for homedir the specified mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash
   * @param mountTableName - the mount table.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,setUMask,"org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)",400,402,"/**
* Sets M3 configuration value from underlying file system permission mask.
* @param conf Configuration object
* @param umask FS permission mask to convert
*/","* Set the user file creation mask (umask)
   * @param conf configuration.
   * @param umask umask.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,setCodecClasses,"org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)",156,169,"/**
* Configures compression codecs for Hadoop IO.
* @param conf Hadoop Configuration object
* @param classes List of Class objects (codec implementations)
*/","* Sets a list of codec classes in the configuration. In addition to any
   * classes specified using this method, {@link CompressionCodec} classes on
   * the classpath are discovered using a Java ServiceLoader.
   * @param conf the configuration to modify
   * @param classes the list of classes to set",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,setDefaultCompressionType,"org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)",262,265,"/**
* Sets compression type in configuration.
* @param job Configuration object
* @param val Compression type value
*/","* Set the default compression type for sequence files.
   * @param job the configuration to modify
   * @param val the new compression type (none, block, record)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),107,119,"/**
 * Retrieves configuration for proxy user filtering.
 * @param filterConfig FilterConfig instance
 * @return Configuration object or null if not configured
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),158,174,"/**
* Initializes configuration with filter-specific properties.
* @param filterConfig FilterConfig object
* @return Configuration instance
*/","* Returns the proxyuser configuration. All returned properties must start
   * with <code>proxyuser.</code>'
   * <p>
   * Subclasses may override this method if the proxyuser configuration is 
   * read from other place than the filter init parameters.
   *
   * @param filterConfig filter configuration object
   * @return the proxyuser configuration properties.
   * @throws ServletException thrown if the configuration could not be created.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,prepareConf,org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String),175,191,"/**
* Creates configuration for mask functionality based on the given provider.
* @param providerName name of the data provider
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,init,org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[]),78,126,"/**
* Parses command line arguments and executes corresponding actions.
* @param args array of command line arguments
* @return 0 on success, non-zero on failure
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop credential create alias [-provider providerPath]
   * % hadoop credential list [-provider providerPath]
   * % hadoop credential check alias [-provider providerPath]
   * % hadoop credential delete alias [-provider providerPath] [-f]
   * </pre>
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setAuthenticationMethod,"org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)",741,748,"/**
* Sets the authentication method in configuration.
* @param authenticationMethod authentication method to use
* @param conf Hadoop configuration object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setInt,"org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)",1582,1584,"/**
* Sets a mask value with the given name and integer value.
* @param name unique identifier for the mask
* @param value integer value to be assigned to the mask
*/","* Set the value of the <code>name</code> property to an <code>int</code>.
   * 
   * @param name property name.
   * @param value <code>int</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setLong,"org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)",1655,1657,"/**
* Sets the mask value for the specified field.
* @param name field identifier
* @param value numeric value to be masked
*/","* Set the value of the <code>name</code> property to a <code>long</code>.
   * 
   * @param name property name.
   * @param value <code>long</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setFloat,"org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)",1684,1686,"/**
* Applies a mask to the given float value and updates it in the model.
* @param name unique identifier for the masked value
* @param value float value to be masked
*/","* Set the value of the <code>name</code> property to a <code>float</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDouble,"org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)",1713,1715,"/**
* Applies a mask to the specified value and updates the internal state.
* @param name unique identifier for the masked value
* @param value numerical value to be masked
*/","* Set the value of the <code>name</code> property to a <code>double</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBoolean,"org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)",1750,1752,"/**
* Masks a field with a specified boolean value.
* @param name field name
* @param value true to mask, false otherwise
*/","* Set the value of the <code>name</code> property to a <code>boolean</code>.
   * 
   * @param name property name.
   * @param value <code>boolean</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setTimeDuration,"org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1868,1870,"/**
* Adds a time duration mask with specified name and value in given unit.
* @param name name of the mask
* @param value time duration value to add
* @param unit TimeUnit for the added duration (e.g. SECONDS, MILLISECONDS)","* Set the value of <code>name</code> to the given time duration. This
   * is equivalent to <code>set(&lt;name&gt;, value + &lt;time suffix&gt;)</code>.
   * @param name Property name
   * @param value Time duration
   * @param unit Unit of time",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStorageSize,"org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2039,2041,"/**
* Calculates and stores a masked value in storage.
* @param name identifier for stored value
* @param value numerical value to be processed
* @param unit associated measurement unit
*/","* Sets Storage Size for the specified key.
   *
   * @param name - Key to set.
   * @param value - The numeric value to set.
   * @param unit - Storage Unit to be used.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setPattern,"org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)",2089,2092,"/**
* Applies mask to string using regular expression.
* @param name input string
* @param pattern compiled regex pattern
*/","* Set the given property to <code>Pattern</code>.
   * If the pattern is passed as null, sets the empty pattern which results in
   * further calls to getPattern(...) returning the default value.
   *
   * @param name property name
   * @param pattern new value",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStrings,"org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])",2405,2407,"/**
* Applies mask to input string.
* @param name input string to be masked
* @param values variable number of strings to apply as a mask
*/","* Set the array of string values for the <code>name</code> property as 
   * as comma delimited values.  
   * 
   * @param name property name.
   * @param values The values",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setSocketAddr,"org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)",2578,2580,"/**
* Masks an IP address with a given name.
* @param name host name to mask
* @param addr IP address to mask
*/","* Set the socket address for the <code>name</code> property as
   * a <code>host:port</code>.
   * @param name property name.
   * @param addr inetSocketAddress addr.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setClass,"org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)",2811,2815,"/**
* Configures mask for a given class.
* @param name unique identifier
* @param theClass Class to configure
* @param xface interface to verify against
*/","* Set the value of the <code>name</code> property to the name of a 
   * <code>theClass</code> implementing the given interface <code>xface</code>.
   * 
   * An exception is thrown if <code>theClass</code> does not implement the 
   * interface <code>xface</code>. 
   * 
   * @param name property name.
   * @param theClass property value.
   * @param xface the interface implemented by the named class.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,readFields,org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput),3949,3962,"/**
* Reads and processes FUNC_MASK data from input stream.
* @param in DataInput stream containing FUNC_MASK data
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File),169,171,"/**
* Recursively checks if a directory is empty.
* @param dir directory to check
* @return true if the directory is empty, false otherwise
*/","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir dir.
   * @return fully delete status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,"org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)",282,316,"/**
* Tries to delete all contents of a directory and its subdirectories.
* @param dir the directory to delete
* @param tryGrantPermissions whether to attempt to grant permissions before deleting
* @return true if all deletions were successful, false otherwise
*/","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @param tryGrantPermissions if 'true', try grant +rwx permissions to this
   * and all the underlying directories before trying to delete their contents.
   * @return fully delete contents status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData),129,157,"/**
* Processes PathData to update file system status.
* @param item PathData object containing file system data
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)",392,397,"/**
* Opens file stream with specified buffer size.
* @param f Path to the file
* @param bufferSize size of read/write buffer
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",399,409,"/**
* Opens a file stream for reading, buffering data with specified size.
* @param fd path handle to local file system
* @param bufferSize buffer capacity in bytes
* @return buffered input stream or throws IOException if error occurs
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,append,"org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",535,545,"/**
* Opens an output stream for appending to the specified file.
* @param f Path of the file
* @param bufferSize buffer size for optimized writes
* @param progress Progressable object for monitoring write progress
* @return FSDataOutputStream instance or throws IOException on error
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,truncate,"org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)",675,698,"/**
* Truncates a file to a specified length.
* @param f Path to the file
* @param newLength desired file size in bytes
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,listStatus,org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path),729,766,"/**
* Retrieves file status for a given path.
* @param f the input path
*/","* {@inheritDoc}
   *
   * (<b>Note</b>: Returned list is not sorted in any given order,
   * due to reliance on Java's {@link File#list()} API.)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileLinkStatusInternal,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path),1248,1285,"/**
* Computes and returns the file status for a given path.
* @param f path to compute file status for
*/","* Deprecated. Remains for legacy support. Should be removed when {@link Stat}
   * gains support for Windows and other operating systems.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,fixFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",532,550,"/**
* Updates file status with mask and qualifies path.
* @param orig original FileStatus object
* @param qualified qualified Path object
* @return updated FileStatus object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)",370,414,"/**
* Deletes an SFTP file or directory.
* @param channel SFTP connection
* @param file Path to delete
* @param recursive whether to delete recursively
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream),354,356,"/**
* Initializes BZip2 compression input stream from given InputStream.
* @param in underlying InputStream
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",200,211,"/**
* Creates a compressed input stream for seeking and decompression.
* @param seekableIn input stream to compress
* @param decompressor decompressor instance
* @param start compression start position
* @param end compression end position
* @param readMode read mode
* @return BZip2CompressionInputStream object
*/","* Creates CompressionInputStream to be used to read off uncompressed data
   * in one of the two reading modes. i.e. Continuous or Blocked reading modes
   *
   * @param seekableIn The InputStream
   * @param start The start offset into the compressed stream
   * @param end The end offset into the compressed stream
   * @param readMode Controls whether progress is reported continuously or
   *                 only at block boundaries.
   *
   * @return CompressionInputStream for BZip2 aligned at block boundaries",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)",483,522,"/**
* Reads a specified length from the compressed data.
* @param b compressed byte array
* @param off starting offset in bytes
* @param len number of bytes to read
* @return end-of-block marker if reached, otherwise input value
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)",133,159,"/**
* Processes and sets metric fields on the source object.
* @param source Object to process
* @param field Field containing metric annotations
*/","* Change the declared field {@code field} in {@code source} Object to
   * {@link MutableMetric}",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,init,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration),57,84,"/**
* Processes SubsetConfiguration and updates tags for each context.
* @param conf SubsetConfiguration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName(),262,265,"/**
* Calls M1 operation on underlying file system implementation. 
* @return result of M1 operation as string or null if failed. 
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getCanonicalServiceName,org.apache.hadoop.fs.FilterFs:getCanonicalServiceName(),312,315,"/**
* Calls underlying file system's m1() method.
* @return result from myFs.m1()
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getConf,"org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)",120,123,"/**
* Retrieves function mask value from configuration.
* @param conf Configuration object
* @param t Storage type enumeration value
* @param name Function name string
*/","* Get the configured values for different StorageType.
   * @param conf - absolute or fully qualified path
   * @param t - the StorageType
   * @param name - the sub-name of key
   * @return the file system of the path",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getTransferMode,org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration),188,209,"/**
* Resolves FTP transfer mode from configuration.
* @param conf Configuration object
* @return int representing transfer mode (e.g. BLOCK, STREAM, COMPRESSED)
*/","* Set FTP's transfer mode based on configuration. Valid values are
   * STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.
   * <p>
   * Defaults to BLOCK_TRANSFER_MODE.
   *
   * @param conf
   * @return",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setDataConnectionMode,"org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",222,240,"/**
* Configures FTP client data connection mode based on configuration.
* @param client FTPClient instance
* @param conf Configuration object with data connection mode setting
*/","* Set the FTPClient's data connection mode based on configuration. Valid
   * values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,
   * PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.
   * <p>
   * Defaults to ACTIVE_LOCAL_DATA_CONNECTION_MODE.
   *
   * @param client
   * @param conf
   * @throws IOException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,"org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)",245,249,"/**
* Builds function mask by concatenating mount table name and configuration constant.
* @param conf Mule configuration object
* @param mountTableName name of the mount table
*/","* Get the value of the home dir conf value for specified mount table
   * @param conf - from this conf
   * @param mountTableName - the mount table
   * @return home dir value, null if variable is not in conf",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getUMask,org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration),325,349,"/**
* Resolves user permissions from configuration, or uses default umask if not specified.
* @param conf Configuration object to parse umask from
* @return FsPermission object representing the resolved umask
*/","* Get the user file creation mask (umask)
   * 
   * {@code UMASK_LABEL} config param has umask value that is either symbolic 
   * or octal.
   * 
   * Symbolic umask is applied relative to file mode creation mask; 
   * the permission op characters '+' clears the corresponding bit in the mask, 
   * '-' sets bits in the mask.
   * 
   * Octal umask, the specified bits are set in the file mode creation mask.
   *
   * @param conf configuration.
   * @return FsPermission UMask.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",791,796,"/**
* Initializes DiskBlockFactory with configuration and sets up local directory allocator.
* @param keyToBufferDir key to buffer directory in configuration
* @param conf Hadoop Configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClasses,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration),111,147,"/**
* Retrieves configurable CompressionCodecs and merges with provider codecs.
* @param conf Hadoop configuration object
* @return List of available CompressionCodec classes
*/","* Get the list of codecs discovered via a Java ServiceLoader, or
   * listed in the configuration. Codecs specified in configuration come
   * later in the returned list, and are considered to override those
   * from the ServiceLoader.
   * @param conf the configuration to look in
   * @return a list of the {@link CompressionCodec} classes",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDefaultCompressionType,org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration),251,255,"/**
* Returns the compression type based on the provided configuration.
* @param job Configuration object containing compression type
* @return CompressionType enum value or RECORD if not specified
*/","* Get the compression type for the reduce outputs
   * @param job the job config to look in
   * @return the kind of compression to use",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,setConf,org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration),135,142,"/**
* Configures Hadoop SOCKS server settings from the given Configuration object.
* @param conf Hadoop configuration instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isSingleSwitchByScriptPolicy,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy(),135,138,"/**
* Checks if net topology script file name is not set in configuration.
* @return true if not set, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createWebAppContext,"org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)",834,860,"/**
* Creates a WebAppContext instance for the given builder and app directory.
* @param b Builder object
* @param adminsAcl AccessControlList instance
* @param appDir Application directory path
* @return Configured WebAppContext instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,stringifySecurityProperty,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String),314,333,"/**
* Generates a formatted string for the given property, 
* incorporating its value and configuration setting if applicable.
* @param property the property to process
* @return a string representation of the property's value and configuration
*/","* Turn a security property into a nicely formatted set of <i>name=value</i>
   * strings, allowing for either the property or the configuration not to be
   * set.
   *
   * @param property the property to stringify
   * @return the stringified property",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateHadoopTokenFiles,org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration),521,553,"/**
* Resolves Hadoop token file locations and processes tokens.
* @param conf Hadoop configuration object
*/","* Validate that hadoop.token.files (if specified) exist and are valid.
   * @throws ClassNotFoundException
   * @throws SecurityException
   * @throws NoSuchMethodException
   * @throws KerberosDiagsFailure",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore(),314,339,"/**
* Initializes and loads key store with password.
* @throws IOException on keystore creation or loading errors
*/","* Open up and initialize the keyStore.
   *
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword(),341,346,"/**
* Checks if function mask is enabled by comparing environment and config password values.
* @throws IOException if an I/O error occurs
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getLocalHostName,org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration),255,272,"/**
* Retrieves the DNS configuration mask from the given Configuration object.
* @param conf Configuration object (may be null)
* @return String representing the DNS configuration or an empty string if not found
*/","* Retrieve the name of the current host. Multihomed hosts may restrict the
   * hostname lookup to a specific interface and nameserver with {@link
   * org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_INTERFACE_KEY}
   * and {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_NAMESERVER_KEY}
   *
   * @param conf Configuration object. May be null.
   * @return
   * @throws UnknownHostException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getClientPrincipal,"org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)",404,413,"/**
* Retrieves the user associated with a given protocol and configuration.
* @param protocol Class of the protocol to fetch user from
* @param conf Configuration object used for fetching user
* @return The fetched user ID or null if not found
*/","* Look up the client principal for a given protocol. It searches all known
   * SecurityInfo providers.
   * @param protocol the protocol class to get the information for
   * @param conf configuration object
   * @return client principal or null if it has no client principal defined.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword(),308,313,"/**
* Checks if keystore password is set.
* @return true if password is set, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getMetricsTimeUnit,org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration),189,204,"/**
* Retrieves the TimeUnit for RPC metrics based on configuration.
* @param conf Hadoop Configuration object
* @return TimeUnit instance or default if invalid config value
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,validateSslConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration),196,221,"/**
* Validates that all required SSL configuration parameters are set.
* @param config Configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String),1320,1328,"/**
* Returns a string representation of the function mask, or null if not found.
* @param name input parameter for m1() method
*/","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>null</code> if no such property exists. 
   * If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned. 
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setIfUnset,"org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)",1499,1503,"/**
* Updates or inserts a key-value pair in the underlying data structure.
* @param name unique identifier for the key
* @param value associated value for the key
*/","* Sets a property if it is currently unset.
   * @param name the property name
   * @param value the new value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1906,1914,"/**
* Converts and returns a duration value from string representation.
* @param name input string to parse
* @param defaultValue default value if parsing fails
* @param defaultUnit unit of default value
* @param returnUnit unit to return the result in
* @return parsed duration value or default value","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d). If no unit is
   * provided, the default unit is applied.
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param defaultUnit Default time unit if no valid suffix is provided.
   * @param returnUnit The unit used for the returned value.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1916,1924,"/**
* Resolves a time unit value from the given name or returns the default value.
* @param name unique identifier for the time unit
* @param defaultValue fallback value if not found
* @param defaultUnit default time unit (e.g. TimeUnit.SECONDS)
* @param returnUnit desired output time unit (e.g. TimeUnit.MILLISECONDS)
* @return long representation of the resolved time unit in the specified unit",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)",1988,2005,"/**
* Calculates a double value based on the given name, default value, and target unit.
* @param name unique identifier
* @param defaultValue fallback value if calculation is invalid
* @param targetUnit desired storage unit for result
* @return calculated double value or default value if failed
*/","* Gets the Storage Size from the config, or returns the defaultValue. The
   * unit of return value is specified in target unit.
   *
   * @param name - Key Name
   * @param defaultValue - Default Value -- e.g. 100MB
   * @param targetUnit - The units that we want result to be in.
   * @return double -- formatted in target Units",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2017,2030,"/**
* Converts value from string representation to the specified unit.
* @param name string representation of the value
* @param defaultValue default value if conversion fails
* @param targetUnit desired storage unit for result
* @return converted double value or defaultValue if invalid input
*/","* Gets storage size from a config file.
   *
   * @param name - Key to read.
   * @param defaultValue - The default value to return in case the key is
   * not present.
   * @param targetUnit - The Storage unit that should be used
   * for the return value.
   * @return - double value in the Storage Unit specified.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPattern,"org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)",2067,2079,"/**
* Returns a pattern object from the given string, using the default if invalid or null.
* @param name unique identifier
* @param defaultValue fallback pattern to use if input is invalid
*/","* Get the value of the <code>name</code> property as a <code>Pattern</code>.
   * If no such property is specified, or if the specified value is not a valid
   * <code>Pattern</code>, then <code>DefaultValue</code> is returned.
   * Note that the returned value is NOT trimmed by this method.
   *
   * @param name property name
   * @param defaultValue default value
   * @return property value as a compiled Pattern, or defaultValue",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStringCollection,org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String),2310,2313,"/**
* Extracts and returns collection of strings from input using m1 and m2 methods.
* @param name input string to process
*/","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s.  
   * If no such property is specified then empty collection is returned.
   * <p>
   * This is an optimized version of {@link #getStrings(String)}
   * 
   * @param name property name.
   * @return property value as a collection of <code>String</code>s.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,org.apache.hadoop.conf.Configuration:getStrings(java.lang.String),2324,2327,"/**
* Calls M1 to process input string and then passes result to M2.","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then <code>null</code> is returned.
   * 
   * @param name property name.
   * @return property value as an array of <code>String</code>s, 
   *         or <code>null</code>.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,"org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])",2339,2346,"/**
* Returns default values or parsed string representations from a backing system.
* @param name unique identifier
* @param defaultValue array of fallback values to use when parsing fails
* @return String[] containing either the parsed value(s) or the default values
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStringCollection,org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String),2356,2363,"/**
* Retrieves a collection of strings from the underlying data source.
* @param name unique identifier for lookup
* @return non-empty collection if found, otherwise an empty collection
*/","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s, trimmed of the leading and trailing whitespace.  
   * If no such property is specified then empty <code>Collection</code> is returned.
   *
   * @param name property name.
   * @return property value as a collection of <code>String</code>s, or empty <code>Collection</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String),2374,2377,"/**
* Calls m1 to retrieve string representation and then passes it to m2 from StringUtils.
* @param name input string
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then an empty array is returned.
   * 
   * @param name property name.
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or empty array.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,"org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])",2389,2396,"/**
* Returns array of strings from default values or parsed string.
* @param name input name for parsing
* @param defaultValue default values to return if parsing fails
* @return Array of strings, either parsed result or default values
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropsWithPrefix,org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String),3032,3043,"/**
* Retrieves configuration map from properties based on prefix.
* @param confPrefix prefix to filter properties
*/","* Constructs a mapping of configuration and includes all properties that
   * start with the specified configuration prefix.  Property names in the
   * mapping are trimmed to remove the configuration prefix.
   *
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties with prefix stripped",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendJSONProperty,"org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3861,3881,"/**
* Adds a configuration parameter to the JSON output.
* @param name parameter name
* @param redactor ConfigRedactor instance for data processing
*/","* Write property and its attributes as json format to given
   * {@link JsonGenerator}.
   *
   * @param jsonGen json writer
   * @param config configuration
   * @param name property name
   * @throws IOException raised on errors performing I/O.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",39,65,"/**
* Computes changes in configuration properties between two configurations.
* @param newConf new configuration
* @param oldConf old configuration
* @return Collection of property changes
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,reconfigureProperty,"org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)",223,241,"/**
* Updates a configuration property with a new value.
* @param property unique property identifier
* @param newVal new value for the property (may be null)
* @throws ReconfigurationException if update fails or is not allowed
*/","* {@inheritDoc}
   *
   * This method makes the change to this objects {@link Configuration}
   * and calls reconfigurePropertyImpl to update internal data structures.
   * This method cannot be overridden, subclasses should instead override
   * reconfigurePropertyImpl.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String),46,51,"/**
* Calls superclass's m1 and logs a message with redacted value.
* @param name input string to pass to superclass
* @return result from superclass's m1 method
*/",* See {@link Configuration#get(String)}.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,create,"org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)",83,90,"/**
* Retrieves a NodeFencer instance from configuration using the specified key.
* @param conf configuration object
* @param confKey unique configuration key
* @return NodeFencer instance or null if not configured
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getDefaultMountTableName,org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration),260,263,"/**
* Retrieves the default mount table name from configuration.
* @param conf configuration object
*/","* Get the name of the default mount table to use. If
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE_NAME_KEY} is specified,
   * it's value is returned. Otherwise,
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE} is returned.
   *
   * @param conf Configuration to use.
   * @return the name of the default mount table to use.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getCodecClassName,"org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)",256,285,"/**
* Returns the erasure code function mask based on the provided codec.
* @param conf Configuration object
* @param codec Name of the erasure code to use (e.g. RS, XOR, etc.)
* @return Function mask as a string or null if not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,isNativeBzip2Loaded,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration),47,70,"/**
* Determines if native Bzip2 library is loaded and ready for use.
* @param conf Configuration object containing library settings
* @return true if native library loaded, false otherwise
*/","* Check if native-bzip2 code is loaded &amp; initialized correctly and
   * can be loaded for this job.
   * 
   * @param conf configuration
   * @return <code>true</code> if native-bzip2 is loaded &amp; initialized
   *         and can be loaded for this job, else <code>false</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getDefaultSocketFactory,org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration),121,130,"/**
* Creates a SocketFactory instance based on configuration settings.
* @param conf Hadoop Configuration object
*/","* Get the default socket factory as specified by the configuration
   * parameter <tt>hadoop.rpc.socket.factory.default</tt>
   * 
   * @param conf the configuration
   * @return the default socket factory as specified in the configuration or
   *         the JVM default socket factory if the configuration does not
   *         contain a default socket factory property.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,load,org.apache.hadoop.net.TableMapping$RawTableMapping:load(),93,125,"/**
* Loads network topology mapping from file and returns as a map.
* @return Map of node IDs to names or null on failure
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,getUsernameFromConf,org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration),133,146,"/**
* Extracts and returns the user ID from configuration.
* @param conf Configuration object
*/",* Retrieve the static username from the configuration.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setEnabledProtocols,org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory),665,696,"/**
* Resets SSL exclusions based on configured protocols.
* @param sslContextFactory SslContextFactory instance to modify
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,parseStaticMapping,org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration),164,191,"/**
* Initializes and populates the static user-to-groups map from configuration.
* @param conf Hadoop configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printConfOpt,org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String),907,909,"/**
* Sets an option to its unset value.
* @param option name of the option
*/","* Print a configuration option, or {@link #UNSET} if unset.
   *
   * @param option option to print",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,<init>,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration),868,873,"/**
* Initializes TruststoreKeystore object from Hadoop configuration.
* @param conf Hadoop Configuration object
*/","* Configuration for the ZooKeeper connection when SSL/TLS is enabled.
     * When a value is not configured, ensure that empty string is set instead of null.
     *
     * @param conf ZooKeeper Client configuration",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDirContext,org.apache.hadoop.security.LdapGroupsMapping:getDirContext(),653,706,"/**
* Creates an LDAP context with specified configuration.
* @return DirContext object or null if initialization fails
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForUserCreds,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean),882,899,"/**
* Performs Kerberos ticket management.
* @param force whether to refresh the ticket regardless of conditions
*/","* Spawn a thread to do periodic renewals of kerberos credentials. NEVER
   * directly call this method. This method should only be used for ticket cache
   * based kerberos credentials.
   *
   * @param force - used by tests to forcibly spawn thread",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,getHostnameVerifier,org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration),206,210,"/**
* Creates an instance of HostnameVerifier based on configuration.
* @param conf Configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getAuthenticationMethod,org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration),730,739,"/**
* Retrieves authentication method from configuration.
* @param conf Configuration object
* @return AuthenticationMethod enum or null if invalid
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getCodecClasses,"org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",105,140,"/**
* Retrieves configured crypto codec classes for the given cipher suite.
* @param conf Hadoop configuration object
* @param cipherSuite Cipher suite identifier
* @return List of CryptoCodec class types or null if not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),83,102,"/**
* Initializes security configuration and secure random instance.
* @param conf Configuration object with security settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,setConf,org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration),83,90,"/**
* Initializes configuration and secure random device path.
* @param conf Configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration),403,417,"/**
* Initializes KeyProvider with Hadoop configuration.
* @param conf Hadoop Configuration object
*/","* Constructor.
   * 
   * @param conf configuration for the provider",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",143,146,"/**
* Initializes the builder with context and configuration.
* @param context application context
* @param conf overall job configuration
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAcls,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration),97,109,"/**
* Retrieves a list of ACLs from ZooKeeper configuration.
* @param conf Hadoop configuration object
* @return List of ACLs or empty list if not found
*/","* Utility method to fetch the ZK ACLs from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @return acl list.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getHashType,org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration),64,68,"/**
* Computes a function mask based on configuration.
* @param conf Configuration object
*/","* This utility method converts the name of the configured
   * hash type to a symbolic constant.
   * @param conf configuration
   * @return one of the predefined constants",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getEnumSet,"org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)",1803,1809,"/**
* Retrieves an EnumSet from configuration data.
* @param key configuration key
* @param enumClass Enum class to retrieve values for
* @param ignoreUnknown whether to ignore unknown values
* @return EnumSet of values or null if not found
*/","* Build an enumset from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param key key to look for
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values raise an exception?
   * @return a mutable set of the identified enum values declared in the configuration
   * @param <E> enumeration type
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   *           or there are two entries in the enum which differ only by case.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRange,"org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)",2296,2298,"/**
* Creates IntegerRange instance based on input string.
* @param name input string
* @param defaultValue default value to use if parsing fails
* @return IntegerRange object or null if parsing failed
*/","* Parse the given attribute as a set of integer ranges.
   * @param name the attribute name
   * @param defaultValue the default value if it is not set
   * @return a new set of ranges from the configured value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,<init>,org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration),44,55,"/**
* Initializes the ConfigRedactor with a Configuration object.
* @param conf Hadoop configuration
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,"org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)",56,62,"/**
* Overridden method to fetch a string value with logging.
* @param name the name of the field
* @param defaultValue default value if not found
* @return the fetched string value or default value
*/","* See {@link Configuration#get(String, String)}.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getParentZnode,org.apache.hadoop.ha.ZKFailoverController:getParentZnode(),384,391,"/**
* Constructs a ZooKeeper node path with the FUNC_MASK suffix.
* @return the constructed path as a string
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)",182,185,"/**
* Wraps a single boolean value in a boxed primitive. 
* @param key unique identifier for the value
* @param value single boolean value to box
*/","* Set optional boolean parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)",202,205,"/**
* Computes and returns a mask value using provided key and value.
* @param key arbitrary string identifier
* @param value numeric value to be masked
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)",232,235,"/**
 * Applies bitwise mask operation to the given double value.
 * @param key unique identifier for the mask operation
 * @param value double value to be masked
 * @return result of the mask operation (type B not specified) 
 */","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)",268,271,"/**
* Wraps primitive boolean value in a boxed type.","* Set mandatory boolean option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)",273,276,"/**
* Computes and returns the functional mask value based on input key and value.
* @param key unique identifier or key string
* @param value numerical value to be masked
* @return computed B-value
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)",283,286,"/**
* Computes and returns a function mask based on input key and value.
* @param key unique identifier for the function
* @param value numerical value to be processed
*/","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)",319,321,"/**
 * Invokes M3 processing on the given URI after applying intermediate transformations.
 * @param conf configuration object
 * @param uri input URI to process
 */","Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])",167,175,"/**
* Invokes M2 with provided configuration and targets.
* @param conf Hadoop Configuration
* @param mountTableName name of mounted table
* @param src source data path
* @param settings optional settings string (defaults to ""minReplication=2,repairOnRead=true"")
* @param targets URI array for target tables
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,addMappingProvider,"org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)",162,168,"/**
* Registers a provider with the specified class and name.
* @param providerName unique provider identifier
* @param providerClass Class of the GroupMappingServiceProvider instance to register
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setBlockSize,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)",126,128,"/**
 * Sets BZip2 compression block size in configuration. 
 * @param conf Configuration object to update
 * @param blockSize compression block size value
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setWorkFactor,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)",135,137,"/**
* Sets BZIP2 compression work factor in configuration.
* @param conf Configuration object to update
* @param workFactor BZIP2 compression work factor value
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,setIndexInterval,"org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)",380,382,"/**
* Updates index interval configuration value.
* @param conf Configuration object
* @param interval new index interval value
*/","* Sets the index interval and stores it in conf.
     * @see #getIndexInterval()
     *
     * @param conf configuration.
     * @param interval interval.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setPingInterval,"org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)",175,178,"/**
* Sets IPC ping interval in configuration.
* @param conf Hadoop Configuration object
* @param pingInterval interval value (in ms)
*/","* set the ping interval value in configuration
   * 
   * @param conf Configuration
   * @param pingInterval the ping interval",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setConnectTimeout,"org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)",233,235,"/**
* Sets IPC client connect timeout in configuration.
* @param conf Configuration object to update
* @param timeout new timeout value (in milliseconds)
*/","* set the connection timeout value in configuration
   * 
   * @param conf Configuration
   * @param timeout the socket connect timeout value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setIsNestedMountPointSupported,"org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)",279,281,"/**
* Sets nested mount point support configuration.
* @param conf Hadoop configuration object
* @param isNestedMountPointSupported true to enable or false to disable support
*/","* Set the bool value isNestedMountPointSupported in config.
   * @param conf - from this conf
   * @param isNestedMountPointSupported - whether nested mount point is supported",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])",242,248,"/**
* Applies mask configuration based on provided key and values.
* @param key unique identifier for the configuration
* @param values additional values required by the configuration
*/","* Set an array of string values as optional parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])",318,324,"/**
* Validates and processes input key and values.
* @param key unique identifier
* @param values variable number of related values
*/","* Set a string array as mandatory option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)",2624,2629,"/**
* Creates an InetSocketAddress based on the provided name and address.
* @param name network interface name
* @param addr initial address to modify
* @return modified address or null if creation fails
*/","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address.
   * @param name property name.
   * @param addr InetSocketAddress of a listener to store in the given property
   * @return InetSocketAddress for clients to connect",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,setProtocolEngine,"org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)",211,217,"/**
* Configures RPC engine for given protocol.
* @param conf configuration object
* @param protocol the protocol to configure
* @param engine the engine class to use
*/","* Set a protocol to use a non-default RpcEngine if one
   * is not specified in the configuration.
   * @param conf configuration to use
   * @param protocol the protocol interface
   * @param engine the RpcEngine impl",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,delete,"org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",707,721,"/**
* Verifies and processes a directory path.
* @param p directory Path to verify
* @param recursive whether to perform recursive operations
* @return true if successful, false otherwise
*/","* Delete the given path to a file or directory.
   * @param p the path to delete
   * @param recursive to delete sub-directories
   * @return true if the file or directory and all its contents were deleted
   * @throws IOException if p is non-empty and recursive is false",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File),267,269,"/**
* Wraps call to m1 with default recursive parameter.
* @param dir directory object
* @return true if directory is empty or null, false otherwise
*/","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @return fullyDeleteContents Status.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path),567,574,"/**
* Resolves file status for the given path.
* @param f Path to resolve
* @return FileStatus object or throws exception if access fails
*/","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path),611,628,"/**
* Fetches file statuses for the given path.
* @param f Path to fetch statuses for
* @return Array of FileStatus objects or null if not found
*/","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFileSystem#getFileStatus(Path f)}
   *
   * Note: In ViewFileSystem, by default the mount links are represented as
   * symlinks.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(),524,528,"/**
* Extracts a single byte value from the underlying data stream.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)",62,74,"/**
* Initializes MetricsSourceBuilder with source object and factory.
* @param source Object to extract metrics from
* @param factory MutableMetricsFactory instance
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDirWithMode,"org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",777,802,"/**
* Creates directory with specified permissions.
* @param p path to create directory in
* @param p2f file object representing the directory
* @param permission FsPermission object specifying directory mode
* @return true if successful, false otherwise
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1226,1236,"/**
* Creates a new FSDataOutputStream instance with default permissions.
* @param f file path
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size for I/O operations
* @param replication data block replication factor
* @param blockSize block size in bytes
* @param progress progress monitor for long-running operations
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getUMask,org.apache.hadoop.fs.FileContext:getUMask(),585,587,"/**
* Returns file system permissions based on umask or default configuration. 
* @return FsPermission object representing file system permissions","* 
   * @return the umask of this FileContext",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createFactory,"org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)",129,144,"/**
* Creates a BlockFactory instance based on the specified type.
* @param keyToBufferDir directory path for buffered data
* @param configuration application configuration
* @param name factory type (e.g. DATA_BLOCKS_BUFFER_ARRAY)
*/","* Create a factory.
   *
   * @param keyToBufferDir Key to buffer directory config for a FS.
   * @param configuration  factory configurations.
   * @param name           factory name -the option from {@link CommonConfigurationKeys}.
   * @return the factory, ready to be initialized.
   * @throws IllegalArgumentException if the name is unknown.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,<init>,org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration),177,191,"/**
* Initializes compression codecs factory with user-specified or default codecs.
* @param conf Configuration object containing codec settings
*/","* Find the codecs specified in the config value io.compression.codecs 
   * and register them. Defaults to gzip and deflate.
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",81,90,"/**
* Initializes AbstractJavaKeyStoreProvider with URI and Configuration.
* @param uri key store location
* @param conf configuration settings
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,replacePattern,"org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)",235,243,"/**
* Constructs a FUNC_MASK string from hostname and service components.
* @param components array of service component strings
* @param hostname host name or IP address to be used in the mask
* @return FUNC_MASK string representation",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,registerProtocolAndImpl,"org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1101,1135,"/**
* Configures RPC protocol based on given parameters.
* @param rpcKind Rpc kind
* @param protocolClass Protocol class
* @param protocolImpl Protocol implementation object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,"org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)",71,79,"/**
* Retrieves a URI from the configuration for the given key.
* @param conf Configuration object
* @param configKeyName unique configuration key name
* @return URI object or null if not found or invalid
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,"org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)",1340,1343,"/**
* Returns user profile data or default value if not found.
* @param name unique user identifier
* @param defaultValue default value to return if profile is missing
* @return UserProfile object or default value as String
*/","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>defaultValue</code> if no such property exists. 
   * See @{Configuration#getTrimmed} for more details.
   * 
   * @param name          the property name.
   * @param defaultValue  the property default value.
   * @return              the value of the <code>name</code> or defaultValue
   *                      if it is not set.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInt,"org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)",1546,1555,"/**
* Converts string to integer using custom functions m1 and m2.
* @param name input string
* @param defaultValue default value to return if conversion fails
* @return integer representation of the string or defaultValue if failed
*/","* Get the value of the <code>name</code> property as an <code>int</code>.
   *   
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>int</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as an <code>int</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLong,"org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)",1599,1608,"/**
* Converts string to long value using hexadecimal or decimal conversion.
* @param name string identifier
* @param defaultValue default long value to return when conversion fails
* @return long value or defaultValue if conversion fails
*/","* Get the value of the <code>name</code> property as a <code>long</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>long</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLongBytes,"org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)",1624,1629,"/**
* Converts string to binary prefix value and returns it as a long.
* @param name input string
* @param defaultValue default value if conversion fails
* @return binary prefix value or default value if not found
*/","* Get the value of the <code>name</code> property as a <code>long</code> or
   * human readable format. If no such property exists, the provided default
   * value is returned, or if the specified value is not a valid
   * <code>long</code> or human readable format, then an error is thrown. You
   * can use the following suffix (case insensitive): k(kilo), m(mega), g(giga),
   * t(tera), p(peta), e(exa)
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>,
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFloat,"org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)",1671,1676,"/**
* Retrieves and converts a float value from the system using the given name.
* @param name identifier for retrieving the float value
* @param defaultValue default value to return if retrieval fails
* @return retrieved float value or defaultValue if not available
*/","* Get the value of the <code>name</code> property as a <code>float</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>float</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>float</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDouble,"org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)",1700,1705,"/**
* Retrieves and parses a numeric mask value from configuration.
* @param name unique identifier for the mask
* @return numeric value or default value if not found
*/","* Get the value of the <code>name</code> property as a <code>double</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>double</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>double</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getBoolean,"org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)",1727,1742,"/**
* Returns the boolean value of a string parameter or a default value.
* @param name unique identifier
* @param defaultValue default boolean value to return on failure
*/","* Get the value of the <code>name</code> property as a <code>boolean</code>.  
   * If no such property is specified, or if the specified value is not a valid
   * <code>boolean</code>, then <code>defaultValue</code> is returned.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @return property value as a <code>boolean</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)",2730,2739,"/**
* Retrieves a class by name or returns the default class.
* @param name Class name to search for
* @param defaultValue Default class to return if not found
* @return Retrieved class object or default class
*/","* Get the value of the <code>name</code> property as a <code>Class</code>.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBooleanIfUnset,"org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)",1759,1761,"/**
* Sets a mask value based on provided name and boolean value.
* @param name unique identifier for the mask
* @param value true/false state of the mask
*/","* Set the given property, if it is currently unset.
   * @param name property name
   * @param value new value",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1884,1886,"/**
* Returns the value of the specified resource with default value and time unit.
* @param name resource name
* @param defaultValue default value to use if not found
* @param unit time unit for returned value
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param unit Unit to convert the stored property, if it exists.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1888,1890,"/**
* Returns value in specified time unit.
* @param name unknown parameter
* @param defaultValue default value not used
* @param unit time unit to convert to (duplicated)
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java,getProviders,org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),73,112,"/**
* Retrieves a list of CredentialProvider instances from the given Configuration.
* @param conf Configuration object containing CredentialProvider paths
* @return List of CredentialProvider objects or an empty list if none found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderFactory.java,getProviders,org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),62,81,"/**
* Retrieves a list of KeyProviders from the given Configuration.
* @param conf input configuration
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseServiceUserNames,"org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)",409,413,"/**
* Retrieves a set of user identifiers from configuration.
* @param ns namespace string
* @param conf Configuration object
* @return Set of user IDs or empty set if not found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,getPackages,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages(),65,73,"/**
* Initializes package mask by populating from AVRO_REFLECT_PACKAGES.
* @param packages Set of package identifiers
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getRawCoderNames,"org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)",168,174,"/**
* Retrieves configuration values as an array of strings for the specified erasure code.
* @param conf Hadoop Configuration object
* @param codecName name of the erasure code to fetch values for
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getSaslProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)",136,150,"/**
* Builds a map of SASL properties from configuration.
* @param conf Hadoop Configuration object
* @param configKey key to fetch QOP values for
* @param defaultQOP default QualityOfProtection value
* @return Map of SASL properties or empty if not found
*/","* A util function to retrieve specific additional sasl property from config.
   * Used by subclasses to read sasl properties used by themselves.
   * @param conf the configuration
   * @param configKey the config key to look for
   * @param defaultQOP the default QOP if the key is missing
   * @return sasl property associated with the given key",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getKeyFiles,org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles(),220,222,"/**
* Retrieves collection of string masks.
* @return Collection of String values or empty if none found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration),35,45,"/**
* Initializes a set of valid proxy servers from configuration.
* @param conf Hadoop Configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInts,org.apache.hadoop.conf.Configuration:getInts(java.lang.String),1567,1574,"/**
* Converts string array to integer array using m1 and m2 functions.
* @param name input string
* @return int array representation of input or null if invalid
*/","* Get the value of the <code>name</code> property as a set of comma-delimited
   * <code>int</code> values.
   * 
   * If no such property exists, an empty array is returned.
   * 
   * @param name property name
   * @return property value interpreted as an array of comma-delimited
   *         <code>int</code> values",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurations,"org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)",1971,1978,"/**
* Calculates duration masks for the given name and time unit.
* @param name unique identifier
* @param unit TimeUnit to apply (e.g. SECONDS, MILLISECONDS)
* @return array of duration values in specified units
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClasses,"org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])",2703,2718,"/**
* Resolves function mask by name, returning default values if not found.
* @param name unique identifier for the function mask
* @param defaultValue array of Class<?> objects to use as fallback
* @return array of Class<?> objects or defaultValue if not resolved
*/","* Get the value of the <code>name</code> property
   * as an array of <code>Class</code>.
   * The value of the property specifies a list of comma separated class names.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the property name.
   * @param defaultValue default value.
   * @return property value as a <code>Class[]</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFile,"org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)",2861,2874,"/**
* Searches for a valid local directory based on the given properties and path.
* @param dirsProp unique directory identifier
* @param path file or directory path to search for
* @return File object of the first matching directory or null if not found
* @throws IOException if no valid directories are found in the property
*/","* Get a local file name under a directory named in <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,<init>,org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration),58,67,"/**
* Initializes SerializationFactory with configured serializations.
* @param conf Configuration object containing serialization settings
*/","* <p>
   * Serializations are found by reading the <code>io.serializations</code>
   * property from <code>conf</code>, which is a comma-delimited list of
   * classnames.
   * </p>
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,setConf,org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration),60,73,"/**
* Initializes configuration for RPC security mask.
* @param conf Hadoop Configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,getFilterParams,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",229,232,"/**
* Retrieves configuration properties from Hadoop Configuration object.
* @param conf Hadoop Configuration instance
* @param confPrefix prefix to filter properties in the config
* @return Map of string key-value pairs or null if empty
*/","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java,getFilterParams,"org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",80,83,"/**
* Retrieves configuration settings as a map from M1 source.
* @param conf Configuration object
* @param confPrefix prefix to filter settings
* @return Map of key-value pairs or empty if not found
*/","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",356,374,"/**
* Adds properties from the configuration with a specified prefix to the FSBuilder.
* @param builder FSBuilder instance
* @param conf Configuration object
* @param prefix Property prefix (including dot if not empty)
* @param mandatory whether properties are required or optional
*/","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * <pre>
   *   fs.example.s3a.option becomes ""s3a.option""
   *   fs.example.fs.io.policy becomes ""fs.io.policy""
   *   fs.example.something becomes ""something""
   * </pre>
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,parseChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",67,70,"/**
 * Computes property changes between two configuration objects.
 * @param newConf new configuration values
 * @param oldConf previous configuration values
 * @return Collection of PropertyChange objects or empty if no changes
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,printConf,"org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)",88,130,"/**
* Generates an HTML form to display and apply configuration changes.
* @param out PrintWriter for output
* @param reconf Reconfigurable object containing current and new configurations
*/",* Print configuration options that can be changed.,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",67,70,"/**
* Calls m2 with intermediate results from m1.
* @param conf configuration object
* @param src source string
* @param target target URI
*/","* Add a link to the config for the default mount table
   * @param conf - add the link to this conf
   * @param src - the src path name
   * @param target - the target URI link",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)",91,93,"/**
* Invokes M2 operation with configuration and target URI.
* @param conf Hadoop Configuration object
* @param target Target URI to process
*/","* Add a LinkMergeSlash to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)",114,116,"/**
 * Invokes M2 with results of M1 and current configuration.
 * @param conf Hadoop Configuration object
 * @param target target URI
 */","* Add a LinkFallback to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])",137,139,"/**
 * Invokes another instance of M2 with results from M1 and specified targets.
 * @param conf Configuration object
 * @param targets array of target URIs
 */","* Add a LinkMerge to the config for the default mount table.
   *
   * @param conf configuration.
   * @param targets targets array.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)",209,212,"/**
* Recursively calls itself with user-specific configuration and home directory.
* @param conf Configuration object
* @param homedir User's home directory path
*/","* Add config variable for homedir for default mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration),235,237,"/**
 * Recursively fetches configuration and executes function m1 to generate result.
 * @param conf Configuration object
 */","* Get the value of the home dir conf value for default mount table
   * @param conf - from this conf
   * @return home dir value, null if variable is not in conf",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",94,104,"/**
* Creates an M6 ErasureEncoder instance based on the provided configuration and options.
* @param conf Configuration object
* @param options ErasureCodecOptions object
*/","* Create encoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure encoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",112,122,"/**
* Creates an ErasureDecoder instance based on the provided configuration and options.
* @param conf Configuration object
* @param options ErasureCodecOptions object
* @return ErasureDecoder instance or null if creation fails
*/","* Create decoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure decoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getLibraryName,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration),72,78,"/**
* Determines compressor name based on configuration.
* @param conf configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2CompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration),86,90,"/**
* Returns compressor class based on configuration.
* @param conf Hadoop Configuration object
*/","* Return the appropriate type of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 compressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2DecompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration),109,113,"/**
* Returns decompressor class based on configuration.
* @param conf Hadoop Configuration object
*/","* Return the appropriate type of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Decompressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration),121,124,"/**
* Determines the decompressor to use based on configuration.
* @param conf Configuration object
*/","* Return the appropriate implementation of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactory,"org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)",96,110,"/**
* Retrieves a SocketFactory instance based on configuration and class type.
* @param conf Hadoop Configuration object
* @param clazz Class to determine factory implementation for
* @return SocketFactory instance or null if not found
*/","* Get the socket factory for the given class according to its
   * configuration parameter
   * <tt>hadoop.rpc.socket.factory.class.&lt;ClassName&gt;</tt>. When no
   * such parameter exists then fall back on the default socket factory as
   * configured by <tt>hadoop.rpc.socket.factory.class.default</tt>. If
   * this default socket factory is not configured, then fall back on the JVM
   * default socket factory.
   * 
   * @param conf the configuration
   * @param clazz the class (usually a {@link VersionedProtocol})
   * @return a socket factory",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,resolve,org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List),127,147,"/**
* Maps list of node names to their corresponding rack IDs.
* @param names list of node names
* @return A list of rack IDs or default rack ID if not found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(),149,160,"/**
* Reloads and updates the functional mask mapping.
* @throws NullPointerException if m1() returns null
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,initFilter,"org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",122,128,"/**
* Configures filter for static user access control.
* @param container FilterContainer instance
* @param conf Configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)",520,524,"/**
* Constructs a Hadoop Zookeeper factory with Kerberos authentication.
* @param zkPrincipal Zookeeper principal
* @param kerberosPrincipal Kerberos principal
* @param kerberosKeytab Kerberos keytab file path
*/","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.
     * @param kerberosPrincipal Optional. Use along with kerberosKeytab.
     * @param kerberosKeytab Optional. Use along with kerberosPrincipal.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,goUpGroupHierarchy,"org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)",592,618,"/**
* Recursively fetches LDAP groups based on input DN and hierarchy level.
* @param groupDNs set of initial group DNs
* @param goUpHierarchy maximum recursion depth
* @param groups set to store all found groups
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,init,org.apache.hadoop.security.ssl.SSLFactory:init(),194,204,"/**
* Initializes SSL/TLS context and socket factory.
* @throws GeneralSecurityException on security errors
* @throws IOException on I/O errors
*/","* Initializes the factory.
   *
   * @throws  GeneralSecurityException thrown if an SSL initialization error
   * happened.
   * @throws IOException thrown if an IO error happened while reading the SSL
   * configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,isSimpleAuthentication,org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration),427,430,"/**
* Determines if function masking is enabled based on configuration.
* @param conf Configuration object
*/","* Is the authentication method of this configuration ""simple""?
   * @param conf configuration to check
   * @return true if auth is simple (i.e. not kerberos)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,setConfiguration,org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration),63,85,"/**
* Configures security masks based on authentication type.
* @param conf Hadoop Configuration object
*/","* Set the static configuration to get and evaluate the rules.
   * <p>
   * IMPORTANT: This method does a NOP if the rules have been set already.
   * If there is a need to reset the rules, the {@link KerberosName#setRules(String)}
   * method should be invoked directly.
   * 
   * @param conf the new configuration
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthMethods,"org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)",3472,3491,"/**
* Initializes list of accepted authentication methods based on configuration and secret manager.
* @param secretManager Secret manager instance or null
* @param conf Configuration object
* @return List of AuthMethod objects
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,"org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",59,88,"/**
* Finds and returns a valid CryptoCodec instance based on the given configuration and cipher suite.
* @param conf Configuration object
* @param cipherSuite CipherSuite object
* @return CryptoCodec instance or null if not found
*/","* Get crypto codec for specified algorithm/mode/padding.
   * 
   * @param conf
   *          the configuration
   * @param cipherSuite
   *          algorithm/mode/padding
   * @return CryptoCodec the codec object. Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider),115,127,"/**
* Copies configuration from another instance.
* @param other source JavaKeyStoreProvider instance
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration),92,95,"/**
* Resolves hash based on configuration.
* @param conf application configuration
*/","* Get a singleton instance of hash function of a type
   * defined in the configuration.
   * @param conf current configuration
   * @return defined hash type, or null if type is invalid",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,buildFlagSet,"org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",318,325,"/**
* Retrieves a flag set from configuration based on the given enum class.
* @param enumClass Class of enumeration to fetch flags for
* @param conf Configuration object
* @param key Key to retrieve flags under
* @param ignoreUnknown Whether to ignore unknown flags
* @return FlagSet containing fetched flags or null if not found
*/","* Build a FlagSet from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param enumClass class of enum
   * @param conf configuration
   * @param key key to look for
   * @param ignoreUnknown should unknown values raise an exception?
   * @param <E> enumeration type
   * @return a mutable FlagSet
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)",690,720,"/**
* Binds server socket to specified address and range of ports.
* @param socket server socket instance
* @param address address to bind to
* @param backlog maximum number of pending connections
* @param conf configuration object
* @param rangeConf string representation of port range (optional)
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)",3622,3640,"/**
* Generates and writes XML mask for a property.
* @param propertyName name of the property to mask
* @param out output writer
* @param config configuration object
*/","* Write out the non-default properties in this configuration to the
   * given {@link Writer}.
   * <ul>
   * <li>
   * When property name is not empty and the property exists in the
   * configuration, this method writes the property and its attributes
   * to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is null or empty, this method writes all the
   * configuration properties and their attributes to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is not empty but the property doesn't exist in
   * the configuration, this method throws an {@link IllegalArgumentException}.
   * </li>
   * </ul>
   * @param propertyName xml property name.
   * @param out the writer to write to.
   * @param config configuration.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)",3832,3850,"/**
* Masks sensitive configuration properties and writes the result to a JSON stream.
* @param config Configuration object with sensitive properties
* @param out Writer to output the masked JSON data
*/","*  Writes out all properties and their attributes (final and resource) to
   *  the given {@link Writer}, the format of the output would be,
   *
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *
   *  It does not output the properties of the configuration object which
   *  is loaded from an input stream.
   *  <p>
   *
   * @param config the configuration
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,<init>,org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration),37,41,"/**
* Initializes configuration with logging capabilities.
* @param conf underlying Hadoop configuration
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,confirmFormat,org.apache.hadoop.ha.ZKFailoverController:confirmFormat(),301,317,"/**
* Confirms user intent to clear failover information from ZooKeeper.
* @return true if confirmation is successful, false otherwise
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)",192,195,"/**
 * Applies mask function to input string with specified integer value.
 * @param key input string subject to masking
 * @param value integer value used in masking operation
 */","* Set optional int parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)",197,200,"/**
* Applies mask to input value using provided key.
* @param key unique identifier for masking operation
* @param value value to be masked
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)",212,215,"/**
* Maps float value to its equivalent long representation and passes both to underlying function.
* @param key unique identifier for mapping operation
* @param value float value to be converted and mapped
*/","* Set optional float parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)",222,225,"/**
 * Calculates a mask by applying value to the specified key.
 * @param key unique identifier for the calculation
 * @param value numeric value used in the calculation
 */","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)",293,296,"/**
* Maps string key to integer value using m1 function.
* @param key unique identifier as a string
* @param value integer value to be mapped
* @return integer result of mapping or null if failed
*/","* Set mandatory int option.
   *
   * @see #must(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)",298,301,"/**
 * Applies a mask function to the input value.
 * @param key unique identifier for the mask operation
 * @param value input value to be masked
 * @return result of the mask operation as type B
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)",303,306,"/**
* Converts a float mask value to an integer using m1 function.
* @param key unique identifier
* @param value float mask value to be converted
* @return int result of conversion or null if invalid
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)",308,311,"/**
* Returns mask value using provided key and value.
* @param key unique identifier
* @param value numerical value to be masked
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",177,180,"/**
* Invokes m2 with intermediate results from m1 and default target. 
* @param conf configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,handleEmptyDstDirectoryOnWindows,"org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)",646,673,"/**
* Renames a source file and deletes an empty destination directory.
* @param src source file path
* @param srcFile source file object
* @param dst destination directory path
* @param dstFile destination directory object
* @return true if operation completes successfully, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,makeSource,org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object),36,39,"/**
* Creates a MetricsSource instance from the provided object.
* @param source Object to create MetricsSource from
*/","* Make an metrics source from an annotated object.
   * @param source  the annotated object.
   * @return a metrics source",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,newSourceBuilder,org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object),41,44,"/**
* Creates a metrics source builder with default factory.
* @param source metrics source object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDir,org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File),773,775,"/**
* Checks function mask by reading file contents.
* @param p2f File object containing function data
* @return true if valid, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirsWithOptionalPermission,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",818,839,"/**
* Validates file creation by checking destination and parent directory existence.
* @param f path to be created
* @param permission FsPermission object
* @return true if creation is valid, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)",1205,1211,"/**
* Creates an FSDataOutputStream with specified parameters.
* @param f file to write
* @param overwrite whether to overwrite existing file
* @param bufferSize buffer size for I/O operations
* @param replication HDFS block replication count
* @param blockSize HDFS block size in bytes
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,create,"org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",681,706,"/**
* Creates or overwrites a file at the specified path with the given permissions and options.
* @param f file path
* @param createFlag file creation flags
* @param opts additional file creation options
* @return FSDataOutputStream for writing to the file
*/","* Create or overwrite file on indicated path and returns an output stream for
   * writing into the file.
   * 
   * @param f the file name to open
   * @param createFlag gives the semantics of create; see {@link CreateFlag}
   * @param opts file creation options; see {@link Options.CreateOpts}.
   *          <ul>
   *          <li>Progress - to report progress on the operation - default null
   *          <li>Permission - umask is applied against permission: default is
   *          FsPermissions:getDefault()
   * 
   *          <li>CreateParent - create missing parent path; default is to not
   *          to create parents
   *          <li>The defaults for the following are SS defaults of the file
   *          server implementing the target path. Not all parameters make sense
   *          for all kinds of file system - eg. localFS ignores Blocksize,
   *          replication, checksum
   *          <ul>
   *          <li>BufferSize - buffersize used in FSDataOutputStream
   *          <li>Blocksize - block size for file blocks
   *          <li>ReplicationFactor - replication for blocks
   *          <li>ChecksumParam - Checksum parameters. server default is used
   *          if not specified.
   *          </ul>
   *          </ul>
   * 
   * @return {@link FSDataOutputStream} for created file
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>f</code> already exists
   * @throws FileNotFoundException If parent of <code>f</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>f</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,mkdir,"org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",799,816,"/**
* Creates a new directory with specified permissions and creates parent if necessary.
* @param dir  directory path
* @param permission file system permissions
* @param createParent whether to create parent directories if needed
*/","* Make(create) a directory and all the non-existent parents.
   * 
   * @param dir - the dir to make
   * @param permission - permissions is set permission{@literal &~}umask
   * @param createParent - if true then missing parent dirs are created if false
   *          then parent must exist
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If directory <code>dir</code> already
   *           exists
   * @throws FileNotFoundException If parent of <code>dir</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>dir</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>dir</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>dir</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,main,org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[]),301,358,"/**
* Compresses or decompresses data using specified codecs.
* @param args command line arguments: -in to encode, -out to decode
*/","* A little test program.
   * @param args arguments.
   * @throws Exception exception.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",47,50,"/**
* Initializes KeyStoreProvider with URI and configuration.
* @param uri URI to initialize provider
* @param conf Hadoop configuration",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",54,57,"/**
* Initializes a new instance of LocalKeyStoreProvider with given URI and configuration.
* @param uri URI of the key store
* @param conf Apache Hadoop configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)",185,196,"/**
* Resolves FUNC_MASK from principal configuration and hostname.
* @param principalConfig principal configuration string
* @param hostname host name to resolve against
* @return resolved FUNC_MASK string or original principal config if invalid
*/","* Convert Kerberos principal name pattern to valid Kerberos principal
   * names. It replaces hostname pattern with hostname, which should be
   * fully-qualified domain name. If hostname is null or ""0.0.0.0"", it uses
   * dynamically looked-up fqdn of the current host instead.
   * 
   * @param principalConfig
   *          the Kerberos principal name conf value to convert
   * @param hostname
   *          the fully-qualified domain name used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)",212,227,"/**
* Replaces hostname in principal config with client IP.
* @param principalConfig configuration string
* @param addr client's network address
* @return modified principal config or original if invalid
*/","* Convert Kerberos principal name pattern to valid Kerberos principal names.
   * This method is similar to {@link #getServerPrincipal(String, String)},
   * except 1) the reverse DNS lookup from addr to hostname is done only when
   * necessary, 2) param addr can't be null (no default behavior of using local
   * hostname when addr is null).
   * 
   * @param principalConfig
   *          Kerberos principal name pattern to convert
   * @param addr
   *          InetAddress of the host used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,addProtocol,"org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1218,1222,"/**
* Initializes server with RPC configuration.
* @param rpcKind Rpc kind (e.g. TCP, UDP)
* @param protocolClass Protocol class for data serialization
* @param protocolImpl Implementation of the protocol
* @return This server object for method chaining
*/","* Add a protocol to the existing server.
     * @param rpcKind - input rpcKind
     * @param protocolClass - the protocol class
     * @param protocolImpl - the impl of the protocol that will be called
     * @return the server (for convenience)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProvider,"org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)",59,64,"/**
* Creates a KeyProvider instance using configuration and key name.
* @param conf Configuration object
* @param configKeyName Name of the configuration key
* @return KeyProvider instance or null if not found
*/","* Creates a new KeyProvider from the given Configuration
   * and configuration key name.
   *
   * @param conf Configuration
   * @param configKeyName The configuration key name
   * @return new KeyProvider, or null if no provider was found.
   * @throws IOException if the KeyProvider is improperly specified in
   *                             the Configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration),66,69,"/**
* Returns an instance of URI using the provided configuration.
* @param conf application configuration
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultUri,org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration),297,304,"/**
* Creates a URI for the function mask based on the provided configuration.
* @param conf Configuration object
*/","* Get the default FileSystem URI from a configuration.
   * @param conf the configuration to use
   * @return the uri of the default filesystem",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,setConf,org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration),95,102,"/**
* Configures function mask settings from the provided configuration.
* @param conf the Hadoop configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKinitExecutable,org.apache.hadoop.security.KDiag:validateKinitExecutable(),715,727,"/**
* Initializes Kerberos environment by executing kinit command.
* @param none
* @return none
*/","* A cursory look at the {@code kinit} executable.
   *
   * If it is an absolute path: it must exist with a size > 0.
   * If it is just a command, it has to be on the path. There's no check
   * for that -but the PATH is printed out.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)",2566,2570,"/**
* Creates an InetSocketAddress from the given name and default values.
* @param name hostname or service name
* @param defaultAddress default IP address (used if name is empty)
* @param defaultPort default port number
* @return InetSocketAddress object representing the resolved address and port
*/","* Get the socket address for <code>name</code> property as a
   * <code>InetSocketAddress</code>.
   * @param name property name.
   * @param defaultAddress the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)",2596,2614,"/**
* Resolves and constructs an InetSocketAddress based on provided properties.
* @param hostProperty property for host value
* @param addressProperty property for address value
* @param defaultAddressValue default address to use if property is null
* @param addr existing InetSocketAddress to reuse
* @return constructed InetSocketAddress or null if invalid properties
*/","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address. If the host and address
   * properties are configured the host component of the address will be combined
   * with the port component of the addr to generate the address.  This is to allow
   * optional control over which host name is used in multi-home bind-host
   * cases where a host can have multiple names
   * @param hostProperty the bind-host configuration name
   * @param addressProperty the service address configuration name
   * @param defaultAddressValue the service default address configuration value
   * @param addr InetSocketAddress of the service listener
   * @return InetSocketAddress for clients to connect",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initializeMetadataCache,org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration),108,113,"/**
* Initializes and configures metadata cache with specified size.
* @param conf configuration object with cache size settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build(),2971,2990,"/**
* Resolves file system link to FSDataInputStream.
* @return FSDataInputStream object or null if not found
*/","* Perform the open operation.
     *
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build(),4941,4958,"/**
* Opens file for reading and returns a stream.
* @return FSDataInputStream object or null if failed
*/","* Perform the open operation.
     * Returns a future which, when get() or a chained completion
     * operation is invoked, will supply the input stream of the file
     * referenced by the path/path handle.
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,setConfigurationFromURI,"org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)",97,135,"/**
* Configures SFTP connection settings from URI and Configuration.
* @param uriInfo URI containing host, port, user, and password information
* @param conf Configuration object to store SFTP settings
*/","* Set configuration from UI.
   *
   * @param uri
   * @param conf
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,connect,org.apache.hadoop.fs.sftp.SFTPFileSystem:connect(),143,157,"/**
* Retrieves an SFTP channel for the specified host.
* @return ChannelSftp object or throws IOException if failed
*/","* Connecting by using configuration parameters.
   *
   * @return An FTPClient instance
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)",180,185,"/**
* Initializes ChecksumFSInputChecker with the given file and stream buffer size.
* @param fs ChecksumFileSystem instance
* @param file Path to the input file
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,initFromFS,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS(),113,116,"/**
* Retrieves and sets the buffer size based on the system configuration. 
* @param IO_FILE_BUFFER_SIZE_KEY system key for buffer size configuration
* @param IO_FILE_BUFFER_SIZE_DEFAULT default buffer size value
*/",* Initialize from a filesystem.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,create,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)",515,544,"/**
* Creates or appends FSDataOutputStream for the given item.
* @param item PathData object
* @param lazyPersist whether to use lazy persist or not
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path),996,999,"/**
* Opens an input stream to the file at the specified path.
* @param f Path to the file
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file to open
   * @throws IOException IO failure
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle),1014,1017,"/**
* Opens an FSDataInputStream from the given file descriptor.
* @param fd PathHandle representing the file descriptor
*/","* Open an FSDataInputStream matching the PathHandle instance. The
   * implementation may encode metadata in PathHandle to address the
   * resource directly and verify that the resource referenced
   * satisfies constraints specified at its construciton.
   * @param fd PathHandle object returned by the FS authority.
   * @throws InvalidPathHandleException If {@link PathHandle} constraints are
   *                                    not satisfied
   * @throws IOException IO failure
   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}
   *                                       not overridden by subclass
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path),1516,1519,"/**
* Creates an output stream to the specified file.
* @param f Path to the file
*/","* Append to an existing file (optional operation).
   * Same as
   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}
   * @param f the existing file to be appended.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,"org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)",1558,1561,"/**
* Creates an output stream to the specified file.
* @param f Path to the file
* @param appendToNewBlock Flag to append to new block or not
*/","* Append to an existing file (optional operation).
   * @param f the existing file to be appended.
   * @param appendToNewBlock whether to append data to a new block
   * instead of the end of the last partial block
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,setConf,org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration),83,93,"/**
* Initializes configuration for local file system.
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)",156,163,"/**
* Calculates the optimal buffer size for a stream based on provided parameters.
* @param bytesPerSum the number of bytes per sum
* @param bufferSize the desired buffer size
* @return the optimal buffer size
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration),3657,3663,"/**
* Initializes the cache with a semaphore based on configuration.
* @param conf Configuration object
*/","* Instantiate. The configuration is used to read the
     * count of permits issued for concurrent creation
     * of filesystem instances.
     * @param conf configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)",2948,2974,"/**
* Initializes Sorter with file system, comparator, and configuration.
* @param fs the file system to use
* @param comparator comparator for sorting
* @param keyClass class of key objects
* @param valClass class of value objects
* @param conf Hadoop configuration
* @param metadata metadata for the sort operation
*/","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.
     * @param metadata input metadata.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBlockSize,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration),130,133,"/**
* Returns the block size for bzip2 compression.
* @param conf Configuration object
* @return Block size value or default if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getWorkFactor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration),139,142,"/**
* Returns the bzip2 compress work factor from configuration.
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,"org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",60,67,"/**
* Creates a compression output stream with the specified compressor and buffer size.
* @param out target output stream
* @param compressor compression algorithm to use
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,"org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",86,93,"/**
* Creates a compression input stream using the provided decompressor.
* @param in input stream to be compressed
* @param decompressor decompression engine
* @return CompressionInputStream instance
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,"org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
* Creates a CompressionOutputStream with LZ4 compression.
* @param out OutputStream to compress
* @param compressor Compressor instance for compression
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,"org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",146,153,"/**
* Creates a compression-aware input stream using LZ4 decompression.
* @param in the original input stream
* @param decompressor the decompressor instance
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createDecompressor,org.apache.hadoop.io.compress.Lz4Codec:createDecompressor(),170,176,"/**
* Creates a LZ4 decompressor with the specified buffer size.
* @return Decompressor instance or null if failed
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,"org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",122,130,"/**
* Returns a CompressionOutputStream instance based on compression method.
* @param out OutputStream to write compressed data to
* @param compressor Compressor instance (used for CompressorStream)
* @return CompressionOutputStream instance or throws IOException if error occurs
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",177,186,"/**
* Creates a compression input stream based on configuration.
* @param in the input stream to compress
* @param decompressor the decompression object
* @return CompressionInputStream instance or throws IOException if failed
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}, and return a 
   * stream for uncompressed data.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionLevel,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration),88,92,"/**
* Retrieves ZStandard compression level from configuration.
* @param conf Configuration object
* @return ZStandard compression level (0-22)
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration),108,111,"/**
* Retrieves Zstandard buffer size configuration value.
* @param conf Configuration object
* @return integer buffer size or default if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,"org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",51,60,"/**
* Creates a compression output stream with optional compression.
* @param out target output stream
* @param compressor compressor instance or null for no compression
* @return CompressionOutputStream object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,"org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",83,93,"/**
* Creates a compression-aware input stream using the provided decompressor.
* @param in original input stream
* @param decompressor decompression instance or null to create a new one
* @return CompressionInputStream object wrapping the original stream
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,"org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
* Creates a CompressionOutputStream instance with specified buffer size and overhead.
* @param out OutputStream to compress
* @param compressor Compressor instance for compression
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createCompressor,org.apache.hadoop.io.compress.SnappyCodec:createCompressor(),111,117,"/**
* Creates a Snappy compressor instance with specified buffer size.
* @return Compressor object or null if creation fails
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,"org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",143,150,"/**
* Creates a compression-aware input stream using the provided decompressor.
* @param in original input stream
* @param decompressor Decompressor instance for decompressing data
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createDecompressor,org.apache.hadoop.io.compress.SnappyCodec:createDecompressor(),167,173,"/**
* Creates a Snappy decompressor instance with specified buffer size.
* @return Decompressor object for decompressing Snappy compressed data
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)",114,118,"/**
* Configures and calls internal m2 function with default buffer size.
* @param in input stream
* @param out output stream
* @param conf configuration object
*/","* Copies from one stream to another. <strong>closes the input and output streams 
   * at the end</strong>.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)",130,134,"/**
* Wraps input/output streams with default buffer size.
* @param in input stream to wrap
* @param out output stream to wrap
* @param conf configuration object for buffer size
* @param close flag to close the streams after use
*/","* Copies from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getChunkBufferSize,org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration),142,145,"/**
* Calculates a function-specific buffer size mask based on configuration.
* @param conf Configuration object
* @return Function mask value or default buffer size if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSInputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration),147,149,"/**
* Calculates function mask value based on configuration.
* @param conf application configuration
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSOutputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration),151,153,"/**
* Calculates and returns the function mask value based on the specified configuration attribute.
* @param conf Configuration object with attributes
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getBufferSize,org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration),1738,1740,"/**
 * Retrieves and returns the function mask from the configuration.
 * @param conf Configuration object
 */",Get the configured buffer size,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),156,166,"/**
* Initializes configuration for the script.
* @param conf Configuration object
*/","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",566,581,"/**
* Creates a ServerConnector instance with specified configuration.
* @param server the server to connect
* @param httpConfig HTTP configuration settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,doOp,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)",167,234,"/**
* Attempts to execute a provider call with failover and retry logic.
* @param op ProviderCallable to execute
* @param currPos current position in the provider list
* @param isIdempotent whether operation is idempotent
* @return result of provider call or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration),341,344,"/**
* Initializes options from Hadoop Configuration object.
* @param conf Hadoop configuration to read options from
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,getBufferSize,org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration),65,68,"/**
* Retrieves configuration value for crypto buffer size.
* @param conf Configuration object
* @return default crypto buffer size if not set in config
*/","* Read crypto buffer size.
   *
   * @param conf configuration.
   * @return hadoop.security.crypto.buffer.size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseNumLevels,"org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)",391,411,"/**
* Retrieves the number of priority levels for a given namespace.
* @param ns namespace identifier
* @param conf configuration object
* @return numLevels integer value or throws exception if invalid
*/","* Read the number of levels from the configuration.
   * This will affect the FairCallQueue's overall capacity.
   * @throws IllegalArgumentException on invalid queue count",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getRpcTimeout,org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration),829,832,"/**
* Returns a configuration mask value based on IPC client RPC timeout settings.
* @param conf Configuration object to retrieve values from
*/","* Get the RPC time from configuration;
   * If not set in the configuration, return the default value.
   *
   * @param conf Configuration
   * @return the RPC timeout (ms)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getPingInterval,org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration),187,190,"/**
* Retrieves function mask from configuration.
* @param conf Configuration object
* @return function mask value or default if not set","* Get the ping interval from configuration;
   * If not set in the configuration, return the default value.
   * 
   * @param conf Configuration
   * @return the ping interval",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcTimeout,org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration),221,226,"/**
* Calculates the function mask value from configuration.
* @param conf Configuration object
* @return Function mask value or 0 if invalid
*/","* The time after which a RPC will timeout.
   *
   * @param conf Configuration
   * @return the timeout period in milliseconds.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,init,"org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",65,90,"/**
* Initializes weights for the given namespace and configuration.
* @param namespace unique namespace identifier
* @param conf Configuration object to fetch weight values from
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)",94,96,"/**
* Initializes a LineReader object with specified input stream and configuration.
* @param in InputStream to read from
* @param conf Configuration object containing file buffer size settings
*/","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>.
   * @param in input stream
   * @param conf configuration
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])",138,144,"/**
* Initializes a LineReader with input stream and configuration.
* @param in InputStream to read from
* @param conf Configuration object for buffer size
* @param recordDelimiterBytes delimiter bytes for line separation
*/","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>, and using a custom delimiter of array of
   * bytes.
   * @param in input stream
   * @param conf configuration
   * @param recordDelimiterBytes The delimiter
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getInt,"org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)",87,92,"/**
* Calls superclass method with provided name and default value.
* Logs the result if not equal to the default value.
* @param name property name
* @param defaultValue default value to log against
* @return property value from superclass
*/","* See {@link Configuration#getInt(String, int)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,setConf,org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration),337,345,"/**
* Initializes the client with configuration and HA timeouts.
* @param conf client configuration
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getSshConnectTimeout,org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout(),215,218,"/**
* Returns function timeout mask value.
* @return Function timeout mask value
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getGracefulFenceTimeout,org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration),82,86,"/**
* Calculates HA fence timeout mask from configuration.
* @param conf Configuration object containing HA settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getRpcTimeoutToNewActive,org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration),88,92,"/**
* Retrieves functional mask value from configuration.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setTimeout,"org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",173,177,"/**
* Configures FTP client with custom timeout settings.
* @param client FTPClient instance to configure
* @param conf Configuration object containing timeout settings
*/","* Set the FTPClient's timeout based on configuration.
   * FS_FTP_TIMEOUT is set as timeout (defaults to DEFAULT_TIMEOUT).",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)",77,93,"/**
* Retrieves a long integer value from options using the given key.
* If parsing fails, returns the provided default value and logs an error. 
* @param key option identifier
* @param defVal default long value to return if parsing fails
* @return parsed long value or default value","* Get a long value with resilience to unparseable values.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,canBeSafelyDeleted,org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData),129,149,"/**
* Determines whether to delete a file system resource based on size and user confirmation.
* @param item PathData object containing file system information
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(),2753,2757,"/**
* Returns the block size mask value.
*/","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.
   * @deprecated use {@link #getDefaultBlockSize(Path)} instead
   * @return default block size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)",49,52,"/**
* Initializes DistributedFilesystem with given configuration and file system interval.
* @param path filesystem path
* @param conf configuration settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getInterval,org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval(),58,67,"/**
* Computes the function mask value.
* @return default interval or configured interval from conf
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getJitter,org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter(),118,129,"/**
* Returns the jitter mask value.
* @return long jitter mask value or default if not configured
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,ensureInitialized,org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized(),1039,1048,"/**
* Initializes and sets the cache timeout for user ID mappings.
* @param none
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)",106,128,"/**
* Initializes ShellBasedIdMapping with configuration and options.
* @param conf Configuration object
* @param constructFullMapAtInit Whether to build full map at initialization
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",71,78,"/**
* Initializes DelegationTokenSecretManager with configuration and token kind.
* @param conf Configuration object
* @param tokenKind Kind of delegation token (e.g. ""HADOOP_TOKEN"")
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayPeriodMillis,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)",359,377,"/**
* Calculates the decay scheduler period in milliseconds.
* @param ns namespace
* @return Decay scheduler period or default value if not specified
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceInit,org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration),74,79,"/**
* Initializes logging thresholds from configuration.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getLong,"org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)",97,102,"/**
* Calls superclass method to fetch a value and logs the result.
* @param name key to retrieve
* @param defaultValue default value if not found
* @return fetched or default value
*/","* See {@link Configuration#getLong(String, long)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,<init>,"org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",115,135,"/**
* Initializes the HealthMonitor with configuration and target service.
* @param conf Configuration object
* @param target HAServiceTarget instance to monitor
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration),166,180,"/**
* Initializes Bloom filter with parameters from Configuration.
* @param conf configuration object containing bloom filter settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getFloat,"org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)",77,82,"/**
* Calls superclass method with default value and logs result.
* @param name parameter name
* @param defaultValue default value to use if not found
* @return float value or default value
*/","* See {@link Configuration#getFloat(String, float)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayFactor,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)",339,357,"/**
* Calculates decay factor for given namespace.
* @param ns namespace to fetch factor from
* @param conf configuration object containing factors
* @return decay factor value (between 0 and 1) or throws exception if invalid
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",87,99,"/**
* Initializes configuration and intervals for trash management.
* @param conf Configuration object
* @param fs FileSystem instance
* @param home Home directory Path
*/","* @deprecated Use {@link #initialize(Configuration, FileSystem)} instead.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",101,118,"/**
* Initializes FS configuration from Hadoop Configuration.
* @param conf Hadoop Configuration object
* @param fs FileSystem instance
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,processRawArguments,org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList),102,122,"/**
* Handles missing fs.defaultFS configuration for a specific command.
* @param args list of arguments passed to the command
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem),1966,1984,"/**
* Closes and disables caching for child file systems.
* @param fs parent file system
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,isNestedMountPointSupported,org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration),270,272,"/**
 * Checks if nested mount point support is enabled in configuration.
 * @param conf configuration object
 */","* Check the bool config whether nested mount point is supported. Default: true
   * @param conf - from this conf
   * @return whether nested mount point is supported",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)",1410,1426,"/**
* Initializes InternalDirOfViewFs object with provided parameters.
* @param dir directory of view file system
* @param cTime creation time of the file system
* @param ugi user information for access control
* @param uri URI of the file system
* @param config configuration settings
* @param fsState file system state
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createCompressor,org.apache.hadoop.io.compress.Lz4Codec:createCompressor(),111,120,"/**
* Creates an LZ4 compressor instance with specified buffer size and HC usage.
* @return Compressor object; null if configuration is invalid
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,handleChecksumException,org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException),2792,2801,"/**
* Handles checksum exceptions by skipping or re-throwing errors.
* @throws IOException if not skipping errors
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getMultipleLinearRandomRetry,"org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)",179,201,"/**
* Creates a retry policy based on configuration settings.
* @param conf Hadoop Configuration object
* @param retryPolicyEnabledKey key for enabling/disabling retry policy
* @param defaultRetryPolicyEnabled default enabled state
* @param retryPolicySpecKey key for specifying retry policy spec
* @param defaultRetryPolicySpec default retry policy specification
* @return RetryPolicy object or null if not created
*/","* Return the MultipleLinearRandomRetry policy specified in the conf,
   * or null if the feature is disabled.
   * If the policy is specified in the conf but the policy cannot be parsed,
   * the default policy is returned.
   * 
   * Retry policy spec:
   *   N pairs of sleep-time and number-of-retries ""s1,n1,s2,n2,...""
   * 
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @return the MultipleLinearRandomRetry policy specified in the conf,
   *         or null if the feature is disabled.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,isSupported,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported(),283,283,"/**
* Returns true if this object has a mask value, false otherwise.",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addPrometheusServlet,org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration),818,828,"/**
* Configures and initializes Prometheus support.
* @param conf Hadoop configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultApps,"org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)",926,973,"/**
* Configures and sets up Jetty contexts for logs and static content.
* @param parent ContextHandlerCollection instance
* @param appDir application directory path
* @param conf Hadoop configuration object
*/","* Add default apps.
   *
   * @param parent contexthandlercollection.
   * @param appDir The application directory
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultServlets,org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration),985,995,"/**
* Configures servlets for the application.
* @param configuration Application configuration
*/","* Add default servlets.
   * @param configuration the hadoop configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,create,"org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)",112,123,"/**
* Retrieves JVM metrics for a given process and session.
* @param processName identifier of the process
* @param sessionId unique session ID
* @return JvmMetrics object or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,initFilter,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",39,52,"/**
* Enables or disables CORS filtering based on configuration.
* @param container FilterContainer instance
* @param conf Configuration object for filter settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate(),242,250,"/**
* Fetches and validates the data provider.
* @return true if successful, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getServerFailOverEnable,"org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",119,141,"/**
* Retrieves failover enable status for a namespace.
* @param namespace the namespace to check
* @param conf configuration object
* @return true if enabled, false otherwise
*/","* Return boolean value configured by property 'ipc.<port>.callqueue.overflow.trigger.failover'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.callqueue.overflow.trigger.failover',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_CALLQUEUE_SERVER_FAILOVER_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"" + ""."" + Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffByResponseTimeEnabled,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)",467,472,"/**
* Retrieves configuration value for decay scheduler backoff response time.
* @param ns namespace
* @param conf Configuration object
* @return true if enabled, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",1325,1342,"/**
* Initializes a new IPC client with the given configuration.
* @param valueClass class of the data to be processed
* @param conf Hadoop configuration for client settings
* @param factory socket factory for establishing connections
*/","* Construct an IPC client whose values are of the given {@link Writable}
   * class.
   *
   * @param valueClass input valueClass.
   * @param conf input configuration.
   * @param factory input factory.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",920,927,"/**
* Checks if IPC backoff is enabled based on configuration.
* @param prefix key prefix
* @param conf Hadoop Configuration object
* @return true if enabled, false otherwise
*/",* Get from config if client backoff is enabled on that port.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)",941,953,"/**
* Checks and returns IPC backoff enable flag for given namespace and port.
* @param namespace cluster or application namespace
* @param port service port number
* @param conf Configuration object containing key-value pairs
* @return true if enabled, false otherwise
*/","* Return boolean value configured by property 'ipc.<port>.backoff.enable'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.backoff.enable',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_BACKOFF_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromConfig,org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String),2513,2524,"/**
* Retrieves password mask for the given user.
* @param name user identifier
* @return password character array or null if not found
*/","* Fallback to clear text passwords in configuration.
   * @param name the property name.
   * @return clear text password or null",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getBoolean,"org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)",67,72,"/**
* Calls superclass's m1 with provided parameters and logs result.
* @param name property name
* @param defaultValue default boolean value to use if not found
* @return true if property exists and is enabled, false otherwise
*/","* See {@link Configuration#getBoolean(String, boolean)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileSystemClass,"org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)",3559,3594,"/**
* Resolves the file system class based on scheme and configuration.
* @param scheme file system scheme
* @param conf Hadoop Configuration object
* @return Class of FileSystem implementation or null if not found
*/","* Get the FileSystem implementation class of a filesystem.
   * This triggers a scan and load of all FileSystem implementations listed as
   * services and discovered via the {@link ServiceLoader}
   * @param scheme URL scheme of FS
   * @param conf configuration: can be null, in which case the check for
   * a filesystem binding declaration in the configuration is skipped.
   * @return the filesystem
   * @throws UnsupportedFileSystemException if there was no known implementation
   *         for the scheme.
   * @throws IOException if the filesystem could not be loaded",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createFileSystem,"org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",169,181,"/**
* Resolves and instantiates AbstractFileSystem implementation for the given URI.
* @param uri file system URI
* @param conf Hadoop configuration
* @return AbstractFileSystem instance or throws UnsupportedFileSystemException if not found
*/","* Create a file system instance for the specified uri using the conf. The
   * conf is used to find the class name that implements the file system. The
   * conf is also passed to the file system for its configuration.
   *
   * @param uri URI of the file system
   * @param conf Configuration for the file system
   * 
   * @return Returns the file system for the given URI
   *
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *           not found",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,loadMappingProviders,org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders(),147,160,"/**
* Initializes and validates mapping providers.
* @param conf configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolEngine,"org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)",220,230,"/**
* Retrieves an RpcEngine instance for the specified protocol class.
* @param protocol target protocol class
* @param conf configuration object
* @return RpcEngine instance or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)",796,802,"/**
* Returns the BlockingQueue implementation class for IPC calls.
* @param prefix configuration prefix
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",816,827,"/**
* Instantiates a blocking call queue class based on configuration.
* @param namespace namespace identifier
* @param port port number
* @param conf configuration object
* @return Class of BlockingQueue<Call> or null if not found
*/","* Return class configured by property 'ipc.<port>.callqueue.impl' if it is
   * present. If the config is not present, default config (without port) is
   * used to derive class i.e 'ipc.callqueue.impl', and derived class is
   * returned if class value is present and valid. If default config is also
   * not present, default class {@link LinkedBlockingQueue} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)",829,853,"/**
* Resolves a custom RPC scheduler class based on configuration.
* @param prefix configuration key prefix
* @param conf Hadoop Configuration object
* @return Custom RpcScheduler Class or default implementation
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",869,898,"/**
* Returns an instance of RpcScheduler based on configuration.
* @param namespace namespace identifier
* @param port network port number
* @param conf system Configuration object
* @return RpcScheduler class or null if not found
*/","* Return class configured by property 'ipc.<port>.scheduler.impl' if it is
   * present. If the config is not present, and if property
   * 'ipc.<port>.callqueue.impl' represents FairCallQueue class,
   * return DecayRpcScheduler. If config 'ipc.<port>.callqueue.impl'
   * does not have value FairCallQueue, default config (without port) is used
   * to derive class i.e 'ipc.scheduler.impl'. If default config is also not
   * present, default class {@link DefaultRpcScheduler} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)",2758,2772,"/**
* Resolves class by name, applying interface validation and method invocation.
* @param name class name
* @param defaultValue default class to return if not found
* @param xface interface to validate against and invoke methods on
* @return Class<? extends U> or null if not found
*/","* Get the value of the <code>name</code> property as a <code>Class</code>
   * implementing the interface specified by <code>xface</code>.
   *   
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * An exception is thrown if the returned class does not implement the named
   * interface. 
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @param xface the interface implemented by the named class.
   * @param <U> Interface class type.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getInternal,"org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)",3689,3765,"/**
* Fetches or creates a FileSystem instance based on the provided key.
* @param uri URI of the filesystem
* @param conf Configuration object
* @param key unique identifier for the filesystem
* @return FileSystem object or null if not found
*/","* Get the FS instance if the key maps to an instance, creating and
     * initializing the FS if it is not found.
     * If this is the first entry in the map and the JVM is not shutting down,
     * this registers a shutdown hook to close filesystems, and adds this
     * FS to the {@code toAutoClose} set if {@code ""fs.automatic.close""}
     * is set in the configuration (default: true).
     * @param uri filesystem URI
     * @param conf configuration
     * @param key key to store/retrieve this FileSystem in the cache
     * @return a cached or newly instantiated FileSystem.
     * @throws IOException If an I/O error occurred.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),81,102,"/**
* Initializes a SQLDelegationTokenSecretManager instance with configuration settings.
* @param conf Configuration object containing secret manager properties
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,setConf,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),61,72,"/**
* Initializes configuration with parent class and updates security group shell command timeout.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,getShutdownTimeout,org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration),180,191,"/**
* Calculates the function timeout mask based on configuration.
* @param conf Configuration object
* @return Function timeout mask in milliseconds
*/","* Get the shutdown timeout in seconds, from the supplied
   * configuration.
   * @param conf configuration to use.
   * @return a timeout, always greater than or equal to {@link #TIMEOUT_MINIMUM}",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,getCredentialProvider,org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider(),144,166,"/**
* Retrieves a valid credential provider based on user configuration.
* @return CredentialProvider object or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromCredentialProviders,org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String),2478,2506,"/**
* Retrieves a password mask by name from registered credential providers.
* @param name the key to fetch
* @return char[] password mask or null if not found
*/","* Try and resolve the provided element name as a credential provider
   * alias.
   * @param name alias of the provisioned credential
   * @return password or null if not found
   * @throws IOException when error in fetching password",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,getKeyProvider,org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider(),191,213,"/**
* Retrieves a valid KeyProvider instance based on user preferences.
* @return KeyProvider object or NO_VALID_PROVIDERS error code
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,accept,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class),55,63,"/**
* Checks if a class matches the function mask.
* @param c Class to check
* @return true if the class matches, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",176,202,"/**
* Retrieves a RawErasureEncoder instance based on the provided configuration and codec name.
* @param conf Configuration object
* @param codecName Name of the codec to use
* @param coderOptions ErasureCoderOptions for the encoder
* @return A RawErasureEncoder instance or throws an exception if creation fails",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",204,230,"/**
* Creates a RawErasureDecoder instance based on the provided configuration and codec.
* @param conf Configuration object
* @param codecName Name of the codec
* @param coderOptions ErasureCoderOptions to use for decoding
* @return RawErasureDecoder instance or throws exception if creation fails
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,createSession,"org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)",118,128,"/**
* Establishes a SSH session with the given host and parameters.
* @param host target hostname
* @param args SSH connection arguments (user, port)
* @return established Session object or null on failure
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(),31,33,"/**
* Initializes the system with default configuration. 
* @param config initial configuration settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseCapacityWeights,"org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)",417,440,"/**
* Determines and returns a set of priority-based weights.
* @param priorityLevels number of priority levels
* @param ns namespace string
* @param conf configuration object
* @return array of positive integer weights or throws exception if invalid
*/","* Read the weights of capacity in callqueue and pass the value to
   * callqueue constructions.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,<init>,"org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",58,111,"/**
* Initializes RPC metrics for the given server and configuration.
* @param server RPC server instance
* @param conf Configuration object with metrics settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseThresholds,"org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)",379,407,"/**
* Converts configuration thresholds to decimal values.
* @param ns namespace
* @param conf Configuration object
* @param numLevels number of levels in the decay scheduler
* @return array of decimal threshold values or an empty array if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,<init>,"org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",56,79,"/**
* Initializes Weighted Round Robin Multiplexer with numQueues and weights.
* @param aNumQueues number of queues
* @param ns namespace for configuration
* @param conf configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffResponseTimeThreshold,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)",433,456,"/**
* Retrieves response time thresholds from configuration.
* @param ns namespace
* @param conf Configuration object
* @return array of response time thresholds in milliseconds or null if empty
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterInitializers,org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration),894,916,"/**
* Initializes filter instances from configuration.
*@param conf Hadoop Configuration object
*@return FilterInitializer array or null if failed to initialize
*/",Get an array of FilterConfiguration specified in the conf,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInstances,"org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)",2787,2798,"/**
* Retrieves a list of objects implementing the specified interface.
* @param name filter string
* @param xface target interface class
* @return List of objects implementing xface or null if none found
*/","* Get the value of the <code>name</code> property as a <code>List</code>
   * of objects implementing the interface specified by <code>xface</code>.
   * 
   * An exception is thrown if any of the classes does not exist, or if it does
   * not implement the named interface.
   * 
   * @param name the property name.
   * @param xface the interface implemented by the classes named by
   *        <code>name</code>.
   * @param <U> Interface class type.
   * @return a <code>List</code> of objects implementing <code>xface</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,<init>,"org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",60,73,"/**
* Initializes the DefaultStringifier with a Configuration and Class.
* @param conf configuration object
* @param c class to serialize/deserialize
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,"org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)",1292,1354,"/**
* Configures and initializes the compression pipeline.
* @param config configuration object
* @param outStream output stream to write compressed data
* @param ownStream whether this method owns the output stream
* @param key class of the key to serialize
* @param val class of the value to serialize
* @param compCodec optional compression codec to use
* @param meta metadata object
* @param syncIntervalVal synchronization interval value
*/",Initialize.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,getFactory,org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration),330,335,"/**
* Returns a serialization factory instance, initializing it if necessary.
* @param conf Hadoop configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/IngressPortBasedResolver.java,setConf,org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration),65,79,"/**
* Configures and initializes the port-to-QOP mapping.
* @param conf Configuration object containing ingress port properties
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,setConf,org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration),91,109,"/**
* Initializes configuration with SASL settings.
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",159,166,"/**
* Calls FutureIO's m1 with provided arguments.
* @param builder FSBuilder instance
* @param conf Hadoop configuration
* @param prefix string prefix
* @param mandatory whether operation is mandatory
*/","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, boolean)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",330,340,"/**
* Configures FSBuilder with optional and mandatory prefixes.
* @param builder the FSBuilder to configure
* @param conf configuration settings
* @param optionalPrefix prefix for optional data
* @param mandatoryPrefix prefix for mandatory data
*/","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * See {@link #propagateOptions(FSBuilder, Configuration, String, boolean)}.
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",99,103,"/**
* Calculates property changes between two configurations.
* @param newConf new configuration instance
* @param oldConf previous configuration instance
* @return Collection of PropertyChange objects or empty collection if no changes
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doGet,"org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",199,212,"/**
* Handles FUNC_MASK requests by fetching and rendering user profile data.
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeLibraryChecker.java,main,org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]),45,156,"/**
* Validates native libraries and winutils on Windows.
* @param args command line arguments, can be -a to check all or -h for help
*/","* A tool to test native library availability.
   * @param args args.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getCompressorType,org.apache.hadoop.io.compress.BZip2Codec:getCompressorType(),137,140,"/**
* Returns compressor class based on configuration.
* @return Compressor subclass (e.g. Bzip2Compressor)","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getDecompressorType,org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType(),218,221,"/**
* Returns decompression factory instance based on configuration.
* @return Class of Decompressor implementation (e.g. Bzip2Decompressor)","* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createDecompressor,org.apache.hadoop.io.compress.BZip2Codec:createDecompressor(),228,231,"/**
* Returns the decompression mask using Bzip2 factory.","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping:reloadCachedMappings(),82,86,"/**
* Calls superclass method m1(), followed by calling and chaining method m1() on the result of method m2().
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List),162,167,"/**
 * Calls itself recursively with an empty list of names. 
 * @param names list of user names (currently unused)
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String),510,512,"/**
 * Constructs HadoopZookeeperFactory instance with the given ZooKeeper principal.
 * @param zkPrincipal ZooKeeper principal to use
 */","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,lookupGroup,"org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)",438,475,"/**
* Retrieves user functional masks by ID.
* @param result SearchResult object
* @param c DirContext instance
* @param goUpHierarchy flag to traverse hierarchy
* @return Set of String group identifiers or empty set if not found
*/","* Perform the second query to get the groups of the user.
   *
   * If posixGroups is enabled, use use posix gid/uid to find.
   * Otherwise, use the general group member attribute to find it.
   *
   * @param result the result object returned from the prior user lookup.
   * @param c the context object of the LDAP connection.
   * @return a list of strings representing group names of the user.
   * @throws NamingException if unable to find group names",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,main,org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[]),87,93,"/**
* Processes command-line arguments and prints converted Kerberos names.
* @param args array of user IDs to process
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration),99,103,"/**
* Creates a CryptoCodec instance based on the specified configuration.
* @param conf Configuration object with cipher suite settings
*/","* Get crypto codec for algorithm/mode/padding in config value
   * hadoop.security.crypto.cipher.suite
   * 
   * @param conf
   *          the configuration
   * @return CryptoCodec the codec object Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)",685,688,"/**
 * Initializes server socket with specified address and backlog.
 * @param socket ServerSocket instance
 * @param address InetSocketAddress for binding
 * @param backlog maximum queue size
 */","* A convenience method to bind to a given address and report 
   * better exceptions if the address is not a valid host.
   * @param socket the socket to bind
   * @param address the address to bind to
   * @param backlog the number of connections allowed in the queue
   * @throws BindException if the address can't be bound
   * @throws UnknownHostException if the address isn't a valid host name
   * @throws IOException other random errors from bind",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)",3642,3645,"/**
* Writes a property value to the output stream.
* @param propertyName name of the property to write
* @param out output stream writer
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)",3787,3804,"/**
* Dumps property value to output writer.
* @param config configuration object
* @param propertyName name of the property to dump
* @param out output writer
*/","*  Writes properties and their attributes (final and resource)
   *  to the given {@link Writer}.
   *  <ul>
   *  <li>
   *  When propertyName is not empty, and the property exists
   *  in the configuration, the format of the output would be,
   *  <pre>
   *  {
   *    ""property"": {
   *      ""key"" : ""key1"",
   *      ""value"" : ""value1"",
   *      ""isFinal"" : ""key1.isFinal"",
   *      ""resource"" : ""key1.resource""
   *    }
   *  }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is null or empty, it behaves same as
   *  {@link #dumpConfiguration(Configuration, Writer)}, the
   *  output would be,
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is not empty, and the property is not
   *  found in the configuration, this method will throw an
   *  {@link IllegalArgumentException}.
   *  </li>
   *  </ul>
   *  <p>
   * @param config the configuration
   * @param propertyName property name
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException when property name is not
   *   empty and the property is not found in configuration
   *",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,formatZK,"org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)",282,299,"/**
* Configures function mask based on force and interactive flags.
* @param force whether to override existing configuration
* @param interactive whether running in interactive mode
* @return error code (0 for success, ERR_CODE_FORMAT_DENIED otherwise)
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSystemSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource(),561,567,"/**
* Initializes system source metrics adapter with configuration.
* @param prefix metric prefix
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)",221,243,"/**
* Builds and registers metrics with provided name, description, and source.
* @param name metric name
* @param desc metric description
* @param source underlying data object
* @return original source object unchanged
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path),808,811,"/**
* Checks if a file exists in the specified directory.
* @param f Path to the directory to check
*/","* Creates the specified directory hierarchy. Does not
   * treat existence as an error.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",813,816,"/**
* Checks if path matches given file system permissions.
* @param f Path object to check
* @param permission FsPermission value to match against
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build(),728,748,"/**
* Generates FSDataOutputStream with specified flags and options.
* @return FSDataOutputStream object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/BouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
* Initializes provider with URI and configuration.
* @param uri unique identifier of the key store
* @param conf application configuration settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
* Initializes a new instance of the Java Key Store provider.
* @param uri URI of the key store
* @param conf Configuration settings for the provider
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
* Initializes key store provider with URI and configuration.",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalBouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
* Initializes a new instance of LocalBouncyCastleFipsKeyStoreProvider with the specified URI and configuration.
* @param uri URI for key store location
* @param conf Key store provider configuration
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initSpnego,"org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)",1356,1375,"/**
* Configures Kerberos authentication filter with principal and keytab from configuration.
* @param conf Configuration object
* @param hostName Host name for principal construction
* @param authFilterConfigurationPrefixes Prefixes for auth filter config properties
* @param usernameConfKey Key for user principal in config
* @param keytabConfKey Key for HTTP keytab in config
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,getFilterConfigMap,"org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)",66,91,"/**
* Configures filter properties with the given prefix.
* @param conf Configuration object
* @param prefix property name prefix
* @return Map of filtered configuration properties
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,createCuratorClient,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)",185,241,"/**
* Creates a CuratorFramework instance with custom configuration and Zookeeper settings.
* @param conf application Configuration
* @param namespace Zookeeper namespace
* @return initialized CuratorFramework object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setJaasConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig),585,597,"/**
* Configures ZK client with Kerberos authentication.
* @param zkClientConfig ZK client configuration
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerPrincipal,org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),303,356,"/**
* Validates and fetches server's Kerberos principal name.
* @param authType SaslAuth object
* @return Server's Kerberos principal name or null if not found
*/","* Get the remote server's principal.  The value will be obtained from
   * the config and cross-checked against the server's advertised principal.
   * 
   * @param authType of the SASL client
   * @return String of the server's principal
   * @throws IOException - error determining configured principal",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,initProtocolMetaInfo,org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration),1200,1209,"/**
* Initializes RPC service for Protocol Meta Info.
* @param conf Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",231,242,"/**
* Creates a KeyProvider instance based on the provided token and configuration.
* @param token authentication token
* @param conf system configuration
* @return KeyProvider object or null if creation fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",100,140,"/**
* Initializes a LoadBalancingKMSClientProvider instance.
* @param uri the token service URI
* @param providers array of KMS client providers
* @param seed random seed for shuffling (0 for deterministic behavior)
* @param conf configuration object containing retry policy settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeHarURI,"org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)",218,254,"/**
* Reconstructs a valid URI from the given raw URI, applying HARP (HTTP Archive Protocol) rules.
*@param rawURI input URI to process
*@return reconstructed URI or null if not applicable
*/","* decode the raw URI to get the underlying URI
   * @param rawURI raw Har URI
   * @return filtered URI of the underlying fileSystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration),288,290,"/**
* Wraps m1 output in a new FileSystem instance.
* @param conf Configuration object
*/","* Returns the configured FileSystem implementation.
   * @param conf the configuration to use
   * @return FileSystem.
   * @throws IOException If an I/O error occurred.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,initialize,"org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",339,350,"/**
* Configures function mask settings based on provided URI and configuration.
* @param name URI to determine scheme from
* @param conf Configuration object for resolving symlinks
*/","* Initialize a FileSystem.
   *
   * Called after the new FileSystem instance is constructed, and before it
   * is ready for use.
   *
   * FileSystem implementations overriding this method MUST forward it to
   * their superclass, though the order in which it is done, and whether
   * to alter the configuration before the invocation are options of the
   * subclass.
   * @param name a URI whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration
   * @throws IOException on any failure to initialize this instance.
   * @throws IllegalArgumentException if the URI is considered invalid.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration),621,623,"/**
 * Wraps the output of m1() in an HDFS filesystem. 
 * @param conf Hadoop configuration
 */","* Returns a unique configured FileSystem implementation for the default
   * filesystem of the supplied configuration.
   * This always returns a new FileSystem object.
   * @param conf the configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkPath,org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path),792,825,"/**
* Validates and normalizes a file system path.
* @param path the path to validate
*/","* Check that a Path belongs to this FileSystem.
   *
   * The base implementation performs case insensitive equality checks
   * of the URIs' schemes and authorities. Subclasses may implement slightly
   * different checks.
   * @param path to check
   * @throws IllegalArgumentException if the path is not considered to be
   * part of this FileSystem.
   *",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)",2539,2556,"/**
* Resolves InetSocketAddress from properties or defaults.
* @param hostProperty property for hostname
* @param addressProperty property for address (may override hostname)
* @param defaultAddressValue default address value if not set
* @param defaultPort default port number
* @return InetSocketAddress object with resolved values","* Get the socket address for <code>hostProperty</code> as a
   * <code>InetSocketAddress</code>. If <code>hostProperty</code> is
   * <code>null</code>, <code>addressProperty</code> will be used. This
   * is useful for cases where we want to differentiate between host
   * bind address and address clients should use to establish connection.
   *
   * @param hostProperty bind host property name.
   * @param addressProperty address property name.
   * @param defaultAddressValue the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FutureDataInputStreamBuilder.java,build,org.apache.hadoop.fs.FutureDataInputStreamBuilder:build(),47,50,"/**
* Opens and returns a stream to the function mask data.
* @throws Exception if operation fails or is unsupported
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,open,"org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)",507,539,"/**
* Opens an SFTP input stream for a file.
* @param f the file path
* @param bufferSize buffer size
* @return FSDataInputStream object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,create,"org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",545,589,"/**
* Creates an FSDataOutputStream for writing to a remote file.
* @param f Path to the remote file
* @return FSDataOutputStream instance or null if creation fails
*/","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",603,612,"/**
* Transfers file from source to destination via SFTP.
* @param src source file path
* @param dst destination file path
* @return true if transfer is successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",614,623,"/**
* Recursively synchronizes a remote file with the local file system.
* @param f Path to remote file
* @param recursive whether to synchronize recursively
* @return true if synchronization was successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path),625,634,"/**
* Retrieves file statuses from an SFTP server.
* @param f Path to the file or directory
* @return Array of FileStatus objects or null if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(),657,673,"/**
* Retrieves the SFTP home directory path.
* @return Path to home directory or null on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",688,697,"/**
* Executes SFTP command on the specified file with given permissions.
* @param f Path to file
* @param permission File system permissions
* @return true if operation was successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),699,708,"/**
* Retrieves file status for the given path using SFTP protocol.
* @param f Path to the file
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,read,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)",230,246,"/**
* Reads from input stream and performs checksum verification.
* @param position current read position
* @param b data buffer
* @param off offset into buffer
* @param len length to read
* @return number of bytes read or 0 if end-of-stream reached
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",91,96,"/**
* Initializes a new DataInputStreamBuilder instance from the specified file system and path.
* @param fileSystem File system to operate on
* @param path Path to initialize builder with
*/","* Constructor.
   * @param fileSystem owner FS.
   * @param path path",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",103,108,"/**
* Initializes data input stream builder with given file system and path handle.
* @param fileSystem non-null file system instance
* @param pathHandle non-null path handle instance
*/","* Constructor with PathHandle.
   * @param fileSystem owner FS.
   * @param pathHandle path handle",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFileOnInstance,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",472,498,"/**
* Opens file for reading based on provided read policies.
* @param instance DynamicWrappedIO instance
* @param fs FileSystem object
* @param status FileStatus object
* @param readPolicies string specifying read policies
* @return FSDataInputStream object or null if failed
*/","* Open a file.
   * <p>
   * If the WrappedIO class is found, uses
   * {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)} with
   * {@link #PARQUET_READ_POLICIES} as the list of read policies and passing down
   * the file status.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param instance dynamic wrapped IO instance.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getInputStreamForFile,org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile(),63,66,"/**
* Retrieves a file stream based on previously computed masks.
* @throws IOException if an I/O error occurs while accessing files.",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])",291,298,"/**
* Retrieves file system permissions for the specified path using a given password.
* @param p the Path object
* @param password security credentials
* @return FsPermission object or throws IOException and other exceptions if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkAppend,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem),483,495,"/**
* Checks if file system allows appending operations.
* @param fs FileSystem object to check
* @return true if append allowed, false otherwise
*/","* Test whether the file system supports append and return the answer.
   *
   * @param fs the target file system",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2934,2937,"/**
* Constructs a Sorter instance with specified parameters.
* @param fs FileSystem object
* @param comparator RawComparator for sorting
* @param keyClass Class of the key data type
* @param valClass Class of the value data type
* @param conf Configuration object
*/","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration),72,76,"/**
* Initializes Bzip2 compressor with configuration.
* @param conf Hadoop Configuration object
*/","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reinit,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration),108,122,"/**
* Reinitializes the compressor based on provided configuration.
* @param conf Compression configuration object
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's block size and
   * and work factor.
   * 
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
* Creates and returns an OutputStream that masks the original stream.
* @param downStream original output stream
* @param compressor compression object
* @param downStreamBufferSize buffer size for downstream stream
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,org.apache.hadoop.io.SequenceFile$Reader:init(boolean),2022,2161,"/**
* Initializes and reads SequenceFile metadata.
* @param tempReader whether to read temporary data
*/","* Initialize the {@link Reader}
     * @param tmpReader <code>true</code> if we are constructing a temporary
     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, 
     *                  and hence do not initialize every component; 
     *                  <code>false</code> otherwise.
     * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
* Decompresses data from provided InputStream.
* @param downStream input stream to decompress
* @param decompressor Decompressor instance
* @param downStreamBufferSize buffer size for downstream processing
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reinit,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration),112,120,"/**
* Reinitializes the compressor based on provided configuration.
* @param conf Compression configuration
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   *
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration),94,99,"/**
* Calculates buffer size based on configuration.
* @param conf Configuration object
* @return Buffer size or ZStandard compressor value if zero
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration),101,106,"/**
* Calculates function mask value based on configuration.
* @param conf configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,writeStreamToFile,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)",499,512,"/**
* Writes input to target file with optional lazy persistence and direct mode.
* @param in InputStream containing data
* @param target PathData object for output location
* @param lazyPersist whether to persist lazily
* @param direct whether to operate directly (true) or not (false)
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,printToStdout,org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream),98,104,"/**
* Streams data from input stream to output using M3 protocol.
* @param in input stream containing data to be processed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendValue,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int),554,575,"/**
* Creates a DataOutputStream for writing a value of specified length.
* @param length length of the value to be written (or -1 for dynamic length)
* @return DataOutputStream instance or null if an error occurs
*/","* Obtain an output stream for writing a value into TFile. This may only be
     * called right after a key appending operation (the key append stream must
     * be closed).
     * 
     * @param length
     *          The expected length of the value. If length of the value is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream. Advertising the value size up-front
     *          guarantees that the value is encoded in one chunk, and avoids
     *          intermediate chunk buffering.
     * @throws IOException raised on errors performing I/O.
     * @return DataOutputStream.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)",496,513,"/**
* Initializes RBlockState with compression algorithm and FS input stream.
* @param compressionAlgo compression algorithm to use
* @param fsin file system input stream
* @param region block region metadata
* @param conf configuration settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)",119,139,"/**
* Creates a WBlockState object with the specified compression algorithm and output streams.
* @param compressionAlgo Algorithm instance for compression
* @param fsOut FSDataOutputStream to write compressed data to
* @param fsOutputBuffer BytesWritable buffer for compressed data
* @param conf Configuration object for file system settings
*/","* @param compressionAlgo
       *          The compression algorithm to be used to for compression.
       * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),135,139,"/**
* Calls parent and child methods to perform initialization.
* @param conf configuration object passed to both calls
*/","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),130,138,"/**
* Initializes configuration for M1 operation.
* @param conf Configuration object
*/","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpsChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",583,632,"/**
* Configures SSL/TLS server connector with custom settings.
* @param server the server instance
* @param httpConfig HTTP configuration object
* @return configured ServerConnector instance
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String),251,264,"/**
* Retrieves a new token with given renewer.
* @param renewer the entity performing the renewal
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),266,274,"/**
* Executes M1 operation on the KMS client.
* @param token security token
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),276,285,"/**
* Executes a KMS client operation using the provided token.
* @param token Token object for authentication
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String),326,344,"/**
* Retrieves an encrypted key version using the given encryption key name.
* @param encryptionKeyName name of the encryption key to use
* @return EncryptedKeyVersion object or throws an exception if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),346,364,"/**
* Decrypts and retrieves a KeyVersion using the provided encrypted key.
* @param encryptedKeyVersion Encrypted key to decrypt
* @return Decrypted KeyVersion object or throws exception if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),366,384,"/**
* Executes M2 operation on the specified EncryptedKeyVersion.
* @param ekv Encrypted key version to process
* @return EncryptedKeyVersion result or throws an exception if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List),386,404,"/**
* Executes encryption key version operations on the KMS client.
* @param ekvs list of encrypted key versions to process
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String),406,414,"/**
* Retrieves a KeyVersion object for the specified version name.
* @param versionName name of the key version to fetch
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys(),416,424,"/**
* Executes the M1 function with KMS client provider and caches result.
* @throws IOException if an error occurs during execution
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[]),426,434,"/**
* Retrieves metadata for the given names.
* @param names variable number of name strings
* @return array of metadata objects or null on exception
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String),436,445,"/**
* Executes KMSClientProvider's m1 method with the given name.
* @param name user-provided string value
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String),447,455,"/**
* Calls m1 on KMS client provider with the given name.
* @param name user-provided identifier
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String),457,465,"/**
* Retrieves metadata using the provided KMS client.
* @param name metadata identifier
* @return Metadata object or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",467,476,"/**
* Executes M1 operation with provided name, material and options.
* @param name unique identifier or name of the key
* @param material byte array for encryption/decryption
* @param options configuration parameters for the operation
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",478,495,"/**
* Retrieves a KeyVersion using the KMSClientProvider.
* @param name key identifier
* @param options configuration options
* @throws NoSuchAlgorithmException if algorithm not found
* @throws IOException on I/O error
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,options,org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration),433,435,"/**
* Creates an instance of Options from a Configuration object.
* @param conf Configuration object to initialize options with
* @return Initialized Options object
*/","* A helper function to create an options object.
   * @param conf the configuration to use
   * @return a new options object",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",142,145,"/**
* Initializes a cryptographically secure input stream with given parameters.
* @param in input stream to encrypt
* @param codec encryption algorithm configuration
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)",130,135,"/**
* Constructs a CryptoOutputStream instance with the given parameters.
* @param out OutputStream to be wrapped
* @param codec CryptoCodec instance for encryption/decryption
* @param key Encryption key
* @param iv Initialization vector
* @param streamOffset Stream offset (in bytes)
* @param closeOutputStream Whether to close the underlying OutputStream",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1679,1709,"/**
* Initializes ConnectionId with given parameters.
* @param address InetSocketAddress for connection
* @param protocol Class of communication protocol
* @param ticket UserGroupInformation for authentication
* @param rpcTimeout RPC timeout in milliseconds
* @param connectionRetryPolicy Retry policy for connections
* @param conf Configuration object for client settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getTimeout,org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration),202,213,"/**
* Calculates function mask based on configuration.
* @param conf Configuration object
* @return Function mask value or -1 if not found
*/","* The time after which a RPC will timeout.
   * If ping is not enabled (via ipc.client.ping), then the timeout value is the 
   * same as the pingInterval.
   * If ping is enabled, then there is no timeout value.
   * 
   * @param conf Configuration
   * @return the timeout period in milliseconds. -1 if no timeout value is set
   * @deprecated use {@link #getRpcTimeout(Configuration)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,parseMetaData,org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData(),1166,1236,"/**
* Processes master and archive indexes to fetch store metadata.
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,<init>,"org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",61,80,"/**
* Initializes FailoverController with configuration and request source.
* @param conf Hadoop Configuration
* @param source RequestSource instance
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,connect,org.apache.hadoop.fs.ftp.FTPFileSystem:connect(),141,167,"/**
* Establishes an FTP connection using configuration settings.
* @throws IOException if login or other operations fail
*/","* Connect to the FTP server using configuration parameters *
   * 
   * @return An FTPClient instance
   * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getPositiveLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)",61,69,"/**
* Returns a long value for the given key, using a default value if it's negative.
* @param key unique identifier
* @param defVal default value to use when key value is invalid
* @return valid long value or default value
*/","* Get a long value with resilience to unparseable values.
   * Negative values are replaced with the default.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(),1282,1286,"/**
* Calls underlying fs.m1() function. 
* @return result of fs.m1() as a long value.",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(),939,954,"/**
* Creates FsServerDefaults object with checksum and buffer settings.
* @throws IOException if configuration cannot be loaded
*/","* Return a set of server default configuration values.
   * @return server default configuration values
   * @throws IOException IO failure
   * @deprecated use {@link #getServerDefaults(Path)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),2766,2768,"/**
* Returns a constant value equivalent to calling m1() with no parameters.
* This is likely a wrapper around another function call.","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.  The given path will be used to
   * locate the actual filesystem.  The full path does not have to exist.
   * @param f path of file
   * @return the default block size for the path's filesystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(),426,429,"/**
* Calls method m1() from FileService instance.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,reportChecksumFailure,"org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",99,149,"/**
* Moves a bad file to the designated 'bad_files' directory.
* @param p Path to the bad file
*/","* Moves files to a bad file directory on the same device, so that their
   * storage will not be reused.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),43,48,"/**
* Initializes DU with cached space used values.
* @param builder CachingGetSpaceUsed configuration
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),60,66,"/**
* Constructs a caching get space used object using the provided builder.
* @param path path to cache
* @param interval time interval for caching
* @param jitter randomization value for caching
* @param initialUsed initial used space value
*/","* This is the constructor used by the builder.
   * All overriding classes should implement this.
   *
   * @param builder builder.
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,<init>,org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),34,40,"/**
* Initializes WindowsGetSpaceUsed with specified parameters.
* @param builder configuration builder instance
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOwner,org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor),934,954,"/**
* Retrieves file owner information using platform-specific methods.
* @param fd FileDescriptor to query
* @return owner username as a string, or null on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration),135,137,"/**
* Constructs ShellBasedIdMapping with given configuration.
* @param conf Hadoop Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initHM,org.apache.hadoop.ha.ZKFailoverController:initHM(),323,328,"/**
* Initializes and configures a HealthMonitor instance.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,<init>,"org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",79,82,"/**
 * Initializes default trash policy with given file system and configuration. 
 * @param fs the file system to operate on
 * @param conf the Hadoop configuration 
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,close,org.apache.hadoop.fs.viewfs.ViewFileSystem:close(),1986,2007,"/**
* Executes M1 operation with optional inner cache and/or external file system traversal.
* @throws IOException on I/O errors
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer),2565,2584,"/**
* Reads compressed sequence file data into the given DataOutputBuffer.
* @param buffer Data output buffer to store read data
* @return Key length or -1 if end of sequence
* @throws IOException on unsupported block-compressed sequences
*/","@deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getDefaultRetryPolicy,"org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)",59,85,"/**
* Returns a RetryPolicy instance based on configuration settings.
* @param conf Hadoop Configuration object
* @param retryPolicyEnabledKey key for enabling/disabling retry policy
* @param defaultRetryPolicyEnabled default enabled state of retry policy
* @param retryPolicySpecKey key for specifying retry policy spec
* @param defaultRetryPolicySpec default retry policy specification
* @param remoteExceptionToRetry exception to retry on remote nodes
* @return RetryPolicy instance or TRY_ONCE_THEN_FAIL if not configured","* Return the default retry policy set in conf.
   * 
   * If the value retryPolicyEnabledKey is set to false in conf,
   * use TRY_ONCE_THEN_FAIL.
   * 
   * Otherwise, get the MultipleLinearRandomRetry policy specified in the conf
   * and then
   * (1) use multipleLinearRandomRetry for
   *     - remoteExceptionToRetry, or
   *     - IOException other than RemoteException, or
   *     - ServiceException; and
   * (2) use TRY_ONCE_THEN_FAIL for
   *     - non-remoteExceptionToRetry RemoteException, or
   *     - non-IOException.
   *     
   *
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @param remoteExceptionToRetry    The particular RemoteException to retry
   * @return the default retry policy.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec(),273,273,"/**
* Returns compression codec mask as an integer value. 
* This is an abstract method and should be implemented by concrete subclasses. 
* @throws IOException if any I/O error occurs during operation
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
* Decompresses input stream using provided decompressor.
* @param downStream input stream to be decompressed
* @param decompressor decompression engine
* @param downStreamBufferSize buffer size for downstream processing
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
* Creates a masked OutputStream to compress data before writing it to the specified output stream.
* @param downStream target output stream
* @param compressor compression algorithm instance
* @param downStreamBufferSize buffer size for compressed data
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,init,"org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)",59,64,"/**
* Returns JVM metrics function mask based on process name and session ID.
* @param processName process identifier
* @param sessionId unique session identifier
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",1349,1351,"/**
* Initializes a new Client instance with default socket factory.
* @param valueClass client data class
* @param conf configuration object
*/","* Construct an IPC client with the default SocketFactory.
   * @param valueClass input valueClass.
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)",50,68,"/**
* Retrieves a cached or newly created Client instance.
* @param conf configuration object
* @param factory socket factory used to create the client
* @return configured Client object
*/","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @param valueClass Class of the expected response
   * @return an IPC client",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration),73,85,"/**
* Initializes the FsUrlStreamHandlerFactory with a Hadoop Configuration.
* @param conf Hadoop configuration
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,createURLStreamHandler,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String),87,111,"/**
* Resolves and returns a URLStreamHandler for the given protocol.
* @param protocol the name of the protocol (e.g. ""file"")
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,excludeIncompatibleCredentialProviders,"org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)",141,199,"/**
* Updates configuration with credential provider path based on file system class.
* @param config existing Configuration object
* @param fileSystemClass target FileSystem class
* @return updated Configuration object or original if no changes
*/","* There are certain integrations of the credential provider API in
   * which a recursive dependency between the provider and the hadoop
   * filesystem abstraction causes a problem. These integration points
   * need to leverage this utility method to remove problematic provider
   * types from the existing provider path within the configuration.
   *
   * @param config the existing configuration with provider path
   * @param fileSystemClass the class which providers must be compatible
   * @return Configuration clone with new provider path
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,get,"org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",263,266,"/**
* Retrieves an instance of AbstractFileSystem based on the provided URI and configuration.
* @param uri unique identifier
* @param conf system-wide configuration settings
*/","* The main factory method for creating a file system. Get a file system for
   * the URI's scheme and authority. The scheme of the <code>uri</code>
   * determines a configuration property name,
   * <tt>fs.AbstractFileSystem.<i>scheme</i>.impl</tt> whose value names the
   * AbstractFileSystem class.
   * 
   * The entire URI and conf is passed to the AbstractFileSystem factory method.
   * 
   * @param uri for the file system to be created.
   * @param conf which is passed to the file system impl.
   * 
   * @return file system for the given URI.
   * 
   * @throws UnsupportedFileSystemException if the file system for
   *           <code>uri</code> is not supported.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,setConf,org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),138,145,"/**
* Updates configuration and initializes combined mapping providers.
* @param conf Configuration object updated with new settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)",179,187,"/**
* Retrieves ProtocolMetaInfoPB object using the provided proxy and configuration.
* @param proxy Object to be proxied
* @param conf Configuration object for RPC operation
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,build,org.apache.hadoop.ipc.RPC$Builder:build(),975,991,"/**
* Initializes Server configuration and returns a configured instance.
* @throws IOException if initialization fails
* @throws HadoopIllegalArgumentException on missing configuration settings
*/","* @return Build the RPC Server.
     * @throws IOException on error
     * @throws HadoopIllegalArgumentException when mandatory fields are not set",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",139,146,"/**
* Creates and configures a TrashPolicy instance based on the provided configuration. 
* @param conf Configuration object
* @return configured TrashPolicy object
*/","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @param home the home directory
   * @return an instance of TrashPolicy
   * @deprecated Use {@link #getInstance(Configuration, FileSystem)} instead.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",156,162,"/**
* Creates and configures a TrashPolicy instance based on configuration settings.
* @param conf Configuration object containing policy settings
* @param fs FileSystem object used for policy initialization
* @return An initialized TrashPolicy instance
*/","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @return an instance of TrashPolicy",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountTableConfigLoader,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration),181,203,"/**
* Loads a custom MountTableConfigLoader instance based on configuration.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getKlass,org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass(),74,89,"/**
* Resolves the function mask for getting space used, considering OS and configuration.
*@return Class<? extends GetSpaceUsed> implementation class
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,"org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)",77,83,"/**
 * Creates a network topology instance based on the provided configuration and factory.
 * @param conf Configuration object
 * @param factory InnerNode factory used to initialize the topology
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)",68,75,"/**
* Resolves domain name using a dynamically loaded resolver class.
* @param conf configuration object
* @param configKey key for resolving the resolver class
*/","* This function gets the instance based on the config.
   *
   * @param conf Configuration
   * @param configKey config key name.
   * @return Domain name resolver.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getInstance,org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration),52,58,"/**
* Resolves SASL properties resolver instance from configuration.
* @param conf Hadoop Configuration object
* @return resolved SaslPropertiesResolver instance or null if not found
*/","* Returns an instance of SaslPropertiesResolver.
   * Looks up the configuration to see if there is custom class specified.
   * Constructs the instance by passing the configuration directly to the
   * constructor to achieve thread safety using final fields.
   * @param conf configuration.
   * @return SaslPropertiesResolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,"org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)",104,153,"/**
* Initializes groups configuration and caching.
* @param conf Hadoop configuration
* @param timer Timer instance for caching and reloading
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateSasl,org.apache.hadoop.security.KDiag:validateSasl(java.lang.String),733,748,"/**
* Resolves and initializes SASL property resolver.
* @param saslPropsResolverKey unique key for the resolver
*/","* Try to load the SASL resolver.
   * @param saslPropsResolverKey key for the SASL resolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getInstance,org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration),47,53,"/**
* Instantiates and returns the configured impersonation provider.
* @param conf Hadoop configuration object
*/","* Returns an instance of ImpersonationProvider.
   * Looks up the configuration to see if there is custom class specified.
   * @param conf
   * @return ImpersonationProvider",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),84,100,"/**
* Initializes and configures the random number generator.
* @param conf Configuration object with security settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,<init>,"org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)",205,209,"/**
* Initializes HookEntry with specified Runnable and priority.
* @param hook  Runnable to be executed
* @param priority execution priority value
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,shutdownExecutor,org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration),142,162,"/**
* Executes executor shutdown with configurable timeout.
* @param conf configuration object
*/","* Shutdown the executor thread itself.
   * @param conf the configuration containing the shutdown timeout setting.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordFromCredentialProviders,"org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",893,906,"/**
* Retrieves user password from configuration or uses default value.
* @param config Configuration object
* @param alias unique alias identifier
* @param defaultPass default password to use if not found in config
* @return Password as String or defaultPass if failed to retrieve
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPassword,org.apache.hadoop.conf.Configuration:getPassword(java.lang.String),2418,2428,"/**
* Retrieves password mask using multiple methods.
* @param name user's unique identifier
* @return password mask as a char array or null if not found
*/","* Get the value for a known password configuration element.
   * In order to enable the elimination of clear text passwords in config,
   * this method attempts to resolve the property name as an alias through
   * the CredentialProvider API and conditionally fallsback to config.
   * @param name property name
   * @return password
   * @throws IOException when error in fetching password",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",131,137,"/**
* Creates an instance of RawErasureEncoder based on configuration and codec.
* @param conf configuration object
* @param codec encoding format (e.g., ""default"")
* @param coderOptions options for erasure coding
* @return RawErasureEncoder instance or null if invalid input","* Create RS raw encoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw encoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",146,152,"/**
* Creates and returns a RawErasureDecoder instance based on the provided configuration and codec.
* @param conf Configuration object
* @param codec Codec name
* @param coderOptions ErasureCoder options
*/","* Create RS raw decoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw decoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,tryFence,"org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",80,115,"/**
* Attempts to establish a connection and perform fencing on the specified HA target.
* @param target HAServiceTarget object
* @param argsStr string representation of Args object containing user credentials and other parameters
* @return true if successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,isProxyServer,org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String),47,52,"/**
* Checks if given IP address is masked by a proxy server.
* @param remoteAddr IP address to check
* @return true if masked, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,<init>,"org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)",78,96,"/**
* Initializes CallQueueManager with specified configuration.
* @param backingClass queue implementation class
* @param schedulerClass scheduler implementation class
* @param clientBackOffEnabled enable client back-off feature
* @param maxQueueSize maximum queue size
* @param namespace name of the call queue
* @param conf Hadoop Configuration object",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,swapQueue,"org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",464,495,"/**
* Updates and replaces the RpcScheduler instance with a new one, 
* initializing a BlockingQueue with custom settings.
*@param schedulerClass class of the new scheduler
*@param queueClassToUse class of the blocking queue
*@param maxSize maximum size of the queue
*@param ns namespace for configuration
*@param conf application configuration","* Replaces active queue with the newly requested one and transfers
   * all calls to the newQ before returning.
   *
   * @param schedulerClass input schedulerClass.
   * @param queueClassToUse input queueClassToUse.
   * @param maxSize input maxSize.
   * @param ns input ns.
   * @param conf input configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,create,"org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",115,118,"/**
* Creates RpcMetrics instance with server and configuration.
* @param server server object
* @param conf configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)",119,154,"/**
* Initializes FairCallQueue with specified configuration.
* @param priorityLevels number of priority levels
* @param capacity total queue capacity
* @param ns namespace for metrics
* @param capacityWeights weights for each queue's capacity
* @param serverFailOverEnabled flag to enable server failover
* @param conf configuration object
*/","* Create a FairCallQueue.
   * @param priorityLevels the total size of all multi-level queue
   *                       priority policies
   * @param capacity the total size of all sub-queues
   * @param ns the prefix to use for configuration
   * @param capacityWeights the weights array for capacity allocation
   *                        among subqueues
   * @param serverFailOverEnabled whether or not to enable callqueue overflow trigger failover
   *                              for stateless servers when RPC call queue is filled
   * @param conf the configuration to read from
   * Notes: Each sub-queue has a capacity of `capacity / numSubqueues`.
   * The first or the highest priority sub-queue has an excess capacity
   * of `capacity % numSubqueues`",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initializeWebServer,"org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])",726,795,"/**
* Configures web server with specified settings and handlers.
* @param name application name
* @param hostName HTTP host name
* @param pathSpecs additional path specifications (not used)
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseCostProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",281,309,"/**
* Retrieves a CostProvider instance for the given namespace.
* @param ns namespace string
* @param conf Configuration object
* @return CostProvider instance or DefaultCostProvider if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseIdentityProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",312,337,"/**
* Retrieves IdentityProvider instance based on configuration and namespace.
* @param ns namespace string
* @param conf configuration object
* @return IdentityProvider instance or default user provider if not specified
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,store,"org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)",110,117,"/**
* Serializes and stores an item in the configuration.
* @param conf Configuration object
* @param item Item to serialize (generic type K)
* @param keyName Unique identifier for the stored item
*/","* Stores the item in the configuration with the given keyName.
   * 
   * @param <K>  the class of the item
   * @param conf the configuration to store
   * @param item the object to be stored
   * @param keyName the name of the key to use
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,load,"org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",130,140,"/**
* Retrieves a serialized object of type K from the configuration.
* @param conf Configuration object
* @param keyName unique key name
* @param itemClass class type of the returned object
* @return K object or null if not found
*/","* Restores the object from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,storeArray,"org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)",153,171,"/**
* Configures key-value pairs in Hadoop Configuration.
* @param conf Hadoop configuration
* @param items array of values to configure
* @param keyName name of the key to set
*/","* Stores the array of items in the configuration with the given keyName.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use 
   * @param items the objects to be stored
   * @param keyName the name of the key to use
   * @throws IndexOutOfBoundsException if the items array is empty
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,loadArray,"org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",184,203,"/**
* Retrieves an array of items of type K from configuration using keyName and itemClass.
* @param conf Configuration object
* @param keyName unique identifier for the items in the config
* @param itemClass class of the items to be retrieved
* @return Array of items of type K or null on error","* Restores the array of objects from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1257,1266,"/**
* Initializes a Writer object with the given configuration and file system.
* @param fs FileSystem instance
* @param conf Configuration settings
* @param name Path to write data to
* @param keyClass Class of key data type
* @param valClass Class of value data type
* @param bufferSize Buffer size for I/O operations
* @param replication Replication factor for data
* @param blockSize Block size for I/O operations
* @param progress Progressable instance for tracking progress
* @param metadata Metadata associated with the write operation
*/","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param bufferSize input bufferSize.
     * @param replication input replication.
     * @param blockSize input blockSize.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,copy,"org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)",346,361,"/**
* Serializes and deserializes data of type T between src and dst using a custom serialization framework.
* @param conf Configuration object for serialization
* @param src Source object to serialize
* @param dst Destination object to deserialize into
* @return Deserialized destination object or null on failure
*/","* Make a copy of the writable object using serialization to a buffer.
   * @param src the object to copy from
   * @param dst the object to copy into, which is destroyed
   * @param <T> Generics Type.
   * @param conf configuration.
   * @return dst param (the copy)
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",140,149,"/**
* Wraps {@link FutureIO#m1(FSBuilder, Configuration, String, String)} call.
* @param builder FSBuilder instance
* @param conf configuration object
* @param optionalPrefix prefix for optional parameters
* @param mandatoryPrefix prefix for mandatory parameters
*/","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, String)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,run,org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run(),116,162,"/**
* Reconfigures the system based on provided configuration changes.
* @param none
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,doGetGroups,"org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)",509,557,"/**
* Fetches user's group membership by ID.
* @param user unique user identifier
* @param goUpHierarchy flag to include parent groups (0 = no, > 0 = yes)
* @return Set of group DNs or an empty set if not found
*/","* Perform LDAP queries to get group names of a user.
   *
   * Perform the first LDAP query to get the user object using the user's name.
   * If one-query is enabled, retrieve the group names from the user object.
   * If one-query is disabled, or if it failed, perform the second query to
   * get the groups.
   *
   * @param user user name
   * @return a list of group names for the user. If the user can not be found,
   * return an empty string array.
   * @throws NamingException if unable to get group names",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String),285,305,"/**
* Generates an EncryptedKeyVersion by re-encrypting the specified key.
* @param encryptionKeyName name of the encryption key to use
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List),356,408,"/**
* Encrypts and decrypts key versions in the input list using provided keys.
* @param ekvs List of EncryptedKeyVersion objects to process
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),433,457,"/**
* Decrypts and returns a KeyVersion object.
* @param encryptedKeyVersion Encrypted key version to decrypt
* @return Decrypted KeyVersion object or throws an exception if decryption fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer),3593,3595,"/**
 * Writes output to the specified writer.
 * @param out target output writer
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)",95,105,"/**
* Writes configuration property with specified format.
* @param conf Hadoop Configuration object
* @param out Writer to output data
* @param format Output format (JSON or XML)
* @param propertyName Property name to write
*/",* Guts of the servlet - extracted for easy testing.,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources(),539,543,"/**
* Initializes filter and config settings.
* @param sourceFilter updated filter value
* @param sourceConfigs list of source configurations
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",555,568,"/**
* Creates a streaming output file with specified parameters.
* @param f Path of the file to be created
* @param overwrite whether to overwrite existing file (if any)
* @param createParent whether to create parent directories (if needed)
* @param bufferSize buffer size for writing data
* @param replication block replication factor
* @param blockSize block size in bytes
* @param progress progress monitor object
* @param permission file permissions
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1179,1202,"/**
* Creates a symbolic link from the given target and link paths.
* @param target source file path
* @param link destination link path
* @param createParent whether to create parent directory if missing
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterProperties,"org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)",872,886,"/**
* Builds a Properties object by merging filter configurations from provided prefixes.
* @param conf global configuration
* @param prefixes list of prefix filters
* @return merged Properties object or null if failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,createFilterConfig,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration),42,51,"/**
* Initializes function mask configuration.
* @param conf Configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",57,64,"/**
* Configures authentication filter in the given FilterContainer.
* @param container target FilterContainer
* @param conf configuration object
*/","* Initializes hadoop-auth AuthenticationFilter.
   * <p>
   * Propagates to hadoop-auth AuthenticationFilter configuration all Hadoop
   * configuration properties prefixed with ""hadoop.http.authentication.""
   *
   * @param container The filter container
   * @param conf Configuration for run-time parameters",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),159,183,"/**
* Initializes ZKDelegationTokenSecretManager with configuration settings.
* @param conf configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)",555,576,"/**
* Creates a ZooKeeper client with custom configuration.
* @param connectString zookeeper connection string
* @param sessionTimeout zookeeper session timeout in milliseconds
* @param watcher zookeeper event listener
* @param canBeReadOnly whether to allow read-only connections
* @param zkClientConfig custom zookeeper client configuration
* @return ZooKeeper client instance or throws exception on failure",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,createSaslClient,org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),211,270,"/**
* Creates a SASL client for authentication based on the provided auth type.
* @param authType SaslAuth object containing authentication details
* @return SaslClient instance or null if authentication fails
*/","* Try to create a SaslClient for an authentication type.  May return
   * null if the type isn't supported or the client lacks the required
   * credentials.
   * 
   * @param authType - the requested authentication method
   * @return SaslClient for the authType or null
   * @throws SaslException - error instantiating client
   * @throws IOException - misc errors",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renew,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",189,208,"/**
* Retrieves a new delegation token using the provided key provider.
* @param token Token to renew
* @param conf Configuration object
* @return New token ID or throws IOException on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancel,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",210,229,"/**
* Cancels a delegation token.
* @param token Token to cancel
* @param conf Configuration
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)",89,92,"/**
* Creates a LoadBalancingKMSClientProvider instance.
* @param providerUri URI of the load balancing KMS client provider
* @param providers array of KMS client providers to balance
* @param conf configuration object for this instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",94,98,"/**
* Constructs a LoadBalancingKMSClientProvider instance for testing.
* @param providers array of KMS client providers
* @param seed random seed value
* @param conf configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getFS,org.apache.hadoop.fs.FsShell:getFS(),79,84,"/**
* Returns the file system instance, initializing it if not already created.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,initialize,"org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",493,500,"/**
* Initializes and configures the object with URI information.
* @param uriInfo URI containing configuration details
* @param conf overall system configuration
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,<init>,"org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",49,57,"/**
* Initializes a DelegateToFileSystem instance with given configuration.
* @param theUri URI of the file system
* @param theFsImpl implementation of the file system
* @param supportedScheme supported scheme for the file system
* @param authorityRequired whether authority is required for access
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes M1 with URI and configuration.
* @param name unique identifier URI
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,initialize,"org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",102,133,"/**
* Configures FTP connection parameters from URI and Configuration.
* @param uri FTP connection URI
* @param conf Configuration object to store settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,initialize,"org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",130,135,"/**
* Initializes and configures Hadoop mapper.
* @param uri input URI for data processing
* @param conf Hadoop job configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFileSystem,"org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",3604,3629,"/**
* Creates a FileSystem instance based on the provided URI and configuration.
* @param uri unique file system identifier
* @param conf Hadoop Configuration object
* @return initialized FileSystem object or null if creation fails
*/","* Create and initialize a new instance of a FileSystem.
   * @param uri URI containing the FS schema and FS details
   * @param conf configuration to use to look for the FS instance declaration
   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.
   * @return the initialized filesystem.
   * @throws IOException problems loading or initializing the FileSystem",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",310,384,"/**
* Initializes and configures the ViewFs instance.
* @param theUri URI to initialize with
* @param conf configuration options
*/","* Called after a new FileSystem instance is constructed.
   * @param theUri a uri whose authority section names the host, port, etc. for
   *        this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,createFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",277,291,"/**
* Initializes and returns a file system instance based on the provided URI.
* @param uri unique identifier for the file system
* @param conf configuration object used to determine the target file system implementation
* @return initialized FileSystem object or throws UnsupportedFileSystemException if not found
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,initialize,"org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",92,104,"/**
* Overridden method m1: loads and initializes resources based on URI.
*@param name input URI
*@param conf configuration object
*/","Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,initialize,"org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",44,53,"/**
* Configures URI scheme and swaps scheme if necessary.
*@param name input URI
*@param conf configuration
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,checkPath,org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path),338,341,"/**
* Calls Filesystem (FS) method m1 with given Path.
* @param path input file/directory path
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,makeQualified,org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path),682,685,"/**
* Computes M4 value by invoking M1 and utilizing internal parameters.
* @param path input path object
*/","* Qualify a path to one which uses this FileSystem and, if relative,
   * made absolute.
   * @param path to qualify.
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified
   * @see Path#makeQualified(URI, Path)
   * @throws IllegalArgumentException if the path has a schema/URI different
   * from this FileSystem.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,resolvePath,org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path),975,978,"/**
* Applies transformations to input path and returns processed result.
* @param p original file system path
*/","* Return the fully-qualified path of path, resolving the path
   * through any symlinks or mount point.
   * @param p path to be resolved
   * @return fully qualified path
   * @throws FileNotFoundException if the path is not present
   * @throws IOException for any other error",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,checkPath,org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path),146,149,"/**
* Calls file system operation on the given path.
* @param path file system path to operate on
*/",Check that a Path belongs to this FileSystem.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,<init>,"org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",55,63,"/**
* Opens Avro file input for reading, specifying sequential read policy.
* @param fc FileContext instance
* @param p Path to the Avro file
*/","Construct given a {@link FileContext} and a {@link Path}.
   * @param fc filecontext.
   * @param p the path.
   * @throws IOException If an I/O error occurred.
   *",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)",2208,2248,"/**
* Recursively copies and optionally deletes source file.
* @param src source path
* @param dst destination path
* @param deleteSource whether to delete the original file
* @param overwrite whether to overwrite existing files
* @return true if successful, false otherwise
*/","* Copy from src to dst, optionally deleting src and overwriting dst.
     * @param src src.
     * @param dst dst.
     * @param deleteSource - delete src if true
     * @param overwrite  overwrite dst if true; throw IOException if dst exists
     *         and overwrite is false.
     *
     * @return true if copy is successful
     *
     * @throws AccessControlException If access is denied
     * @throws FileAlreadyExistsException If <code>dst</code> already exists
     * @throws FileNotFoundException If <code>src</code> does not exist
     * @throws ParentNotDirectoryException If parent of <code>dst</code> is not
     *           a directory
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>src</code> or <code>dst</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * 
     * RuntimeExceptions:
     * @throws InvalidPathException If path <code>dst</code> is invalid",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(),641,645,"/**
 * Returns a path representing functional mask.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4913,4917,"/**
* Constructs an FSDataInputStreamBuilder instance with a given file system and path.
* @param fileSystem the file system to operate on
* @param path the file or directory path to access
*/","* Path Constructor.
     * @param fileSystem owner
     * @param path path to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4924,4928,"/**
* Constructs an FSDataInputStreamBuilder with the given file system and path handle.
* @param fileSystem the file system to operate on
* @param pathHandle the path handle for this builder
*/","* Construct from a path handle.
     * @param fileSystem owner
     * @param pathHandle path handle of file to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",449,454,"/**
* Returns an FSDataInputStream instance based on given parameters.
* @param fs FileSystem object
* @param status FileStatus object
* @param readPolicies string of read policies
*/","* Open a file.
   * <p>
   * If the WrappedIO class is found, use it.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",188,216,"/**
* Retrieves FsPermission for a given file or directory.
* @param path original file/directory Path
* @param backupPath alternate Path to use if original is corrupted
* @return FsPermission object or null on failure
*/","* Try loading from the user specified path, else load from the backup
   * path in case Exception is not due to bad/wrong password.
   * @param path Actual path to load from
   * @param backupPath Backup path (_OLD)
   * @return The permissions of the loaded file
   * @throws NoSuchAlgorithmException
   * @throws CertificateException
   * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadAndReturnPerm,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",252,272,"/**
* Loads FsPermission by executing m2() and updates the file system using m3().
* @param pathToLoad Path to load key store from
* @param pathToDelete Path to delete
* @return Loaded FsPermission or null on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,resetKeyStoreState,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path),586,598,"/**
* Resets KeyStore to a previous state if flushing fails.
* @param path Path object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2921,2924,"/**
* Initializes a Sorter instance with the given configuration.
* @param fs FileSystem instance
* @param keyClass Class of the key used for sorting
* @param valClass Class of the values being sorted
* @param conf Hadoop Configuration object
*/","* Sort and merge files containing the named classes.
     * @param fs input FileSystem.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Compressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration),98,101,"/**
* Returns compressor instance based on configuration.
* @param conf Configuration object
* @return Bzip2Compressor or BZip2DummyCompressor instance
*/","* Return the appropriate implementation of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 compressor.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,initialize,"org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)",1965,1989,"/**
* Initializes mask reader for given file and stream.
* @param filename path to the file
* @param in input stream
* @param start starting position
* @param length length of data to read
* @param conf configuration object
* @param tempReader whether this is a temporary reader
*/",Common work of the constructors.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",137,144,"/**
* Creates a compressed output stream with specified compressor and settings.
* @param out target output stream
* @param compressor compression algorithm to use
* @return CompressionOutputStream instance or null on failure
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createCompressor,org.apache.hadoop.io.compress.ZStandardCodec:createCompressor(),162,167,"/**
* Creates and returns a ZStandard compressor instance with custom settings.
* @return Compressor object implementing ZStandard compression algorithm","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",194,201,"/**
* Creates a compression-aware input stream using the provided decompressor.
* @param in original input stream
* @param decompressor instance to handle compressed data
* @return CompressionInputStream object
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor(),220,224,"/**
* Creates a decompressor instance based on function mask.
* @return ZStandardDecompressor object implementing decompression functionality
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor(),236,241,"/**
* Returns a direct decompressor instance using Z-standard algorithm.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,createReader,"org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",730,734,"/**
* Creates a BlockReader instance for the specified block region.
* @param compressAlgo algorithm to use for compression
* @param region block region to read from
* @return BlockReader instance or null on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)",345,363,"/**
* Creates a BlockAppender instance for writing compressed data.
* @param name unique block name
* @param compressAlgo compression algorithm to use
* @return initialized BlockAppender object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock(),417,436,"/**
* Creates a new Block Appender instance.
* @return BlockAppender object
*/","* Create a Data Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Data Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @return The BlockAppender stream
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration),103,106,"/**
* Initializes ScriptBasedMapping with given configuration.
* @param conf Configuration object to initialize the mapping. 
*/","* Create an instance from the given configuration
   * @param conf configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),85,89,"/**
 * Calls parent and child methods with configuration.
 * @param conf application configuration
 */","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,init,org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[]),80,168,"/**
* Parses command-line arguments and sets up the key provider.
* @param args array of command-line arguments
* @return 0 on success, non-zero on error
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop key create keyName [-size size] [-cipher algorithm]
   *    [-provider providerPath]
   * % hadoop key roll keyName [-provider providerPath]
   * % hadoop key list [-provider providerPath]
   * % hadoop key delete keyName [-provider providerPath] [-i]
   * % hadoop key invalidateCache keyName [-provider providerPath]
   * </pre>
   * @param args Command line arguments.
   * @return 0 on success, 1 on failure.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",33,36,"/**
* Initializes a CryptoFSDataInputStream with the given FSDataInputStream, 
* CryptoCodec, and encryption parameters.
* @param in input stream
* @param codec cryptographic codec to use
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)",125,128,"/**
* Constructs a CryptoOutputStream instance with specified parameters.
* @param out OutputStream to write encrypted data
* @param codec CryptoCodec to use for encryption
* @param key Encryption key
* @param iv Initialization vector
* @param streamOffset Stream offset (not used in this constructor)
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getConnectionId,"org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1798,1817,"/**
* Creates a ConnectionId instance with default retry policy if not provided.
* @param addr the InetSocketAddress to connect to
* @param protocol the protocol class for the connection
* @param ticket the UserGroupInformation for authentication
* @param rpcTimeout the RPC timeout in milliseconds
* @param conf the Configuration object for client settings
*/","* Returns a ConnectionId object. 
     * @param addr Remote address for the connection.
     * @param protocol Protocol for RPC.
     * @param ticket UGI
     * @param rpcTimeout timeout
     * @param conf Configuration object
     * @return A ConnectionId instance
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,open,"org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)",276,306,"/**
* Opens and returns a FSDataInputStream for the specified file.
* @param file Path to the file
* @param bufferSize buffer size for reading
* @return FSDataInputStream object or null if failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,create,"org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",312,375,"/**
* Creates an FSDataOutputStream to a file on the HDFS with specified permissions and replication. 
* @param file Path of the file to create
* @param permission File system permissions
* @param overwrite Whether to overwrite existing files
* @return FSDataOutputStream instance or null if creation failed","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",400,409,"/**
* Downloads a file from an FTP server.
* @param file local file path
* @param recursive whether to recursively download directories
* @return true if successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path),468,477,"/**
* Retrieves M3 status for a given file.
* @param file Path to the file
* @return Array of FileStatus objects or null on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),500,509,"/**
* Retrieves file status using FTP client.
* @param file Path object to query
* @return FileStatus object or null on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",574,583,"/**
* Tries to set permissions on a remote file via FTP.
* @param file local Path object
* @param permission FsPermission value to apply
* @return true if successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",631,640,"/**
* Transfers a file between two remote locations using FTP.
* @param src source path
* @param dst destination path
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory(),713,729,"/**
* Retrieves the user's function mask by connecting to FTP server and fetching home directory.
* @return Path object representing the home directory, or null if failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(),1260,1264,"/**
* Delegates call to fs.m1() and returns result.
* @throws IOException if an I/O error occurs
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(),158,162,"/**
* Retrieves FsServerDefaults instance from implementation.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path),963,965,"/**
* Returns FsServerDefaults instance for the given path.
* @param p Path to fetch defaults for
* @return FsServerDefaults instance or throws IOException if an error occurs
*/","* Return a set of server default configuration values.
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @return server default configuration values
   * @throws IOException IO failure",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(),436,439,"/**
* Calls underlying FS implementation's m1() method.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),1288,1292,"/**
* Calls underlying file system's m1 method on the specified Path.
* @param f path to operate on
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)",1089,1096,"/**
* Creates a buffered output stream for the given file.
* @param f file path
* @param overwrite whether to overwrite existing file
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an exception will be thrown.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",1107,1114,"/**
* Wraps a file output stream with progress tracking.
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)",1124,1131,"/**
* Creates an M4 file output stream with specified replication and chunk size.
* @param f file path
* @param replication data replication factor
*/","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @throws IOException IO failure
   * @return output stream1",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)",1143,1149,"/**
* Creates an M4 file output stream.
* @param f Path to file
* @param replication short replication factor
* @param progress Progressable callback
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)",1161,1168,"/**
* Creates an output stream to write data to a file at the specified path.
* @param f target file path
* @param overwrite whether to overwrite existing file
* @param bufferSize buffer size for writing operations
* @return FSDataOutputStream object
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a path with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)",1183,1191,"/**
* Creates an FSDataOutputStream for writing to a file.
* @param f the file path
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size for writing
* @param progress progressable object (not used in this method)
*/","* Create an {@link FSDataOutputStream} at the indicated Path
   * with write-progress reporting.
   *
   * The frequency of callbacks is implementation-specific; it may be ""none"".
   * @param f the path of the file to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),976,988,"/**
* Resolves file system and fetches M3 metadata.
* @param f Path to resolve
* @return long value representing M3 metadata
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),442,445,"/**
* Calls underlying file system's m1() function with provided path.
* @param f Path object representing the file system location
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",130,139,"/**
* Initializes FSDataOutputStreamBuilder with a FileSystem and Path.
* @param fileSystem required file system instance
* @param p required path to configure output settings from
*/","* Constructor.
   *
   * @param fileSystem file system.
   * @param p the path.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),39,42,"/**
* Initializes caching space usage with specified path and interval.
* @param builder configuration builder containing path and interval
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable),2462,2506,"/**
* Reads a compressed or uncompressed key from storage.
* @param key Writable object representing the key to read
*/","* @return Read the next key in the file into <code>key</code>, skipping its
     * value.True if another entry exists, and false at end of file.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object),2707,2752,"/**
* Retrieves a key by its ID, handling compressed and uncompressed cases.
* @param key the key to fetch
*/","* Read the next key in the file, skipping its
     * value.
     *
     * @param key input Object key.
     * @throws IOException raised on errors performing I/O.
     * @return Return null at end of file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,initSingleton,"org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)",129,131,"/**
* Retrieves JVM metrics for a specified process and session.
* @param processName name of the process
* @param sessionId unique session identifier
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",162,170,"/**
* Initializes Invoker instance with client connection and protocol details.
* @param protocol Class of the remote protocol
* @param connId unique ConnectionId
* @param conf Configuration object
* @param factory SocketFactory instance
* @param alignmentContext AlignmentContext for this invocation
*/","* This constructor takes a connectionId, instead of creating a new one.
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration),77,79,"/**
* Creates a new Client instance with default socket factory settings.
* @param conf configuration object
*/","* Construct &amp; cache an IPC client with the default SocketFactory
   * and default valueClass if no cached client exists. 
   * 
   * @param conf Configuration
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",89,91,"/**
 * Creates a new Client instance using provided configuration and socket factory.
 * @param conf client configuration
 * @param factory socket factory for network operations
 */","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists. Default response type is ObjectWritable.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration),370,376,"/**
* Creates an M2 client instance from given configuration.
* @param conf Hadoop configuration to use for client creation
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",170,178,"/**
* Initializes Invoker instance with client connection details.
* @param protocol Class of the remote protocol to use
* @param connId unique identifier for the client connection
* @param conf configuration settings for the client
* @param factory SocketFactory instance for creating sockets
* @param alignmentContext context for alignment operations
*/","* This constructor takes a connectionId, instead of creating a new one.
     *
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration),360,366,"/**
* Creates an M2 client instance.
* @param conf configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(),69,71,"/**
 * Initializes the factory with default configuration.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,isMethodSupported,"org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)",108,147,"/**
* Retrieves protocol signature for the given method name.
* @param rpcProxy RPC proxy object
* @param protocol protocol class
* @param rpcKind RPC kind
* @param version protocol version
* @param methodName target method name to find hash for
* @return true if method hash found, false otherwise
*/","* Returns whether the given method is supported or not.
   * The protocol signatures are fetched and cached. The connection id for the
   * proxy provided is re-used.
   * @param rpcProxy Proxy which provides an existing connection id.
   * @param protocol Protocol for which the method check is required.
   * @param rpcKind The RpcKind for which the method check is required.
   * @param version The version at the client.
   * @param methodName Name of the method.
   * @return true if the method is supported, false otherwise.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,<init>,"org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)",47,76,"/**
* Initializes ZKFC RPC server with configuration and ZooKeeper controller.
* @param conf Hadoop configuration object
* @param bindAddr address to bind the server on
* @param zkfc ZooKeeper failover controller instance
* @param policy service-level authorization security policy (if enabled)
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,"org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",60,63,"/**
* Initializes a new instance of Trash with the specified file system and configuration.
* @param fs file system to use
* @param conf configuration for this instance
*/","* Construct a trash can accessor for the FileSystem provided.
   * @param fs the FileSystem
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,build,org.apache.hadoop.fs.GetSpaceUsed$Builder:build(),144,176,"/**
* Creates and returns a GetSpaceUsed instance.
* @return GetSpaceUsed object or alternative implementation based on platform
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration),73,75,"/**
* Creates a network topology using the given configuration.
* @param conf Configuration object
*/","* Get an instance of NetworkTopology based on the value of the configuration
   * parameter net.topology.impl.
   * 
   * @param conf the configuration to be used
   * @return an instance of NetworkTopology",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",55,59,"/**
* Resolves domain name by combining host and configuration key.
* @param conf Spark configuration
* @param host hostname to resolve with
* @param configKey configuration key to combine with
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfigurationInternal,org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration),105,125,"/**
* Initializes configuration settings for security token service and DNS lookup.
* @param conf Hadoop Configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,"org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",118,125,"/**
* Initializes a new SASL RPC client instance.
* @param ugi UserGroupInformation for authentication
* @param protocol Class of the remote communication protocol
* @param serverAddr Server address to connect to
* @param conf Configuration settings for this client
*/","* Create a SaslRpcClient that can be used by a RPC client to negotiate
   * SASL authentication with a RPC server
   * @param ugi - connecting user
   * @param protocol - RPC protocol
   * @param serverAddr - InetSocketAddress of remote server
   * @param conf - Configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration),100,102,"/**
* Initializes the Groups instance with the provided configuration.
* @param conf Hadoop Configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,"org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)",70,80,"/**
* Configures impersonation and proxy servers based on provided prefix.
* @param conf configuration object
* @param proxyUserPrefix prefix to configure impersonation
*/","* Refreshes configuration using the specified Proxy user prefix for
   * properties.
   *
   * @param conf configuration
   * @param proxyUserPrefix proxy user configuration prefix",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),55,59,"/**
* Calls superclass initialization and initializes Hadoop security engine.
* @param conf Hadoop configuration object",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)",294,305,"/**
* Adds a shutdown hook with specified priority.
* @param shutdownHook Runnable to execute on shutdown
* @param priority hook execution order (lower values run first)
*/","* Adds a shutdownHook with a priority, the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getPasswordString,"org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)",443,450,"/**
* Retrieves and returns a password mask as a string from the configuration.
* @param conf Configuration object
* @param name property name for password mask
* @return masked password string or null if not found
*/","* A wrapper of {@link Configuration#getPassword(String)}. It returns
     * <code>String</code> instead of <code>char[]</code>.
     *
     * @param conf the configuration
     * @param name the property name
     * @return the password string or null",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPassword,"org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",914,927,"/**
* Retrieves password from configuration or returns default value.
* @param conf Configuration object
* @param alias unique alias identifier
* @param defaultPass default password value
* @return Password string or null if not found
*/","* Passwords should not be stored in configuration. Use
   * {@link #getPasswordFromCredentialProviders(
   *            Configuration, String, String)}
   * to avoid reading passwords from a configuration file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,getPassword,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",302,315,"/**
* Retrieves user password from configuration, falling back to default if not found.
* @param conf Configuration object
* @param alias unique identifier for the password
* @param defaultPass default password value
* @return password string or default value if retrieval failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getZKAuthInfos,"org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)",775,791,"/**
* Retrieves ZK authentication info from configuration.
* @param conf Hadoop Configuration object
* @param configKey key for auth info in config file
* @return List of ZKAuthInfo objects or empty list if not found
*/","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @param configKey config key.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder(),52,59,"/**
* Returns a RawErasureEncoder instance.
* @return RawErasureEncoder instance or null if not initialized
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder(),61,67,"/**
* Returns the raw erasure encoder instance.
* @return RawErasureEncoder object or null if initialization failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder(),69,76,"/**
* Returns the XOR raw encoder instance.
* @return RawErasureEncoder object or null if not initialized
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder(),75,81,"/**
* Returns a raw erasure encoder instance or initializes it if not created.
* @return RawErasureEncoder object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,50,"/**
* Creates an ErasureCodingStep for XOR erasure coding.
* @param blockGroup Block group to process
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,51,"/**
* Creates a decoding step for the given block group.
* @param blockGroup block group to process
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder(),52,58,"/**
* Initializes and returns the raw erasure decoder instance.
* @return RawErasureDecoder object or null if failed to initialize
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder(),67,73,"/**
* Initializes and returns a raw erasure decoder instance.
* @return RawErasureDecoder object or null if initialization fails
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshCallQueue,org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration),903,915,"/**
* Configures and initializes the IPC server queue based on given configuration.
* @param conf IPC server configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)",88,94,"/**
* Constructs a FairCallQueue with default weights.
* @param priorityLevels number of priority levels
* @param capacity total queue capacity
* @param ns namespace for the queue
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)",96,102,"/**
* Constructs a FairCallQueue with default weights and failover settings.
* @param priorityLevels number of priority levels
* @param capacity maximum queue capacity
* @param ns name of the namespace
* @param conf configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",236,279,"/**
* Initializes the DecayRpcScheduler with given parameters.
* @param numLevels number of priority levels
* @param ns namespace identifier
* @param conf configuration object
*/","* Create a decay scheduler.
   * @param numLevels number of priority levels
   * @param ns config prefix, so that we can configure multiple schedulers
   *           in a single instance.
   * @param conf configuration to use.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,clone,"org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)",218,227,"/**
* Creates a masked instance of a Writable object.
* @param orig original object to mask
* @param conf configuration for masking operation
* @return masked object or null if error occurs
*/","* Make a copy of a writable object using serialization to a buffer.
   *
   * @param <T> Generics Type T.
   * @param orig The object to copy
   * @param conf input Configuration.
   * @return The copied object",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String),726,760,"/**
* Fetches user-specific mask by ID with retry and failover logic.
* @param user unique user identifier
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),327,354,"/**
* Encrypts key version using provided encrypted key and key provider.
* @param ekv EncryptedKeyVersion instance
* @return updated EncryptedKeyVersion or throws exception if failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream),3589,3591,"/**
* Wraps OutputStream with UTF-8 OutputStreamWriter and calls m1.
*/","* Write out the non-default properties in this configuration to the given
   * {@link OutputStream} using UTF-8 encoding.
   *
   * @param out the output stream to write to.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)",107,110,"/**
* Default constructor for m1, using null as additional argument.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configure,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String),481,486,"/**
* Initializes metrics configuration with specified prefix and executes subsequent setup tasks.
* @param prefix custom prefix for metric IDs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",547,553,"/**
* Wraps the main file output method with a default parameter for stream type.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",592,600,"/**
* Creates a new output stream with specified permissions.
* @param f file path
* @param permission file system permissions
* @param overwrite whether to overwrite existing files
* @param bufferSize I/O buffer size
* @param replication replication factor (ignored for non-HDFS)
* @param blockSize block size (ignored for non-HDFS)
* @param progress progress tracker
* @return FSDataOutputStream instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",602,610,"/**
* Returns a file output stream with specified permissions and settings.
* @param f the file path
* @param permission file system permissions
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size for writing
* @param replication replication factor (0-127)
* @param blockSize block size in bytes
* @param progress progress monitor
* @return FSDataOutputStream instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,constructSecretProvider,"org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)",862,870,"/**
* Creates a signer secret provider instance based on configuration.
* @param b builder with configuration and authentication filter settings
* @param ctx Servlet context for filter initialization
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",53,58,"/**
* Initializes and adds a ProxyUserAuthenticationFilter to the FilterContainer.
* @param container FilterContainer instance
* @param conf Configuration object for filter configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",98,101,"/**
* Initializes ZKSecretManager with configuration and secret token kind.
* @param conf Hadoop Configuration object
* @param tokenKind Secret token kind to manage in ZooKeeper
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)",547,553,"/**
* Initializes a ZooKeeper client with the specified connection string and timeout.
* @param connectString ZooKeeper ensemble connection string
* @param sessionTimeout ZooKeeper session timeout in milliseconds
* @param watcher callback to receive watch events
* @param canBeReadOnly whether this client can be read-only
* @param config configuration for the ZK client (defaults to default config)
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,selectSaslClient,org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List),154,187,"/**
* Selects the first compatible SASL authentication type from the list.
* @param authTypes list of available SASL authentication types
* @return selected SaslAuth object or null if none found
*/","* Instantiate a sasl client for the first supported auth type in the
   * given list.  The auth type must be defined, enabled, and the user
   * must possess the required credentials, else the next auth is tried.
   * 
   * @param authTypes to attempt in the given order
   * @return SaslAuth of instantiated client
   * @throws AccessControlException - client doesn't support any of the auths
   * @throws IOException - misc errors",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,<init>,"org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",50,53,"/**
* Initializes an FTP-based file system with the given configuration.
* @param theUri URI of the FTP server
* @param conf Configuration object for this file system
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFs.java,<init>,"org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",28,31,"/**
* Initializes HDFS file system using the given configuration.
* @param theUri HDFS URI
* @param conf Hadoop configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,"org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",55,59,"/**
* Initializes a raw local file system instance.
* @param theUri URI of the file system
* @param conf Hadoop configuration object
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes the object with URI and configuration.
* @param name URI to initialize with
* @param conf configuration for initialization
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes URI and calls parent class initialization.
* @param name URI to be set
* @param conf configuration object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",150,179,"/**
* Initializes ViewFileSystemOverloadScheme with the given URI and Configuration.
* @param theUri file system URI
* @param conf Hadoop configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",135,140,"/**
* Calls superclass m1 method and then calls m2 with configuration.","* Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDependencies,"org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",336,353,"/**
* Validates and prevents copying between the same file systems.
* @param srcFS source file system
* @param src source path
* @param dstFS destination file system
* @param dst destination path
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",95,104,"/**
* Initializes MultipartUploaderBuilderImpl with the given file system and path.
* @param fileSystem non-null file system instance
* @param p non-null file path to configure uploader for
*/","* Constructor.
   *
   * @param fileSystem fileSystem.
   * @param p path.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)",156,166,"/**
* Initializes a new PathData object from file system, path string, and status.
* @param fs the underlying Hadoop file system
* @param pathString the path as a string
* @param stat file status
*/","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it.
   * @param fs the FileSystem
   * @param pathString a String of the path
   * @param stat the FileStatus (may be null if the path doesn't exist)",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3487,3501,"/**
* Evaluates the capabilities of a given path.
* @param path Path object to check
* @param capability Capability type to verify
* @return True if the path supports the specified capability, false otherwise
*/","* The base FileSystem implementation generally has no knowledge
   * of the capabilities of actual implementations.
   * Unless it has a way to explicitly determine the capabilities,
   * this method returns false.
   * {@inheritDoc}",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),4973,4978,"/**
* Applies file system function mask to the given path.
* @param path input file system path
*/","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem.
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,makeQualified,org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path),124,139,"/**
* Resolves and potentially swaps scheme of a given path.
* @param path the input path to resolve
* @return resolved Path object, with swapped scheme if applicable
*/",Make sure that a path specifies a FileSystem.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,resolvePath,org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path),343,346,"/**
* Delegates file system operation to underlying FS implementation.
* @param p the input path
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,getFileStatus,"org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)",279,290,"/**
* Recursively resolves FileStatus for the given PathData and depth.
* @param item PathData containing file status
* @param depth current recursion depth
* @return resolved FileStatus or null if not found
*/","* Returns the {@link FileStatus} from the {@link PathData} item. If the
   * current options require links to be followed then the returned file status
   * is that of the linked file.
   *
   * @param item
   *          PathData
   * @param depth
   *          current depth in the process directories
   * @return FileStatus
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path),412,420,"/**
* Resolves a file path to its corresponding file system and updates the remaining path.
* @param f input file path
* @return updated Path object or original Path if resolution fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolvePath,org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path),157,160,"/**
* Wraps file system operation on input path.
* @param p the input path to operate on
* @return result of file system operation
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path),91,97,"/**
* Computes function mask by processing input path.
* @param path the input path to process
*/","* @param path
   * @return  full path including the chroot",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2173,2178,"/**
* Copies a file from source to destination.
* @param src source file path
* @param dst destination file path
* @throws various exceptions on copy operation failure
*/","* Copy file from src to dest. See
     * {@link #copy(Path, Path, boolean, boolean)}
     *
     * @param src src.
     * @param dst dst.
     * @throws AccessControlException If access is denied.
     * @throws FileAlreadyExistsException If file <code>src</code> already exists.
     * @throws FileNotFoundException if next file does not exist any more.
     * @throws ParentNotDirectoryException If parent of <code>src</code> is not a
     * directory.
     * @throws UnsupportedFileSystemException If file system for
     * <code>src/dst</code> is not supported.
     * @throws IOException If an I/O error occurred.
     * @return if success copy true, not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4877,4883,"/**
* Creates a new FSDataInputStreamBuilder for the given filesystem and path.
* @param fileSystem filesystem instance
* @param path file or directory to read from
*/","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path.
   * @param fileSystem owner
   * @param path path to read
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4892,4898,"/**
* Creates a builder for an FSDataInputStream.
* @param fileSystem file system instance
* @param pathHandle path handle to the input stream
*/","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path handle.
   * @param fileSystem owner
   * @param pathHandle path handle of file to open.
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadIncompleteFlush,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",229,250,"/**
* Calculates and returns FsPermission for the given paths.
* @param oldPath original path
* @param newPath new path
* @return FsPermission object or null if not found
*/","* The KeyStore might have gone down during a flush, In which case either the
   * _NEW or _OLD files might exists. This method tries to load the KeyStore
   * from one of these intermediate files.
   * @param oldPath the _OLD file created during flush
   * @param newPath the _NEW file created during flush
   * @return The permissions of the loaded file
   * @throws IOException
   * @throws NoSuchAlgorithmException
   * @throws CertificateException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createCompressor,org.apache.hadoop.io.compress.BZip2Codec:createCompressor(),147,150,"/**
 * Returns Bzip2 compressor instance with specified configuration. 
 * @return Compressor object implementing Bzip2 compression algorithm","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String),701,710,"/**
* Retrieves a block reader for the specified name.
* @param name unique identifier
* @return BlockReader object or throws exception if not found
*/","* Stream access to a Meta Block.
     * 
     * @param name
     *          meta block name
     * @return BlockReader input stream for reading the meta block.
     * @throws IOException
     * @throws MetaBlockDoesNotExist
     *           The Meta Block with the given name does not exist.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int),720,728,"/**
* Retrieves a BlockReader for the specified block index.
* @param blockIndex unique identifier of the block to read
*/","* Stream access to a Data Block.
     * 
     * @param blockIndex
     *          0-based data block index.
     * @return BlockReader input stream for reading the data block.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer:close(),303,339,"/**
* Flushes and closes the block appender.
* @throws IOException if an I/O error occurs
*/","* Close the BCFile Writer. Attempting to use the Writer after calling
     * <code>close</code> is not allowed and may lead to undetermined results.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",381,385,"/**
* Creates a block appender with specified name and compression scheme.
* @param name unique block appender identifier
* @param compressionName name of the compression scheme to use
* @return BlockAppender instance or throws exception if failed
*/","* Create a Meta Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Regular Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @param compressionName
     *          The name of the compression algorithm to be used.
     * @return The BlockAppender stream
     * @throws IOException
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String),403,406,"/**
* Creates a new block appender with the given name and uses the result of m1 as meta block. 
* @param name unique block appender identifier
*/","* Create a Meta Block and obtain an output stream for adding data into the
     * block. The Meta Block will be compressed with the same compression
     * algorithm as data blocks. There can only be one BlockAppender stream
     * active at any time. Regular Blocks may not be created after the first
     * Meta Blocks. The caller must call BlockAppender.close() to conclude the
     * block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @return The BlockAppender stream
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock(),639,644,"/**
* Initializes block appender instance.
* @throws IOException on write error
*/","* Check if we need to start a new data block.
     * 
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",42,47,"/**
* Initializes a secure output stream with the given FS data output stream,
* crypto codec, and encryption keys.",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",120,123,"/**
* Initializes a CryptoOutputStream with the given parameters.
* @param out underlying OutputStream
* @param codec encryption codec to use
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory(),707,711,"/**
* Returns a Path representing function masks.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1266,1269,"/**
* Delegates file system server defaults retrieval to underlying FS implementation.
* @param f file path to retrieve defaults for
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),164,167,"/**
* Delegates file system defaults retrieval to implementation.
* @param f path to retrieve defaults for
* @return FsServerDefaults object or null if not found
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1004,1013,"/**
* Resolves server defaults for a given file path.
* @param f the input file path
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),452,455,"/**
* Calls the underlying file system's m1 method on the specified Path.
* @param f the path to operate on
* @return FsServerDefaults object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList),392,424,"/**
* Writes data to output file, optionally appending new block.
* @param args list of input files or stdin
* @throws IOException on write error
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path),1077,1079,"/**
 * Opens an output stream on the specified file.
 * @param f the Path to open
 */","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @throws IOException IO failure
   * @return output stream.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,close,org.apache.hadoop.io.BloomMapFile$Writer:close(),192,204,"/**
* Writes Bloom filter data to file and flushes output.
* @throws IOException if I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path),688,716,"/**
* Recursively attempts to create a file system output stream with a unique ID.
* @param initial the initial path
*/","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, add a suffix, starting with 1
   * and try again. Keep incrementing the suffix until a nonexistent target
   * path is found.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath} are set
   * appropriately.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see if the exists fails",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createOrAppendLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path),789,820,"/**
* Opens or re-opens a file in the specified location for output.
* @param targetFile Path to the desired file
*/","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, open the file for append
   * instead.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath}.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see the append operation fails.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)",293,297,"/**
* Masks instance data in filesystem.
* @param fs FileSystem object
* @param path Path to mask data
* @param instance Instance to mask
* @param overwrite Whether to overwrite existing data
*/","* Save to a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten
   * @param instance instance
   * @throws IOException IO exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1231,1238,"/**
* Initializes a Writer instance with the specified configuration and file system.
* @param fs FileSystem instance
* @param conf Configuration object
* @param name Path to write to
* @param keyClass Class of key values
* @param valClass Class of value objects
* @param progress Progressable instance for reporting progress
* @param metadata Metadata object for tracking file information
*/","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNewFile,org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path),1495,1503,"/**
* Verifies file existence and initializes buffer size.
* @param f Path to the file
* @return True if file exists; False otherwise
*/","* Creates the given Path as a brand-new zero-length file.  If
   * create fails, or if it already existed, return false.
   * <i>Important: the default implementation is not atomic</i>
   * @param f path to use for create
   * @throws IOException IO failure
   * @return if create new file success true,not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4690,4692,"/**
 * Initializes a new FileSystemDataOutputStreamBuilder instance.
 * @param fileSystem underlying file system
 * @param p path to data output stream","* Constructor.
     * @param fileSystem owner
     * @param p path to create",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,"org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",2518,2530,"/**
* Updates or inserts key-value pair into the data store.
* @param key unique identifier
* @param val new value to associate with the key
* @return true if a new entry was created, false otherwise
*/","* Read the next key/value pair in the file into <code>key</code> and
     * <code>val</code>.
     * @return Returns true if such a pair exists and false when at
     * end of file.
     *
     * @param key input key.
     * @param val input val.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,read,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read(),236,257,"/**
* Processes input and writes output to buffer.
* @throws IOException on write or read error
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",143,152,"/**
* Initializes an Invoker instance with specified protocol and address.
* @param protocol the communication protocol to use
* @param addr server endpoint address
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",80,88,"/**
* Creates a ProtocolProxy instance with the specified protocol and client version.
* @param protocol Class representing the protocol
* @param clientVersion Client version as a long value
* @param connId Connection ID
* @param conf Configuration object
* @param factory Socket factory
* @param alignmentContext Alignment context
* @return ProtocolProxy instance or throws IOException if an error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",121,130,"/**
* Creates a ProtocolProxy instance for the specified protocol.
* @param connId Connection ID
* @param conf Configuration object
* @param factory SocketFactory instance
* @return ProtocolProxy<ProtocolMetaInfoPB> or throws IOException if an error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getClient,org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration),279,283,"/**
* Returns an instance of Client using configuration.
* @param conf client configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",219,230,"/**
* Initializes an Invoker instance with specified configuration.
* @param protocol communication protocol
* @param address remote server address
* @param ticket user authentication ticket
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC request timeout
* @param fallbackToSimpleAuth flag for simple auth fallback
* @param alignmentContext context for data alignment
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",103,111,"/**
* Creates a protocol proxy for the given client version and connection.
* @param protocol protocol class
* @param clientVersion client version
* @param connId connection ID
* @param conf configuration
* @param factory socket factory
* @param alignmentContext alignment context
* @return ProtocolProxy instance or throws IOException if an error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",128,137,"/**
* Creates a ProtocolProxy instance for the specified protocol.
* @param connId connection ID
* @param conf configuration
* @param factory socket factory
* @return ProtocolProxy instance or throws IOException if creation fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",150,159,"/**
* Initializes Invoker with given parameters and sets fallback auth flag.
* @param protocol the protocol to use
* @param addr server address
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout timeout for RPC operations
* @param connectionRetryPolicy retry policy for connections
* @param fallbackToSimpleAuth flag for simple auth fallback
* @param alignmentContext context for alignment purposes
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Calls RPC method using RpcClientUtil.
* @param methodName name of the RPC method to call
* @return true if successful, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),71,78,"/**
* Calls the remote procedure to retrieve user mappings.
* @param methodName name of the method to call
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),106,113,"/**
* Executes a remote procedure call using the given method name.
* @param methodName name of the method to invoke
* @return true if the operation was successful, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Executes M2 operation via RPC client.
* @param methodName name of the method to execute
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),61,66,"/**
* Invokes the 'm2' method on the RpcClient with the given method name.
* @param methodName name of the method to invoke
* @return true if successful, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initRPC,org.apache.hadoop.ha.ZKFailoverController:initRPC(),330,334,"/**
* Initializes the RPC server with a specified address and configuration.
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration),50,52,"/**
* Initializes Trash with provided Hadoop Configuration.
* @param conf Hadoop job configuration
*/","* Construct a trash can accessor.
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,main,org.apache.hadoop.fs.DU:main(java.lang.String[]),91,102,"/**
* Executes GetSpaceUsed functionality with optional custom path.
* @param args Array of command-line arguments (path is used if present)
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)",50,53,"/**
* Redirects to m2 with resolved domain name.
* @param conf Configuration object
* @param uri URI object containing a resolved domain name (via m1())
* @param configKey configuration key for resolver
*/","* Create a domain name resolver to convert the domain name in the config to
   * the actual IP addresses of the Namenode/Router/RM.
   *
   * @param conf Configuration to get the resolver from.
   * @param uri the url that the resolver will be used against
   * @param configKey The config key name suffixed with
   *                  the nameservice/yarnservice.
   * @return Domain name resolver.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfiguration,org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration),98,103,"/**
* Updates configuration with mask values.
* @param conf Configuration object to modify
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups),1575,1578,"/**
* Constructs a TestingGroups instance from an existing Groups implementation.
* @param underlyingImplementation the existing Groups object to wrap
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration),471,481,"/**
* Returns a singleton instance of the Groups object, lazily initializing it from the given Configuration.
* @param conf configuration to use for initialization
*/","* Get the groups being used to map user-to-groups.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingServiceWithLoadedConfiguration,org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration),488,495,"/**
* Initializes and returns a singleton instance of Group configuration.
* @param conf Configuration object to initialize from
*/","* Create new groups used to map user-to-groups with loaded configuration.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,init,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig),53,58,"/**
* Initializes and configures proxy user functionality.
* @param filterConfig Filter configuration object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig),177,202,"/**
* Initializes filter with authentication handler and delegated token secret manager.
* @param filterConfig Filter configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration),86,88,"/**
* Configures Hadoop proxy user settings.
* @param conf Configuration object
*/","* Refreshes configuration using the default Proxy user prefix for properties.
   * @param conf configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteOnExit,org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path),1706,1724,"/**
* Verifies a file's mask and initializes shutdown hook if necessary.
* @param f the Path to verify
* @return true if successful, false otherwise
*/","* Mark a path to be deleted on JVM shutdown.
   * 
   * @param f the existing path to delete.
   *
   * @return  true if deleteOnExit is successful, otherwise false.
   *
   * @throws AccessControlException If access is denied
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,register,org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int),61,64,"/**
 * Synchronizes and schedules a shutdown hook with specified priority. 
 * @param priority The priority of the shutdown hook. */","* Register the service for shutdown with Hadoop's
   * {@link ShutdownHookManager}.
   * @param priority shutdown hook priority",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,startupShutdownMessage,"org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)",805,828,"/**
* Logs function execution with customizable arguments and registers shutdown hook.
* @param clazz Class to be logged
* @param args Customizable log message arguments
* @param log Logger instance for logging
*/","* Print a log message for starting up and shutting down
   * @param clazz the class of the server
   * @param args arguments
   * @param log the target log object",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,loadSSLConfiguration,org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration(),455,483,"/**
* Configures SSL/TLS settings from configuration.
* @throws IOException if properties are missing
*/",* Load SSL properties from the SSL configuration.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,loadSslConf,org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration),874,891,"/**
* Initializes SSL/TLS configuration from provided Configuration object.
* @param sslConf Configuration object containing SSL/TLS settings
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordForBindUser,org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String),977,991,"/**
* Retrieves the password for a given key prefix.
* @param keyPrefix unique identifier for the password
* @return password string or null if not found
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createTrustManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)",105,150,"/**
* Initializes and configures the trust manager for SSL connections.
* @param mode SSL factory mode
* @param truststoreType type of trust store to use
* @param truststoreLocation location of trust store file
* @param storesReloadInterval interval at which to reload trust store (0 = disabled)
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createKeyManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)",160,205,"/**
* Initializes SSL factory with a ReloadingX509KeystoreManager.
* @param mode SSLFactory.Mode
* @param keystoreType type of the keystore (e.g. JKS)
* @param storesReloadInterval interval to reload keystore in seconds
*/","* Implements logic of initializing the KeyManagers with the options
   * to reload keystores.
   * @param mode client or server
   * @param keystoreType The keystore type.
   * @param storesReloadInterval The interval to check if the keystore certificates
   *                             file has changed.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAuths,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration),120,123,"/**
* Retrieves ZK authentication info from configuration.
* @param conf Configuration object
* @return list of ZK auth info or null if not found
*/","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initZK,org.apache.hadoop.ha.ZKFailoverController:initZK(),341,382,"/**
* Initializes ZooKeeper client with configuration and authentication settings.
* @throws HadoopIllegalArgumentException if configuration is invalid
* @throws IOException on I/O errors
* @throws KeeperException on ZooKeeper connection issues
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
* Creates an ErasureCodingStep for the given ECBlockGroup.
* @param blockGroup ECBlockGroup object
* @return ErasureCodingStep instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),48,59,"/**
* Creates a new Erasure Coding Step for XOR encoding.
* @param blockGroup ECBlockGroup to process
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
* Creates a step for erasure decoding.
* @param blockGroup ECBlockGroup object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),49,65,"/**
* Creates a mask erasure coding step for the given block group.
* @param blockGroup ECBlockGroup object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String),92,105,"/**
* Converts user group set based on the specified rule.
* @param user unique user identifier
* @return Set of converted group names or original set if no conversion applied
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroups,org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String),357,360,"/**
* Retrieves a list of function masks associated with the given user.
* @param user username to retrieve function masks for
*/","* Returns list of groups for a user.
   * 
   * The LdapCtx which underlies the DirContext object is not thread-safe, so
   * we need to block around this whole method. The caching infrastructure will
   * ensure that performance stays in an acceptable range.
   *
   * @param user get groups for this user
   * @return list of groups for a given user",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,main,org.apache.hadoop.conf.Configuration:main(java.lang.String[]),3945,3947,"/**
 * Configures and executes m1 operation on System.out.
 * @throws Exception if an error occurs during execution
 */","For debugging.  List non-default properties to the terminal and exit.
   * @param args the argument to be parsed.
   * @throws Exception exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,start,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start(),178,194,"/**
* Starts the metrics system.
* @throws MetricsException if system already started
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,<init>,org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder),699,724,"/**
* Initializes an HttpServer2 instance with the provided configuration.
* @param b Builder object containing server settings
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",118,125,"/**
* Initializes a DelegationTokenManager instance based on ZooKeeper configuration.
* @param conf Hadoop Configuration object
* @param tokenKind type of delegation token (Text) 
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,saslConnect,org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams),365,455,"/**
* Performs authentication using the provided IPC streams.
* @param ipcStreams IpcStreams object for communication
* @return AuthMethod enum value representing the chosen authentication method
*/","* Do client side SASL authentication with server via the given IpcStreams.
   *
   * @param ipcStreams ipcStreams.
   * @return AuthMethod used to negotiate the connection
   * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration),42,44,"/**
* Initializes local file system with given configuration.
* @param conf Hadoop configuration object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",37,41,"/**
* Constructs a new FileSystemMultipartUploaderBuilder instance.
* @param fileSystem file system to use for uploads
* @param path directory path where files will be uploaded
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)",111,113,"/**
* Initializes PathData with file system and path string.
* @param fs FileSystem object
* @param pathString absolute path to initialize from
*/","* Looks up the file status for a path.  If the path
   * doesn't exist, then the status will be null
   * @param fs the FileSystem for the path
   * @param pathString a string for a path 
   * @throws IOException if anything goes wrong",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContents,org.apache.hadoop.fs.shell.PathData:getDirectoryContents(),274,285,"/**
* Retrieves file system data for the specified path.
*@return Array of PathData objects representing files and subdirectories
*/","* Returns a list of PathData objects of the items contained in the given
   * directory.
   * @return list of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,maybeIgnoreMissingDirectory,"org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)",2094,2110,"/**
* Throws FileNotFoundException with user-friendly error messages.
* @param fs FileSystem object
* @param path Path to the missing directory
* @param e Initial exception
*/","* Method to call after a FNFE has been raised on a treewalk, so as to
   * decide whether to throw the exception (default), or, if the FS
   * supports inconsistent directory listings, to log and ignore it.
   * If this returns then the caller should ignore the failure and continue.
   * @param fs filesystem
   * @param path path
   * @param e exception caught
   * @throws FileNotFoundException the exception passed in, if rethrown.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",287,292,"/**
* Delegates file system operation to underlying implementation.
* @param path file system path
* @param capability specific capability or feature
* @return true if successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Determines if a read-only connector is supported at the specified path.
* @param path file system path
* @param capability requested capability
* @return true if read-only connector is supported, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1315,1332,"/**
* Evaluates file system capabilities based on path and capability.
* @param path the file system path
* @param capability the requested capability
* @return true if supported, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1350,1371,"/**
* Checks if a path has a specific capability.
* @param path Path to check
* @param capability Capability to verify
* @return true if the path has the capability, false otherwise
*/","* Reject the concat operation; forward the rest to the viewed FS.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return the capability
   * @throws IOException if there is no resolved FS, or it raises an IOE.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1373,1389,"/**
* Resolves file system path by ID and returns enclosing path.
* @param path input filesystem path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1941,1958,"/**
* Resolves path and returns the more enclosing one.
* @param path input path to resolve
* @return most enclosing path or original path if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),735,738,"/**
* Wraps file system operation with FS API.
* @param path input path object
* @return output path object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_getEnclosingRoot,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",202,204,"/**
 * Retrieves a file mask from the given filesystem at the specified location.
 * @param fs FileSystem instance
 * @param path Path to the file
 */","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   * @param fs filesystem
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",740,753,"/**
* Determines whether a path is supported based on its capabilities.
* @param path file system path
* @param capability requested capability
* @return true if supported, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",694,760,"/**
* Renames a file or directory from one location to another.
* @param src source Path object
* @param dst destination Path object
* @return true if rename operation is successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",193,199,"/**
* Wraps m1 output in a FSDataOutputStream.
* @param f file path
* @param permission file permissions
* @param overwrite whether to overwrite existing files
* @param bufferSize I/O buffer size
* @param replication storage replication factor
* @param blockSize block size for storage
* @param progress progress update callback
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",201,208,"/**
* Wraps FSDataOutputStream creation with custom file system logic.
* @param f Path to the file
* @param permission File permissions
* @param flags Create flags
* @param bufferSize Buffer size for I/O operations
* @param replication Replication factor
* @param blockSize Block size for data storage
* @param progress Progress callback
* @return FSDataOutputStream instance
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",210,214,"/**
* Calls superclass method with result of m1() and optional recursion. 
* @param f input file path
* @param recursive whether to traverse subdirectories
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",223,228,"/**
* Invokes parent's m3 method with transformed file status.
* @param fs FileStatus object
* @param start starting block position
* @param len block length
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),230,234,"/**
* Calls parent's m2 with checksum from m1.
* @param f file path to compute checksum for
* @return FileChecksum object or null if error
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",236,240,"/**
* Calls superclass's m2 method with result of m1 and provided file length.
* @param f input file path
* @param length total bytes in the file
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path),242,246,"/**
* Calls superclass's m2() with result of m1().
* @param f input file path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),248,251,"/**
 * Calls parent class's m2 method with result of m1 applied to input Path.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path),259,262,"/**
* Calls superclass's m2 method with result of m1 operation on the given Path.
* @param p file system path to operate on
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path),264,268,"/**
 * Wraps the result of m1() in a call to super's m2(). 
 * @param f input path object
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),270,274,"/**
 * Wraps the result of m1() in a RemoteIterator.
 * @param f Path to process
 * @return iterator over LocatedFileStatus objects
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",276,280,"/**
* Calls superclass method with result of m1(f) and permission.
* @param f file path to process
* @param permission file permissions
* @return true if successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path),282,285,"/**
 * Calls superclass's m2 method with result of m1(f) as argument.
 * @param f directory path to process
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)",287,291,"/**
* Wraps file I/O operation with custom buffering.
* @param f file path
* @param bufferSize buffer size for optimized reading
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",293,297,"/**
* Wraps FSDataOutputStream with optimized buffer size and progress reporting.
* @param f output file path
* @param bufferSize optimal write buffer size
* @param progress progress monitor for reporting
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",299,304,"/**
* Calls parent's move operation with transformed source and destination paths.
* @param src source file path
* @param dst destination file path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",306,311,"/**
* Calls superclass method with transformed path result from m1.
* @param f input file path
* @param username user identifier
* @param groupname group name
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",313,317,"/**
* Calls superclass method with result of m1() and permission. 
* @param f file path to process
* @param permission file permissions for processing
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",319,323,"/**
* Calls superclass method with result of m1() and specified replication.
* @param f file path to process
* @param replication replication factor (short)
* @return true if operation successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",325,329,"/**
* Calls superclass's m2 method with modified file path.
* @param f the file path to modify and pass
* @param mtime last modification time
* @param atime last access time
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",331,335,"/**
* Calls parent's m2 method with result of m1 and ACL specification.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",337,341,"/**
* Calls superclass method with result of m1 and ACL specification.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),343,346,"/**
* Calls superclass's m2() with result of m1() applied to the given path.
* @param path input path to be processed by m1()
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path),348,351,"/**
* Calls superclass method with result of m1 execution.",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",353,356,"/**
* Calls superclass's m2 method with result of m1 and ACL specification.
* @param path directory path
* @param aclSpec list of access control entries
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path),358,361,"/**
* Calls parent's m2 method with result of m1(path).
* @param path file path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",363,367,"/**
* Calls parent's implementation of m2 with transformed path.
* @param path input file path
* @param name attribute name
* @param value attribute value
* @param flag attribute flags
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",369,372,"/**
* Calls parent's m2 method with result of m1 and provided parameters.
* @param path file path
* @param name file name
* @return byte array or throws exception if IO error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
* Calls superclass's m2 method with result of m1(path).
* @param path file system path to process
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",379,383,"/**
* Calls parent's implementation with preprocessed data.
* @param path file system path
* @param names list of field names
* @return map of field values or null if error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)",385,388,"/**
* Calls superclass method with result of m1() and specified new length.
* @param path file path
* @param newLength new file length
* @return true if operation was successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path),390,393,"/**
* Calls superclass's m2() with result of m1() on given file path.
* @param path Path object to process
* @return list of strings or throws IOException if error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",395,398,"/**
* Calls parent's m2() with result of m1() on given Path and name.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",400,403,"/**
* Calls parent's m2 with result of m1 and provided name.
* @param path input path
* @param name additional parameter for m2
* @return Path object or null if an error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",405,409,"/**
* Calls superclass's m2 method with result of m1() on given Path.
* @param path input file system path
* @param snapshotOldName old snapshot name
* @param snapshotNewName new snapshot name
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",411,415,"/**
* Calls parent method with processed path and name.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path),417,420,"/**
* Calls superclass method m2() with result of m1() on input path.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path),422,425,"/**
 * Invokes secondary metadata aggregation on file content.
 * @param f input file path
 * @return aggregated ContentSummary object
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),427,430,"/**
* Calculates quota usage by delegating to underlying storage.
* @param f file path to calculate quota for
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),439,442,"/**
* Calls superclass's m2 with result of m1 on input file.
* @param f Path to file
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),449,452,"/**
* Calls parent's m2 with result of m1 on input file Path.
* @param f input file path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),464,467,"/**
* Calls superclass's m2() with result of m1() on input path.
* @param src file to process
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),469,472,"/**
* Invokes superclass method with result of m1() on specified file path.
* @param src input file path to process
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",474,477,"/**
 * Calls superclass method with result of m1() and policyName.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),479,482,"/**
 * Overridden method to process source file using custom processing logic.
 * @param src Path to source file
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path),484,487,"/**
 * Wraps the output stream builder with an additional layer of configuration.
 * @param path file system path to configure
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path),4761,4765,"/**
* Builds DataInputStream from the given file path.
* @param path file location to read from
* @return FutureDataInputStreamBuilder instance or null if failed
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(Path, int)} unless a subclass
   * executes the open command differently.
   *
   * The semantics of this call are therefore the same as that of
   * {@link #open(Path, int)} with one special point: it is in
   * {@code FSDataInputStreamBuilder.build()} in which the open operation
   * takes place -it is there where all preconditions to the operation
   * are checked.
   * @param path file path
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle),4780,4785,"/**
* Creates a DataInputStreamBuilder instance for the provided file handle.
* @param pathHandle handle to the file path
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass
   * executes the open command differently.
   *
   * If PathHandles are unsupported, this may fail in the
   * {@code FSDataInputStreamBuilder.build()}  command,
   * rather than in this {@code openFile()} operation.
   * @param pathHandle path handle.
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore(),145,176,"/**
* Loads and configures key store with specified password.
* @throws IOException if keystore cannot be loaded or created
*/","* Open up and initialize the keyStore.
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkTFileDataIndex,org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex(),882,893,"/**
* Initializes and loads the TFile index if not already done.
* @throws IOException on errors reading or parsing the index
*/","* Lazily loading the TFile index.
     * 
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String),967,970,"/**
* Reads data from BCF file using specified name.
* @param name unique identifier to read data for
*/","* Stream access to a meta block.``
     * 
     * @param name
     *          The name of the meta block.
     * @return The input stream.
     * @throws IOException
     *           on I/O error.
     * @throws MetaBlockDoesNotExist
     *           If the meta block with the name does not exist.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",617,645,"/**
* Initializes Reader object from FSDataInputStream and configuration.
* @param fin input stream to read from
* @param fileLength total size of the file in bytes
* @param conf configuration settings
*/","* Constructor
     * 
     * @param fin
     *          FS input stream.
     * @param fileLength
     *          Length of the corresponding file
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockReader,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int),2035,2037,"/**
* Fetches a specific block from the BCF file using its index.
* @param blockIndex unique block identifier
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",595,606,"/**
* Starts and returns a Meta Block output stream.
* @param name unique block identifier
* @param compressName compressed block name
* @return DataOutputStream instance or throws exceptions if failed
*/","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile.
     * 
     * @param name
     *          Name of the meta block.
     * @param compressName
     *          Name of the compression algorithm to be used. Must be one of the
     *          strings returned by
     *          {@link TFile#getSupportedCompressionAlgorithms()}.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer:close(),300,343,"/**
* Closes TFile object, writing metadata and index if in READY state.
* @throws IOException on write or close errors
*/","* Close the Writer. Resources will be released regardless of the exceptions
     * being thrown. Future close calls will have no effect.
     * 
     * The underlying FSDataOutputStream is not closed.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String),623,632,"/**
* Starts and writes a Meta Block with the given name.
* @param name unique identifier for the Meta Block
* @return DataOutputStream object for further writing
*/","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile. Data will be compressed using the default
     * compressor as defined in Writer's constructor.
     * 
     * @param name
     *          Name of the meta block.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendKey,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int),527,537,"/**
* Initializes and returns a DataOutputStream for a new key of specified length.
* @param length the length of the key
*/","* Obtain an output stream for writing a key into TFile. This may only be
     * called when there is no active Key appending stream or value appending
     * stream.
     * 
     * @param length
     *          The expected length of the key. If length of the key is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream.
     * @return The key appending output stream.
     * @throws IOException raised on errors performing I/O.
     *",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),459,462,"/**
* Calls parent implementation with result of m1() on given path.
* @param f input file path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",517,557,"/**
* Copies a file from source to destination, optionally deleting the original.
* @param src source file
* @param dstFS destination file system
* @param dst destination path
* @param deleteSource whether to delete the original file
* @return true if successful, false otherwise
*/","* Copy local files to a FileSystem.
   *
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerComplete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",195,243,"/**
* Processes multipart upload and returns the result.
* @param multipartUploadId unique multipart upload identifier
* @param filePath path to file being processed
* @param handleMap map of part handles
* @return PathHandle object representing the processed file
*/","* The upload complete operation.
   * @param multipartUploadId the ID of the upload
   * @param filePath path
   * @param handleMap map of handles
   * @return the path handle
   * @throws IOException failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touch,org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData),162,175,"/**
* Updates PathData with mask operation; creates or updates if necessary.
* @param item PathData object to update
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touchz,org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData),88,90,"/**
* Applies mask operations to PathData.
* @param item PathData object with file system and path
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",742,749,"/**
* Creates and initializes a new output stream to the specified file.
* @param fs FileSystem instance
* @param file destination Path object
* @param permission FsPermission for the new file
* @return FSDataOutputStream instance for writing to the file
*/","* Create a file with the provided permission.
   *
   * The permission of the file is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * The HDFS implementation is implemented using two RPCs.
   * It is understood that it is inefficient,
   * but the implementation is thread-safe. The other option is to change the
   * value of umask in configuration to be 0, but it is not thread-safe.
   *
   * @param fs FileSystem
   * @param file the name of the file to be created
   * @param permission the permission of the file
   * @return an output stream
   * @throws IOException IO failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",1209,1215,"/**
* Initializes a Writer instance with the specified configuration and name.
* @param fs FileSystem instance
* @param conf Configuration object
* @param name Path to write to
* @param keyClass Class of key data type
* @param valClass Class of value data type
*/","* Create the named file.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDir,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir(),661,673,"/**
* Creates a masked log file for the current working directory.
* @throws IOException if an I/O error occurs
*/","* Create a new directory based on the current interval and a new log file in
   * that directory.
   *
   * @throws IOException thrown if an error occurs while creating the
   * new directory or new log file",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,readIndex,org.apache.hadoop.io.MapFile$Reader:readIndex(),577,631,"/**
* Initializes and populates the keys array from the index.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,next,"org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",811,814,"/**
* Calls underlying data storage's method to perform operation with given key and value.
* @param key unique identifier
* @param val associated value
* @return true if successful, false otherwise
*/","* Read the next key/value pair in the map into <code>key</code> and
     * <code>val</code>.  Returns true if such a pair exists and false when at
     * the end of the map.
     *
     * @param key WritableComparable.
     * @param val Writable.
     * @return if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",106,119,"/**
* Creates a ProtocolProxy instance with the given parameters.
* @param protocol target protocol class
* @param clientVersion client version
* @param addr server address
* @param ticket user authentication information
* @param conf configuration object
* @param factory socket factory
* @param rpcTimeout RPC timeout in milliseconds
* @param connectionRetryPolicy retry policy for connections
* @param fallbackToSimpleAuth flag to fall back to simple auth
* @param alignmentContext alignment context
* @return ProtocolProxy instance or null if creation fails
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",348,367,"/**
* Creates a protocol proxy for the given class and client version.
* @param protocol protocol class
* @param clientVersion client version number
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param fallbackToSimpleAuth input fallbackToSimpleAuth.
   * @param alignmentContext input alignmentContext.
   * @return ProtocolProxy.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",113,126,"/**
* Creates a ProtocolProxy instance with the specified parameters.
* @param protocol protocol class to proxy
* @return ProtocolProxy object or null if creation fails
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processArguments,org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList),244,273,"/**
* Trashes child file systems and their contents.
* @param args PathData list
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getTrash,org.apache.hadoop.fs.FsShell:getTrash(),86,91,"/**
* Returns the Trash object or initializes it with data from m1().
* @return Trash object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(),462,464,"/**
 * Initializes and returns a Groups instance with default configuration. 
 */","* Get the groups being used to map user-to-groups.
   * @return the groups being used to map user-to-groups.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,initialize,"org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)",309,354,"/**
* Configures security settings and group information.
* @param conf Configuration object
* @param overrideNameRules Whether to override name rules
*/","* Initialize UGI and related classes.
   * @param conf the configuration to use",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(),73,74,"/**
* Initializes an empty Access Control List (ACL).",* This constructor exists primarily for AccessControlList to be Writable.,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String),85,87,"/**
 * Constructs an AccessControlList from a string representation.
 * @param aclString string containing ACL data in the format ""owner group""
 */","* Construct a new ACL from a String representation of the same.
   * 
   * The String is a a comma separated list of users and groups.
   * The user list comes first and is separated by a space followed 
   * by the group list. For e.g. ""user1,user2 group1,group2""
   * 
   * @param aclString String representation of the ACL",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,"org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)",97,99,"/**
* Constructs an Access Control List (ACL) from user and group identifiers.
* @param users comma-separated list of user IDs
* @param groups comma-separated list of group IDs
*/","* Construct a new ACL from String representation of users and groups
   * 
   * The arguments are comma separated lists
   * 
   * @param users comma separated list of users
   * @param groups comma separated list of groups",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(),58,61,"/**
 * Initializes configuration and calls itself recursively.",* refresh Impersonation rules,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,coreServiceLaunch,"org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",571,647,"/**
* Executes a service instance, managing configuration and shutdown hooks.
* @param conf Configuration object
* @param instance Service instance to execute
* @param processedArgs List of CLI arguments
* @param addShutdownHook Flag to add shutdown hook
* @return Exit code or -1 on error
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,build,org.apache.hadoop.http.HttpServer2$Builder:build(),485,564,"/**
* Initializes and configures an HttpServer2 instance.
* @throws IOException if initialization fails
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,initializeBindUsers,org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers(),950,975,"/**
* Configures and initializes bind user information.
* @param conf configuration object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,init,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode),252,300,"/**
* Initializes SSL configuration based on the specified mode.
* @param mode SSLFactory.Mode value (CLIENT or SERVER)
*/","* Initializes the keystores of the factory.
   *
   * @param mode if the keystores are to be used in client or server mode.
   * @throws IOException thrown if the keystores could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the keystores could not be
   * initialized due to a security error.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,"org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)",149,193,"/**
* Initializes ZK client with configured settings and authentications.
* @param authInfos list of AuthInfo objects
* @param sslEnabled true to enable SSL, false otherwise
*/","* Start the connection to the ZooKeeper ensemble.
   *
   * @param authInfos  List of authentication keys.
   * @param sslEnabled If the connection should be SSL/TLS encrypted.
   * @throws IOException            If the connection cannot be started.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doRun,org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]),202,270,"/**
* Executes the failover controller with optional formatting and configuration.
* @throws Exception if an error occurs during execution
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroups,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String),76,90,"/**
* Modifies user group list based on specified rule.
* @param user user identifier
* @return List of modified usernames or original list if no rule applied
*/","* Returns list of groups for a user.
     * This calls {@link LdapGroupsMapping}'s getGroups and applies the
     * configured rules on group names before returning.
     *
     * @param user get groups for this user
     * @return list of groups for a given user",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,init,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String),148,176,"/**
* Initializes and starts the MetricsSystem instance.
* @param prefix custom metric prefix
*/","* Initialized the metrics system with a prefix.
   * @param prefix  the system will look for configs with the prefix
   * @return the metrics system object itself",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,initTokenManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties),148,163,"/**
* Configures and initializes a TokenManager with properties.
* @param config Properties object containing configuration data
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupSaslConnection,org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams),571,579,"/**
* Initializes and executes SASL authentication method.
* @param streams IpcStreams object to process
* @return AuthMethod result or null if failed
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration),36,38,"/**
 * Initializes LocalFs with the given configuration.
 * @param conf Hadoop Configuration object
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,suffix,org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String),241,243,"/**
 * Constructs a PathData instance with specified file system and extension.
 * @param extension file name extension (e.g., "".txt"")","* Returns a new PathData with the given extension.
   * @param extension for the suffix
   * @return PathData
   * @throws IOException shouldn't happen",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getPathDataForChild,org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData),307,310,"/**
* Creates a new PathData with a mask applied to the given child path.
* @param child PathData object to apply the mask to
*/","* Creates a new object for a child entry in this directory
   * @param child the basename will be appended to this object's path
   * @return PathData for the child
   * @throws IOException if this object does not exist or is not a directory",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,recursePath,org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData),449,466,"/**
* Processes a PathData item to determine its mask.
* @param item PathData object containing file system and path data
*/","*  Gets the directory listing for a path and invokes
   *  {@link #processPaths(PathData, PathData...)}
   *  @param item {@link PathData} for directory to recurse into
   *  @throws IOException if anything goes wrong...",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Determines whether to handle the given capability via parent's implementation.
* @param path file system path
* @param capability desired capability
* @return true if handled by this class, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Custom implementation of FS connector read-only capability check.
* @param path file system path
* @param capability type of capability to check
* @return true if read-only connector is supported, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",495,499,"/**
* Calls parent method with result of m1(path) as parameter.
* @param path file system path
* @param capability capability string
* @return true if successful, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1128,1140,"/**
* Handles path capabilities.
* @param path Path object
* @param capability Capability string
* @return true if capable, false otherwise
*/","* Disable those operations which the checksummed FS blocks.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path),217,221,"/**
* Overloads m1() to simplify usage with default flag enabled.",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,updateFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path),131,136,"/**
* Updates file status with alternative retrieval if temporary is null.
* @param f input file path
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path),804,845,"/**
* Retrieves and aggregates file status information from multiple MRNflyNodes.
* @param f Path to fetch status for
* @return Array of FileStatus objects or throws IOException if errors occur
*/","* Returns the closest non-failing destination's result.
   *
   * @param f given path
   * @return array of file statuses according to nfly modes
   * @throws FileNotFoundException
   * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",866,873,"/**
* Verifies file permissions across all nodes.
* @param f the file to check
* @param permission desired file permissions
* @return true if permissions match on all nodes, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,rename,"org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",739,765,"/**
* Renames a file or directory in the distributed file system.
* @param src original path
* @param dst new path
* @return true if rename operation is successful for all nodes, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(),434,437,"/**
* Recursively calls itself with result of m1().
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(),444,447,"/**
* Recursively computes result using m2 and m1 methods.
* @return short value",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",462,504,"/**
* Copies a file or directory from source to destination.
* @param deleteSource whether to delete the original file after copying
* @param overwrite whether to overwrite existing files at destination
*/","* Copy a file/directory tree within/between filesystems.
   * <p>
   * returns true if the operation succeeded. When deleteSource is true,
   * this means ""after the copy, delete(source) returned true""
   * If the destination is a directory, and mkdirs (dest) fails,
   * the operation will return false rather than raise any exception.
   * </p>
   * The overwrite flag is about overwriting files; it has no effect about
   * handing an attempt to copy a file atop a directory (expect an IOException),
   * or a directory over a path which contains a file (mkdir will fail, so
   * ""false"").
   * <p>
   * The operation is recursive, and the deleteSource operation takes place
   * as each subdirectory is copied. Therefore, if an operation fails partway
   * through, the source tree may be partially deleted.
   * </p>
   * @param srcFS source filesystem
   * @param srcStatus status of source
   * @param dstFS destination filesystem
   * @param dst path of source
   * @param deleteSource delete the source?
   * @param overwrite overwrite files at destination?
   * @param conf configuration to use when opening files
   * @return true if the operation succeeded.
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",578,605,"/**
* Recursively copies or moves a file system subtree.
* @param deleteSource whether to delete source after copy
*/",Copy FileSystem files to local files.,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openFile,org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String),632,639,"/**
* Opens file for read with specified policy.
* @param policy file access policy
*/","* Open a file.
   * @param policy fadvise policy.
   * @return an input stream
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path),709,713,"/**
* Wraps the file system's data input stream builder around the given path.
* @param path path to the data source
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,openFile,"org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)",2002,2012,"/**
* Opens a file for sequential read access with specified buffer size and optional length.
* @param fs FileSystem instance
* @param file Path to the file to open
* @param bufferSize Buffer size in bytes
* @param length Optional file length (>= 0)
* @return FSDataInputStream object or null on failure
*/","* Override this method to specialize the type of
     * {@link FSDataInputStream} returned.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param bufferSize The buffer size used to read the file.
     * @param length The length being read if it is {@literal >=} 0.
     *               Otherwise, the length is not available.
     * @return The opened stream.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)",264,283,"/**
* Fetches masked data from the specified file system and path.
* @param fs file system instance
* @param path file location
* @param status optional file status (may be null)
* @return T object or throws exception if error occurs
*/","* Load from a Hadoop filesystem.
   * If a file status is supplied, it's passed in to the openFile()
   * call so that FS implementations can optimize their opening.
   * @param fs filesystem
   * @param path path
   * @param status status of the file to open.
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws EOFException file status references an empty file
   * @throws IOException IO problems",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle),715,719,"/**
* Wraps file system operation with FS interface.
* @param pathHandle unique file handle
* @return FutureDataInputStreamBuilder for further processing
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFirstKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey(),901,904,"/**
* Calls m1() and returns result from tfileIndex's m2().","* Get the first key in the TFile.
     * 
     * @return The first key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey(),912,915,"/**
* Calls m1() and returns result of tfileIndex.m2().
* @throws IOException if an I/O error occurs
*/","* Get the last key in the TFile.
     * 
     * @return The last key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockContainsKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",985,995,"/**
* Finds the block index for a given key with optional greater-than comparison.
* @param key RawComparable key to search
* @param greater true for ""greater than"" search, false otherwise
* @return Location object representing the block index or end if not found
*/","* if greater is true then returns the beginning location of the block
     * containing the key strictly greater than input key. if greater is false
     * then returns the beginning location of the block greater than equal to
     * the input key
     * 
     * @param key
     *          the input key
     * @param greater
     *          boolean flag
     * @return
     * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long),997,1000,"/**
 * Retrieves location information based on the given record number.
 * @param recNum unique record identifier
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1002,1005,"/**
* Computes M2 score for given location.
* @param location geographic location to compute M2 for
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long),1063,1068,"/**
* Retrieves a RawComparable object from the file system based on the given offset.
* @param offset file offset
* @return RawComparable object or null if not found
*/","* Get a sample key that is within a block whose starting offset is greater
     * than or equal to the specified offset.
     * 
     * @param offset
     *          The file offset.
     * @return the key that fits the requirement; or null if no such key exists
     *         (which could happen if the offset is close to the end of the
     *         TFile).
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",802,818,"/**
* Initializes a Reader object with FSDataInputStream, file length, and configuration.
* @param fsdis input stream for file data
* @param fileLength total size of the file in bytes
* @param conf Hadoop configuration
*/","* Constructor
     * 
     * @param fsdis
     *          FS input stream of the TFile.
     * @param fileLength
     *          The length of TFile. This is required because we have no easy
     *          way of knowing the actual size of the input file through the
     *          File input stream.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initBlock,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int),1548,1559,"/**
* Updates block mask by reading and processing the specified block.
* @param blockIndex index of the block to process
*/","* Load a compressed block for reading. Expecting blockIndex is valid.
       * 
       * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)",380,413,"/**
* Writes key-value pair to output streams.
* @param key byte array containing key data
* @param koff offset into key buffer
* @param klen length of key data
* @param value byte array containing value data
* @param voff offset into value buffer
* @param vlen length of value data
*/","* Adding a new key-value pair to TFile.
     * 
     * @param key
     *          buffer for key.
     * @param koff
     *          offset in key buffer.
     * @param klen
     *          length of key.
     * @param value
     *          buffer for value.
     * @param voff
     *          offset in value buffer.
     * @param vlen
     *          length of value.
     * @throws IOException
     *           Upon IO errors.
     *           <p>
     *           If an exception is thrown, the TFile will be in an inconsistent
     *           state. The only legitimate call after that would be close",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(),454,457,"/**
 * Recursively fetches and returns FsServer Defaults based on root path.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData),148,151,"/**
* Applies mask to PathData item.
* @param item PathData object to process
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),153,160,"/**
* Handles path mask operation.
* @param item PathData object containing mask data
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData),67,77,"/**
* Handles non-zero length files by calling m4.
* @param item PathData object containing file information
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),79,86,"/**
* Validates and processes a PathData object.
* @param item PathData to be validated
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getOutputStreamForKeystore,org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore(),52,56,"/**
* Creates an output stream to write data with specified permissions.
* @return OutputStream object for writing data
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeToNew,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path),607,620,"/**
* Stores key store to a file at the specified path.
* @param newPath path where key store will be stored
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,midKey,org.apache.hadoop.io.MapFile$Reader:midKey(),649,657,"/**
* Retrieves the mask value from the sorted key array.
* @return The middle key or null if count is zero.","* Get the key at approximately the middle of the file. Or null if the
     *  file is empty.
     *
     * @throws IOException raised on errors performing I/O.
     * @return WritableComparable.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,finalKey,org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable),665,681,"/**
* Updates the index by masking a key.
* @param key WritableComparable to mask
*/","* Reads the final key from the file.
     *
     * @param key key to read into
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,"org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)",721,780,"/**
* Searches for key in the data and returns a mask based on the comparison result.
* @param key WritableComparable key to search for
* @param before whether to find previous matching key
* @return int mask indicating comparison result (0: equal, < 0: less, > 0: greater)
*/","* Positions the reader at the named key, or if none such exists, at the
     * key that falls just before or just after dependent on how the
     * <code>before</code> parameter is set.
     * 
     * @param before - IF true, and <code>key</code> does not exist, position
     * file at entry that falls just before <code>key</code>.  Otherwise,
     * position file at record that sorts just after.
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,mergePass,org.apache.hadoop.io.MapFile$Merger:mergePass(),1101,1146,"/**
* Iterates through input data and writes the next entry to output based on comparator rules.
*/","* Merge all input files to output map file.<br>
     * 1. Read first key/value from all input files to keys/values array. <br>
     * 2. Select the least key and corresponding value. <br>
     * 3. Write the selected key and value to output file. <br>
     * 4. Replace the already written key/value in keys/values arrays with the
     * next key/value from the selected input <br>
     * 5. Repeat step 2-4 till all keys are read. <br>",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",97,104,"/**
* Creates a ProtocolProxy instance with the given parameters.
* @param protocol the protocol class
* @param clientVersion the client version
* @param addr the address to connect to
* @param ticket the user's ticket information
* @param conf the configuration settings
* @param factory the socket factory
* @param rpcTimeout the RPC timeout value
* @param connectionRetryPolicy the retry policy for connections
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",299,307,"/**
* Creates a ProtocolProxy instance with default parameters.
* @param protocol the protocol class
* @param clientVersion the client version
* @param addr the socket address
* @param ticket user group information
* @param conf configuration object
* @param factory socket factory
* @param rpcTimeout RPC timeout value
* @param connectionRetryPolicy retry policy for connections
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",322,330,"/**
* Wraps the m5 method with additional parameters into a unified interface.
* @param protocol protocol class
*/","* Construct a client-side proxy object with a ConnectionId.
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param connId input ConnectionId.
   * @param conf input Configuration.
   * @param factory input factory.
   * @param alignmentContext Alignment context
   * @throws IOException raised on errors performing I/O.
   * @return ProtocolProxy.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",93,101,"/**
* Creates a ProtocolProxy object for the specified protocol and client version.
* @param protocol target protocol class
* @param clientVersion client version number
* @param addr server address
* @param ticket user credentials
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC timeout value
* @param connectionRetryPolicy retry policy for connections
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(),125,127,"/**
 * Calls the m1() method to fetch a related resource and then calls its m2() method.
 */","* Returns the Trash object associated with this shell.
   * @return Path to the trash
   * @throws IOException upon error",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path),135,137,"/**
* Calls M1 to fetch and process data, then calls M2 on the result.
* @param path input path
*/","* Returns the current trash location for the path specified
   * @param path to be deleted
   * @return path to the trash
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,ensureInitialized,org.apache.hadoop.security.UserGroupInformation:ensureInitialized(),295,303,"/**
* Runs user group information initialization if not already completed.
* @param None
*/","* A method to initialize the fields that depend on a configuration.
   * Must be called before useKerberos or groups is used.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setConfiguration,org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration),362,366,"/**
* Configures function mask based on provided configuration.
* @param conf Function configuration to apply settings from.","* Set the static configuration for UGI.
   * In particular, set the security authentication mechanism and the
   * group look up service.
   * @param conf the configuration to use",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refreshWithLoadedConfiguration,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",152,200,"/**
* Initializes ACLs and machine lists for services.
* @param conf configuration object
* @param provider policy provider
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,init,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String),68,101,"/**
* Configures ACL and host lists based on configuration prefix.
* @param configurationPrefix user-defined prefix
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getSip,org.apache.hadoop.security.authorize.ProxyUsers:getSip(),116,124,"/**
* Returns the impersonation provider instance or initializes it if not created.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",485,539,"/**
* Launches a service with configurable parameters and error handling.
* @param conf configuration object
* @param instance service instance
* @param processedArgs command-line arguments
* @param addShutdownHook whether to add shutdown hook
* @param execute whether to execute the service
* @return ExitException object or null if successful
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param instance optional instance of the service.
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,setConf,org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),767,864,"/**
* Initializes LDAP configuration and settings.
* @param conf Configuration object containing LDAP properties
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List),138,140,"/**
* Invokes m1 with provided AuthInfo list and default flag value.
* @param authInfos list of authentication information
*/","* Start the connection to the ZooKeeper ensemble.
   * @param authInfos List of authentication keys.
   * @throws IOException If the connection cannot be started.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties),127,132,"/**
* Calls authentication handler and subsequent methods with configuration.
* @param config Properties object containing configuration data
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,"org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",49,52,"/**
* Initializes LocalFs with configuration and URI.","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyStreamToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)",418,434,"/**
* Creates a function mask for the given PathData.
* @param in input stream to write data from
* @param target PathData object with file system and overwrite flags
*/","* If direct write is disabled ,copies the stream contents to a temporary
   * file ""target._COPYING_"". If the copy is successful, the temporary file
   * will be renamed to the real path, else the temporary file will be deleted.
   * if direct write is enabled , then creation temporary file is skipped.
   *
   * @param in     the input stream for the copy
   * @param target where to store the contents of the stream
   * @throws IOException if copy fails",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getTargetPath,org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData),330,342,"/**
* Returns the PathData object with applied mask operation.
* @param src source PathData object
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cleanupAllTmpFiles,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles(),399,407,"/**
* Removes temporary files from disk for each node.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,commit,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit(),409,444,"/**
* Applies file system operations and sets timestamps for each node.
* @throws IOException if any operation fails
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,delete,"org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)",768,793,"/**
* Deletes files and directories recursively while handling exceptions.
* @param f Path to delete
* @param recursive true for recursive deletion
* @return true if successful, false otherwise
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",426,433,"/**
* Copies a file from source to destination and optionally deletes source.
* @param srcFS source file system
* @param src source file path
* @param dstFS destination file system
* @param dst destination file path
* @param deleteSource whether to delete the source after copy
* @param overwrite whether to overwrite existing destination
* @param conf configuration object
*/","* Copy files between FileSystems.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param overwrite overwrite.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,repairAndOpen,"org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)",636,713,"/**
* Fetches MRNflyNode array by ID and returns a FSDataInputStream.
* @param mrNodes array of MRNflyNode objects
* @return FSDataInputStream object or null if not found
*/","* Iterate all available nodes in the proximity order to attempt repair of all
   * FileNotFound nodes.
   *
   * @param mrNodes work set copy of nodes
   * @param f path to repair and open
   * @param bufferSize buffer size for read RPC
   * @return the closest/most recent replica stream AFTER repair",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",570,575,"/**
* Copies a file from source to destination and optionally deletes the source.
* @param srcFS source file system
* @param src source file path
* @param dst destination file
* @param deleteSource whether to delete the source after copy
* @param conf configuration object
* @return true if successful, false otherwise
*/","* Copy FileSystem files to local files.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openForSequentialIO,org.apache.hadoop.fs.shell.PathData:openForSequentialIO(),621,624,"/**
* Opens file in sequential read policy mode. 
* @return File stream input data or null if failed
*/","* Open a file for sequential IO.
   * <p>
   * This uses FileSystem.openFile() to request sequential IO;
   * the file status is also passed in.
   * Filesystems may use to optimize their IO.
   * </p>
   * @return an input stream
   * @throws IOException failure",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,dumpToOffset,org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData),72,77,"/**
* Reads and prints the contents of a file to standard output.
* @param item PathData object containing file information
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,dumpFromOffset,"org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)",105,121,"/**
* Reads and prints data from a file starting at the specified offset.
* @param item PathData object containing file information
* @param offset initial read position (may be adjusted)
* @return final read position or fileSize if fully read
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,openFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path),489,493,"/**
* Invokes the parent's m2 method with the result of m1 on the provided path.
* @param path input file path
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",248,250,"/**
 * Convenience method to fetch data from file system.
 * @param fs FileSystem instance
 * @param path Path to data location
 */","* Load from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws IOException IO problems",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long),1048,1050,"/**
* Computes the function mask value from the given offset.
* @param offset input data offset
*/","* Get the RecordNum for the first key-value pair in a compressed block
     * whose byte offset in the TFile is greater than or equal to the specified
     * offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the RecordNum to the corresponding entry. If no such entry
     *         exists, it returns the total entry count.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum(),1629,1631,"/**
* Retrieves a function mask from the current location.
* @throws IOException on read error
*/","* Get the RecordNum corresponding to the entry pointed by the cursor.
       * @return The RecordNum corresponding to the entry pointed by the cursor.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",1281,1303,"/**
* Initializes a Scanner instance with the specified reader and location bounds.
* @param reader input Reader object
* @param begin starting point of the scan (inclusive)
* @param end ending point of the scan (inclusive)
* @throws IOException if an I/O error occurs while initializing the scanner
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param begin
       *          Begin location of the scan.
       * @param end
       *          End location of the scan.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1396,1429,"/**
* Validates and updates location mask based on provided Location object.
* @param l Location object with various fields (beginLocation, endLocation, etc.)
*/","* Move the cursor to the new location. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @param l
       *          new cursor location. It must fall between the begin and end
       *          location of the scanner.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,advance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance(),1521,1541,"/**
* Validates block mask by performing checks and updates.
* @throws IOException if an I/O error occurs
*/","* Move the cursor to the next key-value pair. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @return true if the cursor successfully moves. False when cursor is
       *         already at the end location and cannot be advanced.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])",355,357,"/**
 * Copies data from key to value array.
 * @param key source byte array
 * @param value destination byte array
 */","* Adding a new key-value pair to the TFile. This is synonymous to
     * append(key, 0, key.length, value, 0, value.length)
     * 
     * @param key
     *          Buffer for key.
     * @param value
     *          Buffer for value.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,flush,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush(),529,584,"/**
* Updates file metadata and renames files if necessary.
*@throws IOException on failure
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable),704,707,"/**
* Retrieves value associated with given key using default retrieval strategy.
* @param key unique key identifier
*/","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.
     *
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)",859,875,"/**
* Fetches the next key based on the given key and value.
* @param key WritableComparable key object
* @param val Writable value object
* @param before whether to seek before or after the key
* @return next WritableComparable key object or null if not found
*/","* Finds the record that is the closest match to the specified key.
     * 
     * @param key       - key that we're trying to find
     * @param val       - data value if key is found
     * @param before    - IF true, and <code>key</code> does not exist, return
     * the first entry that falls just before the <code>key</code>.  Otherwise,
     * return the record that sorts just after.
     * @return          - the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",90,95,"/**
* Creates a ProtocolProxy instance with default Hadoop metrics configuration.
* @param protocol the protocol class
* @param clientVersion the client version
* @param addr the address to connect to
* @param ticket the user's ticket
* @param conf the configuration
* @param factory the socket factory
* @param rpcTimeout the RPC timeout in milliseconds
* @return a ProtocolProxy instance
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",86,91,"/**
* Creates a new ProtocolProxy instance with default Hadoop configuration.
* @param protocol the protocol class to use
* @param clientVersion the client version number
* @param addr the server address
* @param ticket user authentication information
* @param conf Hadoop configuration object (defaulted)
* @param factory socket factory for creating connections
* @param rpcTimeout RPC connection timeout value
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isAuthenticationMethodEnabled,org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),391,396,"/**
* Checks if authentication method matches the current method.
* @param method AuthenticationMethod to compare with the current method
* @return true if methods match, false otherwise
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isKerberosKeyTabLoginRenewalEnabled,org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled(),398,404,"/**
* Returns a flag indicating whether Kerberos keytab login renewal is enabled.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosLoginRenewalExecutor,org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor(),406,412,"/**
* Returns the Kerberos login renewal executor service.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])",1607,1620,"/**
* Returns UserGroupInformation instance based on the provided user and groups.
* @param user unique user identifier
* @param userGroups array of group names for the user
*/","* Create a UGI for testing HDFS and MapReduce
   * @param user the full user principal name
   * @param userGroups the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])",1634,1645,"/**
* Computes and returns UserGroupInformation based on provided parameters.
* @param user user identifier
* @param realUser actual user information
* @param userGroups array of user group memberships
* @return the computed UserGroupInformation object
*/","* Create a proxy user UGI for testing HDFS and MapReduce
   * 
   * @param user
   *          the full user principal name for effective user
   * @param realUser
   *          UGI of the real user
   * @param userGroups
   *          the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation:getGroups(),1790,1799,"/**
* Retrieves a list of group names for the current user.
* @return List of group names or an empty list if retrieval fails
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.
   * @deprecated Use {@link #getGroupsSet()} instead.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation:getGroupsSet(),1806,1814,"/**
* Retrieves a set of group names associated with the current user.
* @return Set of group names or an empty set if failed
*/","* Get the groups names for the user as a Set.
   * @return the set of users with the primary group first. If the command
   *     fails, it returns an empty set.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,doSubjectLogin,"org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)",2042,2073,"/**
* Authenticates and returns UserGroupInformation instance.
* @param subject Subject object
* @param params Login parameters
*/","* Login a subject with the given parameters.  If the subject is null,
   * the login context used to create the subject will be attached.
   * @param subject to login, null for new subject.
   * @param params for login, null for externally managed ugi.
   * @return UserGroupInformation for subject
   * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,init,org.apache.hadoop.fs.FsShell:init(),100,109,"/**
* Initializes and configures the command factory with user info.
* @param commandFactory null to create a new instance
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAclWithLoadedConfiguration,"org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",778,782,"/**
 * Initializes service authorization manager with configuration and policy provider. 
 * @param conf Configuration object
 * @param provider Policy provider instance
 */","* Refresh the service authorization ACL for the service handled by this server
   * using the specified Configuration.
   *
   * @param conf input Configuration.
   * @param provider input provider.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getTestProvider,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider(),52,59,"/**
* Returns a shared, synchronized instance of the DefaultImpersonationProvider.
* @return DefaultImpersonationProvider instance or null if not initialized
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",99,102,"/**
* Invokes M2 on the default instance.
* @param user User information
* @param remoteAddress Remote address of the caller
*/","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the ip address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",111,114,"/**
* Calls second-level authorization on Hadoop framework.
* @param user UserGroupInformation object
* @param remoteAddress remote IP address
*/","* Authorize the superuser which is doing doAs.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the inet address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getDefaultImpersonationProvider,org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider(),140,143,"/**
* Returns an instance of DefaultImpersonationProvider. 
* @return DefaultImpersonationProvider object",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)",464,469,"/**
* Wraps existing functionality with a different parameter set.
* @param conf Hadoop configuration
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,setConf,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),56,66,"/**
* Initializes the conversion rule from configuration.
* @param conf Configuration object
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(),129,131,"/**
* Invokes recursive implementation with an empty list.
* @throws IOException if I/O operation fails
*/","* Start the connection to the ZooKeeper ensemble.
   * @throws IOException If the connection cannot be started.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties),99,126,"/**
* Initializes authentication schemes from configuration.
* @param config Properties object containing scheme settings
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",350,367,"/**
* Copies and synchronizes source PathData with target, preserving raw Xattrs if applicable.
* @param src source data to copy from
* @param target target data to synchronize with
*/","* Copies the source file to the target.
   * @param src item to copy
   * @param target where to copy the item
   * @throws IOException if copy fails",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPathArgument,org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData),247,274,"/**
* Verifies and processes the source path data.
* @param src input PathData object
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,recursePath,org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData),299,328,"/**
* Copies data from source to destination and updates metadata.
*@param src PathData object containing source information
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,close,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close(),378,397,"/**
* Replicates and closes output streams, handling exceptions.
* @throws IOException if replication fails or an I/O error occurs
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",366,371,"/**
* Wraps the main file copy function with a simple wrapper for backwards compatibility.
*/","* Copy files between FileSystems.
   * @param srcFS src fs.
   * @param src src.
   * @param dstFS dst fs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @return if copy success true, not false.
   * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",373,411,"/**
* Copies multiple files from source to destination file system.
* @param srcFS source file system
* @param srcs array of source paths
* @param dstFS destination file system
* @param dst target directory path
* @param deleteSource whether to delete sources after copying
* @param overwrite whether to overwrite existing files
* @param conf configuration object
* @return true if all files copied successfully, false otherwise",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,open,"org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)",579,621,"/**
* Fetches FSDataInputStream for MRNflyNode instances from file system.
* @param f Path to file
* @param bufferSize buffer size for input stream
*/","* Category: READ.
   *
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @return input stream according to nfly flags (closest, most recent)
   * @throws IOException
   * @throws FileNotFoundException iff all destinations generate this exception",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList),91,114,"/**
* Writes PathData items to file system.
* @param items list of PathData objects
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData),106,109,"/**
* Returns input stream from PathData item.
* @param item PathData object containing data to retrieve
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processPath,org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData),63,70,"/**
* Handles file mask operation on given PathData.
* @param item PathData object to process
* @throws IOException upon failure
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processPath,org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData),88,103,"/**
* Processes a PathData item and throws an exception if it's a directory.
* @param item the PathData to process
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(),1077,1079,"/**
* Creates a scanner instance to read from the specified range.
* @throws IOException if an I/O error occurs
*/","* Get a scanner than can scan the whole TFile.
     * 
     * @return The scanner object. A valid Scanner is always returned even if
     *         the TFile is empty.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByRecordNum,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)",1194,1202,"/**
* Creates a Scanner instance for the given record range.
* @param beginRecNum starting record number (clamped to 0 if negative)
* @param endRecNum ending record number (clamped to max allowed value if invalid)
* @return Scanner object or throws IOException on error
*/","* Create a scanner that covers a range of records.
     * 
     * @param beginRecNum
     *          The RecordNum for the first record (inclusive).
     * @param endRecNum
     *          The RecordNum for the last record (exclusive). To scan the whole
     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().
     * @return The TFile scanner that covers the specified range of records.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)",1264,1268,"/**
 * Constructs a new Scanner instance from a Reader at specified offset range.
 * @param reader the underlying Reader to read from
 * @param offBegin offset of beginning of desired data in bytes
 * @param offEnd offset of end of desired data in bytes
 */","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param offBegin
       *          Begin byte-offset of the scan.
       * @param offEnd
       *          End byte-offset of the scan.
       * @throws IOException
       * 
       *           The offsets will be rounded to the beginning of a compressed
       *           block whose offset is greater than or equal to the specified
       *           offset.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",1366,1385,"/**
* Searches for a key in the index, potentially recursing to adjacent locations.
* @param key RawComparable key to search for
* @param beyond whether to search beyond the end of the current location
* @return true if key found, false otherwise
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,rewind,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind(),1437,1439,"/**
* Applies mask to data at specified location.
* @throws IOException on input/output error
*/","* Rewind to the first entry in the scanner. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seek,org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable),692,694,"/**
* Checks if a given key exists in the data structure.
* @param key WritableComparable object to search
* @return true if key is present, false otherwise
*/","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.  Returns true iff the named key exists
     * in this map.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.
     * @return if the named key exists in this map true, not false.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",842,846,"/**
* Shortcut to call m1 with default behavior (not nulling key).
* @param key object to be stored
* @param val value associated with the key
*/","* Finds the record that is the closest match to the specified key.
     * Returns <code>key</code> or if it does not exist, at the first entry
     * after the named key.
     * 
     * @param key key that we're trying to find.
     * @param val data value if key is found.
     * @return the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isSecurityEnabled,org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled(),387,389,"/**
* Returns true if simple authentication is disabled.
*/","* Determine if UserGroupInformation is using Kerberos to determine
   * user identities or is relying on simple authentication
   * 
   * @return true if UGI is working in a secure environment",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logoutUserFromKeytab,org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab(),1158,1189,"/**
* Performs logout operation with Kerberos authentication.
* @throws IOException if an I/O error occurs
*/","* Log the current user out who previously logged in using keytab.
   * This method assumes that the user logged in by calling
   * {@link #loginUserFromKeytab(String, String)}.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if a failure occurred in logout,
   * or if the user did not log in by invoking loginUserFromKeyTab() before.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,checkStat,"org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",282,303,"/**
* Validates file ownership and group membership.
* @param f File object to check
* @param owner Current owner of the file
* @param group Current group of the file
* @param expectedOwner Expected owner of the file
* @param expectedGroup Expected group of the file
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getPrimaryGroupName,org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName(),1655,1661,"/**
* Retrieves a function mask string from the underlying data.
* @throws IOException if the operation fails or there's no primary group
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupNames,org.apache.hadoop.security.UserGroupInformation:getGroupNames(),1778,1781,"/**
* Returns an array of group names based on internal logic.
* @return Array of String group names
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserInList,org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation),235,249,"/**
* Checks if a user has access based on their group membership.
* @param ugi the UserGroupInformation object to check
*/","* Checks if a user represented by the provided {@link UserGroupInformation}
   * is a member of the Access Control List. If user was proxied and
   * USE_REAL_ACLS + the real user name is in the control list, then treat this
   * case as if user were in the ACL list.
   * @param ugi UserGroupInformation to check if contained in the ACL
   * @return true if ugi is member of the list or if USE_REAL_ACLS + real user
   * is in the list",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromSubject,org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject),651,664,"/**
* Retrieves user group information for the given subject.
* @param subject Subject object to fetch info from
*/","* Create a UserGroupInformation from a Subject with Kerberos principal.
   *
   * @param subject             The KerberosPrincipal to use in UGI.
   *                            The creator of subject is responsible for
   *                            renewing credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if the kerberos login fails
   * @return UserGroupInformation",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createLoginUser,org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject),731,803,"/**
* Fetches user group information with optional proxy and token loading.
* @param subject the subject for which to fetch UGI
* @return UserGroupInformation object or null if failed
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",60,105,"/**
* Authenticates and proxies the user by ID.
* @param filterChain Filter chain object
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",243,308,"/**
* Authenticates user and sets up UserGroupInformation for the request.
* @param filterChain Filter chain
* @param request HTTP servlet request
* @param response HTTP servlet response
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,managementOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",220,355,"/**
* Authenticates and processes a Kerberos delegation token operation.
* @param token AuthenticationToken object
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)",134,138,"/**
* Calls deprecated version of m1 with reduced parameters.
* @param user UserGroupInformation instance
* @param remoteAddress Remote address string
* @param conf Configuration object (currently unused)
*/","* This function is kept to provide backward compatibility.
   * @param user user.
   * @param remoteAddress remote address.
   * @param conf configuration.
   * @throws AuthorizationException Authorization Exception.
   * @deprecated use {@link #authorize(UserGroupInformation, String)} instead.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorizeConnection,org.apache.hadoop.ipc.Server$Connection:authorizeConnection(),3018,3039,"/**
* Authenticates and authorizes user based on auth method.
* @throws RpcServerException if authorization fails
*/","* Authorize proxy users to access this server
     * @throws RpcServerException - user is not allowed to proxy",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",140,154,"/**
* Executes path data synchronization with optional parallelization.
* @param src source PathData object
* @param target target PathData object
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,"org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",287,297,"/**
* Validates and processes path data between source and destination.
* @param src source PathData object
* @param dst destination PathData object
*/","* Called with a source and target destination pair
   * @param src for the operation
   * @param dst for the operation
   * @throws IOException if anything goes wrong",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",845,849,"/**
* Applies mask operation to source image.
* @param delSrc whether to delete source image after processing
* @param src input image path
* @param dst output image path
*/",* copies the file in the har filesystem to a local file.,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,rename,"org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",624,644,"/**
* Performs file comparison and copying if necessary.
* @param src source path
* @param dst destination path
* @return true if files are identical or copied successfully
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",991,996,"/**
* Copies or moves a file, optionally deleting the source.
* @param delSrc true to delete the source after copy/move
* @param src source file path
* @param dst destination file path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1002,1007,"/**
* Performs a file operation with masking functionality.
* @param delSrc whether to delete the source file
* @param src source file path
* @param dst destination file path
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,87,"/**
* Performs file transfer with masking functionality.
* @param delSrc whether to delete source after transfer
* @param src source path
* @param dst destination path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",89,93,"/**
* Copies or moves files based on source and destination paths.
* @param delSrc whether to delete the source file
* @param src source file path
* @param dst destination file path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData),88,96,"/**
* Validates and processes path data, handling directory exceptions and checksum verification.
* @param item PathData object to process
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByByteRange,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)",1094,1096,"/**
* Creates a Scanner instance with specified file offset and length.
* @param offset starting position in bytes
* @param length number of bytes to scan
*/","* Get a scanner that covers a portion of TFile based on byte offsets.
     * 
     * @param offset
     *          The beginning byte offset in the TFile.
     * @param length
     *          The length of the region.
     * @return The actual coverage of the returned scanner tries to match the
     *         specified byte-region but always round up to the compression
     *         block boundaries. It is possible that the returned scanner
     *         contains zero key-value pairs even if length is positive.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1318,1331,"/**
* Initializes a Scanner with a Reader and key boundaries.
* @param reader input data source
* @param beginKey starting point for scanning (null for beginning)
* @param endKey ending point for scanning (null for end)
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param beginKey
       *          Begin key of the scan. If null, scan from the first
       *          &lt;K, V&gt; entry of the TFile.
       * @param endKey
       *          End key of the scan. If null, scan up to the last &lt;K, V&gt;
       *          entry of the TFile.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)",1361,1364,"/**
* Creates a cryptographic operation instance from a byte array key.
* @param key byte array containing the key
* @param keyOffset offset within the key array to start reading from
* @param keyLen length of the key in bytes
* @return true if successful, false otherwise
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @return true if we find an equal key; false otherwise.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)",1477,1480,"/**
* Encrypts or decrypts data using specified key and mask.
* @param key encryption/decryption key
* @param keyOffset starting offset of the key in bytes
* @param keyLen length of the key in bytes
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)",1508,1511,"/**
* Encrypts or decrypts data using a provided key.
* @param key encryption/decryption key
* @param keyOffset offset into the key array (assumed to be in bytes)
* @param keyLen length of the key array (in bytes) 
*/","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. The entry returned by the previous entry() call will be
       * invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,seek,org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable),130,134,"/**
 * Calls superclass's implementation of m1 to compare a given WritableComparable object with this instance.
 * @param key the object to be compared (must implement WritableComparable)
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,get,"org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",823,830,"/**
* Returns a writable object with a function mask applied to the given value.
* @param key unique identifier or key
* @param val value to be masked
* @return Writable object if successful, otherwise null
*/","* Return the value for the named key, or null if none exists.
     * @param key key.
     * @param val val.
     * @return Writable if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,commit,org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit(),189,235,"/**
* Authenticates and sets the user subject.
* @throws LoginException if authentication fails
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",579,587,"/**
* Creates a ProtocolProxy instance for the given protocol and configuration.
* @param protocol Class of the protocol to proxy
* @param clientVersion Client version number
* @param connId Connection ID
* @param conf Configuration object
* @param factory Socket factory
* @param alignmentContext Alignment context
* @return Initialized ProtocolProxy object
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @param alignmentContext StateID alignment context
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)",661,677,"/**
* Creates a ProtocolProxy instance with specified parameters.
* @param protocol the target protocol class
* @param clientVersion the client version
* @param addr the server address
* @param ticket the user's identity
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC timeout in milliseconds
* @param connectionRetryPolicy retry policy for connections
* @param fallbackToSimpleAuth flag to enable simple authentication fallback
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate if
   *   a secure client falls back to simple auth
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",698,715,"/**
* Creates a ProtocolProxy instance for the specified protocol and configuration.
* @param protocol class of the protocol to proxy
* @return ProtocolProxy object or null if creation fails
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate
   *   if a secure client falls back to simple auth
   * @param alignmentContext state alignment context
   * @param <T> Generics Type T.
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setFallBackToSimpleAuth,org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean),858,890,"/**
* Configures auth fallback based on server's requirements and client settings.
* @param fallbackToSimpleAuth flag indicating whether to enable simple auth fallback
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRandomRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",126,143,"/**
* Opens and validates a file with specified mode, owner, and group.
* @param f File to open
* @param mode Access mode (e.g. ""r"", ""rw"")
* @param expectedOwner Expected file owner
* @param expectedGroup Expected file group
* @return Opened RandomAccessFile or null on failure
*/","* @return Same as openForRandomRead except that it will run even if security is off.
   * This is used by unit tests.
   *
   * @param f input f.
   * @param mode input mode.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",174,192,"/**
* Reads and validates file metadata.
* @param file the File object to read
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return FSDataInputStream for successful validation, or null if not found
*/","* Same as openFSDataInputStream except that it will run even if security is
   * off. This is used by unit tests.
   *
   * @param file input file.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.
   * @return FSDataInputStream.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)",224,241,"/**
* Opens FileInputStream for a file and validates its ownership.
* @param f the file to open
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return FileInputStream object or null on failure
*/","* @return Same as openForRead() except that it will run even if security is off.
   * This is used by unit tests.
   * @param f input f.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1080,1087,"/**
* Computes file status for the given path.
* @param f input file path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),1089,1128,"/**
* Fetches file status for the given path.
* @param f path to fetch status for
* @return FileStatus object or throws IOException if not found
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1406,1413,"/**
* Calculates ACL status for the given path.
* @param path file system path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1544,1551,"/**
* Returns file status for the given path.
* @param f input file path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1554,1620,"/**
* Retrieves file status for a given path.
* @param f the input path
* @return array of FileStatus objects or null if not found
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1832,1839,"/**
* Calculates ACL status for the given file or directory path.
* @param path file system path to evaluate
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpUGI,"org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",666,690,"/**
* Logs user group information with credentials and secret keys.
* @param title descriptive title
* @param ugi UserGroupInformation object containing credentials and other details
*/","* Dump a UGI.
   *
   * @param title title of this section
   * @param ugi UGI to dump
   * @throws IOException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,print,org.apache.hadoop.security.UserGroupInformation:print(),2022,2032,"/**
* Prints user profile and group information.
* @throws IOException 
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserAllowed,org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation),251,253,"/**
* Checks whether the given UserGroupInformation has functional mask set.
* @param ugi UserGroupInformation object
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLoginUser,org.apache.hadoop.security.UserGroupInformation:getLoginUser(),673,698,"/**
* Retrieves the UserGroupInformation instance using a fallback mechanism.
* @return UserGroupInformation object or null if not found
*/","* Get the currently logged in user.  If no explicit login has occurred,
   * the user will automatically be logged in with either kerberos credentials
   * if available, or as the local OS user, based on security settings.
   * @return the logged in user
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromSubject,org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject),725,729,"/**
* Processes Subject object and generates mask.
* @param subject input data to process
*/","* Log in a user using the given subject
   * @param subject the subject to use when logging in a user, or null to
   * create a new subject.
   *
   * If subject is not null, the creator of subject is responsible for renewing
   * credentials.
   *
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processConnectionContext,org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer),2678,2724,"/**
* Processes RPC header and sets up user authentication context.
* @param buffer RpcWritable.Buffer object containing RPC data
*/","Reads the connection context following the connection header
     * @throws RpcServerException - if the header cannot be
     *         deserialized, or the user is not authorized",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData),276,279,"/**
* Calls m2 with PathData and result of m1 applied to same data.",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",877,880,"/**
 * Copies file contents between two paths using m1 function.
 * @param src source path
 * @param dst destination path
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1174,1181,"/**
 * Creates a Scanner instance with the specified key range.
 * @param beginKey starting key of the range
 * @param endKey ending key of the range
 * @return Scanner object or null if invalid range
 */","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[]),1343,1345,"/**
* Extracts the digest of the provided byte array.
* @param key input data to process
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @return true if we find an equal key.
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[]),1460,1462,"/**
* Initializes cryptographic operation with provided secret key.
* @param key byte array containing encryption key
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to lowerBound(key, 0, key.length). The
       * entry returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[]),1491,1493,"/**
 * Initializes encryption/decryption operation.
 * @param key cryptographic key to use
 */","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. Synonymous to upperBound(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,get,org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable),156,163,"/**
* Returns a maskable comparable object, applying transformation functions if applicable.
* @param key input writable comparable object
* @return transformed key or null if not applicable
*/","* Read the matching key from a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns <code>key</code>, or null if no match exists.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,get,"org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",281,288,"/**
* Calls superclass's m2() method only after verifying the key with m1(). 
* @param key data key to verify and use
* @param val associated value
* @return Writable object if valid or null otherwise
*/","* Fast version of the
     * {@link MapFile.Reader#get(WritableComparable, Writable)} method. First
     * it checks the Bloom filter for the existence of the key, and only if
     * present it performs the real get operation. This yields significant
     * performance improvements for get operations on sparsely populated files.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",558,563,"/**
* Creates a new ProtocolProxy instance with default parameters.
* @param protocol the protocol class to proxy
* @param clientVersion the client version
* @param connId connection ID
* @param conf configuration object
* @param factory socket factory
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",631,641,"/**
* Creates a new ProtocolProxy instance with default Hadoop RPC settings.
* @param protocol the protocol to use
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @return the proxy
   * @throws IOException if any error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupIOstreams,org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean),764,856,"/**
* Establishes IPC connection to the server and authenticates user.
* @param fallbackToSimpleAuth boolean flag to fall back to simple auth if SASL fails
*/","Connect to the server and set up the I/O streams. It then sends
     * a header to the server and starts
     * the connection thread that waits for responses.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRandomRead,"org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",107,114,"/**
* Returns a RandomAccessFile instance for the given file with the specified mode.
* If security checks are enabled, performs additional validation on owner and group.
* @param f the file to access
* @param mode the file access mode (e.g. ""r"", ""w"")
* @param expectedOwner the expected owner of the file
* @param expectedGroup the expected group of the file
* @return a RandomAccessFile instance or null if security checks fail
*/","* @return Open the given File for random read access, verifying the expected user/
   * group constraints if security is enabled.
   * 
   * Note that this function provides no additional security checks if hadoop
   * security is disabled, since doing the checks would be too expensive when
   * native libraries are not available.
   * 
   * @param f file that we are trying to open
   * @param mode mode in which we want to open the random access file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO error occurred or if the user/group does
   * not match when security is enabled.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",156,162,"/**
* Returns an FSDataInputStream for the specified file with optional owner and group filtering.
* @param file File object to open
* @param expectedOwner Optional owner filter (null if ignored)
* @param expectedGroup Optional group filter (null if ignored)
* @return FSDataInputStream or null if access denied
*/","* Opens the {@link FSDataInputStream} on the requested file on local file
   * system, verifying the expected user/group constraints if security is
   * enabled.
   * @param file absolute path of the file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred or the user/group does not
   * match if security is enabled
   * @return FSDataInputStream.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRead,"org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)",208,214,"/**
* Returns a file input stream with access control based on owner and group.
* @param f the file to open
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return FileInputStream object or null if access denied
*/","* Open the given File for read access, verifying the expected user/group
   * constraints if security is enabled.
   *
   * @return Note that this function provides no additional checks if Hadoop
   * security is disabled, since doing the checks would be too expensive
   * when native libraries are not available.
   *
   * @param f the file that we are trying to open
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred, or security is enabled and
   * the user/group does not match",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path),1334,1338,"/**
* Applies filter pipeline to input file path.
* @param f input file path
* @return filtered result or null on failure
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path),1659,1678,"/**
* Calculates content summary by traversing and aggregating file system metadata.
* @param f Path to the root directory
* @return ContentSummary object with aggregated metrics
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path),1680,1694,"/**
* Calculates file system status for the given path.
* @param p Path to calculate status for
* @return FsStatus object summarizing file system metrics
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,userHasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)",1727,1734,"/**
* Checks if a user is an admin by ACL.
* @param servletContext Servlet context
* @param remoteUser username of the user to check
* @return true if user is an admin, false otherwise
*/","* Get the admin ACLs from the given ServletContext and check if the given
   * user is in the ACL.
   *
   * @param servletContext the context containing the admin ACL.
   * @param remoteUser the remote user to check for.
   * @return true if the user is present in the ACL, false if no ACL is set or
   *         the user is not present",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,authorize,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)",88,138,"/**
* Verifies user authorization for a given protocol and address.
* @param user UserGroupInformation object
* @param protocol Class<?> representing the protocol
* @param conf Configuration object
* @param addr InetAddress of the client (optional)
* @throws AuthorizationException if user is not authorized
*/","* Authorize the user to access the protocol being used.
   * 
   * @param user user accessing the service 
   * @param protocol service being accessed
   * @param conf configuration to use
   * @param addr InetAddress of the client
   * @throws AuthorizationException on authorization failure",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",108,135,"/**
* Verifies user impersonation and remote host authorization.
* @param user UserGroupInformation object
* @param remoteAddress InetAddress of the remote client
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCurrentUser,org.apache.hadoop.security.UserGroupInformation:getCurrentUser(),583,594,"/**
* Returns a UserGroupInformation instance based on the current security context.
* @throws IOException if an error occurs during initialization
*/","* Return the current user, including any doAs in the current stack.
   * @return the current user
   * @throws IOException if login fails",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginKeytabBased,org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased(),1417,1421,"/**
* Checks whether a specific mask condition is met.
*/","* Did the login happen via keytab.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginTicketBased,org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased(),1428,1430,"/**
* Combines results from m1 and m2 methods to determine function mask.
* @throws IOException if an I/O error occurs during computation
*/","* Did the login happen via ticket cache.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUserOrFatal,org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction),508,522,"/**
* Executes privileged action with user credentials if authenticated.
* @param action PrivilegedAction to be executed
* @return Result of the action or null if authentication fails
*/","* Perform the given action as the daemon's login user. If the login
   * user cannot be determined, this will log a FATAL error and exit
   * the whole JVM.
   *
   * @param action action.
   * @param <T> generic type T.
   * @return generic type T.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUser,org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction),533,536,"/**
* Executes privileged exception action with user credentials.
* @param action PrivilegedExceptionAction to execute
* @return Result of the action, or null if an error occurs
*/","* Perform the given action as the daemon's login user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> Generics Type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeActive,org.apache.hadoop.ha.ZKFailoverController:cedeActive(int),575,588,"/**
* Cedes control to other processes after specified time period.
* @param millisToCede duration in milliseconds before ceding control
*/","* Request from graceful failover to cede active role. Causes
   * this ZKFC to transition its local node to standby, then quit
   * the election for the specified period of time, after which it
   * will rejoin iff it is healthy.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,gracefulFailoverToYou,org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou(),630,643,"/**
* Executes a privileged operation and performs initialization.
* @throws ServiceFailedException if service fails
* @throws IOException on interrupt or I/O error
*/","* Coordinate a graceful failover to this node.
   * @throws ServiceFailedException if the node fails to become active
   * @throws IOException some other error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])",1132,1137,"/**
* Converts byte arrays to ByteArray objects for scanning.
* @param beginKey starting key bytes or null
* @param endKey ending key bytes or null
* @return Scanner object or throws IOException if error occurs
*/","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1155,1159,"/**
* Decomposes interval into mask bits.
* @param beginKey start of interval (inclusive)
* @param endKey end of interval (exclusive)
* @return Scanner object or null if invalid
*/","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}
     *             instead.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",535,543,"/**
* Creates a ProtocolProxy instance with the given parameters.
* @param protocol protocol class
* @param clientVersion client version number
* @param addr server address
* @param ticket user authentication information
* @param conf configuration object
* @param factory socket creation factory
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param ticket user group information
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",604,613,"/**
* Calls m1 and returns the result of its m2 method.
* @param protocol Class of the protocol
* @param clientVersion Client version
* @param addr Socket address
* @param ticket User group information
* @param conf Configuration object
* @param factory Socket factory
* @param rpcTimeout RPC timeout in milliseconds
*/","* Construct a client-side proxy that implements the named protocol,
   * talking to a server at the named address.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @return the proxy
   * @throws IOException if any error occurs",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,hasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1686,1716,"/**
* Checks authorization for servlet requests.
* @param servletContext Servlet context
* @param request HTTP request
* @return true if authorized, false otherwise
*/","* Does the user sending the HttpServletRequest has the administrator ACLs? If
   * it isn't the case, response will be modified to send an error to the user.
   *
   * @param servletContext servletContext.
   * @param request request.
   * @param response used to send the error response if user does not have admin access.
   * @return true if admin-authorized, false otherwise
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorize,"org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)",3806,3821,"/**
* Authorizes and performs service-specific authorization for the given user.
* @param user UserGroupInformation object
* @param protocolName name of the protocol to use
* @param addr address to authorize for
*/","* Authorize the incoming client connection.
   * 
   * @param user client user
   * @param protocolName - the protocol
   * @param addr InetAddress of incoming connection
   * @throws AuthorizationException when the client isn't authorized to talk the protocol",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FileSystem:getHomeDirectory(),2445,2456,"/**
* Returns a Path object for the user's home directory.
* @return Path object or null if unable to determine home directory
*/","Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   * @return the path.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkAccessPermissions,"org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)",2855,2877,"/**
* Verifies access permissions for a file based on user/group ownership and mode.
* @param stat FileStatus object containing path and permissions
* @param mode FsAction indicating read/write/delete action
* @throws AccessControlException if permission is denied
*/","* This method provides the default implementation of
   * {@link #access(Path, FsAction)}.
   *
   * @param stat FileStatus to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws IOException for any error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(),280,283,"/**
 * Initializes the ViewFileSystem instance with current user and timestamp.
 */","* This is the  constructor with the signature needed by
   * {@link FileSystem#createFileSystem(URI, Configuration)}
   *
   * After this constructor is called initialize() is called.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",227,290,"/**
* Initializes a ViewFs object from the given URI and configuration.
* @param theUri URI of the view file system
* @param conf Configuration for the view file system
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   *
   * @param theUri which must be that of ViewFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)",617,770,"/**
* Initializes ViewFS for the given configuration and URI.
* @param config Configuration object
* @param viewName Name of the mount table (or null to use default)
* @param theUri The URI to initialize ViewFS for
* @param initingUriAsFallbackOnNoMounts Whether to use the URI as a fallback if no mounts are specified in the configuration
*/","* Create Inode Tree from the specified mount-table specified in Config.
   *
   * @param config the mount table keys are prefixed with
   *               FsConstants.CONFIG_VIEWFS_PREFIX.
   * @param viewName the name of the mount table
   *                 if null use defaultMT name.
   * @param theUri heUri.
   * @param initingUriAsFallbackOnNoMounts initingUriAsFallbackOnNoMounts.
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *                                        not found.
   * @throws URISyntaxException if the URI does not have an authority
   *                            it is badly formed.
   * @throws FileAlreadyExistsException there is a file at the path specified
   *                                    or is discovered on one of its ancestors.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)",3881,3889,"/**
* Initializes a Key object from a URI and configuration.
* @param uri Hadoop URI
* @param conf Hadoop Configuration
* @param unique Unique identifier for the key
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory(),461,472,"/**
* Retrieves the user's directory path based on their username.
*@return Path object representing the user's directory
*/","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * 
   * @return current user's home directory.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",283,335,"/**
* Establishes an HTTP connection to the specified URL with the provided authentication token and optional delegation token.
* @param url the target URL
* @param token authentication token
* @param doAs optional user ID to impersonate (null for direct access)
* @return HttpURLConnection object or null on failure
*/","* Returns an authenticated {@link HttpURLConnection}. If the Delegation
   * Token is present, it will be used taking precedence over the configured
   * <code>Authenticator</code>. If the <code>doAs</code> parameter is not NULL,
   * the request will be done on behalf of the specified <code>doAs</code> user.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @param doAs user to do the the request on behalf of, if NULL the request is
   * as self.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",134,153,"/**
* Authenticates URL using delegation token if available, otherwise authenticates with the provided token.
* @param url URL to authenticate
* @param token authentication token
* @throws IOException or AuthenticationException on failure",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getBestUGI,"org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)",606,615,"/**
* Determines the UserGroupInformation object based on ticket cache path and user.
* @param ticketCachePath path to the ticket cache or null
* @param user the user name or null
* @return UserGroupInformation object or throws IOException if an error occurs","* Find the most appropriate UserGroupInformation to use
   *
   * @param ticketCachePath    The Kerberos ticket cache path, or NULL
   *                           if none is specfied
   * @param user               The user name, or NULL if none is specified.
   *
   * @return                   The most appropriate UserGroupInformation
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytabAndReturnUGI,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)",1388,1399,"/**
* Authenticates user with functional mask.
* @param user principal name
* @param path keytab file path
* @return UserGroupInformation object or null on failure
*/","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and login them in. This new user does not affect the currently
   * logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException if the keytab file can't be read
   * @return UserGroupInformation.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,"org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)",1999,2010,"/**
* Logs user group information with masked sensitive data.
* @param log Logger instance
* @param ugi UserGroupInformation object
*/","* Log all (current, real, login) UGI and token info into specified log.
   * @param ugi - UGI
   * @param log - log.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,<init>,org.apache.hadoop.security.alias.UserProvider:<init>(),44,47,"/**
 * Initializes the UserProvider instance with the current user's credentials.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsCurrentUser,org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction),547,550,"/**
* Executes privileged exception action with user context.
* @param action PrivilegedExceptionAction to execute
* @return Result of the action or null if failed
*/","* Perform the given action as the daemon's current user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> generic type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,<init>,org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod),89,120,"/**
* Initializes SaslRpcServer with given authentication method.
* @param authMethod AuthMethod enum value (SIMPLE, TOKEN, KERBEROS, etc.)
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,create,"org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)",122,173,"/**
* Initializes and returns a SaslServer instance based on the specified authentication method.
* @param connection HBase Connection object
* @param saslProperties Map of SASL properties
* @param secretManager Secret manager for token-based authentication
* @return SaslServer instance or null if creation fails
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,<init>,org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration),47,51,"/**
* Initializes UserProvider with configuration and sets current user credentials.
* @param conf Hadoop Configuration object
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDoAsUser,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser(),1129,1134,"/**
* Returns proxy user's function mask.
* @return null if not a proxy user, or the function mask otherwise
*/","* Get the doAs user name.
   *
   * 'actualUGI' is the UGI of the user creating the client
   * It is possible that the creator of the KMSClientProvier
   * calls this method on behalf of a proxyUser (the doAsUser).
   * In which case this call has to be made as the proxy user.
   *
   * @return the doAs user name.
   * @throws IOException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)",411,453,"/**
* Establishes a protocol proxy with retry logic.
* @param protocol data transfer protocol
* @param clientVersion client version
* @param addr server address
* @param conf configuration
* @param rpcTimeout RPC timeout
* @param connectionRetryPolicy retry policy for connections
* @param timeout maximum wait time
* @return ProtocolProxy object or throws exception if failed
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,shouldAuthenticateOverKrb,org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb(),556,569,"/**
* Verifies KERBEROS authentication method for user.
* @return True if authenticated, False otherwise
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRestrictParserDefault,org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object),288,299,"/**
* Checks if resource is a valid function mask.
* @param resource object to verify
* @return true if valid, false otherwise
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$1:run(),1082,1109,"/**
* Executes the main function for mask-related operations.
* @throws IOException on error reading responses
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doKerberosRelogin,org.apache.hadoop.ipc.Server:doKerberosRelogin(),3407,3425,"/**
* Attempts to re-login from IPC Server based on user group membership.
* @throws IOException if an I/O error occurs
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,run,org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[]),173,199,"/**
* Calculates function mask based on provided arguments.
* @param args input array
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,cedeActive,org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int),91,96,"/**
* Cedes control to ZooKeeper after specified time in millis.
* @param millisToCede time to wait before handing over control
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,gracefulFailover,org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover(),98,102,"/**
* Performs cluster maintenance and mask operations.
* @throws IOException if an I/O error occurs
* @throws AccessControlException if access control fails
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])",1113,1117,"/**
* Legacy function to scan for masks between two keys.
* @param beginKey starting key of mask range
* @param endKey ending key of mask range
*/","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",488,494,"/**
* Creates a ProtocolProxy instance with the given parameters.
* @param protocol target protocol class
* @param clientVersion target client version
* @param addr server address to connect to
* @param conf configuration for this proxy
* @param factory socket factory to use
* @return a new ProtocolProxy instance or throws IOException if creation fails
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",511,519,"/**
* Invokes m1 to establish a connection and then calls its m2 method.
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input tocket.
   * @param conf input conf.
   * @param factory input factory.
   * @return the protocol proxy.
   * @throws IOException raised on errors performing I/O.
   *",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",46,54,"/**
* Initializes a ZKFC protocol client-side translator using an InetSocketAddress.
* @param addr address to connect to
* @param conf configuration for the RPC connection
* @param socketFactory factory to create sockets
* @param timeout timeout in milliseconds for the RPC connection
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",74,82,"/**
* Initializes HA service protocol client-side translator.
* @param addr server address
* @param conf configuration parameters
* @param socketFactory socket factory instance
* @param timeout connection timeout in milliseconds
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGet,"org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",325,360,"/**
* Handles FUNC_MASK request by processing log settings and outputting results.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/AdminAuthorizedServlet.java,doGet,"org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",36,45,"/**
* Calls parent handler and child service if child service succeeds.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,isInstrumentationAccessAllowed,"org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1660,1674,"/**
* Checks if servlet context has administrative access.
* @param servletContext Servlet context
* @param request HTTP request
* @param response HTTP response
* @return true if admin access granted, false otherwise
*/","* Checks the user has privileges to access to instrumentation servlets.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE
   * (default value) it always returns TRUE.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to TRUE
   * it will check that if the current user is in the admin ACLS. If the user is
   * in the admin ACLs it returns TRUE, otherwise it returns FALSE.
   *
   * @param servletContext the servlet context.
   * @param request the servlet request.
   * @param response the servlet response.
   * @return TRUE/FALSE based on the logic decribed above.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory(),169,172,"/**
* Calls underlying file system implementation to perform operation 'm1'. 
* @return result of the operation, as returned by fsImpl.m1()",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoot,org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path),3439,3442,"/**
* Generates a masked path by prepending the trash prefix.","* Get the root directory of Trash for current user when the path specified
   * is deleted.
   *
   * @param path the trash root of the path to be determined.
   * @return the default implementation returns {@code /user/$USER/.Trash}",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoots,org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean),3452,3478,"/**
* Retrieves a collection of user trash status by ID, 
* optionally including sub-users' trash if requested. 
* @param allUsers whether to include sub-users' trash
* @return list of FileStatus objects or empty list if failed
*/","* Get all the trash roots for current user or all users.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all the trash root directories.
   *         Default FileSystem returns .Trash under users' home directories if
   *         {@code /user/$USER/.Trash} exists.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory(),297,300,"/**
* Calls underlying file system's m1() method.
* @return result from file system's implementation
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,access,"org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",2840,2844,"/**
* Performs file system access control on the specified Path.
* @param path file system path
* @param mode file system action (e.g. read, write)
*/","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   *
   * Note that the {@link #getFileStatus(Path)} call will be subject to
   * authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException see specific implementation",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,access,"org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1046,1050,"/**
* Authorizes access to a file system resource.
* @param path file system path
* @param mode desired action (read, write, etc.)
*/","* The specification of this method matches that of
   * {@link FileContext#access(Path, FsAction)}
   * except that an UnresolvedLinkException may be thrown if a symlink is
   * encountered in the path.
   *
   * @param path the path.
   * @param mode fsaction mode.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",392,396,"/**
* Initializes and configures view file system.
* @param theUri URI of file system to access
* @param conf configuration object for file system settings","* Convenience Constructor for apps to call directly.
   * @param theUri which must be that of ViewFileSystem
   * @param conf conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>(),123,125,"/**
* Initializes File System Overload Scheme view.
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration),213,216,"/**
* Initializes ViewFs with the given configuration.
* @param conf Hadoop Configuration object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",3877,3879,"/**
* Constructs a new Key instance from a URI and configuration.
* @param uri unique identifier as a URI
* @param conf configuration parameters
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUnique,"org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)",3671,3674,"/**
* Retrieves file system configuration based on URI and configuration.
* @param uri identifier for the file system
* @param conf application configuration settings
*/",The objects inserted into the cache using this method are all unique.,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory(),151,154,"/**
 * Delegates to myFs's m1() method for file system operations.",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",243,271,"/**
* Creates a FileContext instance with default filesystem and configuration.
* @param defFs default file system
* @param aConf configuration settings
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getHomeDirectory,org.apache.hadoop.fs.FileContext:getHomeDirectory(),577,579,"/**
* Returns the result of calling m1() on the default file system.
*/","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * @return the home directory",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getHomeDirectory,org.apache.hadoop.fs.FilterFs:getHomeDirectory(),83,86,"/**
* Invokes file system operation m1(). 
* @return result of myFs.m1()
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",230,235,"/**
* Handles HTTP request with authentication using the provided token.
* @param url requested URL
* @param token authenticated token for authorization
* @return HttpURLConnection object or null if authentication fails
*/","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",268,280,"/**
* Initializes and returns a FileSystem instance for the given URI.
* @param uri file system URI
* @param conf configuration object
* @param user username (for authentication)
*/","* Get a FileSystem instance based on the uri, the passed in
   * configuration and the user.
   * @param uri of the filesystem
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return the filesystem instance
   * @throws IOException failure to load
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   * somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",571,583,"/**
* Initializes and returns a Hadoop File System instance using the provided URI and configuration.
* @param uri file system URI
* @param conf Hadoop configuration object
* @return Initialized FileSystem object
*/","* Returns the FileSystem for this URI's scheme and authority and the
   * given user. Internally invokes {@link #newInstance(URI, Configuration)}
   * @param uri uri of the filesystem.
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return filesystem instance
   * @throws IOException if the FileSystem cannot be instantiated.
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   *         somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromTicketCache,"org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)",627,638,"/**
* Authenticates user with KERBEROS method and returns UserGroupInformation.
* @param ticketCache Kerberos ticket cache
* @param user principal username
*/","* Create a UserGroupInformation from a Kerberos ticket cache.
   * 
   * @param user                The principal name to load from the ticket
   *                            cache
   * @param ticketCache     the path to the ticket cache file
   *
   * @throws IOException        if the kerberos login fails
   * @return UserGroupInformation.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,loginFromKeytab,org.apache.hadoop.security.KDiag:loginFromKeytab(),628,657,"/**
* Initializes and logs in UserGroupInformation using keytab if available.
* @throws IOException on authentication failure or other errors
*/","* Log in from a keytab, dump the UGI, validate it, then try and log in again.
   *
   * That second-time login catches JVM/Hadoop compatibility problems.
   * @throws IOException Keytab loading problems",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytab,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)",1127,1147,"/**
* Fetches and configures user credentials from a keytab file.
* @param user username
* @param path keytab file path
*/","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation),2017,2020,"/**
* Logs in using UserGroupInformation.
* @param ugi instance of UserGroupInformation to use
*/","* Log all (current, real, login) UGI and token info into UGI debug log.
   * @param ugi - UGI
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getActualUgi,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi(),1167,1190,"/**
* Resolves UserGroupInformation based on current context and Kerberos settings.
* @return resolved UserGroupInformation object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildNegotiateResponse,org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List),3445,3467,"/**
* Negotiates SASL protocol based on provided authentication methods.
* @param authMethods list of AuthMethod objects to consider
* @return RpcSaslProto object representing negotiated SASL protocol
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,createSaslServer,org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod),2621,2626,"/**
* Creates a SASL server instance for the given AuthMethod.
* @param authMethod authentication method to use
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",366,372,"/**
* Creates a new ProtocolProxy instance with the given parameters.
* @param protocol class of the protocol to proxy
* @param clientVersion client version number
* @param addr server address
* @param conf configuration object
* @param connTimeout connection timeout in milliseconds
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)",387,394,"/**
* Returns a result of the masked operation on the given configuration.
* @param protocol data transmission protocol
* @param clientVersion client version information
* @param addr network address for communication
* @param conf configuration settings
* @param rpcTimeout RPC request timeout value
* @param timeout overall operation timeout value
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)",261,263,"/**
* Initializes Resource with provided details and default parser restrictions.
* @param resource Object to be wrapped as Resource
* @param name Name of the Resource object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",467,473,"/**
* Executes protocol-specific functionality using a socket.
* @param protocol class implementing the desired protocol
* @param clientVersion client version number
* @param addr server address to connect to
* @param conf configuration settings for the connection
* @param factory factory to create and manage sockets
* @return result of the protocol-specific operation, or null on failure
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input Configuration.
   * @param factory input factory.
   * @throws IOException raised on errors performing I/O.
   * @return proxy.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",773,780,"/**
* Creates a new ProtocolProxy instance with the given parameters.
* @param protocol Class of the protocol to proxy
* @param clientVersion Client version number
* @param addr Remote address to connect to
* @param conf Configuration object for network settings
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   * 
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input configuration.
   * @param <T> Generics Type T.
   * @return a protocol proxy
   * @throws IOException if the thread is interrupted.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,getUgmProtocol,org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol(),97,105,"/**
* Retrieves the GetUserMappings protocol instance.
* @return GetUserMappingsProtocol object
*/","* Get a client of the {@link GetUserMappingsProtocol}.
   * @return A {@link GetUserMappingsProtocol} client proxy.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getZKFCProxy,"org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)",164,173,"/**
* Creates a ZKFC protocol instance with client-side translator.
* @param conf ZooKeeper configuration
* @param timeoutMs connection timeout in milliseconds
*/","* @return a proxy to the ZKFC which is associated with this HA service.
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)",146,156,"/**
* Creates an HA service protocol client with custom configuration and timeouts.
* @param conf the configuration to use
* @param timeoutMs the connection timeout in milliseconds
* @param retries the maximum number of retries
* @param addr the address of the HA service
* @return an HAServiceProtocol instance or null if creation fails
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,isInstrumentationAccessAllowed,"org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",152,156,"/**
 * Delegates HTTP server processing to HttpServer2 instance. 
 * @param request incoming HTTP request
 * @param response outgoing HTTP response
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doGet,"org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1745,1758,"/**
* Handles FUNC_MASK request by invoking m2() method and rendering text/plain response.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileOutputServlet.java,doGet,"org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",48,77,"/**
* Handles instrumentation requests; redirects to auto-refresh if output file is incomplete.
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,doGet,"org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",187,327,"/**
* Handles FUNC_MASK requests by validating input, executing the async profiler, and returning a response. 
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,doGet,"org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",57,83,"/**
* Handles FUNC_MASK HTTP request.
* @param request incoming HttpServletRequest
* @param response HttpServletResponse to be written
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,moveToTrash,org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path),129,204,"/**
* Moves a file or directory to the trash.
* @param path Path object of item to be moved
* @return true if successful, false otherwise
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(),241,244,"/**
* Returns the function mask path.
* @return Path to function mask file
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path),246,249,"/**
* Constructs a new file system path with the specified mask.
* @param path existing file system path
* @return new file system path with applied mask
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),1180,1220,"/**
* Resolves user trash root path based on provided path.
* @param path input path to resolve
*/","* Get the trash root directory for current user when the path
   * specified is deleted.
   *
   * If FORCE_INSIDE_MOUNT_POINT flag is not set, return the default trash root
   * from targetFS.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true,
   * <ol>
   *   <li>
   *     If the trash root for path p is in the same mount point as path p,
   *       and one of:
   *       <ol>
   *         <li>The mount point isn't at the top of the target fs.</li>
   *         <li>The resolved path of path is root (in fallback FS).</li>
   *         <li>The trash isn't in user's target fs home directory
   *            get the corresponding viewFS path for the trash root and return
   *            it.
   *         </li>
   *       </ol>
   *   </li>
   *   <li>
   *     else, return the trash root under the root of the mount point
   *     (/{mntpoint}/.Trash/{user}).
   *   </li>
   * </ol>
   *
   * These conditions handle several different important cases:
   * <ul>
   *   <li>File systems may need to have more local trash roots, such as
   *         encryption zones or snapshot roots.</li>
   *   <li>The fallback mount should use the user's home directory.</li>
   *   <li>Cloud storage systems should not use trash in an implicity defined
   *        home directory, per a container, unless it is the fallback fs.</li>
   * </ul>
   *
   * @param path the trash root of the path to be determined.
   * @return the trash root path.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoot,org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),689,692,"/**
* Invokes file system operation 'm1' on provided file path.
* @param path file system path to operate on
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,run,org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run(),278,320,"/**
* Periodically empties trash and closes file system.
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date),212,220,"/**
* Recursively creates checkpoints in the trashed directories.
* @param date Date object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean),232,239,"/**
* Recursively deletes trash roots, optionally deleting immediately.
* @param deleteImmediately true to bypass trash retention period
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoots,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean),1231,1297,"/**
* Fetches and aggregates user and system trash roots from file systems.
* @param allUsers true to include user-specific trash, false otherwise
*/","* Get all the trash roots for current user or all users.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true, we also return trash roots
   * under the root of each mount point, with their viewFS paths.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all Trash root directories.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoots,org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean),694,697,"/**
* Delegates file status retrieval to underlying filesystem.
* @param allUsers whether to include all users in result
* @return collection of FileStatus objects or empty if none found
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,testAccess,"org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)",110,118,"/**
* Checks if a file system operation is allowed based on access control and existence.
* @param item PathData object containing file path and other details
* @param action FsAction enum value representing the desired operation
* @return true if operation is allowed, false otherwise
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",576,582,"/**
* Resolves and executes file system action on the specified path.
* @param path filesystem path to operate on
* @param mode file system operation mode (e.g. read, write)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,access,"org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",470,474,"/**
* Delegates file system operation to underlying implementation.
* @param path file system path to operate on
* @param mode file system action (e.g. read, write, delete)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",208,211,"/**
* Performs file system action on specified file.
* @param path Path to file
* @param mode File system action (e.g. read, write)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,access,"org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",428,434,"/**
* Resolves file system path and executes corresponding action.
* @param path Path to resolve
* @param mode File system operation mode (e.g. read, write)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,access,"org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",132,137,"/**
* Executes file system operation with specified action and mode.
* @param path file system path
* @param mode file system action (e.g. read, write)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration),403,405,"/**
* Initializes ViewFileSystem with given configuration.
* @param conf Hadoop Configuration object
*/","* Convenience Constructor for apps to call directly.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,addFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",240,244,"/**
* Updates cache with file system data for given URI and configuration.
* @param uri unique identifier of the resource
* @param conf configuration settings
* @param fs file system instance
*/","* This method adds a FileSystem instance to the cache so that it can
   * be retrieved later. It is only for testing.
   * @param uri the uri to store it under
   * @param conf the configuration to store it under
   * @param fs the FileSystem to store
   * @throws IOException if the current user cannot be determined.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,removeFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",246,250,"/**
* Puts file system object into cache at specified URI and configuration. 
* @param uri unique identifier for cache entry
* @param conf configuration parameters for caching
* @param fs file system object to cache
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",3665,3668,"/**
* Creates a FileSystem instance based on URI and configuration.
* @param uri unique file system identifier
* @param conf Hadoop job configuration
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",594,611,"/**
* Resolves a FileSystem object from the given URI and configuration.
* @param uri URI to resolve
* @param config configuration parameters
* @return FileSystem object or throws IOException if resolution fails. 
* If no scheme is specified, it attempts to use default values from the configuration. 
* If still unresolved, it uses cached results.","* Returns the FileSystem for this URI's scheme and authority.
   * The entire URI is passed to the FileSystem instance's initialize method.
   * This always returns a new FileSystem object.
   * @param uri FS URI
   * @param config configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",373,376,"/**
* Creates a FileContext instance based on default file system and configuration.
* @param defFS default file system to use
* @param aConf application configuration
*/","* Create a FileContext with specified FS as default using the specified
   * config.
   * 
   * @param defFS default fs.
   * @param aConf configutration.
   * @return new FileContext with specified FS as default.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",230,235,"/**
* Handles HTTP requests with authentication using the provided token.
* @param url URL to send request to
* @param token Authenticated token for request
* @return HttpURLConnection object or null if not found
*/","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,execute,org.apache.hadoop.security.KDiag:execute(),282,420,"/**
* Configures and checks Hadoop security settings.
* @return true if security is enabled, false otherwise
*/","* Execute diagnostics.
   * <p>
   * Things it would be nice if UGI made accessible
   * <ol>
   *   <li>A way to enable JAAS debug programatically</li>
   *   <li>Access to the TGT</li>
   * </ol>
   * @return true if security was enabled and all probes were successful
   * @throws KerberosDiagsFailure explicitly raised failure
   * @throws Exception other security problems",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,maybeDoLoginFromKeytabAndPrincipal,org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[]),82,106,"/**
* Processes command line arguments with Kerberos handling.
* @param args array of command line arguments
* @return filtered argument array or null if kerberized
*/","* Parse arguments looking for Kerberos keytab/principal.
   * If both are found: remove both from the argument list and attempt login.
   * If only one of the two is found: remove it from argument list, log warning
   * and do not attempt login.
   * If neither is found: return original args array, doing nothing.
   * Return the pruned args array if either flag is present.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,main,org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]),2300,2318,"/**
* Retrieves and displays user group information for current user.
* Optionally, fetches UGI from a keytab if two command-line arguments are provided.
* @param args array of command-line arguments (keytab path and password)
*/","* A test method to print out the current user's UGI.
   * @param args if there are two arguments, read the user from the keytab
   * and print it out.
   * @throws Exception Exception.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)",309,329,"/**
* Configures user authentication using a keytab file.
* @param conf Hadoop configuration
* @param keytabFileKey Key for keytab file in config
* @param userNameKey Key for principal name in config
* @param hostname Current host name
*/","* Login as a principal specified in config. Substitute $host in user's Kerberos principal 
   * name with hostname. If non-secure mode - return. If no keytab available -
   * bail out with an exception
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @param hostname
   *          hostname to use for substitution
   * @throws IOException if the config doesn't specify a keytab",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createConnection,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)",502,537,"/**
* Establishes an authenticated HTTP connection to the specified URL.
* @param url target URL
* @param method HTTP request method (e.g. GET, POST, PUT)
* @return configured HttpURLConnection object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String),1025,1059,"/**
* Retrieves a delegation token with the given renewer.
* @param renewer user to delegate token for
* @return Token object or null on failure
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),1061,1087,"/**
* Renews delegation token and fetches result.
* @param dToken Token object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),1089,1116,"/**
* Cancels delegation token.
* @param dToken Token to cancel
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",3307,3405,"/**
 * Initializes a Server instance with the specified configuration and settings.
 * @param bindAddress server address to bind to
 * @param port server port number
 * @param rpcRequestClass request class for RPC requests
 * @param handlerCount number of handlers to use
 * @param numReaders number of readers to use (or -1 for default)
 * @param queueSizePerHandler maximum queue size per handler (or -1 for default)
 * @param conf server configuration
 * @param serverName server name
 * @param secretManager secret manager instance
 * @param portRangeConfig port range configuration
 */","* Constructs a server listening on the named port and address.  Parameters passed must
   * be of the named class.  The <code>handlerCount</code> determines
   * the number of handler threads that will be used to process calls.
   * If queueSizePerHandler or numReaders are not -1 they will be used instead of parameters
   * from configuration. Otherwise the configuration will be picked up.
   * 
   * If rpcRequestClass is null then the rpcRequestClass must have been 
   * registered via {@link #registerProtocolEngine(RPC.RpcKind,
   *  Class, RPC.RpcInvoker)}
   * This parameter has been retained for compatibility with existing tests
   * and usage.
   *
   * @param bindAddress input bindAddress.
   * @param port input port.
   * @param rpcRequestClass input rpcRequestClass.
   * @param handlerCount input handlerCount.
   * @param numReaders input numReaders.
   * @param queueSizePerHandler input queueSizePerHandler.
   * @param conf input Configuration.
   * @param serverName input serverName.
   * @param secretManager input secretManager.
   * @param portRangeConfig input portRangeConfig.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildSaslNegotiateResponse,org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse(),2603,2619,"/**
* Negotiates SASL protocol with server, potentially using token authentication.
* @throws InterruptedException
* @throws SaslException
* @throws IOException
*/","* Process the Sasl's Negotiate request, including the optimization of 
     * accelerating token negotiation.
     * @return the response to Negotiate request - the list of enabled 
     *         authMethods and challenge if the TOKENS are supported. 
     * @throws SaslException - if attempt to generate challenge fails.
     * @throws IOException - if it fails to create the SASL server for Tokens",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",326,332,"/**
* Creates a ProtocolProxy instance with default timeout.
* @param protocol the protocol class
* @param clientVersion the client version
* @param addr the server address
* @param conf the configuration
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",346,351,"/**
* Performs a masked function call with specified parameters.
* @param protocol protocol class
* @param clientVersion client version number
* @param addr server address
* @param conf configuration object
* @param connTimeout connection timeout value
* @return result of the masked function call or throws IOException if failed
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)",998,1000,"/**
 * Initializes mask operation using input stream and resource name.
 * @param in InputStream containing mask data
 * @param name Name of the resource to process
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param in InputStream to deserialize the object from.
   * @param name the name of the resource because InputStream.toString is not
   * very descriptive some times.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object),253,255,"/**
* Constructs a new Resource instance from an Object.
* @param resource the Object to be wrapped as a Resource
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",728,734,"/**
* Creates an instance of type T based on given protocol and configuration.
* @param protocol Type to instantiate
* @param clientVersion Client version for instantiation
* @param addr Address to bind to
* @param conf Configuration to use
* @return Instance of type T or throws IOException if failed
*/","* Construct a client-side proxy object with the default SocketFactory.
    *
    * @param <T> Generics Type T.
    * @param protocol input protocol.
    * @param clientVersion input clientVersion.
    * @param addr input addr.
    * @param conf input Configuration.
    * @return a proxy instance
    * @throws IOException  if the thread is interrupted.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,run,org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[]),62,79,"/**
* Processes user groups by iterating over input usernames.
* @param args array of usernames to process
*/","* Get the groups for the users given and print formatted output to the
   * {@link PrintStream} configured earlier.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,gracefulFailoverThroughZKFCs,org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget),276,290,"/**
* Performs failover to a HA Service Target node.
* @param toNode target node for failover
* @return 0 on success, -1 on failure
*/","* Initiate a graceful failover by talking to the target node's ZKFC.
   * This sends an RPC to the ZKFC, which coordinates the failover.
   *
   * @param toNode the node to fail to
   * @return status code (0 for success)
   * @throws IOException if failover does not succeed",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeRemoteActive,"org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)",749,756,"/**
* Fetches ZKFC protocol from HAServiceTarget with specified timeout.
* @param remote HAServiceTarget instance
* @param timeout time to wait for response in milliseconds
* @return ZKFCProtocol object or null on failure
*/","* Ask the remote zkfc to cede its active status and wait for the specified
   * timeout before attempting to claim leader status.
   * @param remote node to ask
   * @param timeout amount of time to cede
   * @return the {@link ZKFCProtocol} used to talk to the ndoe
   * @throws IOException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)",131,138,"/**
* Retrieves HA service protocol with custom configuration and retry logic.
* @param conf HAServiceProtocol configuration
* @param timeoutMs socket timeout in milliseconds
* @param retries number of connection retries
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)",140,144,"/**
* Creates an HA service protocol instance with default replication factor.
* @param conf HBase configuration
* @param timeoutMs socket timeout in milliseconds
* @param addr server address to connect to
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,doGet,"org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",175,232,"/**
* Handles JSON-based FUNC_MASK requests.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/","* Process a GET request for the specified resource.
   * 
   * @param request
   *          The servlet request we are processing
   * @param response
   *          The servlet response we are creating",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(),206,210,"/**
* Calls m1 with current date as parameter.
* @throws IOException if an I/O error occurs
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(),222,225,"/**
* Calls m1 with default flag value.
* 
* @throws IOException if an I/O error occurs
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpointsImmediately,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately(),227,230,"/**
* Applies function mask.",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processPath,org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData),77,108,"/**
* Evaluates item based on the specified flag and updates exit code accordingly.
* @param item PathData object to be evaluated
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",253,257,"/**
* Calls superclass method with result of m1() and specified file action.
* @param path file system path
* @param mode file system operation (read, write, etc.)
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",536,558,"/**
* Creates a FileSystem instance from a URI or configuration.
* @param uri file system URI
* @param conf Hadoop Configuration object
* @return created FileSystem instance
*/","* Get a FileSystem for this URI's scheme and authority.
   * <ol>
   * <li>
   *   If the configuration has the property
   *   {@code ""fs.$SCHEME.impl.disable.cache""} set to true,
   *   a new instance will be created, initialized with the supplied URI and
   *   configuration, then returned without being cached.
   * </li>
   * <li>
   *   If the there is a cached FS instance matching the same URI, it will
   *   be returned.
   * </li>
   * <li>
   *   Otherwise: a new FS instance will be created, initialized with the
   *   configuration and URI, cached and returned to the caller.
   * </li>
   * </ol>
   * @param uri uri of the filesystem.
   * @param conf configrution.
   * @return filesystem instance.
   * @throws IOException if the FileSystem cannot be instantiated.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstanceLocal,org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration),631,634,"/**
 * Returns an instance of LocalFileSystem based on configuration.
 * @param conf Hadoop Configuration object
 */","* Get a unique local FileSystem object.
   * @param conf the configuration to configure the FileSystem with
   * @return a new LocalFileSystem object.
   * @throws IOException FS creation or initialization failure.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,getNewInstance,"org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",42,45,"/**
 * Creates an instance of FileSystem with specified configuration and URI.
 * @param uri file system URI
 * @param conf configuration parameters
 */","* Gets new file system instance of given uri.
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getNewInstance,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",234,250,"/**
* Initializes or returns a pre-existing file system for the given URI.
* @param uri unique identifier for the file system
* @param conf configuration parameters
* @return initialized file system or null if not found
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink(),1244,1265,"/**
* Retrieves file statuses from a fallback file system.
* @throws IOException if an I/O error occurs
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem),385,388,"/**
 * Returns an instance of FileContext with the given default file system.
 * @param defaultFS the default file system to use
 */","* Create a FileContext for specified file system using the default config.
   * 
   * @param defaultFS default fs.
   * @return a FileContext with the specified AbstractFileSystem
   *                 as the default FS.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)",456,473,"/**
* Creates FileContext instance based on default filesystem URI and configuration.
* @param defaultFsUri URI of default filesystem
* @param aConf configuration settings
* @throws UnsupportedFileSystemException if filesystem is not supported
*/","* Create a FileContext for specified default URI using the specified config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @param aConf configrution.
   * @return new FileContext for specified uri
   * @throws UnsupportedFileSystemException If the file system with specified is
   *           not supported
   * @throws RuntimeException If the file system specified is supported but
   *         could not be instantiated, or if login fails.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,run,org.apache.hadoop.security.KDiag:run(java.lang.String[]),197,244,"/**
* Parses command line arguments and configures the system.
* @param argv array of command line arguments
* @return 0 on success, -1 on failure.",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,init,org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]),116,177,"/**
* Parses command line arguments and executes corresponding operations.
* @param args array of command line arguments
* @return 0 on success, non-zero on error
*/","* Parse the command line arguments and initialize subcommand.
   * Also will attempt to perform Kerberos login if both -principal and -keytab
   * flags are passed in args array.
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws Exception Exception.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",287,292,"/**
* Calls the overloaded method m2 with an additional parameter from m1.
*/","* Login as a principal specified in config. Substitute $host in
   * user's Kerberos principal name with a dynamically looked-up fully-qualified
   * domain name of the current host.
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @throws IOException if login fails",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)",544,608,"/**
* Fetches an object of type T from the server.
* @param jsonOutput data to send in request body
* @param expectedResponse HTTP status code to expect
* @param klass class of object to fetch
* @param authRetryCount number of retries for authentication errors
* @return object of type T or null if not found
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",1189,1198,"/**
* Initializes a Server instance with custom configuration.
* @param bindAddress server address
* @param port server port
* @param paramClass parameter class
* @param handlerCount number of handlers
* @param numReaders number of readers
* @param queueSizePerHandler queue size per handler
* @param conf configuration object
* @param serverName server name
* @param secretManager secret manager for token identifiers
* @param portRangeConfig port range configuration
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)",3264,3271,"/**
* Initializes a Server instance with default settings.
* @param bindAddress server binding address
* @param port server port number
* @param paramClass parameter class for the server
* @param handlerCount initial handler count
* @param conf configuration object for the server
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)",3273,3280,"/**
* Initializes a new Server instance with specified configuration.
* @param bindAddress server address to bind
* @param port server listening port
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslMessage,org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2327,2385,"/**
* Handles SASL protocol negotiation and authentication based on client message state.
* @param saslMessage client SASL message
* @return RpcSaslProto response or null for success with SIMPLE auth
* @throws SaslException, IOException, AccessControlException, InterruptedException if errors occur
*/","* Process a saslMessge.
     * @param saslMessage received SASL message
     * @return the sasl response to send back to client
     * @throws SaslException if authentication or generating response fails, 
     *                       or SASL protocol mixup
     * @throws IOException if a SaslServer cannot be created
     * @throws AccessControlException if the requested authentication type 
     *         is not supported or trying to re-attempt negotiation.
     * @throws InterruptedException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",305,312,"/**
* Retrieves and processes masked data for a given protocol.
* @param protocol the class of the protocol to process
* @param clientVersion the version of the client requesting data
* @param addr the address of the client
* @param conf configuration settings
* @return an object of type T containing the processed data, or throws IOException","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.lang.String),923,925,"/**
* Masks resource with specified name.
* @param name unique identifier of resource to mask
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param name resource to be added, the classpath is examined for a file 
   *             with that name.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.net.URL),941,943,"/**
* Masks URL using resource processing.
* @param url URL to process
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param url url of the resource to be added, the local filesystem is 
   *            examined directly to find the resource, without referring to 
   *            the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path),959,961,"/**
* Applies mask to specified image file.
* @param file Path to image resource
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param file file-path of resource to be added, the local filesystem is
   *             examined directly to find the resource, without referring to 
   *             the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream),980,982,"/**
* Processes input stream using resource wrapper.
* @param in InputStream to be processed
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * WARNING: The contents of the InputStream will be cached, by this method. 
   * So use this sparingly because it does increase the memory consumption.
   * 
   * @param in InputStream to deserialize the object from. In will be read from
   * when a get or set is called next.  After it is read the stream will be
   * closed.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",66,72,"/**
* Initializes and configures the HA service protocol client-side translator.
* @param addr HA service address
* @param conf configuration settings
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doGracefulFailover,org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover(),660,739,"/**
* Attempts to failover and become the active node.
* @throws ServiceFailedException if failover is unsuccessful
*/","* Coordinate a graceful failover. This proceeds in several phases:
   * 1) Pre-flight checks: ensure that the local node is healthy, and
   * thus a candidate for failover.
   * 2a) Determine the current active node. If it is the local node, no
   * need to failover - return success.
   * 2b) Get the other nodes
   * 3a) Ask the other nodes to yield from election for a number of seconds
   * 3b) Ask the active node to yield from the election for a number of seconds.
   * 4) Allow the normal election path to run in other threads. Wait until
   * we either become unhealthy or we see an election attempt recorded by
   * the normal code path.
   * 5) Allow the old active to rejoin the election, so a future
   * failback is possible.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,createProxy,org.apache.hadoop.ha.HealthMonitor:createProxy(),191,193,"/**
* Returns HA service protocol based on configuration.
* @throws IOException if communication error occurs
*/","* Connect to the service to be monitored. Stubbed out for easier testing.
   *
   * @throws IOException raised on errors performing I/O.
   * @return HAServiceProtocol.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)",126,129,"/**
* Returns HA service protocol with default replication factor.
* @param conf configuration object
* @param timeoutMs network operation timeout in milliseconds
*/","* Returns a proxy to connect to the target HA service for health monitoring.
   * If {@link #getHealthMonitorAddress()} is implemented to return a non-null
   * address, then this proxy will connect to that address.  Otherwise, the
   * returned proxy defaults to using {@link #getAddress()}, which means this
   * method's behavior is identical to {@link #getProxy(Configuration, int)}.
   *
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds
   * @return a proxy to connect to the target HA service for health monitoring
   * @throws IOException if there is an error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxy,"org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)",100,103,"/**
* Computes and returns HA service protocol based on configuration and timeout.
* @param conf Configuration object
* @param timeoutMs timeout value in milliseconds
* @return HAServiceProtocol instance
*/","* @return a proxy to connect to the target HA Service.
   * @param timeoutMs timeout in milliseconds.
   * @param conf Configuration.
   * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initialize,"org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",128,175,"/**
* Initializes and validates the HAR filesystem for fetching user profiles.
* @param name URI of the HAR file
* @param conf Configuration object
*/","* Initialize a Har filesystem per har archive. The 
   * archive home directory is the top level directory
   * in the filesystem that contains the HAR archive.
   * Be careful with this method, you do not want to go 
   * on creating new Filesystem instances per call to 
   * path.getFileSystem().
   * the uri of Har is 
   * har://underlyingfsscheme-host:port/archivepath.
   * or 
   * har:///archivepath. This assumes the underlying filesystem
   * to be used in case not specified.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,moveToAppropriateTrash,"org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",77,122,"/**
* Determines if a file path is within the HDFS trash or not.
* @param fs HDFS FileSystem instance
* @param p Path to check
* @param conf Configuration object with trash interval settings
* @return true if path is in trash, false otherwise
*/","* In case of the symlinks or mount points, one has to move the appropriate
   * trashbin in the actual volume of the path p being deleted.
   *
   * Hence we get the file system of the fully-qualified resolved-path and
   * then move the path p to the trashbin in that volume,
   * @param fs - the filesystem of path p
   * @param p - the path being deleted - to be moved to trash
   * @param conf - configuration
   * @return false if the item is already in the trash or trash is disabled
   * @throws IOException on error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,connect,org.apache.hadoop.fs.FsUrlConnection:connect(),55,75,"/**
* Establishes a connection to the specified URL.
* @throws IOException if URI syntax error or connection fails
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",88,90,"/**
* Constructs a PathData object from a string path and configuration.
* @param pathString string representation of the path
* @param conf Hadoop configuration for file system operations
*/","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param pathString a string for a path
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFSofPath,"org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",427,435,"/**
* Initializes and returns a FileSystem instance based on the provided path.
* @param absOrFqPath absolute or fully qualified file system path
* @param conf Hadoop configuration object
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getNamed,"org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)",477,481,"/**
* Legacy function to create a Hadoop File System instance.
* @param name file system name
* @param conf Hadoop configuration
*/","* @deprecated call {@link #get(URI, Configuration)} instead.
   *
   * @param name name.
   * @param conf configuration.
   * @return file system.
   * @throws IOException If an I/O error occurred.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLocal,org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration),508,511,"/**
* Creates a local file system instance from Hadoop configuration.
* @param conf Hadoop configuration object
* @return LocalFileSystem object or null on failure
*/","* Get the local FileSystem.
   * @param conf the configuration to configure the FileSystem with
   * if it is newly instantiated.
   * @return a LocalFileSystem
   * @throws IOException if somehow the local FS cannot be instantiated.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Constructs a ChRootedFileSystem instance from a given URI and configuration.
* @param uri file system URI
* @param conf Hadoop configuration object
*/","* Constructor.
   * @param uri base file system
   * @param conf configuration
   * @throws IOException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,get,"org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",55,57,"/**
* Creates a new Hadoop file system.
* @param uri URI of the file system
* @param conf configuration parameters
*/","* Gets file system instance of given uri.
   *
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return FileSystem.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",258,275,"/**
* Determines the file system instance based on URI scheme.
* @param uri target URI
* @param conf configuration object
* @return cached or non-cached file system instance
*/","* When ViewFileSystemOverloadScheme scheme and target uri scheme are
     * matching, it will not take advantage of FileSystem cache as it will
     * create instance directly. For caching needs please set
     * ""fs.viewfs.enable.inner.cache"" to true.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getFileSystem,org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration),365,367,"/**
* Creates a new Hadoop file system instance based on configuration.
* @param conf configuration settings
*/","* Return the FileSystem that owns this Path.
   *
   * @param conf the configuration to use when resolving the FileSystem
   * @return the FileSystem that owns this Path
   * @throws java.io.IOException thrown if there's an issue resolving the
   * FileSystem",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getFileSystem,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem(),458,476,"/**
* Initializes or retrieves a FileSystem instance.
* @return the initialized FileSystem object
*/","* Return the supplied file system for testing or otherwise get a new file
   * system.
   *
   * @return the file system to use
   * @throws MetricsException thrown if the file system could not be retrieved",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",129,153,"/**
* Creates or retrieves a FileSystem instance based on the provided URI and configuration.
* @param uri unique file system identifier
* @param config system configuration
* @return FileSystem object or null if not created/retrieved
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1159,1226,"/**
* Retrieves file statuses for the given path and its children.
* @param f the input path
* @return array of file status objects
*/","* {@inheritDoc}
     *
     * Note: listStatus on root(""/"") considers listing from fallbackLink if
     * available. If the same directory name is present in configured mount
     * path as well as in fallback link, then only the configured mount path
     * will be listed in the returned result.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI),440,443,"/**
* Creates a FileContext instance with default configuration.
* @param defaultFsUri URI of the default file system
*/","* Create a FileContext for specified URI using the default config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @return a FileContext with the specified URI as the default FS.
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>defaultFsUri</code> is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration),485,496,"/**
* Resolves and validates the default file system context.
* @param aConf Configuration object
* @throws UnsupportedFileSystemException if invalid or unsupported file system is detected
*/","* Create a FileContext using the passed config. Generally it is better to use
   * {@link #getFileContext(URI, Configuration)} instead of this one.
   * 
   * 
   * @param aConf configration.
   * @return new FileContext
   * @throws UnsupportedFileSystemException If file system in the config
   *           is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration),506,509,"/**
 * Initializes FileContext with mask settings from configuration.
 * @param aConf Configuration object containing file system settings
 */","* @param aConf - from which the FileContext is configured
   * @return a FileContext for the local file system using the specified config.
   * 
   * @throws UnsupportedFileSystemException If default file system in the config
   *           is not supported
   *",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,init,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration),233,265,"/**
* Configures metrics from SubsetConfiguration.
* @param metrics2Properties configuration to apply
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)",539,542,"/**
* Fetches data of type T from HTTP connection.
* @param conn HttpURLConnection to query
* @param jsonOutput JSON output object
* @param expectedResponse Expected response code
* @param klass Class of the data to fetch
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",479,493,"/**
* Initializes a Server instance with the specified configuration.
* @param protocolClass Protocol class to use
* @param protocolImpl Implementation of the protocol
*/","* Construct an RPC server.
     *
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param numReaders number of read threads
     * @param queueSizePerHandler the size of the queue contained
     *                            in each Handler
     * @param verbose whether each call should be logged
     * @param secretManager the server-side secret manager for each token type
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",495,535,"/**
* Initializes a Server instance with the specified configuration and protocol.
* @param protocolClass target protocol class
* @param protocolImpl object implementing the protocol
*/","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param alignmentContext provides server state info on client responses
     * @param numReaders input numReaders.
     * @param portRangeConfig input portRangeConfig.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslProcess,org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2242,2314,"/**
* Establishes SASL context and negotiates QoP.
* @param saslMessage RPC message containing SASL data
*/","* Process saslMessage and send saslResponse back
     * @param saslMessage received SASL message
     * @throws RpcServerException setup failed due to SASL negotiation
     *         failure, premature or invalid connection context, or other state 
     *         errors. This exception needs to be sent to the client. This 
     *         exception will wrap {@link RetriableException}, 
     *         {@link InvalidToken}, {@link StandbyException} or 
     *         {@link SaslException}.
     * @throws IOException if sending reply fails
     * @throws InterruptedException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,readSSLConfiguration,"org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)",162,184,"/**
* Configures SSL settings based on the provided mode.
* @param conf base Configuration object
* @param mode operation mode (CLIENT or SERVER)
* @return configured Configuration object with SSL settings
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refresh,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",140,150,"/**
* Initializes the policy mask configuration.
* @param conf global configuration object
* @param provider policy provider instance
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/HCFSMountTableConfigLoader.java,load,"org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)",57,113,"/**
* Loads and applies the highest version mount-table configuration.
* @param mountTableConfigPath path to mount-table config directory
* @param conf Hadoop configuration object to update
*/","* Loads the mount-table configuration from hadoop compatible file system and
   * add the configuration items to given configuration. Mount-table
   * configuration format should be suffixed with version number.
   * Format: {@literal mount-table.<versionNumber>.xml}
   * Example: mount-table.1.xml
   * When user wants to update mount-table, the expectation is to upload new
   * mount-table configuration file with monotonically increasing integer as
   * version number. This API loads the highest version number file. We can
   * also configure single file path directly.
   *
   * @param mountTableConfigPath : A directory path where mount-table files
   *          stored or a mount-table file path. We recommend to configure
   *          directory with the mount-table version files.
   * @param conf : to add the mount table as resource.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,tryConnect,org.apache.hadoop.ha.HealthMonitor:tryConnect(),170,183,"/**
* Establishes a connection to the local service and initializes the proxy object.
* @throws IOException if connection attempt fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,isOtherTargetNodeActive,"org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)",187,215,"/**
* Tries to activate a target node by ID; returns true if successful or if forced.
* @param targetNodeToActivate node to activate
* @param forceActive whether to force activation despite errors
*/","* Checks whether other target node is active or not
   * @param targetNodeToActivate
   * @return true if other target node is active or some other exception 
   * occurred and forceActive was set otherwise false
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToStandby,org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine),217,234,"/**
* Transitions to standby mode using the provided command line arguments.
* @param cmd CommandLine object containing user input
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkHealth,org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine),292,309,"/**
* Validates service health with given command line arguments.
* @param cmd command line arguments
* @return 0 if successful, -1 otherwise
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getServiceState,org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine),311,324,"/**
* Retrieves service state using the provided command line arguments.
* @param cmd command line object containing service ID
* @return service state mask or -1 on error
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getAllServiceState,org.apache.hadoop.ha.HAAdmin:getAllServiceState(),443,464,"/**
* Retrieves HA service targets and prints their status.
* @return 0 on success or -1 on failure
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeActive,org.apache.hadoop.ha.ZKFailoverController:becomeActive(),408,443,"/**
* Transitions local target to active state.
* @throws ServiceFailedException on failure
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeStandby,org.apache.hadoop.ha.ZKFailoverController:becomeStandby(),514,529,"/**
* Transitions the local target to a standby state.
* @throws Exception if the transition fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doCedeActive,org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int),590,623,"/**
* Cedes active role after specified time, transitioning to standby mode.
* @param millisToCede time in milliseconds to wait before ceding
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,preFailoverChecks,"org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)",109,156,"/**
* Fails over a HAServiceTarget to another, updating its status and protocol.
* @param from current target
* @param target new target
* @param forceActive whether to force the failover even if the target is not ready
*/","* Perform pre-failover checks on the given service we plan to
   * failover to, eg to prevent failing over to a service (eg due
   * to it being inaccessible, already active, not healthy, etc).
   *
   * An option to ignore toSvc if it claims it is not ready to
   * become active is provided in case performing a failover will
   * allow it to become active, eg because it triggers a log roll
   * so the standby can learn about new blocks and leave safemode.
   *
   * @param from currently active service
   * @param target service to make active
   * @param forceActive ignore toSvc if it reports that it is not ready
   * @throws FailoverFailedException if we should avoid failover",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,tryGracefulFence,org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget),168,186,"/**
* Attempts to make the HA service target standby.
* @param svc HAServiceTarget instance
* @return true on success, false otherwise
*/","* Try to get the HA state of the node at the given address. This
   * function is guaranteed to be ""quick"" -- ie it has a short timeout
   * and no retries. Its only purpose is to avoid fencing a node that
   * has already restarted.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,moveToTrash,org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData),151,167,"/**
* Checks if item is in trash by calling Trash.m4() method.
* @param item PathData object containing file system and path
* @return true if item is in trash, false otherwise
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,getInputStream,org.apache.hadoop.fs.FsUrlConnection:getInputStream(),77,83,"/**
* Returns an input stream from the file system.
* @throws IOException if I/O operation fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,recursePath,org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData),345,371,"/**
* Handles PathData item with conditions and potential loop detection.
* @param item PathData object to process
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isPathRecursable,org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData),373,392,"/**
* Checks if a PathData item matches a specific condition.
* @param item the PathData object to evaluate
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,expandArgument,org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String),56,61,"/**
* Processes input string and generates a list of PathData objects.
* @param arg input string to process
* @return List of PathData objects or empty list if processing fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,expandArgument,org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String),81,86,"/**
* Creates a list of path data based on the input string.
* @param arg input string to process
* @return List of PathData objects or empty list if no items created
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemLinkResolver.java,resolve,"org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",71,111,"/**
* Resolves symbolic links to a file system path.
* @param path target file system path
* @return object associated with the resolved path or null if not found
* @throws IOException if link resolution fails due to cyclic loop or symlink resolution disabled
*/","* Attempt calling overridden {@link #doCall(Path)} method with
   * specified {@link FileSystem} and {@link Path}. If the call fails with an
   * UnresolvedLinkException, it will try to resolve the path and retry the call
   * by calling {@link #next(FileSystem, Path)}.
   * @param filesys FileSystem with which to try call
   * @param path Path with which to try call
   * @return Generic type determined by implementation
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",100,102,"/**
 * Initializes a new PathData object using a local file system and URI.
 * @param localPath URI of the path to be accessed
 * @param conf Hadoop configuration for file system operations
 */","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param localPath a local URI
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2571,2576,"/**
* Copies or deletes sources to destination with optional overwriting.
* @param delSrc true to delete source files after copying
* @param overwrite true to overwrite existing destination file
* @param srcs array of source file paths
* @param dst destination file path
*/","* The src files are on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param srcs array of paths which are source
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2588,2593,"/**
* Copies files from source to destination with optional deletion and overwriting.
* @param delSrc whether to delete the source after copy
* @param overwrite whether to overwrite existing files in destination
* @param src source file path
* @param dst destination file path
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",2648,2658,"/**
* Configures and executes a file system operation with masking.
* @param delSrc whether to delete source files
* @param src source path
* @param dst destination path
* @param useRawLocalFileSystem whether to use raw local file system
*/","* The src file is under this filesystem, and the dst is on the local disk.
   * Copy it from the remote filesystem to the local dst name.
   * delSrc indicates if the src will be removed
   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem
   * as the local file system or not. RawLocalFileSystem is non checksumming,
   * So, It will not create any crc files at local.
   *
   * @param delSrc
   *          whether to delete the src
   * @param src
   *          path
   * @param dst
   *          path
   * @param useRawLocalFileSystem
   *          whether to use RawLocalFileSystem as local file system or not.
   *
   * @throws IOException for any IO error",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,confChanged,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration),309,360,"/**
* Configures local directories from given configuration.
* @param conf Hadoop configuration
* @return updated Context object or throws IOException if config is invalid
*/","This method gets called everytime before any read/write to make sure
     * that any change to localDirs is reflected immediately.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLocalPath,"org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)",2828,2848,"/**
* Finds a suitable directory for file creation based on user-specified directories and hash of the target path.
* @param dirsProp comma-separated list of available directories
* @param path target file path
* @return Path object representing the chosen directory or null if not found
* @throws IOException if no valid local directories are found
*/","* Get a local file under a directory named by <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)",94,97,"/**
* Constructs an NflyNode with specified hostname, rack name, and file system.
* @param hostName the hostname
* @param rackName the rack name
* @param uri the URI of the underlying file system
* @param conf the configuration for the file system
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getRawFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",328,340,"/**
* Resolves a file system object from a path.
* @param path file system path
* @param conf configuration options
* @return FileSystem object or null if not found
*/","* This is an admin only API to give access to its child raw file system, if
   * the path is link. If the given path is an internal directory(path is from
   * mount paths tree), it will initialize the file system of given path uri
   * directly. If path cannot be resolved to any internal directory or link, it
   * will throw NotInMountpointException. Please note, this API will not return
   * chrooted file system. Instead, this API will get actual raw file system
   * instances.
   *
   * @param path - fs uri path
   * @param conf - configuration
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountPathInfo,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",351,372,"/**
* Retrieves mount path information for a given file system.
* @param path file system path
* @param conf configuration object
* @return MountPathInfo object or throws exception if not found
*/","* Gets the mount path info, which contains the target file system and
   * remaining path to pass to the target file system.
   *
   * @param path the path.
   * @param conf configuration.
   * @return mount path info.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,expandAsGlob,"org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)",344,396,"/**
* Fetches path data based on a pattern and configuration.
* @param pattern glob pattern
* @param conf Hadoop Configuration object
* @return array of PathData objects or null if not found
*/","* Expand the given path as a glob pattern.  Non-existent paths do not
   * throw an exception because creation commands like touch and mkdir need
   * to create them.  The ""stat"" field will be null if the path does not
   * exist.
   * @param pattern the pattern to expand as a glob
   * @param conf the hadoop configuration
   * @return list of {@link PathData} objects.  if the pattern is not a glob,
   * and does not exist, the list will contain a single PathData with a null
   * stat 
   * @throws IOException anything else goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",1894,1932,"/**
* Initializes a Reader instance from various options.
* @param conf Configuration object
* @param opts Optional parameters (e.g. file path, stream, etc.)
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,"org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",237,254,"/**
* Loads and initializes a DynamicBloomFilter from the specified file.
* @param dirName directory containing the filter
* @param conf configuration object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,dumpInfo,"org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)",96,295,"/**
* Prints detailed information about a BCFile in human-readable format.
* @param file path to the BCFile
* @param out PrintStream to write output to
* @param conf Configuration object used for file operations
*/","* Dump information about TFile.
   * 
   * @param file
   *          Path string of the TFile
   * @param out
   *          PrintStream to output the information.
   * @param conf
   *          The configuration object.
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",225,241,"/**
* Reads token storage file and returns Credentials object.
* @param filename Path to token storage file
* @param conf Configuration object
* @return Credentials object or null on failure
*/","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException  raised on errors performing I/O.
   * @return Credentials.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)",341,347,"/**
* Writes serialized data to a file using the specified format.
* @param filename path to write to
* @param conf configuration for serialization
* @param format output format of serialized data
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI),81,85,"/**
* Invokes superclass method and initializes file system components.
* @param uri URI object representing the resource location
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",129,138,"/**
* Initializes the Java KeyStore provider with configuration and URI.
* @param uri key store location as a URI
* @param conf Hadoop configuration object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,getLibJars,org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration),372,389,"/**
* Builds an array of URLs from a comma-separated list of JAR files in the configuration.
* @param conf Hadoop Configuration object
* @return Array of URLs or null if invalid configuration
*/","* If libjars are set in the conf, parse the libjars.
   * @param conf input Configuration.
   * @return libjar urls
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,initFs,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs(),271,304,"/**
* Creates or updates the file system with specified settings.
* @return true if successful, false otherwise
*/","* Initialize the connection to HDFS and create the base directory. Also
   * launch the flush thread.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(),426,429,"/**
* Returns file context for local file system.
*/","* @return a FileContext for the local file system using the default config.
   * @throws UnsupportedFileSystemException If the file system for
   *           {@link FsConstants#LOCAL_FS_URI} is not supported.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,<init>,org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus),278,289,"/**
* Initializes AvroFileInputStream from a FileStatus.
* @param status FileStatus object containing file metadata
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(),416,419,"/**
* Initializes FileContext with default configuration.
* @throws UnsupportedFileSystemException if file system is unsupported
*/","* Create a FileContext using the default config read from the
   * $HADOOP_CONFIG/core.xml, Unspecified key-values for config are defaulted
   * from core-defaults.xml in the release jar.
   * 
   * @throws UnsupportedFileSystemException If the file system from the default
   *           configuration is not supported
   * @return file context.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String),616,624,"/**
* Retrieves a KeyVersion object by its version name.
* @throws IOException on network errors
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String),626,634,"/**
* Fetches key version by mask.
* @param name unique key identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys(),636,644,"/**
* Fetches list of key names from KMS REST resource.
* @return List of String key names
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[]),675,694,"/**
* Retrieves metadata arrays for given key names.
* @param keyNames variable length array of key identifiers
* @return array of Metadata objects or empty array if not found
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",696,722,"/**
* Generates a KeyVersion with FUNC_MASK algorithm.
* @param name key name
* @param material encryption material (optional)
* @param options algorithm parameters
* @return generated KeyVersion or null on failure
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String),741,750,"/**
* Updates the FUNC_MASK by sending a POST request to the specified resource.
* @param name new mask value
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),801,835,"/**
* Decrypts an EncryptedKeyVersion using the Key Management Service.
* @param encryptedKeyVersion Encrypted key version to decrypt
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),837,864,"/**
* Re-encrypts the given EncryptedKeyVersion.
* @param ekv EncryptedKeyVersion to re-encrypt
* @return new EncryptedKeyVersion or null if failed
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List),866,906,"/**
* Updates encrypted key versions in batch.
* @param ekvs list of EncryptedKeyVersions to update
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String),908,923,"/**
* Retrieves a list of key versions by name.
* @param name unique key identifier
* @return List of KeyVersion objects or empty list if not found
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String),925,933,"/**
* Fetches metadata for a given resource by its name.
* @param name unique resource identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String),935,941,"/**
* Deletes user profile by name.
* @param name unique user identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",380,390,"/**
* Creates a new RPC server instance with specified configuration.
* @param protocol Class of the RPC protocol to use
* @return RPC.Server instance or null if creation fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",446,455,"/**
* Initializes a Server instance with specified parameters.
* @param protocolClass protocol class to use
* @throws IOException on initialization failure
*/","* Construct an RPC server.
     * 
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param numReaders input numReaders.
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getServer,"org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",371,382,"/**
* Creates a new RPC server instance with specified configuration.
* @param protocolClass the service protocol class
* @param secretManager token secret manager for authentication
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)",466,476,"/**
* Initializes a Server instance with default parameters.
* @param protocolClass class of the protocol to use
*/","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param portRangeConfig input portRangeConfig.
     * @param numReaders input numReaders.
     *
     * @deprecated use Server#Server(Class, Object,
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslReadAndProcess,org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer),2178,2196,"/**
* Processes SASL message and applies wrap mask if applicable.
* @param buffer RpcWritable.Buffer containing SASL message
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,<init>,"org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)",136,160,"/**
* Initializes SSLFactory with specified mode and configuration.
* @param mode SSL mode to use
* @param conf Configuration object containing SSL settings
*/","* Creates an SSLFactory.
   *
   * @param mode SSLFactory mode, client or server.
   * @param conf Hadoop configuration from where the SSLFactory configuration
   * will be read.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAcl,"org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",767,769,"/**
* Initializes mask functionality with provided configuration and policy provider.
* @param conf Configuration object
* @param provider PolicyProvider instance
*/","* Refresh the service authorization ACL for the service handled by this server.
   *
   * @param conf input Configuration.
   * @param provider input PolicyProvider.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,loopUntilConnected,org.apache.hadoop.ha.HealthMonitor:loopUntilConnected(),161,168,"/**
* Establishes a connection and waits until a valid proxy is obtained.
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToActive,org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine),155,178,"/**
* Activates a service by ID.
* @param cmd command line arguments
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doFence,org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget),543,566,"/**
* Fences the HAServiceTarget instance with graceful fallback.
* @param target HAServiceTarget instance to fence
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,failover,"org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)",198,260,"/**
* Performs a failover from one HA service target to another.
* @param fromSvc the service target being replaced
* @param toSvc the replacement service target
* @param forceFence whether to fence the old service
* @param forceActive whether to make the new service active immediately
*/","* Failover from service 1 to service 2. If the failover fails
   * then try to failback.
   *
   * @param fromSvc currently active service
   * @param toSvc service to make active
   * @param forceFence to fence fromSvc even if not strictly necessary
   * @param forceActive try to make toSvc active even if it is not ready
   * @throws FailoverFailedException if the failover fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData),110,127,"/**
* Deletes a file or directory based on path data.
* @param item PathData object containing file/directory information
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String),295,304,"/**
* Fetches path data for the given URL or alias.
* @param arg URL or alias to fetch
* @return List of PathData objects
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList),71,89,"/**
* Parses function mask arguments and initializes related data structures.
* @param args list of command line arguments
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String),357,375,"/**
* Fetches PathData objects from URI or path string.
* @param arg URI or path string to process
* @return List of PathData objects or empty list if not found
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getLocalDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList),182,195,"/**
* Initializes path data from provided arguments.
* @param args list of path-related values
*/","*  The last arg is expected to be a local path, if only one argument is
   *  given then the destination will be the current directory 
   *  @param args is the list of arguments
   * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2530,2533,"/**
* Copies files from source paths to destination path, creating mask if necessary.
* @param srcs array of source file paths
* @param dst destination file path
*/","* The src files is on the local disk.  Add it to filesystem at
   * the given dst name, removing the source afterwards.
   * @param srcs source paths
   * @param dst path
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",360,365,"/**
* Calls File System API to perform operation.
* @param delSrc whether to delete source files
* @param overwrite whether to overwrite destination
* @param srcs array of source file paths
* @param dst destination path
*/","* The src files are on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2556,2559,"/**
* Calls overloaded version with default overwrite flag.
* @param delSrc whether to delete source file
* @param src source file path
* @param dst destination file path
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param src path
   * @param dst path
   * @throws IOException IO failure.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",372,377,"/**
* Calls underlying file system's m1() operation.
* @param delSrc whether to delete source after copy
* @param overwrite whether to overwrite destination if it exists
* @param src source path
* @param dst destination path
*/","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2624,2627,"/**
* Calls overloaded version with default overwrite parameter value.
*/","* Copy it a file from a remote filesystem to the local one.
   * delSrc indicates if the src will be removed or not.
   * @param delSrc whether to delete the src
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",394,492,"/**
* Finds a local directory with sufficient space for the given path and size.
* @param pathStr path string
* @param size requested size (SIZE_UNKNOWN if unknown)
* @param conf configuration object
* @param checkWrite whether to check write permissions
* @return Path object or throws DiskErrorException","Get a path from the local FS. If size is known, we go
     *  round-robin over the set of disks (via the configured dirs) and return
     *  the first complete path which has enough space.
     *  
     *  If size is not known, use roulette selection -- pick directories
     *  with probability proportional to their available space.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",518,539,"/**
* Resolves a file path to its first matching local directory.
* @param pathStr file path string
* @param conf configuration object
* @return Path object pointing to the resolved file or null if not found
*/","Get a path from the local FS for reading. We search through all the
     *  configured dirs for the file's existence and return the complete
     *  path to the file when we find one",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",603,610,"/**
* Creates an iterable of file system paths from a given string and configuration.
* @param pathStr input path string
* @param conf Hadoop Configuration object
* @return iterable of Paths or throws IOException on error.","* Get all of the paths that currently exist in the working directories.
     * @param pathStr the path underneath the roots
     * @param conf the configuration to look up the roots in
     * @return all of the paths that exist under any of the roots
     * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)",228,275,"/**
* Initializes an NflyFSystem instance with the given parameters.
* @param uris array of URIs to fetch
* @param conf configuration object
* @param minReplication minimum replication count
* @param nflyFlags enumeration set of flags
* @param fsGetter file system getter (optional)
*/","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @param fsGetter to get the file system instance with the given uri
   * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,runAll,org.apache.hadoop.fs.shell.Command:runAll(),128,142,"/**
* Executes multiple file operations and returns a combined exit code.
* @return 0 on success, -1 on failure
*/","* For each source path, execute the command
   * 
   * @return 0 if it runs successfully; -1 if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArgument,org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String),264,271,"/**
* Retrieves a list of PathData objects for the given argument string.
* @param arg input string to process
*/","* Expand the given argument into a list of {@link PathData} objects.
   * The default behavior is to expand globs.  Commands may override to
   * perform other expansions on an argument.
   * @param arg string pattern to expand
   * @return list of {@link PathData} objects
   * @throws IOException if anything goes wrong...",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getRemoteDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList),203,221,"/**
* Resolves path using provided arguments.
* @param args list of strings for path resolution
*/","*  The last arg is expected to be a remote path, if only one argument is
   *  given then the destination will be the remote user's directory 
   *  @param args is the list of arguments
   *  @throws PathIOException if path doesn't exist or matches too many times",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",1942,1946,"/**
* Creates a new Reader instance from a FileSystem and Path.
* @param fs FileSystem to use
* @param file qualified path to read from
* @param conf Hadoop configuration
*/","* Construct a reader by opening a file from the given file system.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)",1958,1962,"/**
* Initializes a new Reader instance from FSDataInputStream.
* @param in input stream
* @param buffersize buffer size
* @param start starting offset
* @param length data length
* @param conf configuration
*/","* Construct a reader by the given input stream.
     * @param in An input stream.
     * @param buffersize unused
     * @param start The starting position.
     * @param length The length being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Reader.Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,createDataFileReader,"org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",568,575,"/**
* Creates a SequenceFile.Reader instance from the given data file and configuration.
* @param dataFile path to the data file
* @param conf Hadoop Configuration object
* @param options optional reader options (ignored in this method)
*/","* Override this method to specialize the type of
     * {@link SequenceFile.Reader} returned.
     *
     * @param dataFile data file.
     * @param conf configuration.
     * @param options options.
     * @throws IOException raised on errors performing I/O.
     * @return SequenceFile.Reader.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey(),3832,3857,"/**
* Reads and validates input data from a file.
* @return true if valid data is found, false otherwise
*/","* Fills up the rawKey object with the key returned by the Reader.
       * @return true if there is a key returned; false, otherwise
       * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1071,1195,"/**
* Initializes a SequenceFile writer with specified options.
* @param conf Hadoop configuration
* @param opts sequence file options
*/","* Construct a uncompressed writer from a set of options.
     * @param conf the configuration to use
     * @param opts the options used when creating the writer
     * @throws IOException if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,main,org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[]),2348,2366,"/**
* Dumps TFiles and BCFiles from command line.
* @param args array of file paths to dump
*/","* Dumping the TFile information.
   * 
   * @param args
   *          A list of TFile paths.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",335,339,"/**
 * Writes configuration to file in writable format.
 * @param filename path to output file
 * @param conf configuration object
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,doFormattedWrite,"org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)",104,114,"/**
* Masks credentials in the specified file using the given format.
* @param f target file
* @param format serialization format (e.g. PB)
* @param creds credentials to mask
* @param conf configuration settings
*/","Write out a Credentials object as a local file.
   *  @param f a local File object.
   *  @param format a string equal to FORMAT_PB or FORMAT_JAVA.
   *  @param creds the Credentials object to be written out.
   *  @param conf a Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDirIfNeeded,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded(),504,542,"/**
* Triggers flushing and masking of metrics, potentially initializing output stream.
* @throws MetricsException on write errors
*/","* Check the current directory against the time stamp.  If they're not
   * the same, create a new directory and a new log file in that directory.
   *
   * @throws MetricsException thrown if an error occurs while creating the
   * new directory or new log file",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,"org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)",1777,1796,"/**
* Finds and returns a list of matching file paths.
* @param path glob pattern to search for
* @param useLocal whether to use local or global context
* @return List of Path objects or empty list if not found
*/","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @param useLocal use local.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData),123,173,"/**
* Fetches InputStream for compressed or encoded data by PathData.
* @param item PathData containing file metadata
* @return InputStream to read from, or original FSDataInputStream if not compressible
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",724,728,"/**
* Computes functional mask key version using provided name and options.
* @param name functional mask name
* @param options computation options
* @return computed KeyVersion object or throws exception if failed.",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",730,739,"/**
* Executes key versioning with FUNC_MASK operation.
* @param name identifier for the key
* @param material byte array of cryptographic material
* @param options configuration options
* @return KeyVersion object or throws IOException if an error occurs
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersionInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])",752,768,"/**
* Generates a new KeyVersion with specified material.
* @param name unique key identifier
* @param material optional byte array material (base64 encoded)
* @return KeyVersion object or null on failure
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String),319,324,"/**
* Invokes KMS client providers to process key operation.
* @param keyName name of the key to be processed
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",368,378,"/**
* Creates a new RPC server instance with specified configuration.
* @param protocol the protocol class
* @return initialized Server object
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",413,418,"/**
* Initializes a new Server instance with the given protocol class and implementation.
* @param protocolClass protocol class to use
* @param protocolImpl protocol implementation object
* @param conf server configuration
* @param bindAddress address to bind the server to
* @param port server port number","Construct an RPC server.
     * @param protocolClass class
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)",436,445,"/**
* Creates a new Server instance with default properties.
* @param protocolImpl the underlying protocol implementation
*/","* Construct an RPC server.
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param numReaders input numberReaders.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * 
     * @deprecated use Server#Server(Class, Object, 
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcOutOfBandRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2984,3012,"/**
* Handles RPC calls based on the provided header.
* @param header RpcRequestHeaderProto object
* @param buffer RpcWritable.Buffer for processing
*/","* Establish RPC connection setup by negotiating SASL if required, then
     * reading and authorizing the connection header
     * @param header - RPC header
     * @param buffer - stream to request payload
     * @throws RpcServerException - setup failed due to SASL
     *         negotiation failure, premature or invalid connection context,
     *         or other state errors. This exception needs to be sent to the 
     *         client.
     * @throws IOException - failed to send a response back to the client
     * @throws InterruptedException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,connect,org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL),260,284,"/**
* Establishes a secure HTTP(S) connection to the specified URL.
* @param url target URL
*/","* Connect to the URL. Supports HTTP/HTTPS and supports SPNEGO
     * authentication. It falls back to simple authentication if it fails to
     * initiate SPNEGO.
     *
     * @param url the URL address of the daemon servlet
     * @return a connected connection
     * @throws Exception if it can not establish a connection.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",378,428,"/**
* Initializes a KMS client provider with the given URI and configuration.
* @param uri KMS service endpoint URL
* @param conf Client configuration
* @throws IOException if initialization fails
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,runCmd,org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[]),391,441,"/**
* Parses and executes the specified command with optional arguments.
* @param argv array of command-line arguments
* @return non-negative exit code or -1 on error
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,fenceOldActive,org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[]),532,541,"/**
* Executes fencing operation on a given byte array.
* @param data input byte array
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2518,2521,"/**
 * Copies file from source to destination.
 * @param src source path
 * @param dst destination path
 */","* The src file is on the local disk.  Add it to filesystem at
   * the given dst name and the source is kept intact afterwards
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2542,2545,"/**
 * Copies files from source to destination while preserving file masks.
 * @param src source directory path
 * @param dst destination directory path
 */","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name, removing the source afterwards.
   * @param src local path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",349,353,"/**
* Calls underlying file system's 'm1' operation.
* @param delSrc whether to delete source upon completion
* @param src source path
* @param dst destination path
*/","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2601,2603,"/**
 * Copies file from source to destination path.
 * @param src source file path
 * @param dst destination file path
 */","* Copy it a file from the remote filesystem to the local one.
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveToLocalFile,"org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2612,2614,"/**
* Copies file contents from source to destination while preserving metadata.
* @param src input file path
* @param dst output file path
*/","* Copy a file to the local filesystem, then delete it from the
   * remote filesystem (if successfully copied).
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",384,388,"/**
* Calls the file system's m1 method to perform an operation.
* @param delSrc whether to delete source file
* @param src source file path
* @param dst destination file path
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * delSrc indicates if the src will be removed or not.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",162,167,"/**
* Calls M2 function on the allocator context.
* @param pathStr string representation of a path
* @param size file size in bytes
* @param conf configuration object
* @param checkWrite flag to check write permissions
*/","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @param checkWrite ensure that the path is writable
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",500,512,"/**
* Creates a file for storing mask data.
* @param pathStr string representation of the file path
* @param size size of the file
* @param conf configuration object
* @return File object representing the mask data storage location
*/","Creates a file on the local FS. Pass size as 
     * {@link LocalDirAllocator.SIZE_UNKNOWN} if not known apriori. We
     *  round-robin over the set of disks (via the configured dirs) and return
     *  a file on the first path which has enough space. The file is guaranteed
     *  to go away when the JVM exits.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",177,181,"/**
* Calls M2 on the allocator with the given configuration.
* @param pathStr input string for processing
* @param conf configuration object for M2 operation
*/","Get a path from the local FS for reading. We search through all the
   *  configured dirs for the file's existence and return the complete
   *  path to the file when we find one 
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",190,198,"/**
* Executes M2 operation on the given path and configuration.
* @param pathStr input path string
* @param conf configuration object
* @return iterable of file system paths or throws IOException if error occurs
*/","* Get all of the paths that currently exist in the working directories.
   * @param pathStr the path underneath the roots
   * @param conf the configuration to look up the roots in
   * @return all of the paths that exist under any of the roots
   * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)",213,216,"/**
 * Constructs an instance of NflyFSystem with the specified parameters.
 * @param uris array of URIs
 * @param conf configuration object
 * @param minReplication minimum replication level
 * @param nflyFlags set of flags for Nfly keys
 */","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,createFileSystem,"org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)",944,971,"/**
* Creates an Nfly FSystem instance with specified settings.
* @param uris array of URIs
* @param conf configuration object
* @param settings string containing key-value pairs
* @param fsGetter FsGetter instance
* @return new NflyFSystem instance
*/","* Initializes an nfly mountpoint in viewfs.
   *
   * @param uris destinations to replicate writes to
   * @param conf file system configuration
   * @param settings comma-separated list of k=v pairs.
   * @return an Nfly filesystem
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArguments,org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList),243,254,"/**
* Expands input string arguments into PathData objects.
* @param args list of strings to expand
* @return linked list of PathData objects or null on error
*/","*  Expands a list of arguments into {@link PathData} objects.  The default
   *  behavior is to call {@link #expandArgument(String)} on each element
   *  which by default globs the argument.  The loop catches IOExceptions,
   *  increments the error count, and displays the exception.
   * @param args strings to expand into {@link PathData} objects
   * @return list of all {@link PathData} objects the arguments
   * @throws IOException if anything goes wrong...",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",532,556,"/**
* Initializes and configures the SequenceFile reader for data and index files.
* @param dir directory path
* @param comparator optional WritableComparator instance or null to auto-detect type
* @param conf Hadoop configuration object
* @param options additional reader options
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,adjustPriorityQueue,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3598,3609,"/**
* Processes a segment descriptor, updating internal state and potentially reading/writing data.
* @param ms SegmentDescriptor object containing input and control flags
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1609,1620,"/**
* Initializes BlockCompressWriter with configuration and options.
* @param conf Configuration object
* @param options variable number of options
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1539,1542,"/**
* Initializes a Record Compress Writer with configuration and optional settings.
* @param conf Hadoop Configuration object
* @param options variable number of options for customization
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,getTokenFile,"org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",175,220,"/**
* Fetches and processes a token from the specified file.
* @param tokenFile File containing the token
* @param fileFormat Format of the token file
* @param alias Optional alias for the service
* @param service Service designation
* @param url URL to use
* @param renewer Renewal information (if applicable)
* @param conf Configuration settings
*/","Fetch a token from a service and save to file in the local filesystem.
   *  @param tokenFile a local File object to hold the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service use a DtFetcher implementation matching this service text.
   *  @param url pass this URL to fetcher after stripping any http/s prefix.
   *  @param renewer pass this renewer to the fetcher.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,aliasTokenFile,"org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",230,243,"/**
* Configures and writes credentials to a file based on the service and alias.
* @param tokenFile output file path
* @param fileFormat file format (e.g., JSON)
* @param alias user-defined alias for tokens
* @param service target service name
* @param conf configuration settings
*/","Alias a token from a file and save back to file in the local filesystem.
   *  @param tokenFile a local File object to hold the input and output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service only apply alias to tokens matching this service text.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,appendTokenFiles,"org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)",251,264,"/**
* Updates credentials and masks tokens in files based on configuration.
* @param tokenFiles list of files to process
* @param fileFormat format of the files (e.g. CSV, JSON)
* @param conf Hadoop configuration object
*/","Append tokens from list of files in local filesystem, saving to last file.
   *  @param tokenFiles list of local File objects.  Last file holds the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,removeTokenFromFile,"org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",275,291,"/**
* Updates and writes credentials to token file based on alias match.
* @param cancel cancellation flag
* @param tokenFile file for storing updated credentials
* @param fileFormat output format of the written data
* @param alias text identifier to match against tokens
* @param conf configuration object for m10 method
*/","Remove a token from a file in the local filesystem, matching alias.
   *  @param cancel cancel token as well as remove from file.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias remove only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,renewTokenFile,"org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",301,313,"/**
* Updates tokens in the specified file with the given format.
* @param tokenFile file containing tokens
* @param fileFormat desired output format
* @param alias user alias for authentication
* @param conf configuration settings
*/","Renew a token from a file in the local filesystem, matching alias.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias renew only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,importTokenFile,"org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)",323,339,"/**
* Processes a token file and adds it to the credentials configuration.
* @param tokenFile input file containing token data
* @param fileFormat format of the token file
* @param alias optional text alias for the token
* @param base64 encoded token value
* @param conf configuration settings
*/","Import a token from a base64 encoding into the local filesystem.
   * @param tokenFile A local File object.
   * @param fileFormat A string equal to FORMAT_PB or FORMAT_JAVA, for output.
   * @param alias overwrite Service field of fetched token with this text.
   * @param base64 urlString Encoding of the token to import.
   * @param conf Configuration object passed along.
   * @throws IOException Error to import the token into the file.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,putMetrics,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),822,861,"/**
* Writes metrics record and flushes output streams.
* @param record MetricsRecord object containing data to be written
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String),1764,1766,"/**
* Returns list of paths with default recursive search enabled.
* @param path directory to start search from
*/","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   * It operates only on local paths.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist locally",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,expandWildcard,"org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",495,512,"/**
* Validates and extracts JARs from a directory.
* @param finalPaths list of final paths
* @param path directory to process
* @param fs file system instance
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String),771,775,"/**
* Computes key version based on input mask string.
* @param name input mask string
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])",777,786,"/**
* Computes the FUNC_MASK key version based on input name and material.
* @param name string identifier
* @param material byte array data
* @return KeyVersion object or throws IOException/RuntimException if errors occur
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String),497,507,"/**
* Executes M1 operation with the given name.
* @param name input parameter for M1 operation
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])",509,520,"/**
* Creates a new key version using the provided name and material.
* @param name unique identifier for the key
* @param material byte array representing the key's content
* @return new KeyVersion object or throws IOException if creation fails
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String),522,541,"/**
* Generates a new key version using the KMS client provider.
* @param name identifier for the key
* @return KeyVersion object or throws exception on failure
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",398,402,"/**
* Legacy constructor to initialize server with default configuration.
* @param instance object instance
* @param conf server configuration
* @param bindAddress IP address for binding
* @param port server listening port
*/","* Construct an RPC server.
     * @param instance the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * 
     * @deprecated Use #Server(Class, Object, Configuration, String, int)
     * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processOneRpc,org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer),2785,2823,"/**
* Processes a single RPC request from a client.
* @param bb ByteBuffer containing the RPC request
*/","* Process one RPC Request from buffer read from socket stream 
     *  - decode rpc in a rpc-Call
     *  - handle out-of-band RPC requests such as the initial connectionContext
     *  - A successfully decoded RpcCall will be deposited in RPC-Q and
     *    its response will be sent later when the request is processed.
     * 
     * Prior to this call the connectionHeader (""hrpc..."") has been handled and
     * if SASL then SASL has been established and the buf we are passed
     * has been unwrapped from SASL.
     * 
     * @param bb - contains the RPC request header and the rpc request
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,process,org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String),292,311,"/**
* Fetches and processes data from a URL.
* @param urlString URL to connect to
*/","* Configures the client to send HTTP/HTTPS request to the URL.
     * Supports SPENGO for authentication.
     * @param urlString URL and query string to the daemon's web UI
     * @throws Exception if unable to connect",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProviders,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)",311,326,"/**
* Creates an array of KMSClientProviders from a list of hosts.
* @param conf configuration object
* @param origUrl original URL for provider creation
* @param port server port number
* @param hostsPart comma-separated host strings
* @return array of KMSClientProvider objects or null on failure",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,run,org.apache.hadoop.ha.HAAdmin:run(java.lang.String[]),347,361,"/**
* Executes m5 function and returns a mask value, handling exceptions.
* @param argv input arguments
* @return int mask value or -1 on error
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2686,2689,"/**
* Masks output file with temporary file.
* @param fsOutputFile path to output file
* @param tmpLocalFile path to temporary local file
*/","* Called when we're all done writing to the target.
   * A local FS will do nothing, because we've written to exactly the
   * right place.
   * A remote FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.
   * @param fsOutputFile path of output file
   * @param tmpLocalFile path to local tmp file
   * @throws IOException IO failure",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1019,1043,"/**
* Copies file with optional CRC check and compression.
* @param src source path
* @param dst destination path
* @param copyCrc whether to copy checksum file as well
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * If src and dst are directories, the copyCrc parameter
   * determines whether to copy CRC files.
   * @param src src path.
   * @param dst dst path.
   * @param copyCrc copy csc flag.
   * @throws IOException if an I/O error occurs.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",145,148,"/**
* Creates a new Hadoop Path object from the given string and configuration.
* @param pathStr the path string
* @param size the file size
* @param conf the Hadoop configuration
*/","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",211,215,"/**
* Creates an M2 file using the given configuration.
* @param pathStr file path
* @param size file size in bytes
* @param conf M2 creation configuration
* @return created File object or throws IOException if failed
*/","Creates a temporary file in the local FS. Pass size as -1 if not known 
   *  apriori. We round-robin over the set of disks (via the configured dirs) 
   *  and select the first complete path which has enough space. A file is
   *  created on this directory. The file is guaranteed to go away when the
   *  JVM exits.
   *  @param pathStr prefix for the temporary file
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return a unique temporary file
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processRawArguments,org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList),229,232,"/**
 * Applies mask function to input arguments and stores result in output buffer.
 * @param args list of strings to process
 */","* Allows commands that don't use paths to handle the raw arguments.
   * Default behavior is to expand the arguments via
   * {@link #expandArguments(LinkedList)} and pass the resulting list to
   * {@link #processArguments(LinkedList)} 
   * @param args the list of argument strings
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",490,499,"/**
* Opens a SequenceFile reader for the given directory.
* @param dir directory path
* @param conf configuration object
* @param opts additional options (e.g. comparator)
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next(),3565,3591,"/**
* Retrieves and processes the minimum segment descriptor.
* @throws IOException if an I/O error occurs
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",274,294,"/**
* Creates a writer with optional compression based on configuration.
* @param conf Hadoop configuration
* @param opts Writer options
* @return Compressed writer or regular writer if no compression specified
*/","* Create a new Writer with the given options.
   * @param conf the configuration to use
   * @param opts the options to create the file with
   * @return a new Writer
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Get:execute(),241,245,"/**
* Performs function mask operation on a file.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Edit:execute(),271,277,"/**
* Applies mask to token files using M2 operations.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Append:execute(),289,292,"/**
* Processes token files using M2 operations.
* @throws Exception if an error occurs during processing
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Remove:execute(),320,326,"/**
* Executes M2 operation on each token file.
* @throws Exception if an error occurs during the operation
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Renew:execute(),350,355,"/**
* Applies mask to all token files.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Import:execute(),381,385,"/**
* Performs file operations using M2 protocol.
* @throws Exception if an error occurs during execution
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)",1668,1753,"/**
* Generates a JAR file containing the classpath entries and returns it as an array.
* @param inputClassPath input classpath string
* @param pwd current working directory
* @param targetDir target directory for output files
* @param callerEnv environment variables to expand in classpath entries
* @return array with JAR file path and optional unexpanded wildcard classpath entry
*/","* Create a jar file at the given path, containing a manifest with a classpath
   * that references all specified entries.
   *
   * Some platforms may have an upper limit on command line length.  For example,
   * the maximum command line length on Windows is 8191 characters, but the
   * length of the classpath may exceed this.  To work around this limitation,
   * use this method to create a small intermediate jar with a manifest that
   * contains the full classpath.  It returns the absolute path to the new jar,
   * which the caller may set as the classpath for a new process.
   *
   * Environment variable evaluation is not supported within a jar manifest, so
   * this method expands environment variables before inserting classpath entries
   * to the manifest.  The method parses environment variables according to
   * platform-specific syntax (%VAR% on Windows, or $VAR otherwise).  On Windows,
   * environment variables are case-insensitive.  For example, %VAR% and %var%
   * evaluate to the same value.
   *
   * Specifying the classpath in a jar manifest does not support wildcards, so
   * this method expands wildcards internally.  Any classpath entry that ends
   * with * is translated to all files at that path with extension .jar or .JAR.
   *
   * @param inputClassPath String input classpath to bundle into the jar manifest
   * @param pwd Path to working directory to save jar
   * @param targetDir path to where the jar execution will have its working dir
   * @param callerEnv Map {@literal <}String, String{@literal >} caller's
   * environment variables to use for expansion
   * @return String[] with absolute path to new jar in position 0 and
   *   unexpanded wild card entry path in position 1
   * @throws IOException if there is an I/O error while writing the jar file",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,constructUrlsFromClasspath,org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String),107,126,"/**
* Resolves classpath to an array of URLs.
* @param classpath String containing paths and wildcards
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,"org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)",425,488,"/**
* Resolves file paths and masks wildcards from a comma-separated string.
* @param files comma-separated list of file paths
* @param expandWildcard whether to resolve wildcards
* @return formatted string with resolved paths or null if invalid
*/","* takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * @param files the input files argument
   * @param expandWildcard whether a wildcard entry is allowed and expanded. If
   * true, any directory followed by a wildcard is a valid entry and is replaced
   * with the list of jars in that directory. It is used to support the wildcard
   * notation in a classpath.
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,readAndProcess,org.apache.hadoop.ipc.Server$Connection:readAndProcess(),2478,2565,"/**
* Fetches and processes RPC header, handling versioning, authentication,
* and data length parsing. Returns -1 on failure or when done.
*/","* This method reads in a non-blocking fashion from the channel: 
     * this method is called repeatedly when data is present in the channel; 
     * when it has enough data to process one rpc it processes that rpc.
     * 
     * On the first pass, it processes the connectionHeader, 
     * connectionContext (an outOfBand RPC) and at most one RPC request that 
     * follows that. On future passes it will process at most one RPC request.
     *  
     * Quirky things: dataLengthBuffer (4 bytes) is used to read ""hrpc"" OR 
     * rpc request length.
     *    
     * @return -1 in case of error, else num bytes read so far
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException - if the thread is interrupted.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,unwrapPacketAndProcessRpcs,org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[]),2733,2767,"/**
* Processes input token from SASL server, reading and processing data in chunks.
* @param inBuf input byte array from SASL server
*/","* Process a wrapped RPC Request - unwrap the SASL packet and process
     * each embedded RPC request 
     * @param inBuf - SASL wrapped request of one or more RPCs
     * @throws IOException - SASL packet cannot be unwrapped
     * @throws InterruptedException",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGetLevel,org.apache.hadoop.log.LogLevel$CLI:doGetLevel(),236,238,"/**
* Sends a mask request to server.
* @throws Exception if request fails
*/","* Send HTTP/HTTPS request to get log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doSetLevel,org.apache.hadoop.log.LogLevel$CLI:doSetLevel(),246,249,"/**
* Sends log level request to server.
*/","* Send HTTP/HTTPS request to set log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",279,309,"/**
* Resolves KeyProvider for the given URI and configuration.
* @param providerUri URI to resolve
* @param conf Configuration object
* @return LoadBalancingKMSClientProvider instance or null if not found
*/","* This provider expects URIs in the following form :
     * {@literal kms://<PROTO>@<AUTHORITY>/<PATH>}
     *
     * where :
     * - PROTO = http or https
     * - AUTHORITY = {@literal <HOSTS>[:<PORT>]}
     * - HOSTS = {@literal <HOSTNAME>[;<HOSTS>]}
     * - HOSTNAME = string
     * - PORT = integer
     *
     * This will always create a {@link LoadBalancingKMSClientProvider}
     * if the uri is correct.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",408,412,"/**
* Calls underlying file system's implementation of m1.
* @param fsOutputFile output path to write results
* @param tmpLocalFile temporary local path for processing
*/","* Called when we're all done writing to the target.  A local FS will
   * do nothing, because we've written to exactly the right place.  A remote
   * FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createTmpFileForWrite,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",830,838,"/**
* Allocates file system resources based on provided path and configuration.
* @param pathStr input path string
* @param size allocated size
* @param conf configuration object
* @return allocated File object or throws IOException if allocation fails
*/","* Demand create the directory allocator, then create a temporary file.
     * This does not mark the file for deletion when a process exits.
     * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.
     *
     * @param pathStr prefix for the temporary file.
     * @param size    the size of the file that is going to be written.
     * @param conf    the Configuration object.
     * @return a unique temporary file.
     * @throws IOException IO problems",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)",129,132,"/**
 * Convenience wrapper to fetch user profile by ID with default size.
 * @param pathStr string representation of the path
 * @param conf Hadoop configuration object
 */","Get a path from the local FS. This method should be used if the size of 
   *  the file is not known apriori. We go round-robin over the set of disks
   *  (via the configured dirs) and return the first complete path where
   *  we could create the parent directory of the passed path. 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,run,org.apache.hadoop.fs.shell.Command:run(java.lang.String[]),184,201,"/**
* Executes command with arguments and returns a function mask.
* @param argv variable-length array of command-line arguments
*/","* Invokes the command handler.  The default behavior is to process options,
   * expand arguments, and then process each argument.
   * <pre>
   * run
   * |{@literal ->} {@link #processOptions(LinkedList)}
   * \{@literal ->} {@link #processRawArguments(LinkedList)}
   *      |{@literal ->} {@link #expandArguments(LinkedList)}
   *      |   \{@literal ->} {@link #expandArgument(String)}*
   *      \{@literal ->} {@link #processArguments(LinkedList)}
   *          |{@literal ->} {@link #processArgument(PathData)}*
   *          |   |{@literal ->} {@link #processPathArgument(PathData)}
   *          |   \{@literal ->} {@link #processPaths(PathData, PathData...)}
   *          |        \{@literal ->} {@link #processPath(PathData)}*
   *          \{@literal ->} {@link #processNonexistentPath(PathData)}
   * </pre>
   * Most commands will chose to implement just
   * {@link #processOptions(LinkedList)} and {@link #processPath(PathData)}
   * 
   * @param argv the list of command line arguments
   * @return the exit code for the command
   * @throws IllegalArgumentException if called with invalid arguments",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",101,104,"/**
* Initializes a new Reader instance with the specified configuration and path.
* @param fs FileSystem object
* @param file file path
* @param conf configuration settings
*/","* Construct an array reader for the named file.
     * @param fs FileSystem.
     * @param file file.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Initializes a new reader with the specified file system, directory name, 
* writable comparator, and configuration.","* Construct a set reader for the named set using the named comparator.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",510,514,"/**
* Constructs a new Reader instance from a directory name.
* @param dirName directory path
* @param conf configuration object
* @throws IOException if an I/O error occurs
*/","* Construct a map reader for the named map.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",526,530,"/**
* Constructs a Reader from given parameters.
* @param fs FileSystem object
* @param dirName directory name
* @param comparator WritableComparator instance
* @param conf Configuration object
*/","* Construct a map reader for the named map using the named comparator.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator WritableComparator.
     * @param conf Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",213,217,"/**
* Initializes reader with given directory and configuration.
* @param dir directory path
* @param conf Hadoop configuration object
* @param options sequence file reader options (optional)
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cloneFileAttributes,"org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",3406,3422,"/**
* Creates a new Writer instance for compression and writing to an output file.
* @param inputFile input file path
* @param outputFile output file path
* @param prog progress indicator
* @return Writer object for further configuration and usage
*/","* Clones the attributes (like compression of the input file and creates a 
     * corresponding Writer
     * @param inputFile the path of the input file whose attributes should be 
     * cloned
     * @param outputFile the path of the output file 
     * @param prog the Progressable to report status during the file write
     * @return Writer
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,fix,"org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)",934,1014,"/**
* Generates a file mask for the given directory, key and value classes.
* @param fs FileSystem object
* @param dir Path to the directory
* @param keyClass Class of the key Writable
* @param valueClass Class of the value Writable
* @param dryrun Whether this is a dry run or not
* @param conf Configuration object
* @return The generated file mask (long)","* This method attempts to fix a corrupt MapFile by re-creating its index.
   * @param fs filesystem
   * @param dir directory containing the MapFile data and index
   * @param keyClass key class (has to be a subclass of Writable)
   * @param valueClass value class (has to be a subclass of Writable)
   * @param dryrun do not perform any changes, just report what needs to be done
   * @param conf configuration.
   * @return number of valid entries in this MapFile, or -1 if no fixing was needed
   * @throws Exception Exception.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,flush,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)",3217,3251,"/**
* Writes compressed data to file.
* @param count number of records
* @param bytesProcessed processed bytes
* @param compressionType compression type
* @param codec compression codec
* @param done indicates end-of-write
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",312,357,"/**
* Creates a new file system writer for the specified directory.
* @param conf Hadoop configuration
* @param dirName path to the directory where files will be created
* @param opts sequence file options (e.g. key class, comparator)
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",308,315,"/**
* Wraps M5 writer with Hadoop configuration and file system.
* @param fs Hadoop file system
* @param conf Configuration object
* @param name Path to write data to
* @param keyClass Class of key objects
* @param valClass Class of value objects
* @return Wrapped M5 writer
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",330,339,"/**
* Wraps existing methods to create a new writer with specified configuration.
* @param fs file system instance
* @param conf job configuration
* @param name path to the output file
* @param keyClass class of the key data type
* @param valClass class of the value data type
* @param compressionType type of compression to apply
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",355,366,"/**
* Wraps the Hadoop Writer API with legacy method calls.
* @param fs HDFS file system
* @param conf Hadoop configuration
* @param name output path
* @param keyClass output key class
* @param valClass output value class
* @param compressionType compression type
* @param progress progressable callback
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",382,392,"/**
* Wraps a writer with additional configuration and codecs.
* @param fs file system instance
* @param conf configuration object
* @param name path to the writer
* @param keyClass class of key data type
* @param valClass class of value data type
* @param compressionType compression type
* @param codec compression codec
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",410,423,"/**
* Wraps deprecated method for creating a writer with specified parameters.
* @param fs FileSystem object
* @param conf Configuration object
* @param name Path to write to
* @param keyClass Class of key data type
* @param valClass Class of value data type
* @param compressionType Compression type
* @param codec Compression codec
* @param progress Progressable object
* @param metadata Metadata object
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",444,461,"/**
* Wrappers a legacy API call to create a writer.
* @param conf configuration object
* @return the created writer or null if failed
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",539,551,"/**
* Wraps a Hadoop writer with compression and other settings.
* @param fs Hadoop file system
* @param conf Hadoop configuration
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",567,577,"/**
* Wraps the underlying output stream for compression and encoding.
* @param conf configuration
* @param out output stream
* @param keyClass class of key data type
* @param valClass class of value data type
* @param compressionType compression type
* @param codec compression codec
* @param metadata additional metadata
*/","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",592,600,"/**
* Wraps the M5 method with legacy compression functionality.
* @param conf Job configuration
* @param out File output stream
* @param keyClass Key class type
* @param valClass Value class type
* @param compressionType Compression type (deprecated)
* @param codec Compression codec (deprecated)
*/","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)",1632,1635,"/**
* Wraps existing implementation with alternate working directory. 
* @param inputClassPath the input class path
* @param pwd current working directory to be replaced
* @param newPwd new working directory to use instead of {@code pwd}
* @param callerEnv environment variables to pass through
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)",102,105,"/**
* Constructs an ApplicationClassLoader instance with specified classpath and parent loader.
* @param classpath path to application classes
* @param parent parent ClassLoader instance
* @param systemClasses list of system classes to load
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String),405,407,"/**
* Convenience method to fetch file list with default settings.
* @param files directory path to retrieve files from
*/","* Takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * This method does not recognize wildcards.
   *
   * @param files the input files argument
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRead,org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey),1641,1671,"/**
* Processes client connection and updates internal state.
* @param key SelectionKey containing client connection
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,sendLogLevelRequest,org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest(),126,139,"/**
* Executes operation-specific logic based on the provided switch case.
* @throws HadoopIllegalArgumentException if an invalid operation is specified
*/","* Send HTTP/HTTPS request to the daemon.
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",807,817,"/**
* Creates a disk block for data storage.
* @param index unique block identifier
* @param limit maximum size of the block
* @param statistics upload statistics
* @return DataBlock object representing the created block
*/","* Create a temp file and a {@link DiskBlock} instance to manage it.
     *
     * @param index      block index.
     * @param limit      limit of the block.
     * @param statistics statistics to update.
     * @return the new block.
     * @throws IOException IO problems",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getTempFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",649,658,"/**
* Creates a temporary file for function masking.
* @param conf Hadoop configuration
* @param localDirAllocator directory allocator
* @return Path to the temporary file
*/","* Create temporary file based on the file path retrieved from local dir allocator
   * instance. The file is created with .bin suffix. The created file has been granted
   * posix file permissions available in TEMP_FILE_ATTRS.
   *
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @return path of the file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,run,org.apache.hadoop.fs.FsShell:run(java.lang.String[]),300,351,"/**
* Executes a command and returns the exit code.
* @param argv array of command-line arguments
* @return integer exit code or -1 on failure
*/",* run,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",112,114,"/**
 * Initializes a new instance of this class with the specified file system, directory name, and configuration.
 */","* Construct a set reader for the named set.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",219,223,"/**
* Constructs a Reader instance from a directory name. 
* @param dirName Name of the directory to read
* @param conf Hadoop configuration object
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)",225,229,"/**
* Creates a reader instance with given configuration.
* @param fs FileSystem object
* @param dirName directory name as string
* @param comparator WritableComparator instance for comparison
* @param conf Hadoop Configuration object
* @param open whether to open the reader immediately
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",231,235,"/**
* Constructs a Reader instance from the given configuration and comparator.
* @param fs FileSystem instance
* @param dirName directory name
* @param comparator WritableComparator instance
* @param conf Configuration object
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge(),3623,3723,"/**
* Performs a masked iteration over key-value pairs.
*/","This is the single level merge that is called multiple times 
       * depending on the factor size and the number of segments
       * @return RawKeyValueIterator
       * @throws IOException",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,run,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean),3100,3179,"/**
* Processes input files and generates FUNC_MASK values.
* @param deleteInput flag to delete processed input
* @return number of generated FUNC_MASK values
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)",82,89,"/**
* Initializes a SequenceFileWriter with specified configuration and parameters.
* @param conf Hadoop Configuration object
* @param fs   Hadoop FileSystem instance
* @param dirName directory name for the SequenceFile
* @param comparator key-value pair comparison function
* @param compress compression type for the SequenceFile
*/","* Create a set naming the element comparator and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",158,164,"/**
* Initializes a new SequenceFile writer for the specified directory.
* @param conf Hadoop configuration
* @param dir path to the output directory
* @param options additional write options (e.g. compression)
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,55,"/**
* Initializes a Hadoop MapReduce writer with configuration and file system.
* @param conf Hadoop Configuration object
* @param fs Hadoop FileSystem instance
* @param file input/output file path
* @param valClass Class of writable values to be written
*/","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",68,77,"/**
* Initializes a writer for the specified file with configurable settings.
* @param conf Hadoop configuration
* @param fs File system instance
* @param file Path to the output file
* @param valClass Class of writable values being written
* @param compress Compression type (e.g. NONE, DEFLATE)
* @param progress Progressable callback for reporting write progress
*/","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",112,117,"/**
* Constructs a writer using the provided configuration and directory path.
* @param conf Hadoop configuration
* @param dirName name of the output directory
* @param keyClass class of the writable key comparator
* @param valClass class of the writable values
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",132,139,"/**
* Legacy constructor for creating a Writer instance.
* @param conf Hadoop configuration
* @param fs file system
* @param dirName output directory name
* @param keyClass WritableComparable class
* @param valClass value class
* @param compress compression type
* @param progress progressable object
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",155,162,"/**
* Constructs a Hadoop InputFormat instance.
* @param conf Hadoop configuration
* @param fs file system to read from
* @param dirName directory name of the input data
* @param keyClass class of the writable comparable keys
* @param valClass class of the values
* @param compress compression type for output
* @param codec compression codec
* @param progress progressable callback for progress updates
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",175,181,"/**
* Creates a Writer instance from configuration and directory path.
* @param conf Hadoop Configuration
* @param fs HDFS file system
* @param dirName output directory name
* @param keyClass Class of WritableComparable key type
* @param valClass Class of value type
* @param compress compression type
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",192,198,"/**
* Calls the overloaded constructor with default file system and path.
* @param conf configuration object
* @param fs file system to use (default: local FS)
* @param dirName directory name for writing data
* @param comparator key comparator class
* @param valClass value class type
*/","Create the named map using the named key comparator. 
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",210,216,"/**
* Creates a Writer for a deprecated sequence file. 
* @param conf configuration object
* @param fs file system instance
* @param dirName directory name
* @param comparator key comparator
* @param valClass value class
* @param compress compression type
*/","Create the named map using the named key comparator.
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",231,239,"/**
* Constructs a SequenceFileWriter with default settings.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName directory name
* @param comparator key comparator class
* @param valClass value class
* @param compress compression type
* @param progress progressable object
*/","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...)} instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",255,263,"/**
* Wrappers the original constructor with deprecated parameters.
*/","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",1059,1090,"/**
* Initializes map file writer with input files and configuration.
* @param inMapFiles array of input map files
* @param outMapFile output map file path
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",513,522,"/**
* Creates a writer for data with specified configuration and options.
* @param conf Hadoop configuration
* @param name Path to store data
* @param keyClass Class of key data type
* @param valClass Class of value data type
* @param compressionType Compression type (e.g. NONE, GZIP)
* @param codec Compression codec
* @param metadata Additional metadata for writer
* @param createFlag Create flags (e.g. CREATE, OVERWRITE)
* @param opts Optional create options
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fc The context for the specified file.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @param createFlag gives the semantics of create: overwrite, append etc.
   * @param opts file creation options; see {@link CreateOpts}.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,main,org.apache.hadoop.util.Classpath:main(java.lang.String[]),64,113,"/**
* Processes command-line arguments and executes a command.
* @param args array of strings containing command-line arguments
*/","* Main entry point.
   *
   * @param args command-line arguments",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,createClassLoader,"org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)",344,383,"/**
* Creates a ClassLoader instance based on the provided file and work directory.
* @param file input file
* @param workDir working directory
* @return ClassLoader instance or throws MalformedURLException if invalid
*/","* Creates a classloader based on the environment that was specified by the
   * user. If HADOOP_USE_CLIENT_CLASSLOADER is specified, it creates an
   * application classloader that provides the isolation of the user class space
   * from the hadoop classes and their dependencies. It forms a class space for
   * the user jar as well as the HADOOP_CLASSPATH. Otherwise, it creates a
   * classloader that simply adds the user jar to the classpath.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,processGeneralOptions,org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine),291,364,"/**
* Configures Hadoop environment using various command-line options.
* @param line CommandLine object containing configuration options
*/","* Modify configuration according user-specified generic options.
   *
   * @param line User-specified generic options",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop(),1486,1527,"/**
* Processes pending connections and handles incoming data.
* @throws InterruptedException if interrupted while waiting for connections
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,run,org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[]),109,119,"/**
* Executes Hadoop function with provided arguments, catching and handling exceptions.
* @param args array of input arguments
* @return 0 on success, -1 on failure
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getCacheFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",496,500,"/**
* Returns a file path based on configuration and local directory allocator.
* @param conf Hadoop configuration
* @param localDirAllocator local directory allocator instance
*/","* Return temporary file created based on the file path retrieved from local dir allocator.
   *
   * @param conf The configuration object.
   * @param localDirAllocator Local dir allocator instance.
   * @return Path of the temporary file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,isCacheSpaceAvailable,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",621,633,"/**
* Checks if cache space is available for a file of given size.
* @param fileSize the size of the file
* @param conf configuration object
* @param localDirAllocator allocator for local directory
* @return true if cache space is sufficient, false otherwise
*/","* Determine if the cache space is available on the local FS.
   *
   * @param fileSize The size of the file.
   * @param conf The configuration.
   * @param localDirAllocator Local dir allocator instance.
   * @return True if the given file size is less than the available free space on local FS,
   * False otherwise.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)",3313,3319,"/**
* Executes merge queue operation on provided segment descriptors.
* @param segments list of segment descriptors to process
* @param tmpDir temporary directory for merging
*/","* Merges the list of segments of type <code>SegmentDescriptor</code>
     * @param segments the list of SegmentDescriptors
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)",3349,3364,"/**
* Merges input files into a single output stream.
* @param inNames array of input file paths
* @param deleteInputs whether to delete input files after merging
* @param factor merge factor
* @param tmpDir temporary directory for merging
*/","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param factor the factor that will be used as the maximum merge fan-in
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3375,3394,"/**
* Merges input files into a single output file.
* @param inNames array of input file paths
* @param tempDir temporary directory for merged output
* @return iterator over key-value pairs
*/","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param tempDir the directory for creating temp files during merge
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3479,3489,"/**
* Merges and iterates over raw key-value pairs from input files.
* @param inName input file path
* @param indexIn index file path
* @param tmpDir temporary directory for merging
* @return RawKeyValueIterator instance or null on failure
*/","Used by mergePass to merge the output of the sort
     * @param inName the name of the input file containing sorted segments
     * @param indexIn the offsets of the sorted segments
     * @param tmpDir the relative directory to store intermediate results in
     * @return RawKeyValueIterator
     * @throws IOException",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortPass,org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean),3064,3076,"/**
* Executes sorting pass and returns a mask value.
* @param deleteInput whether to delete input data after sorting
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",65,70,"/**
* Configures a new Writer instance with provided settings.
* @param conf Hadoop Configuration object
* @param fs HDFS file system handle
* @param dirName directory name for output files
* @param keyClass WritableComparable class type
* @param compress compression type for SequenceFile writes
*/","* Create a set naming the element class and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",90,97,"/**
* Constructs a Hadoop writer for the given configuration and directory.
* @param conf Hadoop configuration
* @param dirName output directory name
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",99,106,"/**
* Creates a Writer instance with specified configuration and parameters.
* @param conf Hadoop Configuration object
* @param fs  FileSystem to operate on
* @param dirName directory name for output
* @param keyClass Class of the key WritableComparable
* @param valClass Class of the value to be written
* @param compress Compression type (e.g. NONE, RECORD, BLOCK)
* @param progress Progressable object for monitoring progress
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",108,115,"/**
* Legacy constructor for creating a Writer instance.
* @param conf Hadoop configuration
* @param fs File system interface
* @param dirName output directory name
* @param keyClass writable key class type
* @param valClass value class type
* @param compress compression type
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",117,125,"/**
* Constructs a Writer object from deprecated parameters.
* @param conf configuration
* @param fs file system
* @param dirName directory name
* @param comparator writable comparator
* @param valClass value class
* @param compress compression type
* @param codec compression codec
* @param progress progressable object
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",127,134,"/**
* Constructs a Writer from legacy parameters.
* @param conf Configuration object
* @param fs FileSystem instance
* @param dirName Directory name
* @param comparator WritableComparator instance
* @param valClass Value class type
* @param compress Compression type
* @param progress Progressable instance
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",136,142,"/**
* Creates a new Writer instance using the provided configuration and directory name.
* @param conf Configuration object
* @param dirName Name of the directory
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",144,149,"/**
* Legacy constructor for a Writer instance.
* @param conf Hadoop configuration
* @param fs File system handle
* @param dirName Directory name
* @param comparator Key comparator class
* @param valClass Value class type
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",151,156,"/**
* Constructs a MapFileOutput instance with the given configuration and path.",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,main,org.apache.hadoop.io.MapFile:main(java.lang.String[]),1160,1191,"/**
* Maps input file to output file using MapFile API.
* @param args array of input and output file names
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,53,"/**
* Initializes a new writer instance with the specified configuration and file system.
* @param fs Hadoop file system to write to
* @param dirName name of the output directory
* @param keyClass class type for key data in the writable format
*/","* Create the named set for keys of the named class.
     * @deprecated pass a Configuration too
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,merge,"org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",1039,1053,"/**
* Processes input map files, deletes them if requested, and generates output.
* @param inMapFiles array of input map file paths
* @param deleteInputs whether to delete input files after processing
* @param outMapFile output map file path
*/","* Merge multiple MapFiles to one Mapfile.
     *
     * @param inMapFiles input inMapFiles.
     * @param deleteInputs deleteInputs.
     * @param outMapFile input outMapFile.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",480,496,"/**
* Creates a Writer with specified configuration and parameters.
* @param fs FileSystem instance
* @param conf Configuration object
* @param name Path to write to
* @param keyClass Class of key objects
* @param valClass Class of value objects
* @param bufferSize Buffer size for writing
* @param replication Replication factor
* @param blockSize Block size in bytes
* @param createParent Whether to create parent directory if not exists
* @param compressionType Compression type
* @param codec Compression codec instance
* @param metadata Metadata object
* @return Writer instance or null on failure
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param createParent create parent directory if non-existent
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,run,org.apache.hadoop.util.RunJar:run(java.lang.String[]),248,334,"/**
* Runs a JAR file with optional main class.
* @param args array of arguments to pass to the main class
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,parseGeneralOptions,"org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])",572,588,"/**
* Parses command line arguments using GNU parser.
* @param opts Options object to configure parsing
* @param args array of command line arguments
* @return true if parsing successful, false otherwise
*/","* Parse the user-specified options, get the generic options, and modify
   * configuration accordingly.
   *
   * @param opts Options to use for parsing args.
   * @param args User-specified arguments
   * @return true if the parse was successful",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener$Reader:run(),1472,1484,"/**
* Closes the read selector and logs any I/O errors.
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,put,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",370,413,"/**
* Processes a block with the given number and writes data to it if necessary.
* @param blockNumber unique block identifier
* @param buffer ByteBuffer object containing data
* @throws IOException if an I/O error occurs
*/","* Puts the given block in this cache.
   *
   * @param blockNumber the block number, used as a key for blocks map.
   * @param buffer buffer contents of the given block to be added to this cache.
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @throws IOException if either local dir allocator fails to allocate file or if IO error
   * occurs while writing the buffer content to the file.
   * @throws IllegalArgumentException if buffer is null, or if buffer.limit() is zero or negative.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",3331,3337,"/**
* Wraps existing call to overloaded m1 method.
* @param inNames array of input paths
* @param deleteInputs flag for deleting inputs
* @param tmpDir temporary directory path
*/","* Merges the contents of files passed in Path[] using a max factor value
     * that is already set
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,mergePass,org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path),3458,3470,"/**
* Generates a merge function mask.
*/",sort calls this to generate the final merged output,,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,main,org.apache.hadoop.util.RunJar:main(java.lang.String[]),244,246,"/**
 * Runs the main function of the JAR with the provided command line arguments.
 * @param args array of command line arguments
 */","Run a Hadoop job jar.  If the main class is not in the jar's manifest,
   * then it must be provided on the command line.
   *
   * @param args args.
   * @throws Throwable error.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",178,182,"/**
* Parses general command-line options and initializes configuration.
* @param conf Configuration object
* @param options Options object to store parsed values
* @param args Command-line arguments array
*/","* Create a <code>GenericOptionsParser</code> to parse given options as well 
   * as generic Hadoop options. 
   * 
   * The resulting <code>CommandLine</code> object can be obtained by 
   * {@link #getCommandLine()}.
   * 
   * @param conf the configuration to modify  
   * @param options options built by the caller 
   * @param args User-specified arguments
   * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortAndIterate,"org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3032,3052,"/**
* Creates a RawKeyValueIterator instance for the given input files.
* @param inFiles array of input file paths
* @param tempDir temporary directory path
* @param deleteInput whether to delete input files after processing
* @return iterator instance or null if not created
*/","* Perform a file sort from a set of input files and return an iterator.
     * @param inFiles the files to be sorted
     * @param tempDir the directory where temp files are created during sort
     * @param deleteInput should the input files be deleted as they are read?
     * @return iterator the RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",3445,3455,"/**
* Writes key-value pairs from input files to a single output file.
* @param inFiles array of input file paths
* @param outFile path to the output file
*/","Merge the provided files.
     * @param inFiles the array of input path names
     * @param outFile the final output file
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3009,3022,"/**
* Creates a merged output file from multiple input files.
* @param inFiles array of input files to merge
* @param outFile path to the output file
* @param deleteInput flag to delete input files after merging
*/","* Perform a file sort from a set of input files into an output file.
     * @param inFiles the files to be sorted
     * @param outFile the sorted output file
     * @param deleteInput should the input files be deleted as they are read?
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,"org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",1081,1084,"/**
* Initializes parser with configuration and command-line arguments.
* @param conf Configuration object
* @param options Options object
* @param args Command-line argument array
*/",,,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])",135,138,"/**
* Constructs a GenericOptionsParser instance with default configuration.
* @param opts Options object to configure parser
* @param args Command-line arguments array
*/","* Create an options parser with the given options to parse the args.
   * @param opts the options
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[]),145,148,"/**
* Initializes parser with given command-line arguments.
* @param args array of command-line parameters
*/","* Create an options parser to parse the args.
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])",161,164,"/**
* Initializes parser with configuration and command-line arguments.
* @param conf application configuration
* @param args array of command-line options
*/","* Create a <code>GenericOptionsParser</code> to parse only the generic
   * Hadoop arguments.
   * 
   * The array of string arguments other than the generic arguments can be 
   * obtained by {@link #getRemainingArgs()}.
   * 
   * @param conf the <code>Configuration</code> to modify.
   * @param args command-line arguments.
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3060,3062,"/**
 * Copies file from specified input path to output path.
 * @param inFile input file path
 * @param outFile output file path
 */","* The backwards compatible interface to sort.
     * @param inFile the input file to sort.
     * @param outFile the sorted output file.
     * @throws IOException raised on errors performing I/O.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createGenericOptionsParser,"org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])",979,982,"/**
* Creates and returns a MinimalGenericOptionsParser instance.
* @param conf configuration object
* @param argArray array of command-line arguments
*/","* Override point: create a generic options parser or subclass thereof.
   * @param conf Hadoop configuration
   * @param argArray array of arguments
   * @return a generic options parser to parse the arguments
   * @throws IOException on any failure",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,main,org.apache.hadoop.util.ConfTest:main(java.lang.String[]),227,300,"/**
* Validates input files against Hadoop configuration.
* @param args command line arguments
*/",,,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])",62,83,"/**
* Parses CLI arguments and executes main functionality.
* @param conf configuration object
* @param tool Tool instance
* @param args command-line arguments
* @return int result code
*/","* Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after 
   * parsing with the given generic arguments. Uses the given 
   * <code>Configuration</code>, or builds one if null.
   * 
   * Sets the <code>Tool</code>'s configuration with the possibly modified 
   * version of the <code>conf</code>.  
   * 
   * @param conf <code>Configuration</code> for the <code>Tool</code>.
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception Exception.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,parseCommandArgs,"org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)",917,970,"/**
* Parses command line arguments and returns remaining args.
* @param conf Configuration object
* @param args Command line arguments
* @return List of remaining arguments or null on error
*/","* Parse the command arguments, extracting the service class as the last
   * element of the list (after extracting all the rest).
   *
   * The field {@link #commandOptions} field must already have been set.
   * @param conf configuration to use
   * @param args command line argument list
   * @return the remaining arguments
   * @throws ServiceLaunchException if processing of arguments failed",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,exec,"org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])",1052,1056,"/**
* Initializes function mask using Hadoop configuration and command-line arguments.
* @param conf Hadoop configuration object
* @param argv variable number of command-line arguments
* @return initialized mask value or throws exception on failure
*/","* Inner entry point, with no logging or system exits.
   *
   * @param conf configuration
   * @param argv argument list
   * @return an exception
   * @throws Exception Exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,main,org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[]),393,395,"/**
* Executes M2 tool with specified arguments and configuration.
* @throws Exception if execution fails
*/",,,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,main,org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[]),534,537,"/**
* Executes function with provided arguments and masks result.
* @throws Exception if execution fails
*/","* Main program.
   *
   * @param args
   *          Command line arguments
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,main,org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[]),552,555,"/**
* Executes key shell functionality with given arguments.
* @param args array of command-line arguments
*/","* main() entry point for the KeyShell.  While strictly speaking the
   * return is void, it will System.exit() with a return code: 0 is for
   * success and 1 for failure.
   *
   * @param args Command line arguments.
   * @throws Exception raised on errors performing I/O.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])",95,98,"/**
* Calls m2 with result of Tool's m1() and original parameters.
*/","* Runs the <code>Tool</code> with its <code>Configuration</code>.
   * 
   * Equivalent to <code>run(tool.getConf(), tool, args)</code>.
   * 
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,extractCommandOptions,"org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)",896,905,"/**
* Extracts and processes a subset of command-line arguments.
* @param conf configuration object
* @param args list of all command-line arguments
* @return filtered list of string arguments or empty list if none found
*/","* Extract the command options and apply them to the configuration,
   * building an array of processed arguments to hand down to the service.
   *
   * @param conf configuration to update.
   * @param args main arguments. {@code args[0]}is assumed to be
   * the service classname and is skipped.
   * @return the remaining arguments
   * @throws ExitUtil.ExitException if JVM exiting is disabled.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,main,org.apache.hadoop.security.KDiag:main(java.lang.String[]),1062,1072,"/**
* Executes function with given arguments and error handling.
* @param argv array of command-line arguments
*/","* Main entry point.
   * @param argv args list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,main,org.apache.hadoop.fs.FsShell:main(java.lang.String[]),383,395,"/**
* Executes a FsShell command with the provided arguments.
* @param argv array of command-line arguments
*/","* main() has some simple utility methods
   * @param argv the command and its arguments
   * @throws Exception upon error",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,main,org.apache.hadoop.log.LogLevel:main(java.lang.String[]),73,76,"/**
* Executes CLI tool with masked arguments.
* @param args array of command-line arguments
*/","* A command line implementation
   * @param args input args.
   * @throws Exception exception.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,main,org.apache.hadoop.util.FindClass:main(java.lang.String[]),378,386,"/**
* Executes FindClass tool with provided arguments.
* @param args array of command-line arguments
*/","* Main entry point. 
   * Runs the class via the {@link ToolRunner}, then
   * exits with an appropriate exit code. 
   * @param args argument list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchServiceAndExit,org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List),281,315,"/**
* Processes command line arguments and exits with usage message if necessary.
*@param args list of command line arguments
*/","* Launch the service and exit.
   *
   * <ol>
   * <li>Parse the command line.</li> 
   * <li>Build the service configuration from it.</li>
   * <li>Start the service.</li>
   * <li>If it is a {@link LaunchableService}: execute it</li>
   * <li>Otherwise: wait for it to finish.</li>
   * <li>Exit passing the status code to the {@link #exit(int, String)}
   * method.</li>
   * </ol>
   * @param args arguments to the service. {@code arg[0]} is 
   * assumed to be the service classname.",,,True,42
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List),1064,1073,"/**
* Launches a service based on the input list.
* @param argsList contains flags and parameters for service execution
*/",,,,True,43
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,main,org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[]),1043,1045,"/**
* Masks arguments according to specified criteria.
* @param args array of strings to be processed
*/","* This is the JVM entry point for the service launcher.
   *
   * Converts the arguments to a list, then invokes {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[]),1052,1054,"/**
* Recursively calls itself with array result of m1 function.
* @param args variable number of arguments
*/","* Varargs version of the entry point for testing and other in-JVM use.
   * Hands off to {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
