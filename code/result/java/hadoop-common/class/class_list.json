{
    "org.apache.hadoop.fs.impl.FileRangeImpl": {
        "org.apache.hadoop.fs.impl.FileRangeImpl:<init>(long,int,java.lang.Object)": "/**\n* Constructs a FileRangeImpl with specified offset, length, and reference.\n* @param offset starting position in the file\n* @param length number of bytes to read\n* @param reference additional object for context\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:getOffset()": "/**\n* Retrieves the current offset value.\n* @return the current offset as a long\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:getLength()": "/**\n* Returns the length of the object.\n* @return the length as an integer\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:getReference()": "/**\n* Returns the current reference object.\n* @return the reference object\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:getData()": "/**\n* Retrieves data as a ByteBuffer asynchronously.\n* @return CompletableFuture containing the ByteBuffer data\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:setData(java.util.concurrent.CompletableFuture)": "/**\n* Sets the data reader for processing.\n* @param pReader CompletableFuture containing ByteBuffer data\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:setLength(int)": "/**\n* Sets the length property.\n* @param length the new length value to set\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:setOffset(long)": "/**\n* Sets the offset value for positioning.\n* @param offset the new offset value to set\n*/",
        "org.apache.hadoop.fs.impl.FileRangeImpl:toString()": "/**\n* Returns a formatted string representation of the object's range and reference.\n* @return formatted string with range, length, and reference details\n*/"
    },
    "org.apache.hadoop.util.Preconditions": {
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.lang.String,java.lang.Object[])": "/**\n* Validates an expression and throws an exception if false.\n* @param expression condition to check\n* @param errorMsg message template for exception\n* @param errorMsgArgs arguments for formatting the message\n*/",
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.lang.Object)": "/**\n* Validates a boolean expression, throws IllegalArgumentException if false.\n* @param expression condition to check\n* @param errorMessage message to include in exception if check fails\n*/",
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.Object)": "/**\n* Checks if the object is non-null; throws NullPointerException if null.\n* @param obj object to check\n* @param errorMessage message for exception if obj is null\n* @return the non-null object\n*/",
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean)": "/**\n* Validates a boolean expression; throws IllegalArgumentException if false.\n* @param expression condition to validate\n*/",
        "org.apache.hadoop.util.Preconditions:checkState(boolean,java.lang.Object)": "/**\n* Validates an expression; throws IllegalStateException if false.\n* @param expression condition to check\n* @param errorMessage message for the exception if check fails\n*/",
        "org.apache.hadoop.util.Preconditions:checkState(boolean)": "/**\n* Validates a boolean expression; throws IllegalStateException if false.\n* @param expression condition to check\n*/",
        "org.apache.hadoop.util.Preconditions:checkState(boolean,java.lang.String,java.lang.Object[])": "/**\n* Validates an expression and throws an exception with a formatted message if false.\n* @param expression condition to check\n* @param errorMsg message template for exception\n* @param errorMsgArgs arguments for formatting the error message\n*/",
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.String,java.lang.Object[])": "/**\n* Checks if the object is not null; throws NullPointerException with formatted message if null.\n* @param obj the object to check\n* @param message the error message template\n* @param values values for the message formatting\n* @return the original object if not null\n*/",
        "org.apache.hadoop.util.Preconditions:<init>()": "/**\n* Private constructor to prevent instantiation of the Preconditions class.\n*/",
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.util.function.Supplier)": "/**\n* Checks if the object is not null; throws NPE with custom message if it is.\n* @param obj the object to check\n* @param msgSupplier provides the error message if obj is null\n* @return the non-null object\n*/",
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.util.function.Supplier)": "/**\n* Validates a boolean expression; throws IllegalArgumentException if false.\n* @param expression condition to check\n* @param msgSupplier provides the error message\n*/",
        "org.apache.hadoop.util.Preconditions:checkState(boolean,java.util.function.Supplier)": "/**\n* Validates a condition and throws IllegalStateException if false.\n* @param expression condition to check\n* @param msgSupplier provides error message if condition fails\n*/",
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object)": "/**\n* Validates that the object is non-null.\n* @param obj object to check\n* @return the non-null object\n*/"
    },
    "org.apache.hadoop.fs.VectoredReadUtils": {
        "org.apache.hadoop.fs.VectoredReadUtils:isOrderedDisjoint(java.util.List,int,int)": "/**\n* Checks if file ranges are ordered and disjoint based on chunk size and minimum seek.\n* @param input list of FileRange objects to check\n* @param chunkSize size of each chunk for alignment\n* @param minimumSeek minimum distance between ranges\n* @return true if ranges are ordered and disjoint, false otherwise\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:sortRangeList(java.util.List)": "/**\n* Sorts a list of FileRange objects by their offset.\n* @param input list of FileRange objects to sort\n* @return sorted list of FileRange objects\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:roundDown(long,int)": "/**\n* Rounds down the offset to the nearest multiple of chunkSize.\n* @param offset value to round down\n* @param chunkSize size of the chunk for rounding\n* @return rounded down long value\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:roundUp(long,int)": "/**\n* Rounds up the offset to the nearest multiple of chunkSize.\n* @param offset value to round up\n* @param chunkSize size of the chunks for rounding\n* @return rounded value based on chunkSize\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:sliceTo(java.nio.ByteBuffer,long,org.apache.hadoop.fs.FileRange)": "/**\n* Slices a ByteBuffer based on the specified offset and length.\n* @param readData original ByteBuffer to slice from\n* @param readOffset starting position in the original buffer\n* @param request specifies offset and length for slicing\n* @return sliced ByteBuffer containing relevant data\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:<init>()": "/**\n* Prevents instantiation of the VectoredReadUtils class.\n* @throws UnsupportedOperationException if attempted\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange)": "",
        "org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List)": "",
        "org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)": "/**\n* Reads data into a direct ByteBuffer from a specified FileRange.\n* @param range defines the data range to read\n* @param buffer target ByteBuffer for the read data\n* @param operation function to handle read operation\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)": "/**\n* Validates and sorts a list of FileRange objects.\n* @param input list of FileRange objects to validate and sort\n* @param fileLength optional maximum file length for range validation\n* @return sorted list of valid FileRange objects\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)": "/**\n* Merges sorted FileRange objects into CombinedFileRange based on chunk size and limits.\n* @param sortedRanges list of sorted FileRange objects\n* @param chunkSize size of the chunks for merging\n* @param minimumSeek minimum seek distance for merging\n* @param maxSize maximum allowed size for merged ranges\n* @return list of CombinedFileRange objects\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)": "/**\n* Reads data into a ByteBuffer from a PositionedReadable stream.\n* @param stream source for reading data\n* @param range data range to read from the stream\n* @param buffer target ByteBuffer for the read data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List)": "/**\n* Validates and sorts vectored read ranges.\n* @param ranges list of FileRange objects to validate\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)": "/**\n* Reads a range of data from a PositionedReadable stream.\n* @param stream source stream to read from\n* @param range data range to read\n* @param allocate function to allocate ByteBuffer\n* @return CompletableFuture containing the read ByteBuffer\n*/",
        "org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)": "/**\n* Reads data into FileRange objects from a stream after validation and sorting.\n* @param stream source stream to read from\n* @param ranges list of FileRange objects to validate and sort\n* @param allocate function to allocate ByteBuffer\n* @throws EOFException if end of stream is reached unexpectedly\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$Perms": {
        "org.apache.hadoop.fs.Options$CreateOpts$Perms:<init>(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Initializes Perms with specified file system permissions.\n* @param perm file system permissions, must not be null\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$Perms:getValue()": "/**\n* Retrieves the current file system permissions.\n* @return FsPermission object representing the permissions\n*/"
    },
    "org.apache.hadoop.fs.UploadHandle": {
        "org.apache.hadoop.fs.UploadHandle:toByteArray()": "/**\n* Converts the current object to a byte array.\n* @return byte array representation of the object\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem$LruCache": {
        "org.apache.hadoop.fs.HarFileSystem$LruCache:<init>(int)": "/**\n* Constructs an LRU cache with a specified maximum number of entries.\n* @param maxEntries maximum number of entries in the cache\n*/",
        "org.apache.hadoop.fs.HarFileSystem$LruCache:removeEldestEntry(java.util.Map$Entry)": "/**\n* Determines if the eldest entry should be removed.\n* @param eldest the eldest entry in the map\n* @return true if the map size exceeds MAX_ENTRIES, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.Path": {
        "org.apache.hadoop.fs.Path:toUri()": "/**\n* Returns the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.Path:depth()": "/**\n* Calculates the depth of the URI path.\n* @return number of path segments in the URI\n*/",
        "org.apache.hadoop.fs.Path:compareTo(org.apache.hadoop.fs.Path)": "/**\n* Compares this Path object with another by their URI.\n* @param o Path object to compare with\n* @return negative if this URI is less, positive if greater, zero if equal\n*/",
        "org.apache.hadoop.fs.Path:getName()": "/**\n* Retrieves the name from the URI path.\n* @return the name extracted from the URI path\n*/",
        "org.apache.hadoop.fs.Path:equals(java.lang.Object)": "/**\n* Compares this Path object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:hashCode()": "/**\n* Returns the hash code of the URI object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.Path:<init>(java.net.URI)": "/**\n* Initializes Path object with a normalized URI.\n* @param aUri the URI to normalize\n*/",
        "org.apache.hadoop.fs.Path:checkPathArg(java.lang.String)": "/**\n* Validates the path argument for null or empty string.\n* @param path the path string to validate\n* @throws IllegalArgumentException if path is null or empty\n*/",
        "org.apache.hadoop.fs.Path:hasWindowsDrive(java.lang.String)": "/**\n* Checks if the given path has a Windows drive letter.\n* @param path the file path to check\n* @return true if it's a Windows path, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:validateObject()": "/**\n* Validates the object; throws exception if URI is null.\n* @throws InvalidObjectException if the URI is not set\n*/",
        "org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String)": "/**\n* Determines starting position based on Windows drive presence.\n* @param path file path to evaluate\n* @return starting position index as int\n*/",
        "org.apache.hadoop.fs.Path:toString()": "/**\n* Converts URI to string format, preserving illegal characters for glob processing.\n* @return formatted URI string\n*/",
        "org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)": "/**\n* Normalizes file paths by removing duplicates and adjusting slashes for Windows.\n* @param scheme URI scheme, can be null or empty\n* @param path the file path to normalize\n* @return normalized file path as a String\n*/",
        "org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)": "/**\n* Checks if the given path is a Windows absolute path.\n* @param pathString the path string to evaluate\n* @param slashed indicates if the path should have a leading slash\n* @return true if it's a Windows absolute path, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:isUriPathAbsolute()": "/**\n* Checks if the URI path is absolute.\n* @return true if absolute, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Initializes a URI object with given components.\n* @param scheme URI scheme, can be null or empty\n* @param authority URI authority\n* @param path file path to normalize\n* @param fragment URI fragment\n*/",
        "org.apache.hadoop.fs.Path:checkNotSchemeWithRelative()": "/**\n* Validates URI scheme and path; throws exception if scheme is present but path is relative.\n*/",
        "org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull()": "/**\n* Checks if URI path is absolute and scheme/authority are null.\n* @return true if conditions are met, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:isAbsolute()": "/**\n* Determines if the URI path is absolute.\n* @return true if the path is absolute, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Constructs a Path by resolving a child against a parent Path.\n* @param parent the parent Path to resolve against\n* @param child the child Path to be resolved\n*/",
        "org.apache.hadoop.fs.Path:<init>(java.lang.String)": "/**\n* Constructs a Path object from a string, validating and parsing its components.\n* @param pathString the path string to initialize\n* @throws IllegalArgumentException if pathString is invalid\n*/",
        "org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Constructs a Path object with validated and formatted path components.\n* @param scheme URI scheme, can be null or empty\n* @param authority URI authority\n* @param path file path to normalize\n*/",
        "org.apache.hadoop.fs.Path:checkNotRelative()": "/**\n* Validates that the path is absolute; throws exception if it's relative.\n*/",
        "org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a qualified URI Path.\n* @param defaultUri the default URI for scheme and authority\n* @param workingDir the working directory for relative paths\n* @return a qualified Path object\n*/",
        "org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a Path by resolving a child against a parent Path.\n* @param parent the parent Path as a String\n* @param child the child Path as a String\n*/",
        "org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Constructs a Path by resolving a child against a parent Path.\n* @param parent the parent Path to resolve against\n* @param child the child Path as a String\n*/",
        "org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)": "/**\n* Constructs a Path by resolving a child against a parent Path.\n* @param parent the parent Path as a string\n* @param child the child Path to be resolved\n*/",
        "org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path)": "/**\n* Returns a Path without scheme and authority.\n* @param path the original Path object\n* @return modified Path without scheme and authority\n*/",
        "org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Merges two Path objects, normalizing the second path's components.\n* @param path1 first Path, used as base\n* @param path2 second Path, components are merged into path1\n* @return new Path object with combined components\n*/",
        "org.apache.hadoop.fs.Path:getParentUtil()": "/**\n* Retrieves the parent directory of the URI's path.\n* @return Path object representing the parent or null if at root or empty\n*/",
        "org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem)": "/**\n* Converts a FileSystem to a qualified Path.\n* @param fs the FileSystem to be converted\n* @return a qualified Path object\n*/",
        "org.apache.hadoop.fs.Path:getParent()": "/**\n* Retrieves the parent directory of the current URI's path.\n* @return Path object representing the parent or null if at root or empty\n*/",
        "org.apache.hadoop.fs.Path:getOptionalParentPath()": "/**\n* Returns an Optional containing the parent path or empty if at root.\n* @return Optional<Path> parent path or empty if none exists\n*/",
        "org.apache.hadoop.fs.Path:isRoot()": "/**\n* Checks if the current URI is the root directory.\n* @return true if at root, false otherwise\n*/",
        "org.apache.hadoop.fs.Path:suffix(java.lang.String)": "/**\n* Appends suffix to the path name, resolving against the parent path if available.\n* @param suffix string to append to the path name\n* @return new Path object with the appended suffix\n*/",
        "org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a FileSystem instance based on the object's URI and provided configuration.\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if filesystem initialization fails\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem": {
        "org.apache.hadoop.fs.HarFileSystem:getHarAuth(java.net.URI)": "/**\n* Constructs authentication string from URI.\n* @param underLyingUri the URI to extract authentication info from\n* @return formatted authentication string\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getConf()": "/**\n* Retrieves the current configuration from the file system.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getHarVersion()": "/**\n* Retrieves the HAR version from metadata.\n* @return the HAR version number\n* @throws IOException if metadata is null\n*/",
        "org.apache.hadoop.fs.HarFileSystem:decodeString(java.lang.String)": "/**\n* Decodes a URL-encoded string.\n* @param str the encoded string to decode\n* @return the decoded string\n* @throws UnsupportedEncodingException if the encoding is not supported\n*/",
        "org.apache.hadoop.fs.HarFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Creates a PathHandle from FileStatus with optional parameters.\n* @param stat file status information\n* @param opts optional handle options\n* @throws UnsupportedOperationException if not implemented\n*/",
        "org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.PathHandle,int)": "/**\n* Opens a file stream for the given path handle with specified buffer size.\n* @param fd file descriptor handle\n* @param bufferSize size of the buffer for the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Throws an IOException indicating creation is not allowed.\n* @param f path to the file to create\n* @param permission file permissions\n* @param overwrite whether to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @param progress callback for progress updates\n*/",
        "org.apache.hadoop.fs.HarFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Throws an IOException indicating creation is not permitted.\n* @param f path to create\n* @param overwrite whether to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize size of each block\n* @param progress progress callback\n*/",
        "org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Throws IOException as appending to the specified path is not permitted.\n* @param f path to the file\n* @param bufferSize size of the buffer\n* @param progress progress indicator\n* @throws IOException if append operation is attempted\n*/",
        "org.apache.hadoop.fs.HarFileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Throws IOException indicating replication setting is not allowed.\n* @param src source path for replication\n* @param replication desired replication factor\n* @return always throws IOException\n*/",
        "org.apache.hadoop.fs.HarFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Throws IOException to indicate renaming is not permitted.\n* @param src source path to rename\n* @param dst destination path (unused)\n* @throws IOException if rename is attempted\n*/",
        "org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path)": "/**\n* Throws an exception indicating appending to the file is not permitted.\n* @param f the path of the file to append to\n* @throws IOException if append operation is attempted\n*/",
        "org.apache.hadoop.fs.HarFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Throws an IOException indicating truncate operation is not permitted.\n* @param f the file path to truncate\n* @param newLength the desired new length of the file\n* @return always throws IOException\n*/",
        "org.apache.hadoop.fs.HarFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Throws IOException when attempting to delete a file or directory.\n* @param f path to the file or directory to delete\n* @param recursive indicates if deletion should be recursive\n* @throws IOException if deletion is not allowed\n*/",
        "org.apache.hadoop.fs.HarFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Throws an exception indicating mkdirs operation is not allowed.\n* @param f the path to create directories at\n* @param permission the permissions for the new directories\n* @throws IOException always thrown with a specific message\n*/",
        "org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Throws IOException as local file copy is not permitted.\n* @param delSrc indicates if the source should be deleted\n* @param overwrite indicates if existing files should be overwritten\n* @param src source file path\n* @param dst destination file path\n*/",
        "org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)": "/**\n* Throws IOException indicating local file copy is not permitted.\n* @param delSrc flag to delete source files after copy\n* @param overwrite flag to overwrite existing files\n* @param srcs array of source file paths\n* @param dst destination path for the copy\n*/",
        "org.apache.hadoop.fs.HarFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Throws IOException indicating local output start is not permitted.\n* @param fsOutputFile path for the output file in the filesystem\n* @param tmpLocalFile temporary local file path\n*/",
        "org.apache.hadoop.fs.HarFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Throws IOException indicating completion of local output is not permitted.\n* @param fsOutputFile path to the output file in the filesystem\n* @param tmpLocalFile path to the temporary local file\n*/",
        "org.apache.hadoop.fs.HarFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Throws an IOException indicating setting owner is not permitted.\n* @param p the path to set the owner for (ignored)\n* @param username the new owner's username (ignored)\n* @param groupname the new owner's group name (ignored)\n*/",
        "org.apache.hadoop.fs.HarFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Throws IOException indicating setTimes operation is not permitted.\n* @param p file path to set times for\n* @param mtime modified time in milliseconds\n* @param atime access time in milliseconds\n*/",
        "org.apache.hadoop.fs.HarFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Throws IOException indicating permission setting is not allowed.\n* @param p the path for which permission is being set\n* @param permission the desired file system permission\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getUri()": "/**\n* Retrieves the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.HarFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory; currently does nothing.\n* @param newDir the new directory path to set\n*/",
        "org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String)": "",
        "org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)": "",
        "org.apache.hadoop.fs.HarFileSystem:msync()": "",
        "org.apache.hadoop.fs.HarFileSystem:getDefaultReplication()": "/**\n* Retrieves the default replication factor from the filesystem.\n* @return default replication value as short\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the default replication factor for a given file path.\n* @param f the file path to check replication for\n* @return short representing the default replication factor\n*/",
        "org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the path has the specified capability.\n* @param path the Path to check\n* @param capability the capability to validate\n* @return true if capability is present, false otherwise\n*/",
        "org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI)": "/**\n* Canonicalizes the given URI using the file system's method.\n* @param uri the URI to canonicalize\n* @return modified URI with default port, if applicable\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file system status for the specified path.\n* @param p the path for which the status is requested\n* @return FsStatus object with capacity, used, and remaining space\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path)": "/**\n* Computes a non-negative hash code for the given Path.\n* @param p the Path object to hash\n* @return non-negative hash code as an integer\n*/",
        "org.apache.hadoop.fs.HarFileSystem:close()": "/**\n* Closes the file system and handles potential I/O errors.\n* @throws IOException if an I/O error occurs during closure\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getCanonicalUri()": "/**\n* Retrieves the canonical URI by delegating to the filesystem.\n* @return canonicalized URI or null if original URI is invalid\n*/",
        "org.apache.hadoop.fs.HarFileSystem:<init>()": "/**\n* Constructs a HarFileSystem instance; initialize() must be called to configure the file system.\n*/",
        "org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes HarFileSystem with a given FileSystem instance.\n* @param fs FileSystem to associate with this HarFileSystem\n*/",
        "org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path)": "/**** Creates a file output stream builder at the specified path.  \n* @param path destination Path for output stream  \n* @return FSDataOutputStreamBuilder instance  \n*/",
        "org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path)": "/**\n* Appends a file at the specified path.\n* @param path the destination Path for the output stream\n* @return FSDataOutputStreamBuilder for further configuration\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path)": "/**\n* Returns the used length of content at the specified path.\n* @param path location of the file or directory\n* @return length of used content as a long value\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory()": "/**\n* Returns the working directory as a Path object.\n* @return Path representing the working directory\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getHomeDirectory()": "/**\n* Returns the home directory as a Path object.\n* @return Path representing the home directory\n*/",
        "org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path)": "/**\n* Normalizes the given path, ensuring it is absolute and returns a qualified Path object.\n* @param path the input Path to be normalized\n* @return a qualified Path object based on the input\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory()": "/**\n* Retrieves the initial working directory.\n* @return Path representing the initial working directory\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getUsed()": "/**\n* Returns the used length of content at the root path.\n* @return length of used content as a long value\n*/",
        "org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path)": "/**\n* Finds the nearest parent path ending with '.har'.\n* @param p input Path to search from\n* @return Path object or null if not found\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path)": "/**\n* Constructs a Path in HAR format from the given Path.\n* @param path input Path to process\n* @return Path in HAR format or null if not applicable\n*/",
        "org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a relative Path based on the initial string.\n* @param initial base path as a String\n* @param p Path to convert\n* @return relative Path object\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves HAR status for a given file path.\n* @param f the file path to check\n* @return HarStatus object for the file\n*/",
        "org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus)": "/**\n* Converts HarStatus to FileStatus using metadata and path information.\n* @param h HarStatus object containing file/directory info\n* @return FileStatus representation of the HarStatus\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)": "/**\n* Retrieves block locations for a specified file within a byte range.\n* @param file the file status to check\n* @param start starting byte position\n* @param len length of the range\n* @return array of BlockLocation or empty if no blocks found\n*/",
        "org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file as FSDataInputStream; throws if the path is a directory.\n* @param f file path to open\n* @param bufferSize size of the input buffer\n* @return FSDataInputStream for the file\n*/",
        "org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)": "/**\n* Populates statuses with FileStatus from HarStatus children paths.\n* @param parent HarStatus object containing directory info\n* @param statuses list to store FileStatus objects\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus from a given file path.\n* @param f the file path to check\n* @return FileStatus representation of the file\n*/",
        "org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists FileStatus for the given path, returning an array of statuses.\n* @param f the path to list statuses for\n* @return array of FileStatus objects for the path\n*/",
        "org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the metadata cache if not already created.\n* @param conf configuration object for cache settings\n*/",
        "org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Decodes a HAR URI, validating its format and returning a proper URI.\n* @param rawURI the original HAR URI to decode\n* @param conf configuration for default filesystem URI\n* @return decoded URI object\n* @throws IOException if URI is invalid or contains unsupported components\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize()": "/**\n* Returns the default block size in bytes, using deprecated method from filesystem.\n* @return default block size, defaulting to 32MB if not configured\n*/",
        "org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path)": "/**\n* Validates the given filesystem path.\n* @param path the Path object to check\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getServerDefaults()": "/**\n* Retrieves file storage defaults, utilizing a deprecated method.\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)": "/**\n* Returns the default block size for the specified file path.\n* @param f the file path to get the block size for\n* @return default block size in bytes\n*/",
        "org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves the filesystem path from the given Path object.\n* @param p the Path object to resolve\n* @return resolved Path object\n*/",
        "org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file storage defaults for a specified path.\n* @param f the file path to retrieve defaults for\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from source to local destination.\n* @param delSrc flag to delete source after copy\n* @param src source Path, @param dst destination Path\n*/",
        "org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the HAR filesystem with metadata and checks for index files.\n* @param name URI of the HAR filesystem\n* @param conf configuration settings\n* @throws IOException if initialization fails or paths are invalid\n*/"
    },
    "org.apache.hadoop.fs.FileSystem": {
        "org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists at the given path.\n* @param f the file path to check\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:msync()": "/**\n* Throws UnsupportedOperationException indicating msync is not supported.\n* @throws UnsupportedOperationException if method is called\n*/",
        "org.apache.hadoop.fs.FileSystem:getDefaultReplication()": "/**\n* Returns the default replication factor (deprecated).\n* @return default replication value as short\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Creates a file with specified parameters.\n* @param f file path, @param permission access rights, \n* @param flags creation options, @param bufferSize size of buffer, \n* @param replication number of replicas, @param blockSize size of blocks, \n* @param progress callback for progress, @param checksumOpt checksum options\n* @return FSDataOutputStream for writing to the file\n*/",
        "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a non-recursive FSDataOutputStream.\n* @param f file path, @param permission file permissions, @param flags creation flags,\n* @param bufferSize size of buffer, @param replication replication factor, \n* @param blockSize block size, @param progress progress tracker.\n* @throws IOException if the operation is unsupported.\n*/",
        "org.apache.hadoop.fs.FileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner of the specified path to the given username and groupname.\n* @param p path to set the owner for\n* @param username new owner's username\n* @param groupname new owner's group name\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:listStatusIterator(org.apache.hadoop.fs.Path)": "/**\n* Returns an iterator for file statuses in the specified directory.\n* @param p directory path\n* @return RemoteIterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Throws exception indicating removal of default ACL is unsupported.\n* @param path the path for which to remove the default ACL\n*/",
        "org.apache.hadoop.fs.FileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file system permissions for the specified path.\n* @param p the path to set permissions on\n* @param permission the permissions to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:getDefaultPort()": "/**\n* Returns the default port number.\n* @return default port, which is 0\n*/",
        "org.apache.hadoop.fs.FileSystem:getInitialWorkingDirectory()": "/**\n* Retrieves the initial working directory path.\n* @return Path object representing the initial directory, or null if unavailable\n*/",
        "org.apache.hadoop.fs.FileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file link.\n* @param f the path to the file\n* @return FileStatus of the specified file\n*/",
        "org.apache.hadoop.fs.FileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a symbolic link.\n* @param f the path of the symbolic link\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to the specified length.\n* @param f the file path to truncate\n* @param newLength the new length of the file\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.FileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for the specified source path.\n* @param src the source path to set replication for\n* @param replication the desired replication factor\n* @return true if operation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for the specified path.\n* @param p the file path to modify\n* @param mtime the new modification time\n* @param atime the new access time\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:setVerifyChecksum(boolean)": "/**\n* Sets the verifyChecksum flag, but does not perform any action.\n* @param verifyChecksum flag to enable or disable checksum verification\n*/",
        "org.apache.hadoop.fs.FileSystem:supportsSymlinks()": "/**\n* Checks if the system supports symbolic links.\n* @return false, indicating symlinks are not supported\n*/",
        "org.apache.hadoop.fs.FileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link to a target path.\n* @param target the path to the target file\n* @param link the path where the symlink will be created\n* @param createParent flag to create parent directories if needed\n* @throws IOException for I/O errors during link creation\n*/",
        "org.apache.hadoop.fs.FileSystem:areSymlinksEnabled()": "/**\n* Checks if symbolic links are enabled.\n* @return true if symlinks are enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])": "/**\n* Concatenates source files into a target file.\n* @param trg target file path\n* @param psrcs array of source file paths\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Throws UnsupportedOperationException for removing extended attributes.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n*/",
        "org.apache.hadoop.fs.FileSystem:getChildFileSystems()": "/**\n* Returns an array of child FileSystem instances.\n* @return array of FileSystem objects, or null if none exist\n*/",
        "org.apache.hadoop.fs.FileSystem:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the file system path to query\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Throws UnsupportedOperationException for getXAttr method.\n* @param path the file path\n* @param name the attribute name\n*/",
        "org.apache.hadoop.fs.FileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot; throws exception as operation is unsupported.\n* @param path path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n*/",
        "org.apache.hadoop.fs.FileSystem:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Throws UnsupportedOperationException for ACL status retrieval.\n* @param path the file path for which ACL status is requested\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path, but is unsupported.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Throws exception indicating ACL removal is unsupported.\n* @param path the file path whose ACL is attempted to be removed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Throws exception indicating ACL modification is not supported.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n*/",
        "org.apache.hadoop.fs.FileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Unsupported operation for removing ACL entries from a path.\n* @param path the file system path\n* @param aclSpec list of ACL entries to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Attempts to create a snapshot but always throws an exception.\n* @param path the path for which to create a snapshot\n* @param snapshotName the name of the snapshot to create\n* @throws IOException if the operation is unsupported\n*/",
        "org.apache.hadoop.fs.FileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot by name at the specified path, unsupported operation.\n* @param path the location of the snapshot\n* @param snapshotName the name of the snapshot to delete\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FileSystem:cacheSize()": "/**\n* Returns the size of the cache.\n* @return the number of entries in the cache\n*/",
        "org.apache.hadoop.fs.FileSystem:fixName(java.lang.String)": "/**\n* Converts deprecated filesystem names to new formats.\n* @param name old-format filesystem name\n* @return updated filesystem name in new format\n*/",
        "org.apache.hadoop.fs.FileSystem:getScheme()": "/**\n* Retrieves the scheme of the file system.\n* @return UnsupportedOperationException indicating not implemented\n*/",
        "org.apache.hadoop.fs.FileSystem:getName()": "/**\n* Returns the URI as a string; deprecated in favor of a new method.\n* @return URI string representation\n*/",
        "org.apache.hadoop.fs.FileSystem:debugLogFileSystemClose(java.lang.String,java.lang.String)": "/**\n* Logs debug information when closing the file system.\n* @param methodName name of the method closing the file system\n* @param additionalInfo extra information for debugging\n*/",
        "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle,int)": "/**\n* Opens a file stream for the given path handle with specified buffer size.\n* @param fd file descriptor handle\n* @param bufferSize size of the buffer for the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Creates a PathHandle from FileStatus with optional parameters.\n* @param stat file status information\n* @param opt optional handle options\n* @return PathHandle object (not implemented)\n*/",
        "org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory; deprecated in favor of mkdirs.\n* @param f path of the directory to create\n* @param absolutePermission permissions for the directory\n* @return true if the directory was created, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,int)": "/**\n* Appends data to a file at the given path with specified buffer size.\n* @param f the file path to append data to\n* @param bufferSize the size of the buffer for writing\n* @return FSDataOutputStream for writing to the file\n*/",
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)": "/**\n* Appends data to a file, optionally creating a new block.\n* @param f path of the file to append to\n* @param bufferSize size of the buffer for writing\n* @param progress callback for progress updates\n* @param appendToNewBlock flag to indicate new block creation\n* @return FSDataOutputStream for the appended file\n*/",
        "org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path)": "/**\n* Deletes the specified file.\n* @param f the path of the file to delete\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:cancelDeleteOnExit(org.apache.hadoop.fs.Path)": "/**\n* Cancels scheduled deletion of a file path.\n* @param f the file path to cancel deletion for\n* @return true if canceled, false if not found\n*/",
        "org.apache.hadoop.fs.FileSystem:methodNotSupported()": "/**\n* Throws UnsupportedOperationException for unsupported methods in the class.\n*/",
        "org.apache.hadoop.fs.FileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)": "/**\n* Throws UnsupportedOperationException for listing corrupt file blocks.\n* @param path directory path to check for corrupt files\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists located file statuses in a directory.\n* @param f directory path to list files from\n* @param filter filter to apply on listed files\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)": "/**\n* Lists files in a directory, optionally recursively.\n* @param f directory path to list files from\n* @param recursive true for recursive listing, false otherwise\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:resolveLink(org.apache.hadoop.fs.Path)": "/**\n* Resolves a symbolic link for the given path.\n* @param f the path to resolve\n* @throws IOException if an I/O error occurs or symlinks are unsupported\n*/",
        "org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)": "/**\n* Retrieves the checksum of a file.\n* @param f file path\n* @param length expected file length\n* @return FileChecksum object or null\n*/",
        "org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a file path.\n* @param path file path to set the attribute on\n* @param name name of the extended attribute\n* @param value byte array of the attribute value\n* @param flag set flags for attribute modification\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the file path to query\n* @param names list of attribute names to retrieve\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Lists extended attributes for the given path.\n* @param path the file or directory path\n* @return List of extended attribute names\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws UnsupportedOperationException for unsupported storage policy setting.\n* @param path the path to apply storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Throws UnsupportedOperationException for setting storage policy.\n* @param src source path to set policy for\n* @param policyName name of the storage policy\n*/",
        "org.apache.hadoop.fs.FileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws exception as unsetStoragePolicy is not supported.\n* @param src the path for which to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws an exception indicating unsupported storage policy retrieval.\n* @param src the source path for which the policy is requested\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FileSystem:getAllStoragePolicies()": "/**\n* Unsupported operation to retrieve all storage policies.\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FileSystem:getAllStatistics()": "/**\n* Returns a list of all statistics from the statistics table.\n* @return List of Statistics objects\n*/",
        "org.apache.hadoop.fs.FileSystem:printStatistics()": "/**\n* Prints statistics of registered FileSystem classes to the console.\n* @throws IOException if an I/O error occurs during printing\n*/",
        "org.apache.hadoop.fs.FileSystem:createDataOutputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Creates a builder for FSDataOutputStream.\n* @param fileSystem the target FileSystem\n* @param path the destination Path for output stream\n* @return FSDataOutputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.FileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Initiates local output process.\n* @param fsOutputFile path for output file in file system\n* @param tmpLocalFile path for temporary local file\n* @return temporary local file path\n*/",
        "org.apache.hadoop.fs.FileSystem:setWriteChecksum(boolean)": "/**\n* Sets the writeChecksum flag; currently has no effect.\n* @param writeChecksum flag indicating if checksum should be written\n*/",
        "org.apache.hadoop.fs.FileSystem:getDelegationToken(java.lang.String)": "/**\n* Retrieves a delegation token for the specified renewer.\n* @param renewer the entity requesting the token\n* @return Token object or null if not applicable\n*/",
        "org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FileSystem:processDeleteOnExit()": "",
        "org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FileSystem:getStatistics()": "/**\n* Retrieves statistics mapped by their scheme.\n* @return Map of scheme names to Statistics objects\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "",
        "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a non-recursive FSDataOutputStream for a file.\n* @param f file path, @param permission file permissions, @param overwrite allows overwriting,\n* @param bufferSize size of buffer, @param replication replication factor, @param blockSize block size,\n* @param progress progress tracker\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path)": "/**\n* Checks if the given path is a directory.\n* @param f the file path to check\n* @return true if it's a directory, false if not or if it doesn't exist\n*/",
        "org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path)": "/**\n* Returns the length of the specified file.\n* @param f the file path\n* @return length of the file as a long\n*/",
        "org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])": "/**\n* Lists all file statuses in a single batch.\n* @param f path to the directory\n* @param token optional byte array for pagination (ignored)\n* @return DirectoryEntries containing file statuses\n*/",
        "org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists file statuses matching a filter and adds them to results.\n* @param results collection to store matching FileStatus\n* @param f path to list statuses from\n* @param filter criteria to filter file statuses\n*/",
        "org.apache.hadoop.fs.FileSystem:clearStatistics()": "/**\n* Clears all storage statistics by resetting them.\n*/",
        "org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI)": "/**\n* Canonicalizes URI by setting default port if missing.\n* @param uri the URI to canonicalize\n* @return modified URI with default port, if applicable\n*/",
        "org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the file system status.\n* @param p the path for which the status is requested\n* @return FsStatus object with capacity, used, and remaining space\n*/",
        "org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path)": "/**\n* Creates a BulkDelete operation for the specified path.\n* @param path the path for the bulk delete operation\n* @return BulkDelete instance for the operation\n*/",
        "org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers()": "/**\n* Retrieves additional token issuers from child file systems.\n* @return array of DelegationTokenIssuer objects, or null if none exist\n*/",
        "org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path)": "/**\n* Creates a snapshot at the given path.\n* @param path the path for which to create a snapshot\n* @throws IOException if the operation is unsupported\n*/",
        "org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets namespace and storage space quotas for a given path.\n* @param src path to set quotas for\n* @param namespaceQuota maximum allowed namespace size\n* @param storagespaceQuota maximum allowed storage size\n* @throws IOException if an unsupported operation is attempted\n*/",
        "org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)": "/**\n* Sets storage quota by type; throws exception if method is unsupported.\n* @param src path to the source\n* @param type storage type\n* @param quota quota value in bytes\n*/",
        "org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)": "/**\n* Creates a MultipartUploaderBuilder for the given base path.\n* @param basePath directory path for the uploader\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses in a directory.\n* @param f directory path to list files from\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file checksum for the specified path.\n* @param f file path\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])": "/**\n* Sets an extended attribute for a file path.\n* @param path file path to set the attribute on\n* @param name name of the extended attribute\n* @param value byte array of the attribute value\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options and validates mandatory keys.\n* @param path file path to open\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options and validates mandatory keys.\n* @param pathHandle file path handle\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:close()": "/**\n* Closes the file system, logs info, and removes cached key.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:getStorageStatistics()": "/**\n* Retrieves storage statistics as EmptyStorageStatistics.\n* @return StorageStatistics object representing current storage state\n*/",
        "org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path)": "/**\n* Checks if the given path is a file.\n* @param f the path to check\n* @return true if f is a file, false if not or if not found\n*/",
        "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists file statuses from a path based on a filter.\n* @param f path to list statuses from\n* @param filter criteria to filter file statuses\n* @return array of matching FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)": "/**\n* Lists file statuses for given paths using a filter.\n* @param files array of paths to list statuses from\n* @param filter criteria to filter file statuses\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)": "/**\n* Retrieves or creates Statistics for the given scheme and file system class.\n* @param scheme configuration scheme for statistics\n* @param cls file system class type\n* @return Statistics object associated with the class\n*/",
        "org.apache.hadoop.fs.FileSystem:getCanonicalUri()": "/**\n* Retrieves the canonical URI by normalizing the current URI.\n* @return canonicalized URI or null if original URI is invalid\n*/",
        "org.apache.hadoop.fs.FileSystem:getStatus()": "/**\n* Retrieves the file system status with no specific path.\n* @return FsStatus object with capacity, used, and remaining space\n*/",
        "org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path)": "/**\n* Computes ContentSummary for a file or directory at the given path.\n* @param f Path of the file or directory\n* @return ContentSummary object with aggregated counts and length\n*/",
        "org.apache.hadoop.fs.FileSystem:<init>()": "/**\n* Constructs a FileSystem instance by calling the superclass constructor with null.\n*/",
        "org.apache.hadoop.fs.FileSystem:loadFileSystems()": "/**\n* Loads and registers file systems if not already loaded.\n*/",
        "org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path)": "/**\n* Creates a builder for FSDataOutputStream at the specified path.\n* @param path destination Path for output stream\n* @return FSDataOutputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path)": "/**** Appends a file using the specified path. \n* @param path the destination Path for the output stream \n* @return FSDataOutputStreamBuilder instance for further configuration \n*/",
        "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[])": "/**\n* Lists file statuses for given paths using a default filter.\n* @param files array of paths to list statuses from\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Creates a file or appends to it based on flags and path existence.\n* @param f file path to create or append\n* @param absolutePermission permissions for the file\n* @param flag creation options\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize size of the blocks\n* @param progress progress callback\n* @param checksumOpt checksum options\n* @return FSDataOutputStream for the created file\n* @throws IOException if an error occurs during creation\n*/",
        "org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)": "/**\n* Retrieves quota usage for the specified file or directory.\n* @param f Path of the file or directory\n* @return QuotaUsage object based on content summary\n*/",
        "org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the used length of content at the specified path.\n* @param path location of the file or directory\n* @return length of used content as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Closes all FileSystems for the given UserGroupInformation and logs the operation.\n* @param ugi user group information for access control\n* @throws IOException if closing fails for any FileSystem\n*/",
        "org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Retrieves a PathHandle from FileStatus with optional handle options.\n* @param stat file status information\n* @param opt optional handle options\n* @return PathHandle object based on provided parameters\n*/",
        "org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)": "/**\n* Creates directories including parent directories.\n* @param f path to the directory to create\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a non-recursive FSDataOutputStream for a file.\n* @param f file path, @param overwrite allows overwriting,\n* @param bufferSize size of buffer, @param replication replication factor,\n* @param blockSize block size, @param progress progress tracker\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:closeAll()": "/**\n* Closes all file systems and logs the operation.\n* @throws IOException if multiple I/O errors occur during closing\n*/",
        "org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path)": "/**\n* Resolves a relative Path against the working directory.\n* @param p the Path to be resolved\n* @return the absolute Path\n*/",
        "org.apache.hadoop.fs.FileSystem:getUsed()": "/**\n* Returns the used length of content at the root path.\n* @return length of used content as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory and sets its permissions.\n* @param fs file system to use, @param dir directory path, @param permission permissions to apply\n* @return true if directory was created, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)": "/**\n* Retrieves block locations for a file within specified range.\n* @param file the file to retrieve locations from\n* @param start starting byte position\n* @param len length of the range\n* @return array of BlockLocation or empty if no blocks found\n*/",
        "org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory; checks parent existence and permissions.\n* @param f path of the directory to create\n* @param absolutePermission permissions to set for the new directory\n* @param createParent indicates if parent directories should be created if missing\n*/",
        "org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])": "/**\n* Renames a file or directory with options, checking for existence and type compatibility.\n* @param src source path to rename\n* @param dst destination path\n* @param options rename options, e.g., overwrite\n* @throws IOException if an error occurs during renaming\n*/",
        "org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a specified file and byte range.\n* @param p file path; @param start start position; @param len range length\n* @return array of BlockLocation or throws IOException\n*/",
        "org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path)": "/**** Retrieves file statuses matching the specified path pattern. \n* @param pathPattern the pattern to match file paths \n* @return array of matching FileStatus or null if no matches found */",
        "org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Retrieves file statuses matching the specified path pattern and filter.\n* @param pathPattern pattern for file paths\n* @param filter filter to apply on paths\n* @return array of matching FileStatus or null if no matches found\n*/",
        "org.apache.hadoop.fs.FileSystem:getCanonicalServiceName()": "/**\n* Retrieves canonical service name or builds it if child file systems are absent.\n* @return canonical service name or null if child file systems exist\n*/",
        "org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)": "/**\n* Sets the default URI in the given configuration.\n* @param conf Configuration object to update\n* @param uri URI to set as default\n*/",
        "org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Sets the default URI in the given configuration.\n* @param conf Configuration object to update\n* @param uri URI string to set as default\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file with specified parameters and applied umask.\n* @param f file path to create\n* @param overwrite flag to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @param progress callback for progress updates\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)": "/**\n* Creates a file at the specified path with given parameters.\n* @param f file path to create\n* @param overwrite flag to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves default URI from configuration.\n* @param conf configuration object\n* @return URI object representing the default filesystem\n*/",
        "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path)": "/**\n* Opens a file input stream with a specified buffer size.\n* @param f the path to the file\n* @return FSDataInputStream for the file\n*/",
        "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle)": "/**\n* Opens a file stream using the specified path handle and buffer size.\n* @param fd path handle to the file\n* @return FSDataInputStream for the opened file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path)": "/**\n* Appends data to a file at the given path.\n* @param f file path to append data to\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)": "/**\n* Appends data to a file with specified buffer size.\n* @param f path of the file to append to\n* @param appendToNewBlock flag to indicate new block creation\n* @return FSDataOutputStream for the appended file\n*/",
        "org.apache.hadoop.fs.FileSystem:getDefaultBlockSize()": "/**\n* Returns the default block size in bytes (deprecated).\n* @return default block size, defaulting to 32MB if not configured\n*/",
        "org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the FileSystem class for a given scheme from configuration or service files.\n* @param scheme the scheme to find the FileSystem for\n* @param conf configuration containing FileSystem mappings\n* @return Class of the FileSystem for the scheme\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves FileSystem instance using configuration settings.\n* @param conf configuration object\n* @return FileSystem associated with the default URI\n*/",
        "org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes configuration with URI and retrieves statistics.\n* @param name URI for initialization\n* @param conf configuration object\n*/",
        "org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration)": "/**\n* Creates a new FileSystem instance based on configuration.\n* @param conf configuration object\n* @return FileSystem instance\n*/",
        "org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path)": "/**\n* Validates the given path against the expected filesystem URI.\n* @param path the Path object to check\n*/",
        "org.apache.hadoop.fs.FileSystem:getServerDefaults()": "/**\n* Retrieves file storage defaults; deprecated method.\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)": "/**\n* Returns the default block size for a given path.\n* @param f the file path to get the block size for\n* @return default block size in bytes\n*/",
        "org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates and initializes a FileSystem instance from a URI.\n* @param uri the URI for the FileSystem\n* @param conf configuration settings\n* @return initialized FileSystem object\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a qualified URI Path after validation.\n* @param path the Path object to qualify\n* @return a qualified Path object\n*/",
        "org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves and returns the filesystem path from the given Path object.\n* @param p the Path object to resolve\n* @return Path object representing the resolved filesystem path\n*/",
        "org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file storage defaults based on the provided path.\n* @param p the file path to retrieve defaults for\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a file at the specified path with options to overwrite.\n* @param f file path to create\n* @param overwrite flag to overwrite existing file\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file and returns its output stream.\n* @param f file path to create\n* @param progress callback for progress updates\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)": "/**\n* Creates a file at the specified path with replication factor.\n* @param f file path to create\n* @param replication number of file replicas\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file and returns an output stream.\n* @param f file path to create\n* @param replication number of file replicas\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)": "/**\n* Creates a file at the specified path with given parameters.\n* @param f file path to create\n* @param overwrite flag to overwrite existing file\n* @param bufferSize size of the buffer\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file with specified parameters.\n* @param f file path to create\n* @param overwrite flag to overwrite existing file\n* @param bufferSize size of the buffer\n* @param progress callback for progress updates\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the given path supports the specified capability.\n* @param path the Path to check\n* @param capability the capability to verify\n* @return true if the capability is supported, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Returns the enclosing root Path after qualifying the input Path.\n* @param path the Path to qualify\n* @return qualified root Path\n*/",
        "org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Creates a FSDataInputStreamBuilder for the specified file system and path.\n* @param fileSystem the file system to use\n* @param path the path for initialization\n* @return FSDataInputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)": "/**** Creates a FSDataInputStreamBuilder for the specified file system and path handle. \n* @param fileSystem the file system to use \n* @param pathHandle the path handle for the builder \n* @return FSDataInputStreamBuilder instance \n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path)": "/**\n* Creates a file at the specified path, overwriting if it exists.\n* @param f file path to create\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path)": "/**\n* Creates a new file if it doesn't exist.\n* @param f the file path to create\n* @return true if file was created, false if it already exists\n*/",
        "org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path)": "/**\n* Opens a file and returns a FutureDataInputStreamBuilder.\n* @param path the path of the file to open\n* @return FutureDataInputStreamBuilder for the file\n*/",
        "org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle)": "/**** Opens a file for reading.  \n* @param pathHandle the handle of the file to open  \n* @return FutureDataInputStreamBuilder for file operations  \n*/",
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a file with specified permissions.\n* @param fs file system to create the file in\n* @param file path of the file to create\n* @param permission permissions to set on the file\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileSystem:getHomeDirectory()": "/**** Retrieves the home directory path for the current user. \n* @return qualified Path object for the user's home directory */",
        "org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a file status.\n* @param stat file status to check permissions against\n* @param mode action to verify permissions for\n* @throws AccessControlException if access denied\n* @throws IOException for I/O issues\n*/",
        "org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the qualified Path for the user's trash directory.\n* @param path unused in this method\n* @return qualified Path for the trash directory\n*/",
        "org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean)": "/**\n* Retrieves trash roots for users.\n* @param allUsers flag to include all users' trash\n* @return collection of FileStatus for trash directories\n*/",
        "org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a given path and action mode.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n* @throws AccessControlException if access is denied\n* @throws FileNotFoundException if the file does not exist\n* @throws IOException for I/O issues\n*/",
        "org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves FileSystem instance for given URI and user configuration.\n* @param uri the URI of the file system\n* @param conf configuration settings\n* @param user the username for access control\n* @return FileSystem object\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Creates a new FileSystem instance for the given URI and user configuration.\n* @param uri the URI of the file system\n* @param conf configuration settings\n* @param user the username for authentication\n* @return FileSystem instance\n* @throws IOException if file system creation fails\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)": "/**\n* Adds a FileSystem instance to the cache for testing.\n* @param uri the URI of the file system\n* @param conf configuration settings for the file system\n* @param fs the FileSystem instance to be cached\n*/",
        "org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)": "/**\n* Removes a FileSystem from the cache for testing purposes.\n* @param uri the URI of the FileSystem to remove\n* @param conf configuration settings for the FileSystem\n* @param fs the FileSystem instance to be removed\n*/",
        "org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a FileSystem instance based on URI and configuration.\n* @param uri the URI of the filesystem\n* @param config configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if an error occurs while fetching the filesystem\n*/",
        "org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a FileSystem instance for the given URI or defaults based on configuration.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if filesystem initialization fails\n*/",
        "org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration)": "/**\n* Creates a LocalFileSystem instance from the given configuration.\n* @param conf configuration settings for the filesystem\n* @return LocalFileSystem instance\n* @throws IOException if an error occurs while creating the filesystem\n*/",
        "org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the FileSystem for a given path and configuration.\n* @param absOrFqPath path to the filesystem; must be absolute\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws UnsupportedFileSystemException if the filesystem is unsupported\n* @throws IOException if filesystem retrieval fails\n*/",
        "org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a FileSystem instance using a deprecated name format.\n* @param name old-format filesystem name\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if filesystem initialization fails\n*/",
        "org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a LocalFileSystem instance based on the provided configuration.\n* @param conf configuration settings for the filesystem\n* @return LocalFileSystem instance\n* @throws IOException if filesystem initialization fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)": "/**\n* Copies files from local filesystem to a destination.\n* @param delSrc flag to delete source files after copy\n* @param overwrite flag to overwrite existing files\n* @param srcs array of source Paths\n* @param dst destination Path\n* @throws IOException if file copy fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local filesystem to destination; optionally deletes source.\n* @param delSrc flag to delete source after copy, overwrite flag for overwriting\n* @param src source Path, dst destination Path\n* @throws IOException if copy fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Copies a file to the local filesystem.\n* @param delSrc flag to delete source after copy\n* @param src source Path, @param dst destination Path\n* @param useRawLocalFileSystem flag for raw filesystem usage\n* @throws IOException if copy operation fails\n*/",
        "org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)": "/**\n* Moves files from local filesystem to destination.\n* @param srcs array of source Paths to move\n* @param dst destination Path for moved files\n* @throws IOException if file move fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local filesystem to destination, with optional source deletion.\n* @param delSrc flag to delete source after copy\n* @param src source Path\n* @param dst destination Path\n* @throws IOException if copy fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from HDFS to the local filesystem.\n* @param delSrc flag to delete source after copy\n* @param src source Path\n* @param dst destination Path\n* @throws IOException if copy operation fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local filesystem to destination.\n* @param src source Path to copy from\n* @param dst destination Path to copy to\n* @throws IOException if copy fails\n*/",
        "org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Moves a file from local filesystem to destination.\n* @param src source Path to move from\n* @param dst destination Path to move to\n* @throws IOException if the move operation fails\n*/",
        "org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from HDFS to the local filesystem.\n* @param src source Path to copy from\n* @param dst destination Path to copy to\n* @throws IOException if copy operation fails\n*/",
        "org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Moves a file from HDFS to local filesystem and deletes the source.\n* @param src source Path in HDFS\n* @param dst destination Path in local filesystem\n* @throws IOException if the move operation fails\n*/",
        "org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Completes output by moving a temporary local file to the final destination.\n* @param fsOutputFile destination Path for the output file\n* @param tmpLocalFile source Path of the temporary local file\n* @throws IOException if the move operation fails\n*/"
    },
    "org.apache.hadoop.fs.FileStatus": {
        "org.apache.hadoop.fs.FileStatus:getModificationTime()": "/**\n* Retrieves the modification time of the object.\n* @return long representing the modification timestamp\n*/",
        "org.apache.hadoop.fs.FileStatus:getPath()": "/**\n* Retrieves the current path.\n* @return Path object representing the current path\n*/",
        "org.apache.hadoop.fs.FileStatus:getReplication()": "/**\n* Retrieves the block replication factor.\n* @return short representing the replication factor\n*/",
        "org.apache.hadoop.fs.FileStatus:getBlockSize()": "/**\n* Retrieves the size of the block.\n* @return the size of the block as a long value\n*/",
        "org.apache.hadoop.fs.FileStatus:getAccessTime()": "/**\n* Retrieves the access time value.\n* @return access time in milliseconds\n*/",
        "org.apache.hadoop.fs.FileStatus:getPermission()": "/**\n* Retrieves the file system permissions.\n* @return FsPermission object representing the permissions\n*/",
        "org.apache.hadoop.fs.FileStatus:getOwner()": "/**\n* Retrieves the owner of the object.\n* @return String representing the owner's name\n*/",
        "org.apache.hadoop.fs.FileStatus:getGroup()": "/**\n* Retrieves the group associated with the instance.\n* @return the group as a String\n*/",
        "org.apache.hadoop.fs.FileStatus:attributes(boolean,boolean,boolean,boolean)": "/**\n* Generates a set of attribute flags based on boolean inputs.\n* @param acl, crypt, ec, sn flags to include respective attributes\n* @return Set of AttrFlags including specified attributes\n*/",
        "org.apache.hadoop.fs.FileStatus:isDirectory()": "/**\n* Checks if the current object is a directory.\n* @return true if it is a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:isSymlink()": "/**\n* Checks if the current object is a symbolic link.\n* @return true if symlink is not null, otherwise false\n*/",
        "org.apache.hadoop.fs.FileStatus:getLen()": "/**\n* Retrieves the length value.\n* @return the length as a long\n*/",
        "org.apache.hadoop.fs.FileStatus:hasAcl()": "/**\n* Checks if ACL (Access Control List) is present.\n* @return true if ACL exists, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:isEncrypted()": "/**\n* Checks if the current attributes indicate encryption.\n* @return true if encrypted, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:isErasureCoded()": "/**\n* Checks if the current object is erasure coded.\n* @return true if erasure coded, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:isSnapshotEnabled()": "/**\n* Checks if snapshot feature is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:setSymlink(org.apache.hadoop.fs.Path)": "/**\n* Sets the symlink path.\n* @param p Path to be set as the symlink\n*/",
        "org.apache.hadoop.fs.FileStatus:setOwner(java.lang.String)": "/**\n* Sets the owner name, defaulting to empty if null.\n* @param owner the name of the owner\n*/",
        "org.apache.hadoop.fs.FileStatus:setGroup(java.lang.String)": "/**\n* Sets the group name, defaulting to empty if null.\n* @param group the name of the group\n*/",
        "org.apache.hadoop.fs.FileStatus:setPath(org.apache.hadoop.fs.Path)": "/**\n* Sets the path variable to the given Path object.\n* @param p the Path object to be set\n*/",
        "org.apache.hadoop.fs.FileStatus:validateObject()": "/**\n* Validates the FileStatus object for null path or type.\n* @throws InvalidObjectException if path or type is missing\n*/",
        "org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus)": "",
        "org.apache.hadoop.fs.FileStatus:isDir()": "/**\n* Checks if the current object is a directory.\n* @return true if it is a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:isFile()": "",
        "org.apache.hadoop.fs.FileStatus:getSymlink()": "",
        "org.apache.hadoop.fs.FileStatus:equals(java.lang.Object)": "/**\n* Compares this FileStatus object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.FileStatus:hashCode()": "/**\n* Returns the hash code of the URI based on its path.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object)": "/**\n* Compares this FileStatus object with another for ordering.\n* @param o the object to compare with\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.fs.FileStatus:toString()": "/**\n* Returns a string representation of the object with its attributes.\n* @return formatted string of object properties\n*/",
        "org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput)": "/**\n* Serializes the object to DataOutput.\n* @param out output stream for serialization\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)": "/**\n* Constructs a FileStatus object with specified attributes.\n* @param length file length, isdir indicates if it's a directory,\n* @param block_replication number of block replicas, blocksize size of blocks,\n* @param modification_time last modified time, access_time last access time,\n* @param permission file permissions, owner and group of the file,\n* @param symlink path to symlink, path to the file, attr additional attributes.\n*/",
        "org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions, defaulting if null.\n* @param permission FsPermission object or null for default\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)": "/**\n* Constructs a FileStatus object with file attributes.\n* @param length file length, isdir indicates if directory, block_replication number of replicas, \n*               blocksize size of blocks, modification_time last modified time, \n*               access_time last access time, permission file permissions, \n*               owner and group of the file, symlink path to symlink, path to the file, \n*               hasAcl indicates ACL presence, isEncrypted if file is encrypted, \n*               isErasureCoded if file is erasure coded.\n*/",
        "org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput, populating object attributes from FileStatusProto.\n* @param in DataInput stream to read from\n* @throws IOException if reading fails or size is negative\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Constructs a FileStatus object with file attributes.\n* @param length file length, isdir indicates if directory, block_replication number of replicas,\n* @param blocksize size of blocks, modification_time last modified time, access_time last access time,\n* @param permission file permissions, owner and group of the file, symlink path to symlink,\n* @param path to the file.\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)": "/**\n* Constructs a FileStatus object with file attributes.\n* @param length file length, isdir indicates if directory, block_replication number of replicas,\n* @param blocksize size of blocks, modification_time last modified time, access_time last access time,\n* @param permission file permissions, owner and group of the file, path to the file\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus)": "/**\n* Constructs a FileStatus object by copying attributes from another instance.\n* @param other the FileStatus instance to copy from\n* @throws IOException if an I/O error occurs during construction\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>()": "/**\n* Default constructor for FileStatus initializing with default values.\n*/",
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)": "/**\n* Constructs a FileStatus object with file attributes.\n* @param length file length, isdir indicates if directory, block_replication number of replicas,\n* @param blocksize size of blocks, modification_time last modified time, path to the file\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem$HarMetaData": {
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getMasterIndexTimestamp()": "/**\n* Retrieves the master index timestamp.\n* @return master index timestamp as a long value\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getArchiveIndexTimestamp()": "/**\n* Retrieves the timestamp of the archive index.\n* @return long representing the archive index timestamp\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getPartFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the file status for a given path.\n* @param path the file path to check\n* @return FileStatus of the path, or null if not found\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getVersion()": "/**\n* Retrieves the current version number.\n* @return the version as an integer\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData()": "/**\n* Parses metadata from index files and populates store and archive data.\n* @throws IOException if an I/O error occurs during file operations\n*/"
    },
    "org.apache.hadoop.fs.BlockLocation": {
        "org.apache.hadoop.fs.BlockLocation:getOffset()": "/**\n* Returns the current offset value.\n* @return current offset as a long\n*/",
        "org.apache.hadoop.fs.BlockLocation:getLength()": "/**\n* Returns the length value.\n* @return the current length as a long\n*/",
        "org.apache.hadoop.fs.BlockLocation:setOffset(long)": "/**\n* Sets the offset value.\n* @param offset the new offset value to be set\n*/",
        "org.apache.hadoop.fs.BlockLocation:setLength(long)": "/**\n* Sets the length property of the object.\n* @param length the new length value to set\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(org.apache.hadoop.fs.BlockLocation)": "/**\n* Clones a BlockLocation object.\n* @param that the BlockLocation instance to copy from\n*/",
        "org.apache.hadoop.fs.BlockLocation:toString()": "/**\n* Returns a string representation of the object with offset, length, and hosts.\n* @return formatted string including offset, length, and corruption status if applicable\n*/",
        "org.apache.hadoop.fs.BlockLocation:getHosts()": "/**\n* Retrieves an array of host names.\n* @return array of host names\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)": "/**\n* Initializes BlockLocation with provided parameters or defaults.\n* @param names, hosts, cachedHosts, topologyPaths, storageIds arrays for respective data\n* @param storageTypes array of StorageType, offset and length for data location\n* @param corrupt indicates data integrity status\n*/",
        "org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[])": "/**\n* Sets the hosts array, interning strings if not null.\n* @param hosts array of host strings or null to use empty array\n*/",
        "org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[])": "/**\n* Sets cached hosts; intern strings if provided.\n* @param cachedHosts array of host strings or null to reset\n*/",
        "org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[])": "/**\n* Sets names, interning strings if provided; uses EMPTY_STR_ARRAY if null.\n* @param names array of names to set\n* @throws IOException if an error occurs during string interning\n*/",
        "org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[])": "/**\n* Sets topology paths, interning strings if provided.\n* @param topologyPaths array of topology path strings\n*/",
        "org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[])": "/**\n* Sets storage IDs, interning strings if provided; defaults to empty array if null.\n* @param storageIds array of storage ID strings\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)": "/**\n* Constructs BlockLocation with specified names, hosts, and data attributes.\n* @param names, hosts, cachedHosts, topologyPaths arrays for respective data\n* @param offset starting position of data, @param length size of data, @param corrupt data integrity status\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)": "/**\n* Constructs BlockLocation with specified names, hosts, and data attributes.\n* @param names array of block names, @param hosts array of host addresses, \n* @param topologyPaths array of topology paths, @param offset starting position, \n* @param length size of data, @param corrupt data integrity status\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)": "/**\n* Constructs BlockLocation with specified names, hosts, offset, length, and integrity status.\n* @param names array of block names, @param hosts array of host addresses, \n* @param offset starting position, @param length size of data, @param corrupt data integrity status\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)": "/**\n* Constructs BlockLocation with specified names, hosts, and data attributes.\n* @param names array of block names, @param hosts array of host addresses,\n* @param topologyPaths array of topology paths, @param offset starting position,\n* @param length size of data\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)": "/**\n* Constructs BlockLocation with specified names, hosts, offset, and length.\n* @param names array of block names, @param hosts array of host addresses,\n* @param offset starting position, @param length size of data\n*/",
        "org.apache.hadoop.fs.BlockLocation:<init>()": "/**\n* Default constructor for BlockLocation, initializing with empty arrays and zero values.\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem$HarStatus": {
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getPartName()": "/**\n* Retrieves the name of the part.\n* @return the name of the part as a String\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getStartIndex()": "/**\n* Retrieves the starting index value.\n* @return the start index as a long\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getModificationTime()": "/**\n* Returns the modification time of the object.\n* @return long representing the modification time in milliseconds\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:isDir()": "/**\n* Checks if the current object is a directory.\n* @return true if it is a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getLength()": "/**\n* Retrieves the length value.\n* @return the length as a long\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$1": {
        "org.apache.hadoop.fs.FileSystem$Statistics$1:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Copies data from another Statistics object.\n* @param other the Statistics object to copy from\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData": {
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:add(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData)": "/**\n* Adds statistics from another StatisticsData object to this instance.\n* @param other StatisticsData to aggregate values from\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesRead()": "/**\n* Retrieves the total number of bytes read.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesWritten()": "/**\n* Returns the total number of bytes written.\n* @return long representing bytes written\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getReadOps()": "/**\n* Retrieves the number of read operations performed.\n* @return count of read operations as an integer\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getLargeReadOps()": "/**\n* Retrieves the count of large read operations.\n* @return number of large read operations\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getWriteOps()": "/**\n* Retrieves the number of write operations performed.\n* @return total count of write operations\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadLocalHost()": "/**\n* Returns the total bytes read from the local host.\n* @return total bytes as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfOneOrTwo()": "/**\n* Retrieves the number of bytes read for one or two distances.\n* @return long representing bytes read distance\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfThreeOrFour()": "/**\n* Retrieves the number of bytes read for distance of three or four.\n* @return long representing bytes read distance\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfFiveOrLarger()": "/**\n* Retrieves the count of bytes read with a distance of five or larger.\n* @return long representing the byte count\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadErasureCoded()": "/**\n* Retrieves the number of bytes read in erasure-coded format.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getRemoteReadTimeMS()": "/**\n* Retrieves the remote read time in milliseconds.\n* @return remote read time as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:negate()": "/**\n* Negates all metrics related to bytes and operations.\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:toString()": "/**\n* Returns a string summary of read and write operations statistics.\n* @return formatted string with bytes read/written and operation counts\n*/"
    },
    "org.apache.hadoop.fs.StorageStatistics": {
        "org.apache.hadoop.fs.StorageStatistics:<init>(java.lang.String)": "/**\n* Initializes StorageStatistics with the given name.\n* @param name the name of the storage statistics\n*/",
        "org.apache.hadoop.fs.StorageStatistics:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.fs.StorageStatistics:getScheme()": "/**\n* Retrieves the scheme as a String.\n* @return null, indicating no scheme is available\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics": {
        "org.apache.hadoop.fs.FileSystem$Statistics:getScheme()": "/**\n* Retrieves the current scheme value.\n* @return the scheme as a String\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:<init>(java.lang.String)": "/**\n* Initializes Statistics with a specified scheme.\n* @param scheme configuration scheme for statistics\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getThreadStatistics()": "/**\n* Retrieves thread-specific statistics, creating if absent.\n* @return StatisticsData object for the current thread\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getAllThreadLocalDataSize()": "/**\n* Returns the size of all thread-local data.\n* @return number of entries in allData collection\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator)": "",
        "org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Initializes Statistics from another Statistics object.\n* @param other the Statistics object to copy from\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead()": "/**\n* Calculates total bytes read from statistics data.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten()": "/**\n* Calculates total bytes written by aggregating statistics.\n* @return total bytes written as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getReadOps()": "/**\n* Calculates total read operations from Statistics data.\n* @return total number of read operations\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps()": "/**\n* Calculates total large read operations.\n* @return sum of large read operations from statistics data\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps()": "/**\n* Retrieves the total write operations count.\n* @return total number of write operations\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime()": "/**\n* Calculates total remote read time in milliseconds.\n* @return aggregated remote read time from StatisticsData\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getData()": "/**\n* Aggregates and returns StatisticsData from StatisticsAggregator.\n* @return aggregated StatisticsData instance\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded()": "/**\n* Calculates total bytes read from erasure-coded data.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:toString()": "/**\n* Returns a string representation of aggregated statistics.\n* @return aggregated statistics as a String\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:reset()": "/**\n* Resets statistics by aggregating and negating total data.\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int)": "/**\n* Retrieves bytes read based on specified distance.\n* @param distance distance category (0-4 or 5+)\n* @return total bytes read as a long value\n*/"
    },
    "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator": {
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:<init>(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData)": "/**\n* Initializes LongStatisticIterator with provided StatisticsData.\n* @param data the StatisticsData to iterate over\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:hasNext()": "/**\n* Checks if there are more keys to iterate over.\n* @return true if more keys exist, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:remove()": "/**\n* Throws UnsupportedOperationException to indicate removal is not allowed.\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next()": "/**\n* Retrieves the next LongStatistic using a key from KEYS array.\n* @return LongStatistic with key and corresponding value\n*/"
    },
    "org.apache.hadoop.fs.FileSystemStorageStatistics": {
        "org.apache.hadoop.fs.FileSystemStorageStatistics:isTracked(java.lang.String)": "/**\n* Checks if the specified key is in the tracked keys.\n* @param key the key to check for tracking\n* @return true if the key is tracked, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)": "",
        "org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme()": "/**\n* Retrieves the current scheme value.\n* @return scheme as a String from stats\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Initializes FileSystemStorageStatistics with name and validated statistics.\n* @param name identifier for the statistics\n* @param stats FileSystem.Statistics object, must not be null or contain null data\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics()": "/**\n* Retrieves an iterator for long statistics.\n* @return LongStatisticIterator for accessing long statistics\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String)": "/**\n* Retrieves a Long value associated with the given key.\n* @param key the identifier for the data to fetch\n* @return Long value or null if not found\n*/",
        "org.apache.hadoop.fs.FileSystemStorageStatistics:reset()": "/**\n* Resets statistics by aggregating and negating total data.\n*/"
    },
    "org.apache.hadoop.fs.AbstractFileSystem": {
        "org.apache.hadoop.fs.AbstractFileSystem:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Throws exception for unsupported listing of extended attributes.\n* @param path directory path to list attributes for\n* @throws IOException if an I/O error occurs or operation is unsupported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws UnsupportedOperationException for getStoragePolicy method.\n* @param src path of the source file\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a specified path.\n* @param path the target file path\n* @param name the name of the attribute\n* @param value the value to set for the attribute\n* @param flag additional options for setting the attribute\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Throws an exception indicating ACL removal is unsupported.\n* @param path the path for which ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the file link status for a given path.\n* @param f the file path to check status for\n* @return FileStatus object representing the file's status\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getUri()": "/**\n* Retrieves the URI instance.\n* @return myUri the current URI object\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Throws UnsupportedOperationException for ACL status retrieval.\n* @param path the file path for which ACL status is requested\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Unsupported operation for retrieving extended attributes.\n* @param path the file path to get attributes from\n* @param names list of attribute names to retrieve\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Throws exception indicating removal of extended attribute is unsupported.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getFsStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves filesystem status for the specified path.\n* @param f the path to check the filesystem status\n* @return FsStatus of the specified path\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot by directory and name, unsupported operation.\n* @param snapshotDir directory of the snapshot\n* @param snapshotName name of the snapshot to delete\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Throws UnsupportedOperationException for truncate functionality.\n* @param f path to the file to truncate\n* @param newLength desired new length of the file\n* @return always throws an exception\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a given path; always throws UnsupportedOperationException.\n* @param path the file system path to set the policy for\n* @param policyName the name of the storage policy to apply\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Throws exception for unsupported getXAttr operation.\n* @param path the file system path\n* @param name the attribute name to retrieve\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server default settings.\n* @param f file path (unused in this implementation)\n* @return FsServerDefaults object\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses for a given path.\n* @param f the path to list file statuses\n* @return an iterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path; unsupported operation.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to set\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)": "/**\n* Returns an iterator for file statuses in the specified path.\n* @param f the path to list file statuses\n* @return RemoteIterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Throws exception for unsupported getXAttrs operation on the given path.\n* @param path the file path to retrieve extended attributes\n* @throws IOException if the operation is unsupported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Throws an exception indicating snapshot creation is unsupported.\n* @param path the path for which to create a snapshot\n* @param snapshotName the name of the snapshot\n* @throws IOException if snapshot creation is attempted\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:isValidName(java.lang.String)": "/**\n* Validates the given name by checking for prohibited patterns.\n* @param src the name to validate\n* @return true if valid, false if contains prohibited elements\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Throws exception indicating ACL modification is unsupported.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Throws exception as ACL entry removal is not supported for the class.\n* @param path the path from which to remove ACL entries\n* @param aclSpec list of ACL entries to be removed\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Throws exception indicating removal of default ACL is unsupported.\n* @param path the path for which default ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot; unsupported operation throws an exception.\n* @param path the file path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws exception indicating storage policy satisfaction is unsupported.\n* @param path the file system path to check\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws exception as unsetStoragePolicy is unsupported for this class.\n* @param src the path to the storage policy\n* @throws UnsupportedOperationException if invoked\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getAllStoragePolicies()": "/**\n* Throws exception as getAllStoragePolicies is not supported.\n* @throws UnsupportedOperationException if called\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:supportsSymlinks()": "/**\n* Checks if the system supports symbolic links.\n* @return false indicating symlinks are not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Attempts to create a symbolic link at the specified path.\n* @param target path to the target file\n* @param link path where the symlink will be created\n* @param createParent whether to create parent directories if they don't exist\n* @throws IOException if symlinks are not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Throws an error indicating method must be overridden for link resolution.\n* @param f the path to resolve\n* @throws IOException if link resolution fails\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getDelegationTokens(java.lang.String)": "/**\n* Retrieves delegation tokens for the specified renewer.\n* @param renewer the identifier for the token renewer\n* @return an empty list of tokens\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getInitialWorkingDirectory()": "/**\n* Retrieves the initial working directory path.\n* @return Path object representing the initial working directory, or null if not set\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:msync()": "/**\n* Throws UnsupportedOperationException for unsupported msync method.\n* @throws UnsupportedOperationException if called\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:printStatistics()": "/**\n* Prints statistics for each FileSystem in the STATISTICS_TABLE.\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:newInstance(java.lang.Class,java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a new instance of the specified class using URI and Configuration.\n* @param theClass the class type to instantiate\n* @param uri the URI to pass to the constructor\n* @param conf the Configuration object for the constructor\n* @return a new instance of the specified class\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getBaseUri(java.net.URI)": "/**\n* Constructs base URI from the provided URI.\n* @param uri the original URI\n* @return base URI with scheme and authority\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)": "/**\n* Throws an exception indicating unsupported operation for listing corrupt file blocks.\n* @param path the file path to check for corrupt blocks\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:hashCode()": "/**\n* Returns the hash code of the object's URI.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:equals(java.lang.Object)": "/**\n* Compares this AbstractFileSystem with another for equality.\n* @param other object to compare\n* @return true if equal based on myUri, false otherwise\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:methodNotSupported()": "/**\n* Throws UnsupportedOperationException for unsupported methods in the class.\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getStatistics()": "/**\n* Retrieves the current statistics.\n* @return Statistics object containing statistical data\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])": "/**\n* Sets an extended attribute for a specified path.\n* @param path the target file path\n* @param name the name of the attribute\n* @param value the value to set for the attribute\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)": "/**\n* Validates URI scheme against supported scheme.\n* @param uri URI to check\n* @param supportedScheme expected scheme for validation\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path)": "/**\n* Opens a file stream with specified path and default buffer size.\n* @param f file path to open\n* @return FSDataInputStream for the file\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options and validates mandatory keys.\n* @param path file path to open\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI)": "/**\n* Retrieves or creates Statistics for a given URI.\n* @param uri the URI to extract the scheme and base URI\n* @return Statistics object associated with the base URI\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)": "/**\n* Creates a MultipartUploaderBuilder for the given base path.\n* @param basePath directory path for uploads\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)": "/**\n* Constructs a URI based on scheme, authority, and port requirements.\n* @param uri input URI to validate and modify\n* @param supportedScheme expected scheme for the URI\n* @param authorityNeeded indicates if authority is required\n* @param defaultPort fallback port if none is specified\n* @return modified URI based on input parameters\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics()": "/**\n* Retrieves a map of URIs to Statistics objects.\n* @return Map containing URI-Statistics pairs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:clearStatistics()": "/**\n* Clears all statistics by resetting each entry in the STATISTICS_TABLE.\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)": "/**\n* Initializes AbstractFileSystem with validated URI and statistics.\n* @param uri input URI to process\n* @param supportedScheme expected URI scheme\n* @param authorityNeeded indicates if authority is required\n* @param defaultPort fallback port if none is specified\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path)": "/**\n* Validates the given path's URI against the current object's URI.\n* @param path the Path object to check for validity\n* @throws InvalidPathException if the path is invalid based on scheme, authority, or port\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the URI path from a Path object.\n* @param p the Path to extract the URI from\n* @return the URI path as a String\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves and returns the path of a given Path object.\n* @param p the Path to resolve\n* @return Path object of the resolved file status\n* @throws exceptions related to file access and resolution\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])": "/**\n* Creates an FSDataOutputStream for the specified path with options.\n* @param f path to create the output stream for\n* @param createFlag flags for creation behavior\n* @param opts additional creation options\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a qualified URI Path.\n* @param path the Path to be qualified\n* @return a qualified Path object\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/***************************************\n* Checks if the specified path supports a capability.\n* @param path the Path object to check\n* @param capability the capability string to verify\n* @return true if capability is supported, false otherwise\n***************************************/",
        "org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Returns the qualified enclosing root Path for the given Path.\n* @param path the Path to qualify\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Renames a file or directory, handling overwrite and symlink cases.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @param overwrite flag to allow overwriting existing destination\n* @throws various exceptions for access and file state issues\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])": "/**\n* Renames a file or directory, optionally allowing overwrite.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @param options optional rename options\n* @throws various exceptions for access and file state issues\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName()": "/**\n* Retrieves the canonical service name using current URI and default port.\n* @return formatted service name or null if authority is absent\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates an AbstractFileSystem instance for the given URI and configuration.\n* @param uri the URI of the file system\n* @param conf the Hadoop configuration\n* @return AbstractFileSystem instance\n* @throws UnsupportedFileSystemException if the file system is unsupported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves AbstractFileSystem for the given URI and configuration.\n* @param uri the URI of the file system\n* @param conf the Hadoop configuration\n* @return AbstractFileSystem instance\n* @throws UnsupportedFileSystemException if the file system is unsupported\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory()": "/**\n* Retrieves the home directory path for the current user.\n* @return Path object representing the user's home directory\n*/",
        "org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a specified path.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n* @throws AccessControlException if access denied\n* @throws IOException for I/O issues\n*/"
    },
    "org.apache.hadoop.fs.PathIOException": {
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a PathIOException with a specified path and error message.\n* @param path the file path causing the exception\n* @param error the error message to be displayed\n*/",
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathIOException with specified path, error message, and cause.\n* @param path the file path related to the error\n* @param error the error message\n* @param cause the underlying cause of the exception\n*/",
        "org.apache.hadoop.fs.PathIOException:setOperation(java.lang.String)": "/**\n* Sets the operation type.\n* @param operation the operation to be set\n*/",
        "org.apache.hadoop.fs.PathIOException:setTargetPath(java.lang.String)": "/**\n* Sets the target path for the operation.\n* @param targetPath the path to be set as the target\n*/",
        "org.apache.hadoop.fs.PathIOException:formatPath(java.lang.String)": "/**\n* Formats the given path by surrounding it with backticks and single quotes.\n* @param path the path to format\n* @return formatted path as a String\n*/",
        "org.apache.hadoop.fs.PathIOException:withFullyQualifiedPath(java.lang.String)": "/**\n* Sets the fully qualified path and returns the updated PathIOException instance.\n* @param fqPath the fully qualified path to set\n* @return updated PathIOException instance\n*/",
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String)": "/**\n* Constructs a PathIOException with a specified path.\n* @param path the file path causing the exception\n*/",
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathIOException with specified path and cause.\n* @param path the file path related to the error\n* @param cause the underlying cause of the exception\n*/",
        "org.apache.hadoop.fs.PathIOException:getMessage()": "/**\n* Constructs a detailed message string for the operation.\n* @return concatenated message with formatted paths and cause details\n*/",
        "org.apache.hadoop.fs.PathIOException:getPath()": "/**** Returns a Path object initialized with the current path string. */",
        "org.apache.hadoop.fs.PathIOException:getTargetPath()": "/**\n* Returns a Path object from targetPath or null if targetPath is not set.\n* @return Path object or null if targetPath is null\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder": {
        "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:getThisBuilder()": "/**\n* Returns the current instance of FileSystemDataOutputStreamBuilder.\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build()": "/**\n* Builds and returns an FSDataOutputStream based on specified flags and path.\n* @return FSDataOutputStream for file operations\n* @throws IOException if creation or appending fails\n*/",
        "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Constructs a FileSystemDataOutputStreamBuilder with specified file system and path.\n* @param fileSystem the file system to use\n* @param p the path for the output stream\n*/"
    },
    "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder": {
        "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:getThisBuilder()": "/**\n* Returns the current instance of FCDataOutputStreamBuilder.\n* @return this instance of FCDataOutputStreamBuilder\n*/",
        "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)": "/**\n* Constructs FCDataOutputStreamBuilder with file context and path.\n* @param fc FileContext for file operations\n* @param p Path to the file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build()": "/**\n* Builds and creates a file with specified options.\n* @return FSDataOutputStream for the created file\n*/"
    },
    "org.apache.hadoop.fs.protocolPB.PBHelper": {
        "org.apache.hadoop.fs.protocolPB.PBHelper:<init>()": "/**\n* Private constructor to prevent instantiation of PBHelper class.\n*/",
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission)": "",
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus)": "/**\n* Converts FileStatus to FileStatusProto.\n* @param stat FileStatus object to convert\n* @return FileStatusProto representation of the FileStatus\n*/",
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto)": "/**\n* Converts FsPermissionProto to FsPermission.\n* @param proto FsPermissionProto containing permission data\n* @return FsPermission object initialized with permissions\n*/",
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto)": "/**\n* Converts FileStatusProto to FileStatus.\n* @param proto file status protocol object\n* @return corresponding FileStatus object\n*/"
    },
    "org.apache.hadoop.fs.permission.FsPermission": {
        "org.apache.hadoop.fs.permission.FsPermission:toShort()": "/**\n* Converts internal state to a short value based on actions and sticky bit.\n* @return short representation of the combined actions\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getUserAction()": "/**\n* Retrieves the user's action.\n* @return FsAction representing the user's current action\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getGroupAction()": "/**\n* Retrieves the group action permission.\n* @return FsAction representing the group's permission\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getOtherAction()": "/**\n* Retrieves the 'otheraction' permission setting.\n* @return FsAction representing the other action\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getAclBit()": "/**\n* Retrieves the ACL bit; deprecated for subclasses to override instead.\n* @return false, indicating no ACL bit support\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getEncryptedBit()": "/**\n* Returns false; method is deprecated and should not be used.\n* @return always false\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getErasureCodedBit()": "/**\n* Returns false; method is deprecated and should not be used.\n* @return always false\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getStickyBit()": "/**\n* Retrieves the value of the sticky bit.\n* @return true if sticky bit is set, false otherwise\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:<init>()": "/**\n* Private constructor for FsPermission class; prevents instantiation.\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:set(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)": "/**\n* Sets user, group, and other actions along with the sticky bit flag.\n* @param u user action\n* @param g group action\n* @param o other action\n* @param sb sticky bit status\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Copies permissions from another FsPermission object.\n* @param other the FsPermission to copy from\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:equals(java.lang.Object)": "/**\n* Compares this FsPermission with another object for equality.\n* @param obj the object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:validateObject()": "/**\n* Validates the object for required actions and sticky bit.\n* @throws InvalidObjectException if any action or sticky bit is null\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getUnmasked()": "/**\n* Returns unmasked file system permissions.\n* @return FsPermission object representing unmasked permissions\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getMasked()": "/**\n* Retrieves the masked file system permissions.\n* @return FsPermission object representing masked permissions\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput)": "/**\n* Writes a short representation of the object's state to output.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:toExtendedShort()": "/**\n* Returns a short value; deprecated in favor of toShort().\n* @return short representation of the internal state\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:toOctal()": "/**\n* Converts internal state to octal representation.\n* @return short octal value derived from internal state\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:hashCode()": "/**\n* Returns the hash code based on the internal state.\n* @return int hash code derived from the short representation\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:toString()": "",
        "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)": "/**\n* Constructs FsPermission with user, group, other actions, and sticky bit.\n* @param u user action, g group action, o other action, sb sticky bit status\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:fromShort(short)": "/**\n* Sets FsAction values based on a short input.\n* @param n short value encoding user, group, other actions and sticky bit\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Constructs FsPermission with user, group, and other actions.\n* @param u user action, g group action, o other action\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:<init>(short)": "/**\n* Initializes FsPermission using a mode value.\n* @param mode short encoding for permissions and sticky bit\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput)": "/**\n* Reads fields from input and sets FsAction values.\n* @param in DataInput source for reading short values\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput)": "/**\n* Reads FsPermission from DataInput stream.\n* @param in input stream to read from\n* @return FsPermission object created from stream data\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Applies a umask to the current FsPermission.\n* @param umask FsPermission to apply as a mask\n* @return new FsPermission with applied umask\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:<init>(int)": "/**\n* Constructs FsPermission from a mode value.\n* @param mode integer encoding for permissions and sticky bit\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getDefault()": "/**\n* Returns the default file system permissions.\n* @return FsPermission object with default permissions set to 00777\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getDirDefault()": "/**\n* Returns default directory permissions as FsPermission.\n* @return FsPermission object with default mode 00777\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getFileDefault()": "/****\n* Retrieves default file permissions.\n* @return FsPermission object with default permissions set to 0666\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault()": "/**\n* Retrieves the default cache pool permissions.\n* @return FsPermission object with default permissions\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String)": "/**\n* Converts a Unix symbolic permission string to an FsPermission object.\n* @param unixSymbolicPermission Unix permission string\n* @return FsPermission object or null if input is null\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String)": "/**\n* Constructs FsPermission from a mode string.\n* @param mode string representation of permissions\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:createImmutable(short)": "/**\n* Creates an immutable file system permission.\n* @param permission short encoding for permissions\n* @return ImmutableFsPermission instance\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets the user mask in the configuration.\n* @param conf configuration object to update\n* @param umask permissions to format as octal\n*/",
        "org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the umask from configuration.\n* @param conf configuration object\n* @return FsPermission with the parsed umask value\n*/"
    },
    "org.apache.hadoop.util.StringInterner": {
        "org.apache.hadoop.util.StringInterner:weakIntern(java.lang.String)": "/**\n* Interns the given string for memory optimization.\n* @param sample string to intern, or null\n* @return interned string or null if input is null\n*/",
        "org.apache.hadoop.util.StringInterner:strongIntern(java.lang.String)": "/**\n* Interns a string using a strong reference.\n* @param sample the string to intern, or null\n* @return interned string or null if input is null\n*/",
        "org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[])": ""
    },
    "org.apache.hadoop.fs.FsServerDefaults": {
        "org.apache.hadoop.fs.FsServerDefaults:<init>()": "/**\n* Constructor for FsServerDefaults class.\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte,boolean)": "/**\n* Initializes FsServerDefaults with configuration parameters for file storage.\n* @param blockSize size of blocks in bytes\n* @param bytesPerChecksum bytes per checksum\n* @param writePacketSize size of write packets\n* @param replication number of data replicas\n* @param fileBufferSize size of file buffer\n* @param encryptDataTransfer flag for data encryption\n* @param trashInterval time before files are moved to trash\n* @param checksumType type of data checksum\n* @param keyProviderUri URI for key provider\n* @param storagepolicy storage policy identifier\n* @param snapshotTrashRootEnabled flag for snapshot trash root\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:getTrashInterval()": "/**\n* Retrieves the current trash interval value.\n* @return long representing the trash interval in milliseconds\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:getFileBufferSize()": "/**\n* Retrieves the current file buffer size.\n* @return the size of the file buffer in bytes\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:getBytesPerChecksum()": "/**\n* Retrieves the number of bytes per checksum.\n* @return bytesPerChecksum value\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:getReplication()": "/**\n* Returns the current replication value.\n* @return short representing the replication level\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:getBlockSize()": "/**\n* Retrieves the size of the block.\n* @return long representing the block size\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:getChecksumType()": "/**\n* Retrieves the type of checksum.\n* @return DataChecksum.Type representing the checksum type\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)": "",
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)": "/**\n* Initializes FsServerDefaults with specified configuration parameters.\n* @param blockSize size of each data block\n* @param bytesPerChecksum size of bytes per checksum\n* @param writePacketSize size of write packets\n* @param replication number of data replicas\n* @param fileBufferSize size of the file buffer\n* @param encryptDataTransfer flag for data encryption\n* @param trashInterval interval for trash management\n* @param checksumType type of data checksum\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)": "/**\n* Initializes FsServerDefaults with configuration parameters for file storage.\n* @param blockSize size of the data blocks\n* @param bytesPerChecksum size of bytes per checksum\n* @param writePacketSize size of write packets\n* @param replication number of data replicas\n* @param fileBufferSize size of the buffer for file operations\n* @param encryptDataTransfer flag for data encryption during transfer\n* @param trashInterval time interval for trash management\n* @param checksumType type of data checksum\n* @param keyProviderUri URI for the key provider\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput)": "/**\n* Writes object data to output stream.\n* @param out output stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream into class attributes.\n* @param in input stream to read from\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.FsServerDefaults$1": {
        "org.apache.hadoop.fs.FsServerDefaults$1:<init>()": "/**\n* Constructor for FsServerDefaults initializing default server settings.\n*/"
    },
    "org.apache.hadoop.io.WritableFactories": {
        "org.apache.hadoop.io.WritableFactories:setFactory(java.lang.Class,org.apache.hadoop.io.WritableFactory)": "/**\n* Associates a WritableFactory with a specific class.\n* @param c class to associate with the factory\n* @param factory WritableFactory instance to set\n*/",
        "org.apache.hadoop.io.WritableFactories:<init>()": "/**\n* Private constructor for singleton instance of WritableFactories.\n*/",
        "org.apache.hadoop.io.WritableFactories:getFactory(java.lang.Class)": "/**\n* Retrieves the WritableFactory associated with the given class.\n* @param c the class for which to get the factory\n* @return WritableFactory or null if not found\n*/",
        "org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a new Writable instance with the given configuration.\n* @param c class type of Writable to instantiate\n* @param conf configuration for the Writable instance\n* @return new Writable instance or null if factory not found\n*/",
        "org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class)": "/**\n* Creates a new Writable instance of the specified class.\n* @param c class type of Writable to instantiate\n* @return new Writable instance or null if factory not found\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts": {
        "org.apache.hadoop.fs.Options$CreateOpts:<init>()": "/**\n* Private constructor for CreateOpts class to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts:getOpt(java.lang.Class,org.apache.hadoop.fs.Options$CreateOpts[])": "/**\n* Retrieves the first matching option of specified type from varargs.\n* @param clazz the class type to match\n* @param opts variable number of CreateOpts\n* @return matched option or null if not found\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission)": "",
        "org.apache.hadoop.fs.Options$CreateOpts:blockSize(long)": "",
        "org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int)": "",
        "org.apache.hadoop.fs.Options$CreateOpts:repFac(short)": "/**\n* Creates a ReplicationFactor instance from a short value.\n* @param rf positive replication factor\n* @return ReplicationFactor object\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short)": "/**\n* Creates a BytesPerChecksum instance with the specified CRC value.\n* @param crc positive bytes per checksum value\n* @return BytesPerChecksum object\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Creates a ChecksumParam using the specified ChecksumOpt.\n* @param csumOpt configuration for checksum options\n* @return new ChecksumParam instance\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable)": "/**\n* Creates a Progress instance using the provided Progressable object.\n* @param prog the Progressable object for tracking progress\n* @return a new Progress object\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts:createParent()": "",
        "org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent()": "/**\n* Creates a CreateParent instance without creating a parent.\n* @return CreateParent object with createPar set to false\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$BlockSize": {
        "org.apache.hadoop.fs.Options$CreateOpts$BlockSize:<init>(long)": "/**\n* Constructs a BlockSize with a positive value.\n* @param bs the size of the block, must be greater than 0\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$BlockSize:getValue()": "/**\n* Retrieves the block size value.\n* @return the current block size as a long\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$BufferSize": {
        "org.apache.hadoop.fs.Options$CreateOpts$BufferSize:<init>(int)": "/**\n* Constructs a BufferSize with a positive size.\n* @param bs positive buffer size\n* @throws IllegalArgumentException if bs is less than or equal to 0\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$BufferSize:getValue()": "/**\n* Retrieves the current buffer size.\n* @return the size of the buffer as an integer\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor": {
        "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor:<init>(short)": "/**\n* Constructs a ReplicationFactor with a positive replication value.\n* @param rf the replication factor, must be greater than 0\n* @throws IllegalArgumentException if rf is not positive\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor:getValue()": "/**\n* Returns the current replication value.\n* @return short representation of the replication value\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum": {
        "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum:<init>(short)": "/**\n* Initializes BytesPerChecksum with a positive value.\n* @param bpc bytes per checksum, must be greater than 0\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum:getValue()": "/**\n* Returns the value of bytes per checksum.\n* @return bytesPerChecksum integer value\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam": {
        "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam:<init>(org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Initializes ChecksumParam with the specified ChecksumOpt.\n* @param csumOpt option for checksum configuration\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam:getValue()": "/**\n* Retrieves the current checksum option.\n* @return ChecksumOpt object representing the checksum option\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$Progress": {
        "org.apache.hadoop.fs.Options$CreateOpts$Progress:<init>(org.apache.hadoop.util.Progressable)": "/**\n* Initializes Progress with a non-null Progressable instance.\n* @param prog the Progressable object to be associated\n* @throws IllegalArgumentException if prog is null\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$Progress:getValue()": "/**\n* Retrieves the current progress value.\n* @return Progressable object representing the progress\n*/"
    },
    "org.apache.hadoop.fs.Options$CreateOpts$CreateParent": {
        "org.apache.hadoop.fs.Options$CreateOpts$CreateParent:<init>(boolean)": "/**\n* Initializes the CreateParent with the specified flag.\n* @param createPar indicates whether to create a parent\n*/",
        "org.apache.hadoop.fs.Options$CreateOpts$CreateParent:getValue()": "/**\n* Retrieves the value of createParent.\n* @return true if createParent is set, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.Options$Rename": {
        "org.apache.hadoop.fs.Options$Rename:valueOf(byte)": "/**\n* Retrieves Rename enum by byte code.\n* @param code byte representing enum index\n* @return Rename enum or null if code is out of bounds\n*/"
    },
    "org.apache.hadoop.fs.AvroFSInput": {
        "org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FSDataInputStream,long)": "/**\n* Initializes AvroFSInput with input stream and length.\n* @param in input stream for reading data\n* @param len length of the data to read\n*/",
        "org.apache.hadoop.fs.AvroFSInput:read(byte[],int,int)": "/**\n* Reads bytes from the input stream into the specified byte array.\n* @param b byte array to store read bytes\n* @param off offset in the array to start storing bytes\n* @param len maximum number of bytes to read\n* @return number of bytes read, or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.AvroFSInput:close()": "/**\n* Closes the underlying stream, releasing any associated resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.AvroFSInput:seek(long)": "/**\n* Seeks to a specified position in the input stream.\n* @param p desired position to seek to in the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AvroFSInput:tell()": "/**\n* Returns the current position of the input stream.\n* @return current position as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)": "/**\n* Initializes AvroFSInput with file context and path.\n* @param fc FileContext for file operations\n* @param p Path to the file\n* @throws IOException if file operations fail\n*/"
    },
    "org.apache.hadoop.fs.FileContext": {
        "org.apache.hadoop.fs.FileContext:openFile(org.apache.hadoop.fs.Path)": "/**\n* Opens a file and returns a data input stream builder.\n* @param path the file path to open\n* @return FSDataInputStreamBuilder for the specified file\n*/",
        "org.apache.hadoop.fs.FileContext:util()": "/**\n* Retrieves the Util instance.\n* @return Util object\n*/",
        "org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path)": "/**\n* Creates an FSDataOutputStream for the specified file path.\n* @param f the file path to create the output stream for\n* @return FSDataOutputStreamBuilder for further configuration\n*/",
        "org.apache.hadoop.fs.FileContext:getTracer()": "/**\n* Retrieves the current Tracer instance.\n* @return Tracer object representing the current tracer\n*/",
        "org.apache.hadoop.fs.FileContext:resolve(org.apache.hadoop.fs.Path)": "/**\n* Resolves a file path in the filesystem.\n* @param f the path to resolve\n* @return the resolved Path object\n* @throws FileNotFoundException if the file does not exist\n* @throws UnresolvedLinkException if the path contains unresolved links\n* @throws AccessControlException if access is denied\n* @throws IOException for other I/O errors\n*/",
        "org.apache.hadoop.fs.FileContext:getWorkingDirectory()": "/**\n* Returns the current working directory path.\n* @return Path object representing the working directory\n*/",
        "org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves AbstractFileSystem for given URI and configuration under specified user permissions.\n* @param user user permissions context\n* @param uri the URI of the file system\n* @param conf configuration settings\n* @return AbstractFileSystem instance\n* @throws UnsupportedFileSystemException if the file system is unsupported\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:getAllStoragePolicies()": "/**\n* Retrieves all block storage policies.\n* @return Collection of BlockStoragePolicySpi objects\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves and returns the specified file path.\n* @param f the path to resolve\n* @return resolved Path object\n* @throws exceptions related to file access and resolution\n*/",
        "org.apache.hadoop.fs.FileContext:msync()": "/**\n* Invokes msync on the default file system.\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if msync is unsupported\n*/",
        "org.apache.hadoop.fs.FileContext:printStatistics()": "/**\n* Invokes printStatistics on AbstractFileSystem to display system statistics.\n*/",
        "org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Validates file system dependencies for source and destination paths.\n* @param qualSrc source path, @param qualDst destination path\n* @throws IOException if paths are invalid for copying\n*/",
        "org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI)": "/**\n* Retrieves Statistics for the specified URI.\n* @param uri the URI to extract statistics from\n* @return Statistics object associated with the URI\n*/",
        "org.apache.hadoop.fs.FileContext:getAllStatistics()": "/**\n* Retrieves a map of URIs to Statistics objects.\n* @return Map containing URI-Statistics pairs\n*/",
        "org.apache.hadoop.fs.FileContext:clearStatistics()": "/**\n* Clears all statistics by resetting each entry in the STATISTICS_TABLE.\n*/",
        "org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path)": "/**\n* Fixes relative path by resolving against working directory if not absolute.\n* @param p the Path to check and potentially resolve\n* @return resolved Path object\n*/",
        "org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path)": "/**** Retrieves the AbstractFileSystem for a given absolute or fully qualified Path. \n* @param absOrFqPath the Path object to resolve\n* @return AbstractFileSystem for the path\n* @throws UnsupportedFileSystemException if the file system is unsupported\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a qualified URI Path based on the default filesystem.\n* @param path the Path to be qualified\n* @return a qualified Path object\n*/",
        "org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory at the given path.\n* @param f the Path to delete\n* @param recursive true to delete recursively, false otherwise\n* @return true if deletion was successful\n*/",
        "org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path)": "/**** Opens a file for reading and resolves relative paths. \n* @param f the Path to the file\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file stream with a specified buffer size.\n* @param f the Path of the file to open\n* @param bufferSize size of the buffer for the input stream\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to the specified length.\n* @param f the Path of the file to truncate\n* @param newLength the new length of the file\n* @return true if truncation is successful\n*/",
        "org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified file.\n* @param f the file path to update\n* @param replication the new replication factor\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions for the specified path.\n* @param f the Path to set permissions on\n* @param permission the new FsPermission to apply\n*/",
        "org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner of a file or directory.\n* @param f the Path to the file or directory\n* @param username new owner's username\n* @param groupname new owner's group name\n*/",
        "org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a file.\n* @param f file path, mtime modification time, atime access time\n*/",
        "org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the checksum of a file at the given path.\n* @param f the file path to check\n* @return FileChecksum object\n*/",
        "org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status for a given path.\n* @param f the Path to check\n* @return FileStatus of the file\n*/",
        "org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a given path.\n* @param path the Path to check permissions for\n* @param mode the FsAction defining the access type\n*/",
        "org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file link.\n* @param f the path of the file link\n* @return FileStatus object representing the file link status\n*/",
        "org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Resolves the target of a symbolic link in the filesystem.\n* @param f the Path of the symbolic link\n* @return resolved Path of the link target\n*/",
        "org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a file within specified byte range.\n* @param f file path, @param start byte offset, @param len number of bytes\n* @return array of BlockLocation objects\n*/",
        "org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves filesystem status for a given path or defaults if null.\n* @param f the Path for which to get the status\n* @return FsStatus object representing the filesystem status\n*/",
        "org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link to a target path.\n* @param target the target file or directory\n* @param link the link to create\n* @param createParent indicates if parent directories should be created\n*/",
        "org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for the given path.\n* @param f the Path to list statuses for\n* @return an iterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path)": "/**\n* Lists corrupt file blocks for a given path.\n* @param path the Path to check for corrupt file blocks\n* @return RemoteIterator of Paths with corrupt file blocks\n*/",
        "org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses for a given path.\n* @param f the path to list statuses for\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path)": "/**\n* Resolves abstract file systems from a given path.\n* @param f the path to resolve\n* @return set of AbstractFileSystem objects\n*/",
        "org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for the specified path.\n* @param path the Path to modify ACLs for\n* @param aclSpec list of AclEntry to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes ACL entries from the specified path.\n* @param path the Path to modify\n* @param aclSpec list of ACL entries to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the default ACL for the specified path.\n* @param path the Path from which to remove the default ACL\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL from the specified path.\n* @param path the Path from which to remove the ACL\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the Path to set ACL for\n* @param aclSpec list of AclEntry to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for a given path.\n* @param path the Path to check ACL status\n* @return AclStatus object for the specified path\n*/",
        "org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**** Sets an extended attribute for the specified path.\n* @param path the Path to set the attribute on\n* @param name the name of the extended attribute\n* @param value the value of the extended attribute\n* @param flag the flags for setting the attribute\n*/",
        "org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the file system path\n* @param name the name of the attribute to retrieve\n* @return byte array of the attribute value\n*/",
        "org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the Path to get extended attributes from\n* @return a map of attribute names and their byte values\n*/",
        "org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the file system path\n* @param names list of attribute names to fetch\n* @return map of attribute names to their byte array values\n*/",
        "org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes extended attribute by name from the specified path.\n* @param path the file system path; @param name the attribute name to remove\n*/",
        "org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Lists extended attributes for a given file path.\n* @param path the file path to list attributes for\n* @return List of extended attribute names\n*/",
        "org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a filesystem snapshot with a given name.\n* @param path the Path to create a snapshot for\n* @param snapshotName the name of the snapshot\n* @return Path of the created snapshot\n*/",
        "org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot in the filesystem.\n* @param path the Path of the snapshot\n* @param snapshotOldName current snapshot name\n* @param snapshotNewName new snapshot name\n*/",
        "org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot for the specified path.\n* @param path the path to the snapshot\n* @param snapshotName the name of the snapshot to delete\n*/",
        "org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Satisfies storage policy for the given path.\n* @param path the Path to apply the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets storage policy for the specified path.\n* @param path the file system path to update\n* @param policyName the name of the storage policy to apply\n*/",
        "org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for the specified path.\n* @param src the path to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the storage policy for a given path.\n* @param path the Path to resolve and get policy for\n* @return BlockStoragePolicySpi associated with the path\n*/",
        "org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the specified path has the given capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server defaults for the specified path.\n* @param path the path to resolve\n* @return FsServerDefaults object\n*/",
        "org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path)": "/**\n* Creates a MultipartUploader for the given base path.\n* @param basePath the base path for the uploader\n* @return MultipartUploaderBuilder instance\n*/",
        "org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)": "/**\n* Sets checksum verification for a specified file path.\n* @param verifyChecksum true to enable, false to disable\n* @param f the file path to apply the setting\n*/",
        "org.apache.hadoop.fs.FileContext:processDeleteOnExit()": "/**\n* Deletes paths marked for exit deletion and clears the tracking set.\n*/",
        "org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory to a new absolute path.\n* @param newWDir the new working directory path\n* @throws IOException if the path is invalid or a file is specified\n*/",
        "org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)": "/**\n* Validates destination path for file copy.\n* @param srcName source file name, or null for directory checks\n* @param dst destination Path\n* @param overwrite flag to allow overwriting existing files\n*/",
        "org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves delegation tokens for a specified path and renewer.\n* @param p the path to resolve\n* @param renewer the identifier for the token renewer\n* @return list of delegation tokens\n*/",
        "org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])": "/**\n* Sets an extended attribute for the specified path.\n* @param path the Path to set the attribute on\n* @param name the name of the extended attribute\n* @param value the value of the extended attribute\n*/",
        "org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path)": "/**\n* Creates a filesystem snapshot at the specified path.\n* @param path the Path to create a snapshot for\n* @return Path of the created snapshot\n*/",
        "org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])": "/**** Renames a file or directory, ensuring source and destination are on the same filesystem. \n* @param src source path to rename \n* @param dst destination path for the rename \n* @param options optional rename options \n* @throws various exceptions for access and file state issues \n*/",
        "org.apache.hadoop.fs.FileContext:getUMask()": "/**\n* Returns the umask value, either from cache or configuration.\n* @return FsPermission umask value\n*/",
        "org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])": "/**** Creates a file with specified permissions and create flags. \n* @param f the Path of the file to create\n* @param createFlag flags indicating create options\n* @param opts variable number of CreateOpts for additional settings\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory with specified permissions, optionally creating parents.\n* @param dir directory path to create\n* @param permission desired FsPermission, defaults to dir default if null\n* @param createParent true to create parent directories if needed\n*/",
        "org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path)": "/**\n* Marks a file for deletion on JVM exit.\n* @param f the Path to the file to delete\n* @return true if marked, false if file doesn't exist\n*/",
        "org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FileContext with default filesystem and configuration.\n* @param defFs default filesystem instance\n* @param aConf Hadoop configuration object\n*/",
        "org.apache.hadoop.fs.FileContext:getHomeDirectory()": "/**\n* Retrieves the home directory path for the current user.\n* @return Path object representing the user's home directory\n*/",
        "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a FileContext using the provided filesystem and configuration.\n* @param defFS default filesystem instance\n* @param aConf Hadoop configuration object\n* @return FileContext instance\n*/",
        "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem)": "/**\n* Retrieves FileContext using default filesystem and a new Configuration.\n* @param defaultFS the default filesystem instance\n* @return FileContext instance\n*/",
        "org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates FileContext from URI and configuration.\n* @param defaultFsUri URI of the default filesystem\n* @param aConf Hadoop configuration\n* @return FileContext instance\n* @throws UnsupportedFileSystemException if the filesystem is unsupported\n*/",
        "org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI)": "/**\n* Retrieves FileContext using default configuration.\n* @param defaultFsUri URI of the default filesystem\n* @return FileContext instance\n* @throws UnsupportedFileSystemException if the filesystem is unsupported\n*/",
        "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration)": "/**\n* Creates FileContext from configuration.\n* @param aConf Hadoop configuration\n* @return FileContext instance\n* @throws UnsupportedFileSystemException if URI has no scheme\n*/",
        "org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves FileContext for local filesystem.\n* @param aConf Hadoop configuration\n* @return FileContext instance for local filesystem\n* @throws UnsupportedFileSystemException if the filesystem is unsupported\n*/",
        "org.apache.hadoop.fs.FileContext:getLocalFSFileContext()": "/**\n* Retrieves FileContext for the local filesystem.\n* @return FileContext instance for local filesystem\n* @throws UnsupportedFileSystemException if the filesystem is unsupported\n*/",
        "org.apache.hadoop.fs.FileContext:getFileContext()": "/**\n* Retrieves FileContext using default Hadoop configuration.\n* @return FileContext instance\n* @throws UnsupportedFileSystemException if URI has no scheme\n*/"
    },
    "org.apache.hadoop.fs.FutureDataInputStreamBuilder": {
        "org.apache.hadoop.fs.FutureDataInputStreamBuilder:withFileStatus(org.apache.hadoop.fs.FileStatus)": "/**\n* Sets the file status for the builder.\n* @param status the file status to set, can be null\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.FutureDataInputStreamBuilder:build()": "/**\n* Builds and opens a file asynchronously.\n* @return CompletableFuture of FSDataInputStream\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.FSDataInputStream": {
        "org.apache.hadoop.fs.FSDataInputStream:seek(long)": "/**\n* Seeks to the specified position in the input stream.\n* @param desired position to seek to in the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:getPos()": "/**\n* Retrieves the current position of the input stream.\n* @return current position as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position into a buffer.\n* @param position start position in the source\n* @param buffer byte array to fill\n* @param offset start offset in the buffer\n* @param length number of bytes to read\n* @return number of bytes read\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:readFully(long,byte[],int,int)": "/**\n* Reads bytes into buffer from specified position.\n* @param position starting position in the source\n* @param buffer byte array to fill with data\n* @param offset start offset in buffer\n* @param length number of bytes to read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:setReadahead(java.lang.Long)": "/**\n* Sets the readahead value for input stream.\n* @param readahead the amount of data to prefetch\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if input stream cannot set readahead\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:setDropBehind(java.lang.Boolean)": "/**\n* Sets the drop-behind caching option for the stream.\n* @param dropBehind true to enable, false to disable caching\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if the stream doesn't support this operation\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:seekToNewSource(long)": "/**\n* Seeks to a new source at the specified position.\n* @param targetPos the position to seek to\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:readFully(long,byte[])": "/**\n* Reads bytes into buffer from specified position.\n* @param position the starting position to read from\n* @param buffer the byte array to fill with data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:read(java.nio.ByteBuffer)": "/**\n* Reads data into the provided ByteBuffer.\n* @param buf the ByteBuffer to read data into\n* @return number of bytes read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:getFileDescriptor()": "/**\n* Retrieves the FileDescriptor from the input stream.\n* @return FileDescriptor or null if not applicable\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:toString()": "/**\n* Returns a string representation of the object including 'in' value.\n* @return formatted string of the object's superclass and 'in'\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:read(long,java.nio.ByteBuffer)": "/**\n* Reads bytes into a buffer from a specified position.\n* @param position the position to read from\n* @param buf the buffer to store read bytes\n* @return number of bytes read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:readFully(long,java.nio.ByteBuffer)": "/**\n* Reads bytes into the buffer from a specified position.\n* @param position the position to start reading from\n* @param buf the ByteBuffer to fill with data\n* @throws IOException if an I/O error occurs or operation is unsupported\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:getWrappedStream()": "/**\n* Returns the wrapped InputStream.\n* @return InputStream instance wrapped by this method\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:unbuffer()": "/**\n* Unbuffers the current InputStream using StreamCapabilitiesPolicy.\n* @param in the InputStream to unbuffer\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads()": "/**\n* Delegates to the input stream to get minimum seek size for vector reads.\n* @return minimum seek size in bytes\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads()": "/**\n* Returns the maximum read size for vector reads.\n* @return maximum read size in bytes (1 MB)\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics using the input object.\n* @return IOStatistics object or null if invalid input\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String)": "/**\n* Checks if the InputStream has a specific capability.\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer)": "/**\n* Releases a ByteBuffer, validating its origin and handling exceptions.\n* @param buffer the ByteBuffer to release\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream)": "/**\n* Constructs FSDataInputStream from InputStream.\n* @param in input stream, must be Seekable and PositionedReadable\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)": "/**\n* Reads data into a ByteBuffer, handling access and fallback if necessary.\n* @param bufferPool pool for managing ByteBuffer allocation\n* @param maxLength maximum number of bytes to read\n* @param opts read options\n* @return ByteBuffer containing read data or null if fallback fails\n*/",
        "org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)": "/****  \n* Reads data into a ByteBuffer from the specified pool.  \n* @param bufferPool pool for managing ByteBuffer allocation  \n* @param maxLength maximum number of bytes to read  \n* @return ByteBuffer containing read data  \n*/",
        "org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)": "/**\n* Reads data into FileRange objects using a provided allocation function.\n* @param ranges list of FileRange objects to read data into\n* @param allocate function to allocate ByteBuffer instances\n* @throws IOException if an I/O error occurs during reading\n*/"
    },
    "org.apache.hadoop.fs.impl.OpenFileParameters": {
        "org.apache.hadoop.fs.impl.OpenFileParameters:<init>()": "/**\n* Constructs an instance of OpenFileParameters with default settings.\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:withMandatoryKeys(java.util.Set)": "/**\n* Sets mandatory keys for file parameters.\n* @param keys a set of mandatory key strings\n* @return this OpenFileParameters instance\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:withOptionalKeys(java.util.Set)": "/**\n* Sets optional keys for file parameters.\n* @param keys a set of optional key strings\n* @return the updated OpenFileParameters instance\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:withOptions(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration options and returns the current instance.\n* @param opts configuration settings to apply\n* @return the updated OpenFileParameters instance\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:withStatus(org.apache.hadoop.fs.FileStatus)": "/**\n* Sets the file status and returns the updated OpenFileParameters instance.\n* @param st the new FileStatus to set\n* @return updated OpenFileParameters object\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:withBufferSize(int)": "/**\n* Sets the buffer size and returns the updated OpenFileParameters instance.\n* @param size the new buffer size\n* @return the updated OpenFileParameters object\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:getMandatoryKeys()": "/**\n* Retrieves the set of mandatory keys.\n* @return Set of mandatory key strings\n*/",
        "org.apache.hadoop.fs.impl.OpenFileParameters:getBufferSize()": "/**\n* Retrieves the current buffer size.\n* @return the size of the buffer as an integer\n*/"
    },
    "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus": {
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getLastAccessTime(java.io.File)": "/**\n* Retrieves the last access time of a file in milliseconds.\n* @param f the file to check\n* @return last access time in milliseconds\n* @throws IOException if the file does not exist\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:removeDomain(java.lang.String)": "/**\n* Removes domain from a given string.\n* @param str input string possibly containing a domain\n* @return string without the domain or original if none found\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded()": "/**\n* Checks if permissions are loaded based on owner availability.\n* @return true if owner is not empty, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO()": "/**\n* Loads and sets file permissions using native I/O.\n* @throws IOException if file access fails or path is invalid\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO()": "/**\n* Loads file permission info using a shell command and sets permission, owner, and group.\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)": "/**\n* Constructs a DeprecatedRawLocalFileStatus from a File object.\n* @param f the file to create status for, defaultBlockSize block size, fs file system reference\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo()": "/**\n* Loads permission info using native I/O if available; else, falls back to non-native method.\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission()": "/**\n* Retrieves file system permissions, loading them if not already loaded.\n* @return FsPermission object representing the permissions\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner()": "/**\n* Retrieves the owner's name, loading permissions if not already loaded.\n* @return String representing the owner's name\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup()": "/**\n* Retrieves the group, loading permissions if not already done.\n* @return the group as a String\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput)": "/**\n* Writes the object to output after ensuring permissions are loaded.\n* @param out output stream for serialization\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.StringUtils": {
        "org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)": "/**\n* Converts a Throwable's stack trace to a string.\n* @param e the Throwable to stringify\n* @return stack trace as a String\n*/",
        "org.apache.hadoop.util.StringUtils:toUpperCase(java.lang.String)": "/**\n* Converts the input string to uppercase using English locale.\n* @param str the string to convert\n* @return the uppercase version of the input string\n*/",
        "org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.String[])": "/**\n* Joins an array of strings with a specified separator.\n* @param separator the string to insert between elements\n* @param strings array of strings to join\n* @return concatenated string with separators\n*/",
        "org.apache.hadoop.util.StringUtils:replaceTokens(java.lang.String,java.util.regex.Pattern,java.util.Map)": "/**\n* Replaces tokens in a template string using a specified pattern and replacements map.\n* @param template the string with tokens to replace\n* @param pattern the regex pattern for matching tokens\n* @param replacements map of token names to their replacement values\n* @return the modified string with tokens replaced\n*/",
        "org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.Iterable)": "/**\n* Joins strings with a specified separator.\n* @param separator the string to separate each element\n* @param strings iterable collection of strings\n* @return concatenated string of elements\n*/",
        "org.apache.hadoop.util.StringUtils:popOptionWithArgument(java.lang.String,java.util.List)": "/**\n* Retrieves and removes the argument for a specified option from a list.\n* @param name the option name to find\n* @param args the list of arguments\n* @return the argument value or null if not found\n* @throws IllegalArgumentException if the option requires an argument but none is provided\n*/",
        "org.apache.hadoop.util.StringUtils:toLowerCase(java.lang.String)": "/**\n* Converts input string to lowercase using English locale.\n* @param str the string to convert\n* @return lowercase version of the input string\n*/",
        "org.apache.hadoop.util.StringUtils:popOption(java.lang.String,java.util.List)": "/**\n* Removes and returns true if the specified option is found in args before \"--\".\n* @param name the option to pop from the list\n* @param args the list of arguments to search\n* @return true if the option was found and removed, false otherwise\n*/",
        "org.apache.hadoop.util.StringUtils:split(java.lang.String,char)": "/**\n* Splits a string by a specified separator character.\n* @param str the string to split\n* @param separator the character used for splitting\n* @return an array of split strings, excluding trailing empty results\n*/",
        "org.apache.hadoop.util.StringUtils:stringToURI(java.lang.String[])": "/**\n* Converts an array of strings to an array of URIs.\n* @param str array of string representations of URIs\n* @return array of URI objects or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:uriToString(java.net.URI[])": "/**\n* Converts an array of URIs to a comma-separated string.\n* @param uris array of URI objects\n* @return concatenated string of URIs or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:wrap(java.lang.String,int,java.lang.String,boolean)": "/**\n* Wraps a string to a specified length with optional new lines.\n* @param str input string to wrap\n* @param wrapLength maximum line length\n* @param newLineStr string to use for new lines\n* @param wrapLongWords flag to wrap long words\n* @return wrapped string\n*/",
        "org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String,java.lang.String)": "/**\n* Splits a string into a collection of substrings based on a delimiter.\n* @param str input string to be split\n* @param delim delimiter used for splitting\n* @return Collection of substrings\n*/",
        "org.apache.hadoop.util.StringUtils:getTrimmedStrings(java.lang.String)": "/**\n* Splits and trims input string into an array of substrings.\n* @param str input string to be processed\n* @return array of trimmed substrings or empty array if input is null/empty\n*/",
        "org.apache.hadoop.util.StringUtils:format(java.lang.String,java.lang.Object[])": "/**\n* Formats a string using the specified format and arguments.\n* @param format the format string\n* @param objects the arguments to format\n* @return formatted string\n*/",
        "org.apache.hadoop.util.StringUtils:getStackTrace(java.lang.Thread)": "/**\n* Retrieves the stack trace of a given thread.\n* @param t the thread whose stack trace is to be retrieved\n* @return stack trace as a formatted string\n*/",
        "org.apache.hadoop.util.StringUtils:simpleHostname(java.lang.String)": "/**\n* Extracts the simple hostname from a full hostname.\n* @param fullHostname the complete hostname or IP address\n* @return simple hostname or IP if valid, else original input\n*/",
        "org.apache.hadoop.util.StringUtils:arrayToString(java.lang.String[])": "/**\n* Converts an array of strings to a comma-separated string.\n* @param strs array of strings to convert\n* @return comma-separated string representation of the array\n*/",
        "org.apache.hadoop.util.StringUtils:hexStringToByte(java.lang.String)": "/**\n* Converts a hex string to a byte array.\n* @param hex hexadecimal string to convert\n* @return byte array representation of the hex string\n*/",
        "org.apache.hadoop.util.StringUtils:formatTime(long)": "/**\n* Formats time difference in hours, minutes, and seconds.\n* @param timeDiff time difference in milliseconds\n* @return formatted time string\n*/",
        "org.apache.hadoop.util.StringUtils:formatTimeSortable(long)": "/**\n* Formats time difference in hours, minutes, and seconds.\n* @param timeDiff time difference in milliseconds\n* @return formatted time string (e.g., \"02hrs, 30mins, 45sec\")\n*/",
        "org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String,java.lang.String)": "/**\n* Splits a string into trimmed substrings based on a delimiter.\n* @param str input string to split\n* @param delim delimiter for splitting the string\n* @return collection of non-empty trimmed substrings\n*/",
        "org.apache.hadoop.util.StringUtils:getTrimmedStringsSplitByEquals(java.lang.String)": "/**\n* Splits and trims the input string by equals sign, returning an array of substrings.\n* @param str input string to split\n* @return array of trimmed substrings or an empty array if input is null/empty\n*/",
        "org.apache.hadoop.util.StringUtils:findNext(java.lang.String,char,char,int,java.lang.StringBuilder)": "/**\n* Finds the next separator in a string, considering escape characters.\n* @param str input string to search\n* @param separator character to locate\n* @param escapeChar character used for escaping\n* @param start index to begin the search\n* @param split StringBuilder to accumulate non-separator characters\n* @return index of the next separator or -1 if none found\n*/",
        "org.apache.hadoop.util.StringUtils:hasChar(char[],char)": "/**\n* Checks if the specified character exists in the array.\n* @param chars array of characters to search\n* @param character character to find\n* @return true if found, false otherwise\n*/",
        "org.apache.hadoop.util.StringUtils:toStartupShutdownString(java.lang.String,java.lang.String[])": "/**\n* Constructs a formatted startup/shutdown message.\n* @param prefix string to prepend to each message line\n* @param msg array of message strings to include\n* @return formatted message as a single string\n*/",
        "org.apache.hadoop.util.StringUtils:escapeHTML(java.lang.String)": "/**\n* Escapes HTML special characters in a string.\n* @param string input string to escape\n* @return escaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:popFirstNonOption(java.util.List)": "/**\n* Retrieves and removes the first non-option argument from the list.\n* @param args list of command-line arguments\n* @return the first non-option argument or null if none found\n*/",
        "org.apache.hadoop.util.StringUtils:isAlpha(java.lang.String)": "/**\n* Checks if the string contains only alphabetic characters.\n* @param str input string to check\n* @return true if all characters are letters, false otherwise\n*/",
        "org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])": "/**\n* Joins an array of strings with a specified character separator.\n* @param separator the character to insert between elements\n* @param strings array of strings to join\n* @return concatenated string with separators\n*/",
        "org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)": "",
        "org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)": "/**\n* Converts a delimited string into an array of substrings.\n* @param str input string to be split\n* @param delim delimiter used for splitting\n* @return array of substrings or null if no substrings found\n*/",
        "org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String)": "/**\n* Splits a string into a collection of substrings using a comma as a delimiter.\n* @param str input string to be split\n* @return Collection of substrings\n*/",
        "org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String)": "/**\n* Creates a collection of non-empty trimmed strings from the input.\n* @param str input string to be processed\n* @return Collection of unique trimmed strings\n*/",
        "org.apache.hadoop.util.StringUtils:formatPercent(double,int)": "/**\n* Formats a fraction as a percentage with specified decimal places.\n* @param fraction value to format as a percentage\n* @param decimalPlaces number of decimal places in the output\n* @return formatted percentage string\n*/",
        "org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)": "/**\n* Converts a byte array to a hexadecimal string.\n* @param bytes byte array to convert\n* @param start starting index in the array\n* @param end ending index in the array\n* @return hexadecimal string representation\n*/",
        "org.apache.hadoop.util.StringUtils:limitDecimalTo2(double)": "/**\n* Limits a double to two decimal places.\n* @param d the double value to format\n* @return formatted string representation of the value\n*/",
        "org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)": "/**\n* Formats time difference between start and finish times.\n* @param finishTime end time in milliseconds\n* @param startTime start time in milliseconds\n* @return formatted time string\n*/",
        "org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String)": "/**\n* Splits input string into key-value pairs in a map.\n* @param str input string to process\n* @return Map of key-value pairs derived from the input\n*/",
        "org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)": "/**\n* Splits a string by a separator, considering escape characters.\n* @param str input string to split\n* @param escapeChar character used for escaping\n* @param separator character to split the string\n* @return array of split strings, or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])": "/**\n* Escapes specified characters in a string using an escape character.\n* @param str input string to escape\n* @param escapeChar character used for escaping\n* @param charsToEscape characters that need to be escaped\n* @return escaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])": "/**\n* Unescapes a string by removing escape characters.\n* @param str input string to unescape\n* @param escapeChar character used for escaping\n* @param charsToEscape array of characters that cannot be escaped\n* @return unescaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)": "/**\n* Compares two strings for equality, ignoring case.\n* @param s1 first string, must not be null\n* @param s2 second string, can be null\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.util.StringUtils:getStrings(java.lang.String)": "/**\n* Splits input string into substrings using a comma delimiter.\n* @param str input string to be split\n* @return array of substrings or null if no substrings found\n*/",
        "org.apache.hadoop.util.StringUtils:humanReadableInt(long)": "/**\n* Converts a long integer to a human-readable string format.\n* @param number the long integer to convert\n* @return formatted string representation of the integer\n*/",
        "org.apache.hadoop.util.StringUtils:byteDesc(long)": "/**\n* Converts byte length to a formatted string with a binary prefix.\n* @param len length in bytes\n* @return formatted string representation of the byte length\n*/",
        "org.apache.hadoop.util.StringUtils:byteToHexString(byte[])": "/**\n* Converts a byte array to a hexadecimal string.\n* @param bytes byte array to convert\n* @return hexadecimal string representation\n*/",
        "org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)": "/**\n* Returns formatted finish time with optional duration difference.\n* @param formattedFinishTime formatted end time string\n* @param finishTime end time in milliseconds\n* @param startTime start time in milliseconds\n* @return combined formatted time string\n*/",
        "org.apache.hadoop.util.StringUtils:split(java.lang.String)": "/**\n* Splits a string by a comma, considering escape characters.\n* @param str input string to split\n* @return array of split strings or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:camelize(java.lang.String)": "/**\n* Converts a string to camel case format.\n* @param s input string to camelize\n* @return camelized string\n*/",
        "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)": "/**\n* Escapes a specified character in a string using an escape character.\n* @param str input string to escape\n* @param escapeChar character used for escaping\n* @param charToEscape character that needs to be escaped\n* @return escaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)": "/**\n* Unescapes a string by removing specified escape characters.\n* @param str input string to unescape\n* @param escapeChar character used for escaping\n* @param charToEscape character that cannot be escaped\n* @return unescaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])": "/**\n* Creates a formatted startup message with class and host info.\n* @param classname name of the class starting up\n* @param hostname name of the host\n* @param args additional startup arguments\n* @return formatted startup message as a string\n*/",
        "org.apache.hadoop.util.StringUtils:byteToHexString(byte)": "/**\n* Converts a byte to its hexadecimal string representation.\n* @param b byte to convert\n* @return hexadecimal string representation of the byte\n*/",
        "org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)": "/**\n* Formats finish time with duration difference.\n* @param dateFormat date format to use\n* @param finishTime end time in milliseconds\n* @param startTime start time in milliseconds\n* @return combined formatted time string\n*/",
        "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String)": "/**\n* Escapes special characters in a string.\n* @param str input string to escape\n* @return escaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String)": "/**\n* Unescapes a string by removing specific escape characters.\n* @param str input string to unescape\n* @return unescaped string or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[])": "/**\n* Converts an array of strings to an array of Path objects.\n* @param str array of path strings\n* @return array of Path objects or null if input is null\n*/",
        "org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)": "/**\n* Logs startup message and registers shutdown hook.\n* @param clazz class initiating the startup\n* @param args additional startup arguments\n* @param log Logger for logging events\n*/"
    },
    "org.apache.hadoop.util.Shell$ExitCodeException": {
        "org.apache.hadoop.util.Shell$ExitCodeException:getExitCode()": "/**\n* Retrieves the current exit code.\n* @return the exit code as an integer\n*/",
        "org.apache.hadoop.util.Shell$ExitCodeException:<init>(int,java.lang.String)": "/**\n* Constructs an ExitCodeException with a specific exit code and message.\n* @param exitCode numeric code indicating the type of error\n* @param message description of the exception\n*/",
        "org.apache.hadoop.util.Shell$ExitCodeException:toString()": "/**\n* Returns a string representation of the ExitCodeException.\n* @return formatted string with exit code and message\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getOwner()": "/**\n* Retrieves the owner of the object.\n* @return the owner as a String\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getGroup()": "/**\n* Retrieves the group associated with the object.\n* @return the group as a String\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getMode()": "/**\n* Retrieves the current mode value.\n* @return the current mode as an integer\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:<init>(int,int,int)": "/**\n* Constructs a Stat object with owner, group IDs, and mode.\n* @param ownerId ID of the owner\n* @param groupId ID of the group\n* @param mode access mode of the object\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:<init>(java.lang.String,java.lang.String,int)": "/**\n* Initializes a Stat object with owner, group, and mode.\n* @param owner the owner of the file\n* @param group the group of the file\n* @param mode the file mode (permissions)\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:toString()": "/**\n* Returns a string representation of the Stat object with owner, group, and mode.\n* @return formatted string of Stat details\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream": {
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:close()": "/**\n* Closes the underlying stream and the current object.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:reset()": "/**\n* Throws an IOException indicating reset is not implemented.\n* @throws IOException if reset is called\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[],int,int)": "/**\n* Reads bytes from the underlying stream into the buffer.\n* @param b byte array to store read bytes, @param offset start position in array, \n* @param len maximum number of bytes to read\n* @return number of bytes read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:validatePosition(long)": "/**\n* Validates the given position within a stream's bounds.\n* @param pos position to validate\n* @throws IOException if pos is negative or exceeds stream length\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:getPos()": "/**\n* Returns the current position relative to the start.\n* @return current position offset as long\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seekToNewSource(long)": "/**\n* Indicates if the source should seek to a new position.\n* @param targetPos the position to seek to\n* @return false as this method is not implemented\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)": "",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long)": "",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available()": "/**\n* Returns the number of bytes available to read.\n* @return number of available bytes, capped at Integer.MAX_VALUE\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read()": "/**\n* Reads a single byte from the stream.\n* @return byte value or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[])": "/**\n* Reads bytes into the provided buffer.\n* @param b byte array to store read bytes\n* @return number of bytes read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long)": "",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position into a buffer.\n* @param pos start position in the source\n* @param b byte array to fill\n* @param offset start offset in the buffer\n* @param length number of bytes to read\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)": "/**\n* Reads bytes into buffer from specified position.\n* @param pos starting position in the source\n* @param b byte array to fill with data\n* @param offset start offset in buffer\n* @param length number of bytes to read\n* @throws IOException if an I/O error occurs or not enough bytes are available\n*/",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long)": "",
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean)": ""
    },
    "org.apache.hadoop.fs.FsShell$Help": {
        "org.apache.hadoop.fs.FsShell$Help:processRawArguments(java.util.LinkedList)": "/**\n* Processes command-line arguments and prints help information.\n* @param args list of command-line arguments\n*/"
    },
    "org.apache.hadoop.fs.TrashPolicy": {
        "org.apache.hadoop.fs.TrashPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes configuration with the given parameters.\n* @param conf Configuration settings to apply\n* @param fs FileSystem instance for file operations\n*/",
        "org.apache.hadoop.fs.TrashPolicy:getCurrentTrashDir(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the current trash directory path.\n* @param path base path to derive the trash directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Retrieves TrashPolicy instance from configuration and initializes it.\n* @param conf Hadoop configuration, @param fs FileSystem, @param home home directory path\n* @return initialized TrashPolicy instance\n*/",
        "org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)": "/**\n* Retrieves TrashPolicy instance based on configuration and file system.\n* @param conf configuration settings, @param fs file system for operations\n* @return TrashPolicy instance\n*/"
    },
    "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler": {
        "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:<init>(java.nio.channels.AsynchronousFileChannel,java.util.List,java.nio.ByteBuffer[])": "/**\n* Initializes AsyncHandler with file channel, ranges, and buffers.\n* @param channel asynchronous file channel\n* @param ranges list of file ranges to operate on\n* @param buffers array of byte buffers for data transfer\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:failed(java.lang.Throwable,java.lang.Integer)": "/**\n* Logs failure and completes future for specified range.\n* @param exc throwable error encountered\n* @param r range index associated with the error\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)": "/**\n* Handles completion of read operation, processing results and managing buffers.\n* @param result read result status; -1 indicates failure\n* @param r index of the range being processed\n*/"
    },
    "org.apache.hadoop.fs.Trash": {
        "org.apache.hadoop.fs.Trash:moveToTrash(org.apache.hadoop.fs.Path)": "/**\n* Moves the specified file path to trash.\n* @param path the file path to be moved\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.Trash:isEnabled()": "/**\n* Checks if the trash policy is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.Trash:checkpoint()": "/**\n* Creates a checkpoint for the trash policy.\n* @throws IOException if an I/O error occurs during checkpoint creation\n*/",
        "org.apache.hadoop.fs.Trash:expunge()": "/**\n* Deletes the checkpoint as per the trash policy.\n* @throws IOException if an I/O error occurs during deletion\n*/",
        "org.apache.hadoop.fs.Trash:expungeImmediately()": "/**\n* Immediately expunges checkpoints from the trash policy.\n* @throws IOException if an I/O error occurs during expunging\n*/",
        "org.apache.hadoop.fs.Trash:getCurrentTrashDir()": "/**\n* Retrieves the current trash directory.\n* @return Path to the current trash directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.Trash:getEmptier()": "/**\n* Retrieves a Runnable that empties the trash.\n* @return Runnable for trash emptying process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the current trash directory based on the given path.\n* @param path base path to derive the trash directory\n* @return Path object representing the current trash directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Trash with a FileSystem and Configuration.\n* @param fs the FileSystem to operate on\n* @param conf the Configuration settings\n*/",
        "org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Trash with a FileSystem from the given configuration.\n* @param conf configuration settings\n* @throws IOException if unable to access FileSystem\n*/",
        "org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Moves a file to the appropriate trash based on filesystem configuration.\n* @param fs the FileSystem to operate on\n* @param p the Path of the file to move\n* @param conf the Configuration settings\n* @return true if the move is successful, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$8": {
        "org.apache.hadoop.fs.FileSystem$Statistics$8:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Copies statistics from another Statistics instance.\n* @param other the Statistics object to copy from\n*/"
    },
    "org.apache.hadoop.fs.GlobExpander$StringWithOffset": {
        "org.apache.hadoop.fs.GlobExpander$StringWithOffset:<init>(java.lang.String,int)": "/**\n* Constructs a StringWithOffset with specified string and offset.\n* @param string the input string\n* @param offset the offset value for the string\n*/"
    },
    "org.apache.hadoop.fs.GlobExpander": {
        "org.apache.hadoop.fs.GlobExpander:leftmostOuterCurlyContainingSlash(java.lang.String,int)": "/**\n* Finds the leftmost outer curly brace containing a slash in the file pattern.\n* @param filePattern the string pattern to search within\n* @param offset starting index for the search\n* @return index of the leftmost curly brace or -1 if not found\n*/",
        "org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset)": "/**\n* Expands file patterns with leftmost curly braces into multiple alternatives.\n* @param filePatternWithOffset input pattern with offset\n* @return list of expanded StringWithOffset objects or null if no curly brace found\n*/",
        "org.apache.hadoop.fs.GlobExpander:expand(java.lang.String)": "/**\n* Expands file patterns into a list of fully resolved paths.\n* @param filePattern the input file pattern to expand\n* @return List of fully expanded file paths\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$DirectoryEntries": {
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:<init>(org.apache.hadoop.fs.FileStatus[],byte[],boolean)": "/**\n* Initializes DirectoryEntries with file status, token, and more flag.\n* @param entries array of file status entries\n* @param token optional byte array for pagination\n* @param hasMore indicates if more entries are available\n*/",
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:getEntries()": "/**\n* Retrieves an array of FileStatus entries.\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:hasMore()": "/**\n* Checks if there are more items available.\n* @return true if more items exist, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:getToken()": "/**\n* Retrieves the current token as a byte array.\n* @return byte array representing the token\n*/"
    },
    "org.apache.hadoop.fs.XAttrCodec": {
        "org.apache.hadoop.fs.XAttrCodec:decodeValue(java.lang.String)": "/**\n* Decodes a string value into a byte array based on its prefix.\n* @param value the encoded string to decode\n* @return decoded byte array or UTF-8 bytes if no prefix matches\n*/",
        "org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)": "/**\n* Encodes byte array to string based on specified encoding.\n* @param value byte array to encode\n* @param encoding encoding type (HEX, BASE64, or default)\n* @return encoded string representation\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$3": {
        "org.apache.hadoop.fs.FileSystem$Statistics$3:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Copies statistics from another Statistics object.\n* @param other the Statistics instance to copy from\n*/"
    },
    "org.apache.hadoop.fs.ContentSummary": {
        "org.apache.hadoop.fs.ContentSummary:getLength()": "/**\n* Returns the length of the object.\n* @return length as a long value\n*/",
        "org.apache.hadoop.fs.ContentSummary:getFileCount()": "/**\n* Retrieves the total number of files.\n* @return the count of files as a long value\n*/",
        "org.apache.hadoop.fs.ContentSummary:getDirectoryCount()": "/**\n* Returns the current count of directories.\n* @return number of directories as a long value\n*/",
        "org.apache.hadoop.fs.ContentSummary:write(java.io.DataOutput)": "/**\n* Serializes object data to output stream.\n* @param out DataOutput stream to write data\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.fs.ContentSummary:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput stream and initializes object state.\n* @param in DataInput stream containing serialized fields\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.fs.ContentSummary:getSnapshotLength()": "/**\n* Retrieves the length of the snapshot.\n* @return long representing the snapshot length\n*/",
        "org.apache.hadoop.fs.ContentSummary:getSnapshotFileCount()": "/**\n* Retrieves the count of snapshot files.\n* @return number of snapshot files as a long\n*/",
        "org.apache.hadoop.fs.ContentSummary:getSnapshotDirectoryCount()": "/**\n* Retrieves the count of snapshot directories.\n* @return number of snapshot directories\n*/",
        "org.apache.hadoop.fs.ContentSummary:getSnapshotSpaceConsumed()": "/**\n* Returns the amount of space consumed by snapshots.\n* @return long representing the space in bytes\n*/",
        "org.apache.hadoop.fs.ContentSummary:getErasureCodingPolicy()": "/**\n* Retrieves the current erasure coding policy.\n* @return String representing the erasure coding policy\n*/",
        "org.apache.hadoop.fs.ContentSummary:toErasureCodingPolicy()": "/**\n* Formats and returns the erasure coding policy as a string.\n* @return formatted erasure coding policy string\n*/",
        "org.apache.hadoop.fs.ContentSummary:getHeader(boolean)": "/**\n* Returns header string based on the qOption flag.\n* @param qOption true for ALL_HEADER, false for SUMMARY_HEADER\n* @return corresponding header string\n*/",
        "org.apache.hadoop.fs.ContentSummary:getErasureCodingPolicyHeader()": "/**\n* Retrieves the erasure coding policy header.\n* @return String representing the erasure coding policy header\n*/",
        "org.apache.hadoop.fs.ContentSummary:getSnapshotHeader()": "/**\n* Retrieves the snapshot header string.\n* @return the snapshot header constant\n*/",
        "org.apache.hadoop.fs.ContentSummary:getHeaderFields()": "/**\n* Retrieves the summary header fields.\n* @return an array of header field strings\n*/",
        "org.apache.hadoop.fs.ContentSummary:getQuotaHeaderFields()": "/**\n* Retrieves the quota header fields.\n* @return array of quota header field strings\n*/",
        "org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder)": "/**\n* Initializes ContentSummary from a Builder with various file and directory metrics.\n* @param builder contains values for initializing ContentSummary fields\n*/",
        "org.apache.hadoop.fs.ContentSummary:<init>()": "/**\n* Constructor for ContentSummary class; marked as deprecated.\n*/",
        "org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)": "/**\n* Initializes ContentSummary with various storage metrics.\n* @param length total length of content\n* @param fileCount number of files\n* @param directoryCount number of directories\n* @param quota storage quota\n* @param spaceConsumed used space\n* @param spaceQuota allowed space\n*/",
        "org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object)": "/**\n* Compares this ContentSummary object with another for equality.\n* @param to object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.ContentSummary:hashCode()": "/**\n* Computes hash code based on object attributes and returns it as an integer.\n*/",
        "org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)": "/**\n* Initializes ContentSummary with length, fileCount, and directoryCount.\n* @deprecated use the full constructor instead\n*/",
        "org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)": "/**\n* Formats size as a string; human-readable or raw.\n* @param size the size in bytes\n* @param humanReadable flag for human-readable format\n* @return formatted size string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)": "/**\n* Generates a summary string based on options and storage types.\n* @param qOption flag for quota usage display\n* @param hOption flag for human-readable format\n* @param tOption flag for types quota usage\n* @param xOption flag for excluding snapshot data\n* @param types list of storage types\n* @return formatted summary string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean)": "/**\n* Creates a snapshot string with formatted size details.\n* @param hOption flag for human-readable format\n* @return formatted snapshot string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)": "/**\n* Generates a summary string with specified display options.\n* @param qOption flag for quota usage display\n* @param hOption flag for human-readable format\n* @param xOption flag for excluding snapshot data\n* @return formatted summary string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)": "/**\n* Converts the object to a string representation with options.\n* @param qOption displays quota usage if true\n* @param hOption formats output as human-readable if true\n* @param tOption includes types quota usage if true\n* @param types list of storage types for the summary\n* @return formatted summary string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)": "/**\n* Converts the object to a string representation with options.\n* @param qOption displays quota usage if true\n* @param hOption formats output as human-readable if true\n* @return formatted summary string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toString(boolean)": "/**\n* Returns string representation of the object with quota option.\n* @param qOption displays quota usage if true\n* @return formatted summary string\n*/",
        "org.apache.hadoop.fs.ContentSummary:toString()": "/**\n* Returns string representation of the object with quota usage included.\n* @return formatted summary string\n*/"
    },
    "org.apache.hadoop.fs.GlobalStorageStatistics": {
        "org.apache.hadoop.fs.GlobalStorageStatistics:get(java.lang.String)": "/**\n* Retrieves StorageStatistics by name.\n* @param name the identifier for the storage statistics\n* @return StorageStatistics object or null if name is null or not found\n*/",
        "org.apache.hadoop.fs.GlobalStorageStatistics:reset()": "/**\n* Resets all storage statistics in the map.\n*/",
        "org.apache.hadoop.fs.GlobalStorageStatistics:iterator()": "/**\n* Returns an iterator over StorageStatistics from the map's first entry.\n* @return Iterator for StorageStatistics or null if map is empty\n*/",
        "org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)": ""
    },
    "org.apache.hadoop.fs.FsShell$UnknownCommandException": {
        "org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(java.lang.String)": "/**\n* Constructs an UnknownCommandException with the specified command.\n* @param cmd the unknown command that triggered the exception\n*/",
        "org.apache.hadoop.fs.FsShell$UnknownCommandException:getMessage()": "/**\n* Returns a message indicating the command status.\n* @return formatted message with command or default error message\n*/",
        "org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>()": "/**\n* Constructs an UnknownCommandException with no specific command.\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding": {
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:iostatisticsStore()": "/**\n* Creates a new instance of IOStatisticsStoreBuilder.\n* @return a new IOStatisticsStoreBuilder implementation\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.lang.String,java.lang.Object)": "/**\n* Formats an entry as a string using a specified pattern.\n* @param name the name of the entry\n* @param value the value associated with the entry\n* @return formatted string representation of the entry\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:passthroughFn(java.io.Serializable)": "/**\n* Returns the same object passed as input.\n* @param src input object of type E that extends Serializable\n* @return the same object passed in as parameter\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:dynamicIOStatistics()": "/**\n* Creates and returns a new instance of DynamicIOStatisticsBuilder.\n* @return DynamicIOStatisticsBuilder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:maybeUpdateMinimum(java.util.concurrent.atomic.AtomicLong,long)": "/**\n* Updates dest to sample if sample is less than current value, ensuring atomicity.\n* @param dest destination AtomicLong to update\n* @param sample value to compare and potentially set\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:maybeUpdateMaximum(java.util.concurrent.atomic.AtomicLong,long)": "/**\n* Updates dest with sample if sample is greater than current value.\n* @param dest AtomicLong to update if sample is larger\n* @param sample value to compare and potentially set\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMaximums(java.lang.Long,java.lang.Long)": "/**\n* Returns the maximum of two Long values, handling unset values.\n* @param l first Long value\n* @param r second Long value\n* @return maximum Long value or the other if one is unset\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMinimums(java.lang.Long,java.lang.Long)": "/**\n* Aggregates minimum values, returning the lesser of two or the unset value.\n* @param l first value to compare\n* @param r second value to compare\n* @return minimum of l and r or unset value if either is unset\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:<init>()": "/**\n* Private constructor for IOStatisticsBinding, prevents instantiation.\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:copyMap(java.util.Map,java.util.Map,java.util.function.Function)": "/**\n* Copies entries from source map to destination map using a copy function.\n* @param dest destination map to populate\n* @param source source map to copy from\n* @param copyFn function to clone values\n* @return the populated destination map\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMaps(java.util.Map,java.util.Map,java.util.function.BiFunction,java.util.function.Function)": "/**\n* Merges two maps by copying or aggregating values based on keys.\n* @param dest destination map to aggregate into\n* @param other source map to aggregate from\n* @param aggregateFn function to combine values for existing keys\n* @param copyFn function to copy values for new keys\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateCounters(java.lang.Long,java.lang.Long)": "/**\n* Aggregates two counters, ensuring non-negative values.\n* @param l first counter value\n* @param r second counter value\n* @return sum of non-negative counters\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateGauges(java.lang.Long,java.lang.Long)": "/**\n* Aggregates two Long values.\n* @param l first Long value\n* @param r second Long value\n* @return sum of l and r\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Tracks execution duration of a function, handling failures.\n* @param factory optional tracker factory\n* @param statistic name for the duration statistic\n* @param inputFn function to track\n* @return wrapped function with duration tracking\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackJavaFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Function)": "/**\n* Wraps a function to track its execution duration and handle failures.\n* @param factory optional tracker factory\n* @param statistic name for the statistic being tracked\n* @param inputFn function to be executed and tracked\n* @return a new function that tracks duration and handles exceptions\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfOperation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Tracks the duration of an operation and returns a callable.\n* @param factory optional factory for creating duration trackers\n* @param statistic name for the tracked statistic\n* @param input callable operation to track\n* @return CallableRaisingIOE with duration tracking\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:invokeTrackingDuration(org.apache.hadoop.fs.statistics.DurationTracker,org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Executes a callable while tracking duration; updates tracker on success or failure.\n* @param tracker DurationTracker to monitor execution time\n* @param input CallableRaisingIOE function to execute\n* @return Result of callable execution\n* @throws IOException if the callable raises an IOException\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationConsumer(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.ConsumerRaisingIOE)": "/**\n* Tracks execution duration of a consumer, handling exceptions.\n* @param factory optional tracker factory\n* @param statistic identifier for the statistic\n* @param input consumer to execute\n* @return wrapped consumer with duration tracking\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfCallable(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.concurrent.Callable)": "/**\n* Wraps a Callable to track execution duration and handle failures.\n* @param factory optional tracker factory, @param statistic name for tracking,\n* @param input callable to execute\n* @return Callable that tracks duration and reports failures\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry)": "/**\n* Converts a Map.Entry to a formatted string representation.\n* @param entry a key-value pair from a map\n* @return formatted string of the entry\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Wraps IOStatistics in a SourceWrappedStatistics instance.\n* @param statistics IOStatistics to be wrapped\n* @return SourceWrappedStatistics instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore()": "/**\n* Creates an empty IOStatisticsStore instance.\n* @return an instance of EmptyIOStatisticsStore\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics()": "/**\n* Creates and returns an empty IOStatistics instance.\n* @return IOStatistics instance representing empty statistics\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)": "/**\n* Creates a snapshot of the source map in a ConcurrentHashMap.\n* @param source source map to copy from\n* @param copyFn function to clone values\n* @return populated ConcurrentHashMap\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Tracks duration of an operation and returns its result.\n* @param factory optional duration tracker factory\n* @param statistic name for the tracked statistic\n* @param input callable operation to track\n* @return result of the tracked operation\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)": "/**\n* Creates a PairedDurationTrackerFactory from two duration tracker factories.\n* @param first first duration tracker factory\n* @param second second duration tracker factory\n* @return PairedDurationTrackerFactory instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Creates StorageStatistics from given name, scheme, and IOStatistics source.\n* @param name storage statistics name\n* @param scheme associated scheme\n* @param source IOStatistics for data collection\n* @return StorageStatistics object\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map)": "/**\n* Creates a snapshot of the source map using a passthrough function.\n* @param source the source map to copy from\n* @return a ConcurrentHashMap containing the snapshot\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)": "/**\n* Creates a DurationTracker using a factory or returns a stub if factory is null.\n* @param factory optional factory for creating trackers\n* @param statistic key for tracking duration\n* @return DurationTracker instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics)": "/**\n* Converts StorageStatistics to IOStatistics.\n* @param storageStatistics source of long statistics\n* @return constructed IOStatistics instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)": "/**\n* Measures duration of an invocation and tracks failures.\n* @param factory optional factory for creating trackers\n* @param statistic key for tracking duration\n* @param input function to invoke\n* @return Duration of the invocation\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)": "/**\n* Tracks duration of a supplier execution.\n* @param factory optional tracker factory\n* @param statistic key for tracking duration\n* @param input supplier to execute and track\n* @return result from the supplier\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Aggregates two MeanStatistic objects.\n* @param l left MeanStatistic to copy\n* @param r right MeanStatistic to add\n* @return aggregated MeanStatistic instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)": "/**\n* Tracks the duration of an invocation using a specified factory and statistic.\n* @param factory optional factory for creating trackers\n* @param statistic key for tracking duration\n* @param input function to invoke\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$Windows": {
        "org.apache.hadoop.io.nativeio.NativeIO$Windows:createFileOutputStreamWithMode(java.io.File,boolean,int)": "/**\n* Creates a FileOutputStream with specified append mode and access settings.\n* @param path file to write to\n* @param append true to append, false to overwrite\n* @param mode file access mode\n* @return FileOutputStream for the specified file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$Windows:createDirectoryWithMode(java.io.File,int)": "/**\n* Creates a directory at the specified path with the given mode.\n* @param path the directory path to create\n* @param mode the permissions mode for the new directory\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)": "/**\n* Checks access permissions for a given path.\n* @param path the file or directory path\n* @param desiredAccess the required access rights\n* @return true if access is granted, false otherwise\n*/"
    },
    "org.apache.hadoop.io.IOUtils": {
        "org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,java.io.Closeable[])": "/**\n* Closes provided Closeable resources and logs exceptions if they occur.\n* @param logger Logger for logging exceptions\n* @param closeables resources to be closed\n*/",
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int)": "/**\n* Copies bytes from InputStream to OutputStream using specified buffer size.\n* @param in InputStream source to read bytes from\n* @param out OutputStream destination to write bytes to\n* @param buffSize size of the buffer for byte copying\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IOUtils:readFully(java.io.InputStream,byte[],int,int)": "/**\n* Reads exactly len bytes from the InputStream into the buffer.\n* @param in InputStream to read from\n* @param buf byte array to store read data\n* @param off offset in buf to start storing data\n* @param len number of bytes to read\n* @throws IOException if EOF is reached before len bytes are read\n*/",
        "org.apache.hadoop.io.IOUtils:wrappedReadForCompressedData(java.io.InputStream,byte[],int,int)": "/**\n* Reads compressed data from an InputStream into a byte array buffer.\n* @param is input stream for reading data\n* @param buf byte array to store the read data\n* @param off offset in the buffer to start storing data\n* @param len maximum number of bytes to read\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.io.IOUtils:skipFully(java.io.InputStream,long)": "/**\n* Skips specified bytes in InputStream, throwing EOFException if premature EOF occurs.\n* @param in InputStream to skip bytes from\n* @param len number of bytes to skip\n*/",
        "org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket)": "/**\n* Closes the provided socket if it's not null, ignoring any IOExceptions.\n* @param sock the socket to be closed\n*/",
        "org.apache.hadoop.io.IOUtils:writeFully(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)": "/**\n* Writes entire ByteBuffer to WritableByteChannel.\n* @param bc the channel to write to\n* @param buf the ByteBuffer to be written\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IOUtils:writeFully(java.nio.channels.FileChannel,java.nio.ByteBuffer,long)": "/**\n* Writes entire ByteBuffer content to FileChannel at specified offset.\n* @param fc FileChannel to write to\n* @param buf ByteBuffer containing data to write\n* @param offset starting position in the FileChannel\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IOUtils:listDirectory(java.io.File,java.io.FilenameFilter)": "/**\n* Lists files in a directory filtered by a given criteria.\n* @param dir directory to list files from\n* @param filter criteria for filtering files\n* @return list of file names matching the filter\n*/",
        "org.apache.hadoop.io.IOUtils:fsync(java.nio.channels.FileChannel,boolean)": "/**\n* Forces changes to the file channel; ignores errors for directories.\n* @param channel the FileChannel to sync\n* @param isDir indicates if the channel is for a directory\n* @throws IOException if syncing fails for non-directory channels\n*/",
        "org.apache.hadoop.io.IOUtils:wrapWithMessage(java.io.IOException,java.lang.String)": "/**\n* Wraps an IOException with a custom message.\n* @param exception the original IOException\n* @param msg the custom message for the new exception\n* @return a new exception with the custom message\n*/",
        "org.apache.hadoop.io.IOUtils:readFullyToByteArray(java.io.DataInput)": "/**\n* Reads all bytes from DataInput into a byte array.\n* @param in DataInput source for byte reading\n* @return byte array containing all read bytes\n*/",
        "org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable)": "",
        "org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[])": "/**\n* Closes provided Closeable streams and logs exceptions if they occur.\n* @param streams variable number of Closeable resources to be closed\n*/",
        "org.apache.hadoop.io.IOUtils:fsync(java.io.File)": "/**\n* Syncs a file or directory to ensure changes are written to storage.\n* @param fileToSync the file or directory to sync\n* @throws IOException if file does not exist or syncing fails\n*/",
        "org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)": "/**\n* Wraps an IOException with a custom message based on path and method name.\n* @param path file path related to the error\n* @param methodName name of the method where the error occurred\n* @param exception original IOException to wrap\n* @return wrapped IOException with additional context\n*/",
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)": "/**\n* Copies bytes from InputStream to OutputStream and optionally closes them.\n* @param in InputStream source to read bytes from\n* @param out OutputStream destination to write bytes to\n* @param buffSize size of the buffer for byte copying\n* @param close whether to close streams after copying\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)": "/**\n* Copies bytes from input to output stream up to specified count.\n* @param in input stream, @param out output stream, @param count byte count, @param close close streams\n*/",
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)": "/**\n* Copies bytes from InputStream to OutputStream using buffer size from configuration.\n* @param in InputStream source to read bytes from\n* @param out OutputStream destination to write bytes to\n* @param conf Configuration object for buffer size\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Copies bytes from InputStream to OutputStream with buffer size from Configuration.\n* @param in InputStream source to read bytes from\n* @param out OutputStream destination to write bytes to\n* @param conf Configuration for buffer size\n* @param close whether to close streams after copying\n*/"
    },
    "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream": {
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:close()": "/**\n* Closes the file output stream and aggregates IO statistics.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:flush()": "/**\n* Flushes the output stream to ensure all data is written.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush()": "/**\n* Flushes the output stream to ensure all data is written.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync()": "/**\n* Synchronizes the output stream and flushes data to the file.\n* @throws IOException if an I/O error occurs during flushing or syncing\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)": "/**\n* Writes bytes to output stream and tracks write statistics.\n* @param b byte array to write, @param off start offset, @param len number of bytes to write\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int)": "/**\n* Writes a byte to the output stream and tracks write statistics.\n* @param b byte to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String)": "/**\n* Checks if the capability is supported.\n* @param capability capability string to validate\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore": {
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore:incrementCounter(java.lang.String)": "/**\n* Increments the counter for the given key by 1.\n* @param key unique identifier for the counter\n* @return updated counter value\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore:addSample(java.lang.String,long)": "/**\n* Adds statistical samples for a given key.\n* @param key identifier for the sample\n* @param count value to be added to statistics\n*/"
    },
    "org.apache.hadoop.fs.FSError": {
        "org.apache.hadoop.fs.FSError:<init>(java.lang.Throwable)": "/**\n* Constructs an FSError with the specified cause.\n* @param cause the Throwable that caused this error\n*/"
    },
    "org.apache.hadoop.fs.impl.StoreImplementationUtils": {
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:isProbeForSyncable(java.lang.String)": "/**\n* Checks if the capability is for syncable operations.\n* @param capability the capability string to check\n* @return true if capability is HSYNC or HFLUSH, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:<init>()": "/**\n* Private constructor to prevent instantiation of the utility class.\n*/",
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:objectHasCapability(java.lang.Object,java.lang.String)": "/**\n* Checks if the object has a specific capability.\n* @param object the object to check\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)": "/**\n* Checks if the OutputStream has a specific capability.\n* @param out the OutputStream to check\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)": "/**\n* Checks if the InputStream has a specific capability.\n* @param in the InputStream to check\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.FsUrlStreamHandler": {
        "org.apache.hadoop.fs.FsUrlStreamHandler:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FsUrlStreamHandler with the given configuration.\n* @param conf configuration settings for the handler\n*/",
        "org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL)": "/**\n* Opens a connection to the specified URL.\n* @param url the URL to connect to\n* @return FsUrlConnection instance\n*/",
        "org.apache.hadoop.fs.FsUrlStreamHandler:<init>()": "/**\n* Initializes FsUrlStreamHandler with a default Configuration instance.\n*/"
    },
    "org.apache.hadoop.fs.PartialListing": {
        "org.apache.hadoop.fs.PartialListing:toString()": "/**\n* Returns a string representation of the object with its attributes.\n* @return formatted string of listedPath, partialListing, and exception\n*/",
        "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)": "/**\n* Constructs a PartialListing with a path, listing, and optional exception.\n* @param listedPath the path of the listing\n* @param partialListing a list of items or null if an exception is present\n* @param exception an exception or null if a listing is provided\n*/",
        "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Constructs a PartialListing with a path and a list of items.\n* @param listedPath the path of the listing\n* @param partialListing a list of items or null\n*/",
        "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)": "/**\n* Constructs a PartialListing with a path and optional exception.\n* @param listedPath the path of the listing\n* @param exception an exception or null if a listing is provided\n*/",
        "org.apache.hadoop.fs.PartialListing:get()": "/**\n* Retrieves a list of items or throws an IOException if an error occurs.\n* @return List of items or throws IOException if an exception is present\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$5": {
        "org.apache.hadoop.fs.FileSystem$Statistics$5:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Constructs a deep copy of the given Statistics object.\n* @param other the Statistics instance to copy from\n*/"
    },
    "org.apache.hadoop.fs.StorageType": {
        "org.apache.hadoop.fs.StorageType:asList()": "/**\n* Returns a list of storage types.\n* @return List of StorageType instances\n*/",
        "org.apache.hadoop.fs.StorageType:getNonTransientTypes()": "/**\n* Retrieves a list of non-transient storage types.\n* @return List of non-transient StorageType objects\n*/",
        "org.apache.hadoop.fs.StorageType:getMovableTypes()": "",
        "org.apache.hadoop.fs.StorageType:getTypesSupportingQuota()": "/**\n* Retrieves storage types that support quotas.\n* @return List of StorageType objects supporting quotas\n*/",
        "org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String)": "",
        "org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)": "/**\n* Retrieves configuration value for a specific storage type.\n* @param conf configuration object, @param t storage type, @param name property name\n* @return property value or null if not found\n*/"
    },
    "org.apache.hadoop.fs.StreamCapabilitiesPolicy": {
        "org.apache.hadoop.fs.StreamCapabilitiesPolicy:unbuffer(java.io.InputStream)": "/**\n* Unbuffers the given InputStream if it supports unbuffering capability.\n* @param in the InputStream to unbuffer\n*/"
    },
    "org.apache.hadoop.fs.UnresolvedLinkException": {
        "org.apache.hadoop.fs.UnresolvedLinkException:<init>()": "/**\n* Constructs a new UnresolvedLinkException with no detail message.\n*/",
        "org.apache.hadoop.fs.UnresolvedLinkException:<init>(java.lang.String)": "/**\n* Constructs an UnresolvedLinkException with a specified message.\n* @param msg detail message explaining the exception\n*/"
    },
    "org.apache.hadoop.fs.DUHelper": {
        "org.apache.hadoop.fs.DUHelper:<init>()": "/**\n* Private constructor for DUHelper class to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.DUHelper:getFileSize(java.io.File)": "/**\n* Calculates total size of files in a directory.\n* @param folder directory to calculate size from\n* @return total size in bytes, or 0 if folder is empty\n*/",
        "org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String)": "/**\n* Calculates the total size of a specified folder.\n* @param folder the path to the folder\n* @return total size in bytes\n*/",
        "org.apache.hadoop.fs.DUHelper:check(java.lang.String)": "/**\n* Checks folder usage and returns a formatted usage string.\n* @param folder path to the directory\n* @return formatted string with file and disk usage statistics\n*/",
        "org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String)": "/**\n* Retrieves the total size of the specified folder.\n* @param folder the path to the folder\n* @return total size in bytes\n*/",
        "org.apache.hadoop.fs.DUHelper:main(java.lang.String[])": "/**\n* Prints folder usage based on OS type.\n* @param args command-line arguments; expects folder path as first element\n*/"
    },
    "org.apache.hadoop.fs.statistics.MeanStatistic": {
        "org.apache.hadoop.fs.statistics.MeanStatistic:<init>(long,long)": "/**\n* Initializes MeanStatistic with samples and sum if samples are positive.\n* @param samples number of samples\n* @param sum total sum of the samples\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:<init>()": "/**\n* Constructs a MeanStatistic object.\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:setSamples(long)": "/**\n* Sets the number of samples, ensuring non-negative value.\n* @param samples the number of samples to set\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:setSum(long)": "/**\n* Sets the sum value in a thread-safe manner.\n* @param sum the new sum to be set\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:getSamples()": "/**\n* Retrieves the current number of samples.\n* @return the number of samples as a long value\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:getSum()": "/**\n* Retrieves the current sum value.\n* @return the current sum as a long\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:isEmpty()": "/**\n* Checks if there are no samples.\n* @return true if samples count is zero, otherwise false\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:hashCode()": "/**\n* Computes the hash code based on sum and samples.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:mean()": "/**\n* Calculates the mean of samples.\n* @return mean value as double; returns 0.0 if no samples exist\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:addSample(long)": "/**\n* Adds a sample value, updating the count and sum of samples.\n* @param value the sample value to be added\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)": "/**\n* Sets the sample count and sum in a thread-safe manner.\n* @param sampleCount number of samples to set\n* @param newSum new sum value to set\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Adds another MeanStatistic to this one, updating samples and sum.\n* @param other MeanStatistic to add\n* @return updated MeanStatistic instance\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object)": "/**\n* Compares this MeanStatistic object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:toString()": "/**\n* Returns a string representation of samples, sum, and mean values.\n* @return formatted string with samples, sum, and mean\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:clear()": "/**\n* Resets sample count and sum to zero.\n* Calls setSamplesAndSum with zero values.\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Sets the current object's samples and sum from another MeanStatistic.\n* @param other MeanStatistic object to copy values from\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Clones a MeanStatistic object in a thread-safe manner.\n* @param that MeanStatistic object to copy values from\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:copy()": "/**\n* Creates a clone of the current MeanStatistic object.\n* @return a new MeanStatistic instance with copied values\n*/",
        "org.apache.hadoop.fs.statistics.MeanStatistic:clone()": "/**\n* Clones the current MeanStatistic object.\n* @return a new MeanStatistic instance with copied values\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString": {
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:<init>(org.apache.hadoop.fs.statistics.IOStatisticsSource)": "/**\n* Initializes SourceToString with an optional IOStatisticsSource.\n* @param source optional IOStatisticsSource object\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString()": "/**\n* Returns a string representation of the source IOStatistics.\n* @return formatted string or NULL_SOURCE if source is null\n*/"
    },
    "org.apache.hadoop.fs.statistics.StreamStatisticNames": {
        "org.apache.hadoop.fs.statistics.StreamStatisticNames:<init>()": "/**\n* Private constructor for StreamStatisticNames class.\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsLogging": {
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:<init>()": "/**\n* Private constructor for IOStatisticsLogging class; prevents instantiation.\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:sortedMap(java.util.Map,java.util.function.Predicate)": "/**\n* Creates a sorted map excluding entries based on a predicate.\n* @param source the original map to filter\n* @param isEmpty predicate to determine if a value is empty\n* @return a sorted map with non-empty values\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:demandStringifyIOStatisticsSource(org.apache.hadoop.fs.statistics.IOStatisticsSource)": "/**\n* Converts IOStatisticsSource to a string representation.\n* @param source the IOStatisticsSource to convert\n* @return string representation of the source\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:demandStringifyIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Converts IOStatistics to a string representation.\n* @param statistics IOStatistics object, can be null\n* @return String representation of the statistics\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)": "/**\n* Appends a formatted string representation of a map to StringBuilder.\n* @param sb StringBuilder to append to\n* @param type type description of the map\n* @param map the map to format and append\n* @param separator separator between entries\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Converts IOStatistics to a formatted string.\n* @param statistics IOStatistics object or null\n* @return formatted string representation of statistics or empty string if null\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)": "/**\n* Formats a sorted map into a string and appends it to StringBuilder.\n* @param sb StringBuilder to append to\n* @param type type description of the map\n* @param map the map to format\n* @param isEmpty predicate to filter empty values\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object)": "/**\n* Converts IOStatistics from source to a formatted string.\n* @param source object to extract IOStatistics from\n* @return formatted string or empty string on error\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Formats IOStatistics into a pretty string.\n* @param statistics IOStatistics object or null\n* @return formatted string representation of statistics\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)": "/**\n* Logs IO statistics at debug level if enabled.\n* @param log Logger instance for logging\n* @param message Message to log with statistics\n* @param source Object to extract IOStatistics from\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)": "/**\n* Logs IO statistics at debug level using provided message and source.\n* @param message Message to log with statistics\n* @param source Object to extract IOStatistics from\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)": "/**\n* Logs IOStatistics at specified log level based on source object.\n* @param log Logger instance for logging\n* @param level Desired log level (info, error, warn)\n* @param source Object to extract IOStatistics from\n*/"
    },
    "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream": {
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:<init>(java.io.OutputStream,boolean)": "/**\n* Constructs BufferedIOStatisticsOutputStream with specified OutputStream and sync option.\n* @param out OutputStream to wrap\n* @param downgradeSyncable if true, syncable behavior is downgraded\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:<init>(java.io.OutputStream,int,boolean)": "/**\n* Constructs a BufferedIOStatisticsOutputStream.\n* @param out OutputStream to wrap\n* @param size buffer size\n* @param downgradeSyncable flag for syncable downgrade\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hasCapability(java.lang.String)": "/**\n* Checks if the output stream supports a specified capability.\n* @param capability the capability to check for\n* @return true if capability is supported, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hflush()": "/**\n* Flushes the output stream and synchronizes if Syncable; throws exception if unsupported.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hsync()": "/**\n* Synchronizes output if it is Syncable; otherwise, throws exception or flushes.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics from the output source.\n* @return IOStatistics object or null if invalid\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsSupport": {
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:stubDurationTracker()": "/**\n* Returns a static instance of StubDurationTracker.\n* @return DurationTracker instance for tracking durations\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:<init>()": "/**\n* Private constructor for IOStatisticsSupport class to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:stubDurationTrackerFactory()": "/**\n* Returns the stub implementation of DurationTrackerFactory.\n* @return StubDurationTrackerFactory instance\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object)": "/**\n* Retrieves IOStatistics from source object or returns null if invalid.\n* @param source object to extract IOStatistics from\n* @return IOStatistics object or null if not applicable\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics()": "/**\n* Creates and returns a new IOStatisticsSnapshot instance.\n* @return a new IOStatisticsSnapshot object\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Creates a snapshot of IOStatistics.\n* @param statistics the IOStatistics to snapshot\n* @return a new IOStatisticsSnapshot object\n*/"
    },
    "org.apache.hadoop.fs.statistics.DurationStatisticSummary": {
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:toString()": "/**\n* Returns a string representation of DurationStatisticSummary object.\n* @return formatted string with key, success, count, max, and mean values\n*/",
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Initializes DurationStatisticSummary with key, success status, counts, and statistics.\n* @param key unique identifier for the statistic\n* @param success indicates if the operation was successful\n* @param count total occurrences\n* @param max maximum duration recorded\n* @param min minimum duration recorded\n* @param mean optional MeanStatistic for average duration\n*/",
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)": "/**\n* Fetches duration statistics summary based on key and success status.\n* @param source IOStatistics source for data retrieval\n* @param key unique statistic identifier\n* @param success indicates if the operation was successful\n* @return DurationStatisticSummary object with statistics\n*/",
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)": "/**\n* Retrieves success statistics summary.\n* @param source IOStatistics source for data retrieval\n* @param key unique statistic identifier\n* @return DurationStatisticSummary object with success statistics\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics": {
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:counters()": "/**\n* Retrieves IO statistics counters.\n* @return a map of counter names and their corresponding values\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:gauges()": "/**\n* Retrieves IO statistics as a map of gauge values.\n* @return Map of gauge names to their corresponding values\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:reset()": "/**\n* Resets the state of the object; does nothing in this implementation.\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Constructs StorageStatisticsFromIOStatistics with name, scheme, and IOStatistics.\n* @param name name of the storage statistics\n* @param scheme scheme associated with the statistics\n* @param ioStatistics IOStatistics object for data collection\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String)": "/**\n* Retrieves a Long value by key from counters or gauges.\n* @param key the identifier for the desired Long value\n* @return Long value or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String)": "",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry)": "/**\n* Converts a Map.Entry to a LongStatistic.\n* @param e entry containing a name and long value\n* @return LongStatistic constructed from the entry\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics()": "/**\n* Retrieves an iterator of LongStatistic from counters and gauges.\n* @return Iterator of LongStatistic objects\n*/",
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator()": "/**\n* Returns an iterator of LongStatistic from counters and gauges.\n* @return Iterator of LongStatistic objects\n*/"
    },
    "org.apache.hadoop.fs.StorageStatistics$LongStatistic": {
        "org.apache.hadoop.fs.StorageStatistics$LongStatistic:<init>(java.lang.String,long)": "/**\n* Constructs a LongStatistic with a name and value.\n* @param name the name of the statistic\n* @param value the long value of the statistic\n*/",
        "org.apache.hadoop.fs.StorageStatistics$LongStatistic:getName()": "/**\n* Returns the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.fs.StorageStatistics$LongStatistic:toString()": "/**\n* Returns a string representation of the object in \"name = value\" format.\n* @return formatted string of name and value\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.StubDurationTracker": {
        "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:<init>()": "/**\n* Private constructor for StubDurationTracker; prevents instantiation.\n*/",
        "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:close()": "/**\n* Closes the resource, releasing any associated system resources.\n*/",
        "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:failed()": "/**\n* Handles the failure scenario in the process.\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.StubDurationTrackerFactory": {
        "org.apache.hadoop.fs.statistics.impl.StubDurationTrackerFactory:<init>()": "/**\n* Private constructor for StubDurationTrackerFactory; prevents instantiation.\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap": {
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(java.util.function.Function)": "/**\n* Initializes EvaluatingStatisticsMap with a copy function.\n* @param copyFn function to create copies of elements\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:addFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a function to the evaluators map.\n* @param key unique identifier for the function\n* @param eval function to be associated with the key\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:size()": "/**\n* Returns the number of evaluators.\n* @return count of evaluators in the collection\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:isEmpty()": "/**\n* Checks if the evaluators list is empty.\n* @return true if evaluators is empty, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:containsKey(java.lang.Object)": "/**\n* Checks if the specified key exists in the evaluators map.\n* @param key the key to check for presence\n* @return true if the key exists, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:containsValue(java.lang.Object)": "/**\n* Checks if the collection contains the specified value.\n* @param value the value to search for\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:get(java.lang.Object)": "/**\n* Retrieves an element by key using a stored function.\n* @param key the identifier to look up the element\n* @return the element of type E or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:put(java.lang.String,java.io.Serializable)": "/**\n* Unsupported operation to put a key-value pair in the collection.\n* @param key the key to associate with the value\n* @param value the value to be stored\n* @throws UnsupportedOperationException always thrown\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:remove(java.lang.Object)": "/**\n* Removes the element associated with the specified key.\n* @param key the key of the element to remove\n* @throws UnsupportedOperationException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:putAll(java.util.Map)": "/**\n* Throws UnsupportedOperationException for putAll operation.\n* @param m map of key-value pairs to add\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:clear()": "/**\n* Clears the collection; operation is unsupported and throws an exception.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:keySet()": "/**\n* Retrieves a set of keys from the evaluators map.\n* @return Set of keys present in the evaluators map\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:values()": "/**\n* Applies evaluators to keys and returns a collection of results.\n* @return Collection of evaluated results of type E\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:entrySet()": "/**\n* Returns a set of entries mapping keys to evaluated values.\n* @return Set of entries with key-value pairs after evaluation\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>()": "/**\n* Initializes EvaluatingStatisticsMap with a passthrough function.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot()": "/**\n* Creates a snapshot of the current statistics.\n* @return a map containing the snapshot data\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics": {
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:counters()": "/**\n* Returns an unmodifiable map of counters.\n* @return unmodifiable map of counter names and their values\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:gauges()": "/**\n* Returns an unmodifiable map of gauges.\n* @return Map of gauge names and their values as Long\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:minimums()": "/**\n* Returns an unmodifiable view of the minimums map.\n* @return unmodifiable map of minimum values\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:maximums()": "/**\n* Returns an unmodifiable view of the maximums map.\n* @return a Map of maximum values as key-value pairs\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:meanStatistics()": "/**\n* Returns an unmodifiable view of mean statistics.\n* @return Map of mean statistics with string keys and MeanStatistic values\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a counter function to the counters map.\n* @param key unique identifier for the function\n* @param eval function to be associated with the key\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a gauge function to the evaluators map.\n* @param key unique identifier for the gauge function\n* @param eval function to evaluate the gauge\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a minimum function to the minimums evaluator.\n* @param key unique identifier for the function\n* @param eval function to be associated with the key\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a maximum function to the evaluators map.\n* @param key unique identifier for the function\n* @param eval function to evaluate maximum values\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a mean statistic function to the meanStatistics map.\n* @param key unique identifier for the function\n* @param eval function to be associated with the key\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>()": "/**\n* Initializes a new instance of DynamicIOStatistics.\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.SourceWrappedStatistics": {
        "org.apache.hadoop.fs.statistics.impl.SourceWrappedStatistics:<init>(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Initializes SourceWrappedStatistics with provided IOStatistics source.\n* @param source IOStatistics instance to wrap\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl": {
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:<init>()": "/**\n* Constructs an instance of EmptyIOStatisticsContextImpl.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getInstance()": "/**\n* Retrieves the singleton instance of IOStatisticsContext.\n* @return the EMPTY_CONTEXT instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getID()": "/**\n* Returns the ID as a long value.\n* @return the ID, currently always returns 0\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:reset()": "/**\n* Resets the state of the object to its initial configuration.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator()": "/**\n* Retrieves the IOStatisticsAggregator instance.\n* @return IOStatisticsAggregator from EmptyIOStatisticsStore\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics()": "/**\n* Retrieves IOStatistics instance, always returning EmptyIOStatistics.\n* @return IOStatistics instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot()": "/**\n* Creates and returns a new IOStatisticsSnapshot instance.\n* @return a new IOStatisticsSnapshot object\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore": {
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getInstance()": "/**\n* Returns the singleton instance of IOStatisticsStore.\n* @return the unique IOStatisticsStore instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:<init>()": "/**\n* Private constructor for EmptyIOStatisticsStore to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:counters()": "/**\n* Returns an empty map of counters.\n* @return an empty Map with String keys and Long values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:gauges()": "/**\n* Returns an empty map of gauge metrics.\n* @return Map with gauge names and their values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:minimums()": "/**\n* Returns an empty map of minimum values.\n* @return an empty Map with String keys and Long values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:maximums()": "/**\n* Returns an empty map of maximum values.\n* @return an empty Map with String keys and Long values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:meanStatistics()": "/**\n* Returns an empty map of mean statistics.\n* @return a Map with String keys and MeanStatistic values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMaximumSample(java.lang.String,long)": "/**\n* Adds a maximum sample value associated with a specified key.\n* @param key identifier for the sample\n* @param value the maximum sample value to add\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)": "/**\n* Adds a sample value for the specified mean statistic key.\n* @param key unique identifier for the statistic\n* @param value sample value to add\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMinimumSample(java.lang.String,long)": "/**\n* Adds a minimum sample with the specified key and value.\n* @param key unique identifier for the sample\n* @param value the minimum sample value to add\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)": "/**\n* Adds a timed operation with a specified prefix and duration.\n* @param prefix operation identifier\n* @param duration time duration for the operation\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addTimedOperation(java.lang.String,long)": "/**\n* Adds a timed operation with a specified prefix.\n* @param prefix operation identifier prefix\n* @param durationMillis duration of the operation in milliseconds\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Aggregates statistics and returns success status.\n* @param statistics optional IOStatistics to aggregate\n* @return false as aggregation is not implemented\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getCounterReference(java.lang.String)": "/**\n* Retrieves a counter reference by its key.\n* @param key unique identifier for the counter\n* @return AtomicLong reference or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getGaugeReference(java.lang.String)": "/**\n* Retrieves the gauge reference for the specified key.\n* @param key identifier for the gauge\n* @return AtomicLong reference or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMaximumReference(java.lang.String)": "/**\n* Retrieves the maximum reference value associated with the given key.\n* @param key identifier for the reference\n* @return AtomicLong representing the maximum reference or null\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMeanStatistic(java.lang.String)": "/**\n* Retrieves mean statistic for the given key.\n* @param key identifier for the statistic\n* @return MeanStatistic object or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMinimumReference(java.lang.String)": "/**\n* Retrieves the minimum reference value associated with the given key.\n* @param key identifier for the reference\n* @return AtomicLong representing the minimum reference, or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementCounter(java.lang.String,long)": "/**\n* Increments the counter for the specified key by the given value.\n* @param key unique identifier for the counter\n* @param value amount to increment the counter by\n* @return the updated counter value\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementGauge(java.lang.String,long)": "/**\n* Increments the gauge value associated with the given key.\n* @param key the identifier for the gauge\n* @param value the amount to increment the gauge by\n* @return the new gauge value after increment\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMaximum(java.lang.String,long)": "/**\n* Increments the maximum value associated with the given key.\n* @param key identifier for the value to increment\n* @param value amount to increment the maximum by\n* @return updated maximum value\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMinimum(java.lang.String,long)": "/**\n* Increases the minimum value associated with a key.\n* @param key unique identifier for the value\n* @param value amount to increment\n* @return new minimum value after increment\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:reset()": "/**\n* Resets the state of the object to its initial configuration.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setCounter(java.lang.String,long)": "/**\n* Sets the counter value for a specified key.\n* @param key unique identifier for the counter\n* @param value new value to set for the counter\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setGauge(java.lang.String,long)": "/**\n* Sets the gauge value for the specified key.\n* @param key unique identifier for the gauge\n* @param value the value to set for the gauge\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMaximum(java.lang.String,long)": "/**\n* Sets the maximum value for the specified key.\n* @param key the identifier for the maximum value\n* @param value the maximum value to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Sets the mean statistic for the given key.\n* @param key unique identifier for the statistic\n* @param value MeanStatistic object to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMinimum(java.lang.String,long)": "/**\n* Sets the minimum value for the specified key.\n* @param key the identifier for the value\n* @param value the minimum value to set\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics": {
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:getInstance()": "/**\n* Retrieves the singleton instance of IOStatistics.\n* @return the singleton IOStatistics instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:<init>()": "/**\n* Private constructor for EmptyIOStatistics class.\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:counters()": "/**\n* Returns an empty map of counters.\n* @return a Map with String keys and Long values, currently empty\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:gauges()": "/**\n* Returns an empty map of gauges.\n* @return Map with gauge names and their values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:minimums()": "/**\n* Returns an empty map of minimum values.\n* @return an empty Map with String keys and Long values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:maximums()": "/**\n* Returns an empty map of maximum values.\n* @return an empty Map with String keys and Long values\n*/",
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:meanStatistics()": "/**\n* Returns an empty map of mean statistics.\n* @return Map with String keys and MeanStatistic values\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics": {
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:<init>(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Initializes WrappedIOStatistics with the provided IOStatistics object.\n* @param wrapped the IOStatistics to be wrapped\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:<init>()": "/**\n* Constructor for WrappedIOStatistics class.\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:getWrapped()": "/**\n* Retrieves the wrapped IOStatistics object.\n* @return the wrapped IOStatistics instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters()": "/**\n* Retrieves a map of counters from the wrapped IOStatistics object.\n* @return Map of counter names and their corresponding values\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges()": "/**\n* Returns a map of gauge values.\n* @return Map with gauge names as keys and their values as Longs\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums()": "/**\n* Retrieves minimum values from the wrapped IOStatistics.\n* @return Map of minimum values with their corresponding keys\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums()": "/**\n* Returns a map of maximum values from the wrapped IOStatistics.\n* @return Map with maximum values by key\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics()": "/**\n* Returns mean statistics from the wrapped IOStatistics.\n* @return Map of mean statistics keyed by their names\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Sets the wrapped IOStatistics if none exists; throws exception if already set.\n* @param wrapped the IOStatistics to be wrapped\n*/",
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString()": "/**\n* Converts wrapped IOStatistics to a formatted string representation.\n* @return formatted string or empty if wrapped is null\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl": {
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setAtomicLong(java.util.concurrent.atomic.AtomicLong,long)": "/**\n* Sets the value of an AtomicLong if it's not null.\n* @param aLong the AtomicLong to be updated\n* @param value the new value to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incAtomicLong(java.util.concurrent.atomic.AtomicLong,long)": "/**\n* Increments an AtomicLong by a specified value.\n* @param aLong AtomicLong to increment\n* @param increment value to add; returns current value if zero\n* @return updated AtomicLong value or 0 if null\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:reset()": "/**\n* Resets all metrics in the maps to their initial values.\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Aggregates IOStatistics data into current statistics.\n* @param source source IOStatistics to aggregate\n* @return true if aggregation was successful\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:lookup(java.util.Map,java.lang.String)": "/**\n* Retrieves value from map by key, ensuring it's not null.\n* @param map a map containing key-value pairs\n* @param key the key to look up in the map\n* @return the value associated with the key\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:lookupQuietly(java.util.Map,java.lang.String)": "/**\n* Retrieves value from map by key without throwing exceptions.\n* @param map the source map for lookup\n* @param key the key to search for\n* @return value associated with key or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)": "/**\n* Updates the counter for a given key and logs the action.\n* @param key the identifier for the counter\n* @param value the new value to set for the counter\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)": "/**\n* Updates the maximum value for a given key.\n* @param key identifier for the maximum value\n* @param value new maximum value to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)": "",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)": "",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)": "/**\n* Increments a counter by a specified value if valid.\n* @param key unique identifier for the counter\n* @param value amount to increment; returns old value if negative\n* @return updated counter value or 0 if counter not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)": "/**\n* Increments the maximum value for a given key.\n* @param key identifier for the maximum value\n* @param value amount to increment\n* @return updated maximum value or 0 if key is not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)": "/**\n* Increments the minimum value for a given key.\n* @param key the identifier for the minimum value\n* @param value the amount to increment by\n* @return updated minimum value or 0 if key not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)": "/**\n* Increments the gauge value associated with a key.\n* @param key identifier for the gauge\n* @param value amount to increment the gauge by\n* @return updated gauge value or 0 if key is not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)": "",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)": "/**\n* Updates maximum sample value for a given key if it exists.\n* @param key identifier for the maximum value\n* @param value new sample value to compare\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)": "/**\n* Adds a sample value to the MeanStatistic identified by the key.\n* @param key unique identifier for the MeanStatistic\n* @param value the sample value to be added\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String)": "/**\n* Retrieves the counter reference for a given key.\n* @param key the key to look up in the counter map\n* @return AtomicLong associated with the key or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String)": "/**\n* Retrieves maximum reference value by key.\n* @param key the key to look up in the maximumMap\n* @return AtomicLong value associated with the key\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String)": "/**\n* Retrieves minimum reference value by key.\n* @param key the key to look up in the minimumMap\n* @return AtomicLong value associated with the key or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String)": "/**\n* Retrieves a gauge reference by key.\n* @param key the key to look up in the gauge map\n* @return AtomicLong associated with the key or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String)": "/**\n* Retrieves MeanStatistic by key from the meanStatisticMap.\n* @param key the key to look up in the map\n* @return MeanStatistic object or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)": "/**\n* Records operation duration statistics: mean, min, and max.\n* @param prefix identifier for the statistic group\n* @param durationMillis duration of the operation in milliseconds\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Updates MeanStatistic in the map by key if it exists.\n* @param key identifier for the MeanStatistic\n* @param value new MeanStatistic values to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)": "/**\n* Records operation duration statistics using a prefix and Duration object.\n* @param prefix identifier for the statistic group\n* @param duration operation duration\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)": "/**\n* Tracks duration based on key; returns StatisticDurationTracker or stub instance.\n* @param key unique identifier for tracking duration\n* @param count value to increment the duration counter\n* @return DurationTracker instance for the specified key\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)": "/**\n* Initializes IOStatisticsStore with dynamic counters, gauges, min/max values, and mean statistics.\n* @param counters list of counter names\n* @param gauges list of gauge names\n* @param minimums list of minimum value names\n* @param maximums list of maximum value names\n* @param meanStatistics list of mean statistic names\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl": {
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl:<init>(java.lang.String,java.lang.Object)": "/**\n* Constructs an EntryImpl with a key and value.\n* @param key the entry's key\n* @param value the entry's value\n*/",
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl:getKey()": "/**\n* Retrieves the key associated with the object.\n* @return the key as a String\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker": {
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:<init>(org.apache.hadoop.fs.statistics.DurationTracker,org.apache.hadoop.fs.statistics.DurationTracker)": "/**\n* Initializes a PairedDurationTracker with two DurationTracker instances.\n* @param firstDuration the first DurationTracker\n* @param secondDuration the second DurationTracker\n*/",
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:failed()": "/**\n* Marks both durations as failed.\n*/",
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:close()": "/**\n* Closes both duration resources.\n*/",
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:toString()": "/**\n* Returns the string representation of the firstDuration object.\n* @return String representation of firstDuration\n*/",
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration()": "/**\n* Returns the duration represented by firstDuration.\n* @return Duration object from firstDuration\n*/"
    },
    "org.apache.hadoop.fs.statistics.DurationTracker": {
        "org.apache.hadoop.fs.statistics.DurationTracker:asDuration()": "/**\n* Returns a Duration object representing zero duration.\n* @return Duration.ZERO, indicating no time elapsed\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory": {
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:<init>(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)": "/**\n* Constructs a PairedDurationTracker with local and global duration trackers.\n* @param local  local duration tracker factory\n* @param global global duration tracker factory\n*/",
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)": "/**\n* Tracks duration using global and local sources.\n* @param key identifier for duration tracking\n* @param count number of occurrences to track\n* @return DurationTracker instance for tracking durations\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration": {
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:<init>()": "/**\n* Private constructor for IOStatisticsContextIntegration class.\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:referenceLostContext(java.lang.Long)": "/**\n* Logs a debug message indicating context loss for the given thread ID.\n* @param key the thread ID whose context is lost\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getThreadSpecificIOStatisticsContext(long)": "/**\n* Retrieves IOStatisticsContext for a specific thread ID.\n* @param testThreadId identifier for the thread\n* @return IOStatisticsContext or null if not enabled or found\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:enableIOStatisticsContext()": "/**\n* Enables IO statistics for the current thread if not already enabled.\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:isIOStatisticsThreadLevelEnabled()": "/**\n* Checks if thread-level I/O statistics are enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext()": "/**\n* Retrieves the current IOStatisticsContext based on thread settings.\n* @return IOStatisticsContext for current thread or EMPTY_CONTEXT if disabled\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext)": "/**\n* Sets the IOStatisticsContext for the current thread or removes it if null.\n* @param statisticsContext context to set or null to remove\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long)": "/**\n* Creates a new IOStatisticsContext instance.\n* @param key unique identifier for the context\n* @return newly created IOStatisticsContext instance\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableCounterLong": {
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:value()": "/**\n* Returns the long value of the instance variable.\n* @return long representation of the value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(long)": "/**\n* Increments the value by delta and marks the object as changed.\n* @param delta amount to add to the current value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Records a snapshot of metrics if changed or all is true.\n* @param builder metrics record builder\n* @param all flag to include all metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:incr()": "/**\n* Increments the value by 1 and marks the object as changed.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Initializes MutableCounterLong with metrics info and an initial value.\n* @param info metrics information, must not be null\n* @param initValue initial counter value\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore": {
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore)": "/**\n* Initializes ForwardingIOStatisticsStore with inner IOStatisticsStore.\n* @param innerStatistics the IOStatisticsStore to forward statistics from\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getInnerStatistics()": "/**\n* Retrieves the inner statistics store.\n* @return IOStatisticsStore instance containing statistics\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters()": "",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges()": "/**\n* Retrieves gauge metrics from inner statistics.\n* @return Map of gauge names to their values\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums()": "/**\n* Retrieves minimum statistics from the inner statistics store.\n* @return Map of minimum values with their corresponding keys\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums()": "/**\n* Retrieves maximum values from inner statistics.\n* @return Map of maximum values with their corresponding keys\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics()": "",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)": "",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)": "/**\n* Increments a counter by a specified value for a given key.\n* @param key identifier for the counter\n* @param value amount to increment the counter by\n* @return updated counter value\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)": "/**\n* Sets a counter value for the specified key.\n* @param key the counter's unique identifier\n* @param value the value to set for the counter\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)": "/**\n* Sets a gauge value for the specified key.\n* @param key the gauge identifier\n* @param value the gauge value to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)": "/**\n* Increments the gauge value for a given key.\n* @param key identifier for the gauge\n* @param value amount to increment\n* @return updated gauge value\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)": "/**\n* Sets the maximum value for a given key.\n* @param key the identifier for the value\n* @param value the maximum value to set\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)": "/**\n* Increments the maximum value for a given key.\n* @param key the identifier for the value to increment\n* @param value the amount to add to the maximum\n* @return updated maximum value\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)": "/**\n* Sets the minimum value for a specified key.\n* @param key the identifier for the value\n* @param value the minimum value to be set\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)": "/**\n* Increments the minimum value for a given key.\n* @param key the key to update\n* @param value the value to increment\n* @return updated minimum value\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)": "/**\n* Adds a minimum sample for the specified key and value.\n* @param key unique identifier for the sample\n* @param value the minimum sample value to add\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)": "/**\n* Adds a maximum sample for the specified key.\n* @param key the identifier for the sample\n* @param value the maximum value to be recorded\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Sets the mean statistic for a given key.\n* @param key unique identifier for the statistic\n* @param value MeanStatistic object to be set\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)": "/**\n* Adds a mean statistic sample for the given key and value.\n* @param key identifier for the statistic sample\n* @param value the sample value to be added\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset()": "/**\n* Resets the inner statistics store.\n* Calls getInnerStatistics() to access the statistics.\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String)": "/**\n* Retrieves a counter reference by key from inner statistics.\n* @param key identifier for the counter\n* @return AtomicLong representing the counter value\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String)": "/**\n* Retrieves the maximum reference count for a given key.\n* @param key the identifier for which the maximum reference is requested\n* @return AtomicLong representing the maximum reference count\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String)": "/**\n* Retrieves the minimum reference count for a given key.\n* @param key the identifier for the reference\n* @return AtomicLong representing the minimum reference count\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String)": "/**\n* Retrieves the gauge reference for a given key.\n* @param key the identifier for the gauge\n* @return AtomicLong representing the gauge value\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String)": "/**\n* Retrieves mean statistic for a given key.\n* @param key identifier for the statistic\n* @return MeanStatistic object or null if not found\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)": "/**\n* Adds a timed operation to the statistics.\n* @param prefix operation identifier\n* @param durationMillis duration of the operation in milliseconds\n*/",
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)": "/**\n* Adds a timed operation with a prefix and duration.\n* @param prefix operation identifier\n* @param duration time taken for the operation\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl": {
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withCounters(java.lang.String[])": "/**\n* Adds counter keys to the builder and returns the updated instance.\n* @param keys variable number of counter keys\n* @return updated IOStatisticsStoreBuilderImpl instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withGauges(java.lang.String[])": "/**\n* Adds gauge keys to the statistics store builder.\n* @param keys variable number of gauge keys\n* @return this instance of IOStatisticsStoreBuilderImpl\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMaximums(java.lang.String[])": "/**\n* Adds keys to the maximums list and returns the builder instance.\n* @param keys variable number of maximum keys\n* @return this builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMinimums(java.lang.String[])": "/**\n* Adds keys to the minimums list and returns the builder instance.\n* @param keys variable number of key strings to add\n* @return this builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMeanStatistics(java.lang.String[])": "/**\n* Adds keys to mean statistics and returns the builder instance.\n* @param keys variable-length array of keys to add\n* @return this builder instance for chaining\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[])": "/**\n* Tracks duration statistics for given prefixes.\n* @param prefixes variable-length array of prefix strings\n* @return updated IOStatisticsStoreBuilderImpl instance for chaining\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[])": "/**\n* Tracks samples using provided prefixes and updates the statistics builder.\n* @param prefixes variable-length array of prefix strings\n* @return updated IOStatisticsStoreBuilderImpl instance for chaining\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build()": "/**\n* Builds an IOStatisticsStore instance with predefined statistics lists.\n* @return IOStatisticsStore object initialized with counters, gauges, etc.\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl": {
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:toString()": "/**\n* Returns a string representation of the IOStatisticsContextImpl object.\n* @return formatted string with id, threadId, and ioStatistics values\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getAggregator()": "/**\n* Retrieves the IOStatisticsAggregator instance.\n* @return IOStatisticsAggregator object\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getID()": "/**\n* Retrieves the unique identifier of the object.\n* @return the object's unique ID as a long value\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset()": "/**\n* Resets IOStatisticsContext by clearing its statistical data.\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)": "/**\n* Initializes IOStatisticsContextImpl with thread and context IDs.\n* @param threadId identifier for the thread\n* @param id unique context identifier\n*/",
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot()": "/**\n* Takes a snapshot of IOStatisticsContext.\n* @return IOStatisticsSnapshot containing current statistics\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot": {
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:clear()": "/**\n* Clears all statistical data from counters and gauges.\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:createMaps()": "/**\n* Initializes concurrent maps for counters, gauges, and statistics.\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:counters()": "/**\n* Retrieves the current counters map.\n* @return Map of counter names to their values\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:gauges()": "/**\n* Returns a synchronized map of gauge metrics.\n* @return Map of gauge names and their corresponding values\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:maximums()": "/**\n* Retrieves the maximum values map.\n* @return Map of maximum values with their corresponding keys\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:minimums()": "/**\n* Returns a synchronized map of minimum values.\n* @return Map of minimums with String keys and Long values\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:meanStatistics()": "/**\n* Returns the mean statistics map.\n* @return Map of String keys to MeanStatistic values\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:writeObject(java.io.ObjectOutputStream)": "/**\n* Serializes the object state to the specified ObjectOutputStream.\n* @param s the ObjectOutputStream to write the object state to\n* @throws IOException if an I/O error occurs during serialization\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:readObject(java.io.ObjectInputStream)": "/**\n* Deserializes object state and rebuilds concurrent hash maps from serialized tree maps.\n* @param s input stream for reading serialized object data\n* @throws IOException if an I/O error occurs during deserialization\n* @throws ClassNotFoundException if a class cannot be found during deserialization\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:requiredSerializationClasses()": "/**\n* Returns a list of classes required for serialization.\n* @return List of required serialization classes\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>()": "/**\n* Initializes IOStatisticsSnapshot by creating necessary concurrent maps.\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)": "/**\n* Sets the counter value for a given key.\n* @param key the counter name\n* @param value the value to set for the counter\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)": "/**\n* Sets a gauge metric value by key.\n* @param key the name of the gauge\n* @param value the value to set for the gauge\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)": "/**\n* Sets a maximum value for the specified key.\n* @param key the identifier for the maximum value\n* @param value the maximum value to be set\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)": "/**\n* Sets a minimum value for the specified key.\n* @param key the identifier for the minimum value\n* @param value the minimum value to set\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)": "/**\n* Sets a mean statistic by key.\n* @param key the statistic identifier\n* @param value the MeanStatistic to associate with the key\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer()": "/**\n* Creates a JsonSerialization for IOStatisticsSnapshot.\n* @return JsonSerialization instance for IOStatisticsSnapshot type\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString()": "/**\n* Returns a string representation of the current object using IOStatistics.\n* @return formatted string of IOStatistics or empty string if null\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Creates a snapshot of IOStatistics, copying its counters, gauges, and statistics.\n* @param source the IOStatistics object to snapshot\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Aggregates statistics from the given IOStatistics source.\n* @param source IOStatistics to aggregate data from\n* @return true if aggregation was successful, false if source is null\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Initializes IOStatisticsSnapshot from source or creates maps if source is null.\n* @param source the IOStatistics object for snapshotting\n*/"
    },
    "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream": {
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:<init>(java.io.InputStream)": "/**\n* Initializes BufferedIOStatisticsInputStream with the given InputStream.\n* @param in the InputStream to wrap\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:<init>(java.io.InputStream,int)": "/**\n* Constructs a BufferedIOStatisticsInputStream with specified input stream and buffer size.\n* @param in the input stream to be buffered\n* @param size the size of the buffer\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:hasCapability(java.lang.String)": "/**\n* Checks if the input stream has the specified capability.\n* @param capability name of the capability to check\n* @return true if capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics using the provided input object.\n* @return IOStatistics object or null if invalid input\n*/"
    },
    "org.apache.hadoop.fs.statistics.FileSystemStatisticNames": {
        "org.apache.hadoop.fs.statistics.FileSystemStatisticNames:<init>()": "/**\n* Private constructor for FileSystemStatisticNames class.\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString": {
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:<init>(org.apache.hadoop.fs.statistics.IOStatistics)": "/**\n* Initializes StatisticsToString with optional IOStatistics.\n* @param statistics optional IOStatistics object, can be null\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString()": "/**\n* Returns a string representation of IO statistics or a null source indicator.\n* @return formatted string or NULL_SOURCE if statistics is null\n*/"
    },
    "org.apache.hadoop.fs.statistics.StoreStatisticNames": {
        "org.apache.hadoop.fs.statistics.StoreStatisticNames:<init>()": "/**\n* Private constructor for StoreStatisticNames class to prevent instantiation.\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsSource": {
        "org.apache.hadoop.fs.statistics.IOStatisticsSource:getIOStatistics()": "/**\n* Retrieves IO statistics.\n* @return IOStatistics object or null if not available\n*/"
    },
    "org.apache.hadoop.fs.FSInputChecker": {
        "org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int)": "/**\n* Initializes FSInputChecker with file path and retry count.\n* @param file path to the input file\n* @param numOfRetries number of retry attempts\n*/",
        "org.apache.hadoop.fs.FSInputChecker:available()": "/**\n* Returns the number of bytes available to read.\n* @return number of available bytes, non-negative\n*/",
        "org.apache.hadoop.fs.FSInputChecker:set(boolean,java.util.zip.Checksum,int,int)": "/**\n* Initializes settings for checksum verification and buffer size.\n* @param verifyChecksum flag to enable checksum verification\n* @param sum checksum object for validation\n* @param maxChunkSize maximum size of data chunks\n* @param checksumSize size of the checksum in bytes\n*/",
        "org.apache.hadoop.fs.FSInputChecker:needChecksum()": "/**\n* Determines if a checksum is needed based on verification and sum presence.\n* @return true if checksum is needed, false otherwise\n*/",
        "org.apache.hadoop.fs.FSInputChecker:getPos()": "/**\n* Calculates the current position in the data stream.\n* @return current position as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSInputChecker:resetState()": "/**\n* Resets internal state by clearing count, pos, and checksum.\n*/",
        "org.apache.hadoop.fs.FSInputChecker:readFully(java.io.InputStream,byte[],int,int)": "/**\n* Reads bytes from InputStream into buffer.\n* @param stm InputStream to read from\n* @param buf byte array to store read bytes\n* @param offset starting position in buf\n* @param len maximum number of bytes to read\n* @return total bytes read or -1 on end of stream\n*/",
        "org.apache.hadoop.fs.FSInputChecker:reset()": "/**\n* Resets the stream, throwing an exception as mark/reset is unsupported.\n* @throws IOException if mark/reset is not supported\n*/",
        "org.apache.hadoop.fs.FSInputChecker:mark(int)": "/**\n* Marks the current position in the input stream.\n* @param readlimit maximum number of bytes to read before the mark is invalidated\n*/",
        "org.apache.hadoop.fs.FSInputChecker:markSupported()": "/**\n* Indicates if mark/reset operations are supported.\n* @return false, as mark/reset is not supported\n*/",
        "org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)": "/**\n* Verifies byte sums against expected checksums; throws ChecksumException on mismatch.\n* @param b byte array to verify\n* @param off offset in the byte array\n* @param read number of bytes to read from the array\n*/",
        "org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)": "/**\n* Constructs FSInputChecker with file path and retry settings.\n* @param file path to the input file\n* @param numOfRetries number of retry attempts\n* @param verifyChecksum flag to enable checksum verification\n* @param sum checksum object for validation\n* @param chunkSize maximum size of data chunks\n* @param checksumSize size of the checksum in bytes\n*/",
        "org.apache.hadoop.fs.FSInputChecker:seek(long)": "/**\n* Seeks to a specified position in the stream.\n* @param pos desired position to seek to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSInputChecker:skip(long)": "/**\n* Skips specified number of bytes in the stream.\n* @param n number of bytes to skip; must be positive\n* @return number of bytes skipped\n*/",
        "org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)": "/**\n* Reads a chunk of data and verifies its checksum.\n* @param b byte array to store read data\n* @param off offset in the byte array\n* @param len number of bytes to read\n* @return number of bytes read\n*/",
        "org.apache.hadoop.fs.FSInputChecker:fill()": "/**\n* Fills the internal buffer by reading data with checksum verification.\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int)": "/**** Reads and discards specified bytes; returns total bytes read. \n* @param len number of bytes to read\n* @return total bytes read\n*/",
        "org.apache.hadoop.fs.FSInputChecker:read()": "/**\n* Reads a byte from the buffer, filling it if empty.\n* @return byte value or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)": "/**\n* Reads data into a buffer, filling from a chunk if necessary.\n* @param b destination byte array, @param off start offset, @param len bytes to read\n* @return number of bytes read or -1 if no data available\n*/",
        "org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)": "/**\n * Reads data into a buffer from the input stream.\n * @param b destination byte array, @param off start offset, @param len bytes to read\n * @return number of bytes read or -1 if no data available\n */"
    },
    "org.apache.hadoop.fs.ChecksumFs": {
        "org.apache.hadoop.fs.ChecksumFs:getRawFs()": "/**\n* Retrieves the raw file system instance.\n* @return AbstractFileSystem object representing the file system\n*/",
        "org.apache.hadoop.fs.ChecksumFs:getBytesPerSum()": "/**\n* Retrieves the number of bytes used per checksum.\n* @return int representing bytes per checksum\n*/",
        "org.apache.hadoop.fs.ChecksumFs:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)": "/**\n* Reports checksum failure for a file.\n* @param f file path to check\n* @param in input stream of the file\n* @param inPos position in the input stream\n* @param sums input stream of checksums\n* @param sumsPos position in the checksums stream\n* @return false indicating failure reporting not implemented\n*/",
        "org.apache.hadoop.fs.ChecksumFs:getChecksumLength(long,int)": "/**\n* Calculates the checksum length based on size and bytes per sum.\n* @param size total size in bytes\n* @param bytesPerSum number of bytes per checksum\n* @return calculated checksum length\n*/",
        "org.apache.hadoop.fs.ChecksumFs:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Throws an exception as truncation is unsupported in ChecksumFs.\n* @param f the file path to truncate\n* @param newLength the desired new length of the file\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.ChecksumFs:exists(org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists at the specified path.\n* @param f the path to the file\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFs:setVerifyChecksum(boolean)": "/**\n* Sets the checksum verification flag.\n* @param inVerifyChecksum true to enable, false to disable checksum verification\n*/",
        "org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path)": "/**\n* Checks if the given path is a directory.\n* @param f the file path to check\n* @return true if it's a directory, false if not or if not found\n*/",
        "org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)": "/**\n* Calculates the maximum buffer size based on input parameters.\n* @param bytesPerSum bytes to sum; @param bufferSize current buffer size; @return max buffer size\n*/",
        "org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)": "/**\n* Computes checksum file length using file size.\n* @param file Path to the file\n* @param fileSize total size of the file in bytes\n* @return calculated checksum file length\n*/",
        "org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses for a given path, excluding checksum files.\n* @param f the path to list file statuses\n* @return an iterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists non-checksum file statuses at the given path.\n* @param f directory path to list file statuses\n* @return array of FileStatus excluding checksum files\n*/",
        "org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem)": "/**\n* Initializes ChecksumFs with default bytes per checksum from server settings.\n* @param theFs the AbstractFileSystem instance\n* @throws IOException if an I/O error occurs\n* @throws URISyntaxException if the URI is invalid\n*/",
        "org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path)": "/**** Generates a checksum file path for the given file. \n* @param file the original file's Path\n* @return Path for the checksum file\n*/",
        "org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets replication factor for a file and its checksum if it exists.\n* @param src file path to update replication for\n* @param replication new replication factor\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory, optionally recursively.\n* @param f the Path of the file or directory to delete\n* @param recursive whether to delete directories recursively\n* @return true if deletion was successful, false if not found\n*/",
        "org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file as FSDataInputStream with specified buffer size.\n* @param f file path to open\n* @param bufferSize size of the buffer for reading\n* @return FSDataInputStream for the file\n*/",
        "org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Creates an FSDataOutputStream for writing to a specified path.\n* @param f Path to the file to create\n* @return FSDataOutputStream for the specified path\n*/",
        "org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a source path to a destination path, handling checksums for files.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @throws IOException for I/O errors during rename\n* @throws UnresolvedLinkException for unresolved links\n*/",
        "org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Renames a file or directory, handling checksums and overwrite option.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @param overwrite true to overwrite existing files\n*/"
    },
    "org.apache.hadoop.util.DataChecksum": {
        "org.apache.hadoop.util.DataChecksum:newCrc32()": "/**\n* Creates a new CRC32 checksum instance.\n* @return a new CRC32 object for checksum calculation\n*/",
        "org.apache.hadoop.util.DataChecksum:getBytesPerChecksum()": "/**\n* Retrieves the number of bytes per checksum.\n* @return int representing bytes per checksum value\n*/",
        "org.apache.hadoop.util.DataChecksum:getChecksumSize()": "/**\n* Returns the size of the checksum.\n* @return size of the checksum as an integer\n*/",
        "org.apache.hadoop.util.DataChecksum:getCrcPolynomialForType(org.apache.hadoop.util.DataChecksum$Type)": "/**\n* Retrieves CRC polynomial based on specified type.\n* @param type the CRC type\n* @return corresponding polynomial integer\n* @throws IOException if type is unsupported\n*/",
        "org.apache.hadoop.util.DataChecksum:<init>(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,int)": "/**\n* Constructs a DataChecksum with specified type, checksum, and chunk size.\n* @param type the type of data\n* @param checksum the checksum for data integrity\n* @param chunkSize the size of data chunks\n*/",
        "org.apache.hadoop.util.DataChecksum:getChecksumHeaderSize()": "/**\n* Returns the size of the checksum header in bytes.\n* @return size as an integer, calculated as 1 + SIZE_OF_INTEGER\n*/",
        "org.apache.hadoop.util.DataChecksum:writeHeader(java.io.DataOutputStream)": "/**\n* Writes header information to the output stream.\n* @param out DataOutputStream to write the header data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.DataChecksum:reset()": "/**\n* Resets the summer object and initializes inSum to zero.\n*/",
        "org.apache.hadoop.util.DataChecksum:compare(byte[],int)": "/**\n* Compares checksum from byte array with a stored value.\n* @param buf byte array containing data\n* @param offset starting index for checksum calculation\n* @return true if checksum matches or size is zero, false otherwise\n*/",
        "org.apache.hadoop.util.DataChecksum:getValue()": "/**\n* Retrieves the value from the summer instance.\n* @return long value from the summer\n*/",
        "org.apache.hadoop.util.DataChecksum:update(byte[],int,int)": "/**\n* Updates the summer with byte array data and increments input length.\n* @param b byte array data to update\n* @param off offset to start from in the byte array\n* @param len number of bytes to update\n*/",
        "org.apache.hadoop.util.DataChecksum:update(int)": "/**\n* Updates the summer with value b and increments the inSum counter.\n* @param b value to update the summer\n*/",
        "org.apache.hadoop.util.DataChecksum:toString()": "/**\n* Returns a string representation of the DataChecksum object.\n* @return formatted string with type and chunk size\n*/",
        "org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)": "",
        "org.apache.hadoop.util.DataChecksum:getChecksumSize(int)": "",
        "org.apache.hadoop.util.DataChecksum:getHeader()": "/**\n* Constructs a header byte array with type ID and checksum information.\n* @return byte array representing the header\n*/",
        "org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int)": "/**** Maps an integer to a Checksum Type. \n* @param type integer representation of the checksum Type\n* @throws InvalidChecksumSizeException if the type is invalid\n* @return corresponding Type object\n*/",
        "org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)": "/**\n* Writes value to output stream and optionally resets summer.\n* @param out data output stream\n* @param reset flag to reset summer after writing\n* @return size of the type\n*/",
        "org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)": "/**\n* Writes checksum to buffer; resets summer if requested.\n* @param buf byte array to write checksum\n* @param offset position in buf to start writing\n* @param reset flag to reset the summer object\n* @return size of the type\n*/",
        "org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)": "/**\n* Verifies data integrity using checksums.\n* @param type checksum type, algorithm for checksum calculation, data buffer, CRCs, filename, base position\n* @throws ChecksumException if computed checksum does not match expected value\n*/",
        "org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)": "/**\n* Verifies data chunks against expected CRCs; throws ChecksumException on mismatch.\n* @param type checksum type, algorithm for checksum calculation, data to verify, offsets, and filename\n*/",
        "org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)": "/**\n* Computes chunked sums of a byte array.\n* @param data input byte array for calculations\n* @param dataOffset offset in data array\n* @param dataLength length of data to process\n* @param sums output array for computed sums\n* @param sumsOffset offset in sums array\n*/",
        "org.apache.hadoop.util.DataChecksum:newCrc32C()": "/**\n* Creates a Checksum instance using CRC32C or falls back to PureJavaCrc32C on error.\n* @return Checksum object based on availability\n*/",
        "org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)": "/**\n* Verifies chunked checksums for data integrity.\n* @param data input data buffer\n* @param checksums buffer for expected checksums\n* @param fileName name of the file being processed\n* @param basePos starting position in the file\n* @throws ChecksumException if checksum verification fails\n*/",
        "org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Computes chunked checksums from data and stores them in the checksums ByteBuffer.\n* @param data input data for checksum calculation\n* @param checksums ByteBuffer to store computed checksums\n*/",
        "org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)": "/**\n* Creates a DataChecksum based on type and bytes per checksum.\n* @param type the type of checksum\n* @param bytesPerChecksum size of data chunks\n* @return DataChecksum or null for invalid input\n*/",
        "org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)": "/**\n* Creates a DataChecksum from a byte array and offset.\n* @param bytes byte array containing checksum data\n* @param offset starting position for checksum extraction\n* @return DataChecksum object\n* @throws IOException if the checksum cannot be created\n*/",
        "org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream)": "/**\n* Creates a DataChecksum from input stream data.\n* @param in input stream containing checksum type and size\n* @return DataChecksum object or throws InvalidChecksumSizeException\n*/"
    },
    "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker": {
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:close()": "/**\n* Closes data streams and verifies checksum.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getChecksumFilePos(long)": "/**\n* Calculates checksum file position based on data position.\n* @param dataPos position in data\n* @return corresponding checksum file position\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getChunkPosition(long)": "/**\n* Calculates the starting position of a data chunk.\n* @param dataPos the position within the data\n* @return the chunk starting position\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength()": "/**\n* Retrieves the file length, resolving it if not previously cached.\n* @return the file length as a long\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available()": "/**\n* Returns the total number of bytes available to read.\n* @return total available bytes from data source and super class\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long)": "",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])": "/**\n* Reads a chunk of data and its checksum from specified position.\n* @param pos position to read from; @param buf buffer to store data; @param offset start index; @param len length to read; @param checksum buffer for checksums\n* @return number of bytes read\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long)": "/**\n* Seeks to a specified position in the stream, ensuring it's within file length.\n* @param pos desired position to seek to\n* @throws IOException if seeking past end of file or an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long)": "/**\n* Skips bytes in the stream, adjusting for file length.\n* @param n number of bytes to skip; must not exceed remaining length\n* @return number of bytes actually skipped\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)": "/**\n* Initializes ChecksumFSInputChecker with file and buffer size.\n* @param fs filesystem instance, @param file input file path, @param bufferSize size of buffer\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)": "/**\n* Constructs ChecksumFSInputChecker with filesystem and file path.\n* @param fs filesystem instance, @param file input file path\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position into a buffer.\n* @param position starting position in the file\n* @param b buffer to store read bytes\n* @param off offset in the buffer\n* @param len number of bytes to read\n* @return number of bytes read\n*/"
    },
    "org.apache.hadoop.fs.ChecksumException": {
        "org.apache.hadoop.fs.ChecksumException:<init>(java.lang.String,long)": "/**\n* Constructs a ChecksumException with a message and position.\n* @param description error message\n* @param pos position of the error\n*/"
    },
    "org.apache.hadoop.crypto.CipherSuite": {
        "org.apache.hadoop.crypto.CipherSuite:getAlgorithmBlockSize()": "/**\n* Retrieves the algorithm's block size.\n* @return the size of the algorithm's block\n*/",
        "org.apache.hadoop.crypto.CipherSuite:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.crypto.CipherSuite:setUnknownValue(int)": "/**\n* Sets the unknown value.\n* @param unknown the value to set for unknownValue\n*/",
        "org.apache.hadoop.crypto.CipherSuite:getUnknownValue()": "/**\n* Retrieves the value of unknownValue.\n* @return the current value of unknownValue\n*/",
        "org.apache.hadoop.crypto.CipherSuite:toString()": "/**\n* Returns a string representation of the object with name and algorithm block size.\n* @return formatted string containing object's properties\n*/",
        "org.apache.hadoop.crypto.CipherSuite:getConfigSuffix()": "/**\n* Generates a config suffix from the name by converting parts to lowercase.\n* @return concatenated lowercase suffix with '.' prefix\n*/",
        "org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String)": "/**\n* Converts a string to a CipherSuite enum.\n* @param name the name of the cipher suite\n* @return corresponding CipherSuite or throws IllegalArgumentException if not found\n*/"
    },
    "org.apache.hadoop.fs.FileEncryptionInfo": {
        "org.apache.hadoop.fs.FileEncryptionInfo:toString()": "/**\n* Returns a string representation of the object with key details.\n* @return formatted string with cipher suite, version, and keys\n*/",
        "org.apache.hadoop.fs.FileEncryptionInfo:toStringStable()": "/**\n* Returns a stable string representation of the object's properties.\n* @return formatted string with cipherSuite, version, edek, iv, keyName, and ezKeyVersionName\n*/",
        "org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)": "/**\n* Initializes FileEncryptionInfo with encryption parameters.\n* @param suite cipher suite, @param version protocol version, \n* @param edek encryption data key, @param iv initialization vector, \n* @param keyName name of the key, @param ezKeyVersionName key version\n*/"
    },
    "org.apache.hadoop.fs.HardLink": {
        "org.apache.hadoop.fs.HardLink:<init>()": "/**\n* Initializes a new HardLink instance with default link statistics.\n*/",
        "org.apache.hadoop.fs.HardLink:createHardLink(java.io.File,java.io.File)": "/**\n* Creates a hard link to a specified file.\n* @param file source file to link to\n* @param linkName name of the hard link to create\n* @throws IOException if arguments are null or link creation fails\n*/",
        "org.apache.hadoop.fs.HardLink:createHardLinkMult(java.io.File,java.lang.String[],java.io.File)": "/**\n* Creates hard links for multiple files in a specified directory.\n* @param parentDir directory of source files\n* @param fileBaseNames array of file base names to link\n* @param linkDir directory to create hard links in\n* @throws IOException if parameters are invalid or linkDir doesn't exist\n*/",
        "org.apache.hadoop.fs.HardLink:createIOException(java.io.File,java.lang.String,java.lang.String,int,java.lang.Exception)": "/**\n* Creates an IOException with detailed error information.\n* @param f the file that caused the error\n* @param message descriptive error message\n* @param error specific error string\n* @param exitvalue exit code related to the error\n* @param cause the underlying exception, if any\n* @return constructed IOException\n*/",
        "org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File)": "/**\n* Returns the number of hard links for a given file.\n* @param fileName the file to check for hard link count\n* @return number of hard links\n* @throws IOException if file is null or not found\n*/"
    },
    "org.apache.hadoop.util.Shell$ShellCommandExecutor": {
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:getOutput()": "/**\n* Returns the output as a string or an empty string if output is null.\n* @return output string representation or empty string\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:getExecString()": "/**\n* Retrieves the command execution strings.\n* @return an array of command strings\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:parseExecResult(java.io.BufferedReader)": "/**\n* Reads lines from a BufferedReader and appends them to output.\n* @param lines input source for reading characters\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:close()": "",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:execute()": "/**\n* Executes commands, throwing IOException for null entries.\n* @throws IOException if a null command is found\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:toString()": "/**\n* Converts command execution strings to a formatted string.\n* @return formatted command string representation\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)": "/**\n* Initializes ShellCommandExecutor with command, directory, environment, timeout, and inheritance.\n* @param execString command to execute, @param dir working directory, @param env environment variables,\n* @param timeout execution timeout, @param inheritParentEnv flag for parent env inheritance\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)": "/**\n* Constructs ShellCommandExecutor with command, directory, env, and timeout.\n* @param execString command to execute, @param dir working directory, @param env environment variables, @param timeout execution timeout\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)": "/**\n* Constructs ShellCommandExecutor with command, directory, and environment variables.\n* @param execString command to execute, @param dir working directory, @param env environment variables\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)": "/**\n* Initializes ShellCommandExecutor with command and working directory.\n* @param execString command to execute, @param dir working directory\n*/",
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[])": "/**\n* Initializes ShellCommandExecutor with command array.\n* @param execString command to execute\n*/"
    },
    "org.apache.hadoop.fs.DelegationTokenRenewer": {
        "org.apache.hadoop.fs.DelegationTokenRenewer:getRenewQueueLength()": "/**\n* Returns the current length of the renewal queue.\n* @return int representing the number of items in the queue\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer:<init>(java.lang.Class)": "/**\n* Initializes DelegationTokenRenewer with specified FileSystem class.\n* @param clazz FileSystem class type for the renewer\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer:reset()": "/**\n* Resets the singleton instance and clears its queue.\n* Ensures thread safety during the reset process.\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer:addRenewAction(org.apache.hadoop.fs.FileSystem)": "/**\n* Adds a renewal action for the given file system.\n* @param fs the file system implementing Renewable\n* @return RenewAction object for the specified file system\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer:run()": "/**\n* Continuously processes and renews actions from the queue until interrupted.\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer:removeRenewAction(org.apache.hadoop.fs.FileSystem)": "/**\n* Removes a renewal action from the queue and cancels it if present.\n* @param fs the filesystem instance associated with the renewal action\n* @throws IOException if an I/O error occurs during cancellation\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer:getInstance()": "/**\n* Retrieves the singleton instance of DelegationTokenRenewer.\n* @return DelegationTokenRenewer instance\n*/"
    },
    "org.apache.hadoop.fs.BatchedRemoteIterator": {
        "org.apache.hadoop.fs.BatchedRemoteIterator:<init>(java.lang.Object)": "/**\n* Initializes a BatchedRemoteIterator with a previous key.\n* @param prevKey the key to start iteration from\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator:makeRequest()": "/**\n* Initiates a request to fetch entries, resetting idx and handling empty results.\n* @throws IOException if an I/O error occurs during the request\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded()": "/**\n* Makes a request for entries if needed based on current index and entry availability.\n* @throws IOException if an I/O error occurs during the request\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator:hasNext()": "/**\n* Checks if there are more entries available.\n* @return true if entries exist, false otherwise\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator:next()": "/**\n* Retrieves the next entry and advances the index.\n* @return the next entry of type E\n* @throws IOException if an I/O error occurs\n* @throws NoSuchElementException if no entries are available\n*/"
    },
    "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl": {
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getStatus()": "/**\n* Retrieves the current file status.\n* @return FileStatus representing the current status\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getFS()": "/**\n* Retrieves the FileSystem instance.\n* @return the current FileSystem object\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getThisBuilder()": "/**\n* Returns the current instance of FutureDataInputStreamBuilder.\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getBufferSize()": "/**\n* Retrieves the current buffer size.\n* @return the size of the buffer as an integer\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:withFileStatus(org.apache.hadoop.fs.FileStatus)": "/**\n* Sets the file status and returns the builder instance.\n* @param st the FileStatus to set, or null to clear\n* @return the updated FutureDataInputStreamBuilder\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int)": "/**\n* Sets the buffer size and returns the builder instance.\n* @param bufSize the desired buffer size\n* @return FutureDataInputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder()": "/**\n* Returns the current FutureDataInputStreamBuilder instance.\n* @return FutureDataInputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)": "/**\n* Initializes FutureDataInputStreamBuilderImpl with file context and path.\n* @param fc file context for operations\n* @param path file path to initialize the builder\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS()": "/**\n* Initializes buffer size from file system configuration.\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Constructs FutureDataInputStreamBuilderImpl with a file system and path.\n* @param fileSystem the file system to use\n* @param path the path for initialization\n*/",
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)": "/**\n* Constructs FutureDataInputStreamBuilderImpl with file system and path handle.\n* @param fileSystem the file system to use\n* @param pathHandle the path handle for the builder\n*/"
    },
    "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum": {
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)": "/**\n* Constructs a checksum object with specified parameters.\n* @param bytesPerCRC bytes processed per CRC calculation\n* @param crcPerBlock number of CRCs per block\n* @param md5 MD5 hash for the file\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getCrcType()": "/**\n* Returns the default CRC type used for compatibility.\n* @return DataChecksum.Type representing the CRC type\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getCrcTypeFromAlgorithmName(java.lang.String)": "/**\n* Retrieves checksum type based on algorithm name.\n* @param algorithm name of the checksum algorithm\n* @return DataChecksum.Type corresponding to the algorithm\n* @throws IOException if the algorithm is unknown\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getLength()": "/**\n* Returns the constant length value.\n* @return the predefined LENGTH integer\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>()": "/**\n* Constructs a MD5MD5CRC32FileChecksum object with default parameters.\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName()": "/**\n* Constructs algorithm name using block and CRC parameters.\n* @return concatenated algorithm name string\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt()": "/**\n* Creates ChecksumOpt using CRC type and bytes per checksum.\n* @return ChecksumOpt object with configured checksum options\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput)": "/**\n* Writes CRC data to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString()": "/**\n* Returns a string representation of the algorithm name and MD5 hash.\n* @return formatted string combining algorithm name and MD5\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput stream, populating instance variables.\n* @param in DataInput stream to read from\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes()": "/**\n* Converts the current object to a byte array.\n* @return byte array representation of the object\n*/"
    },
    "org.apache.hadoop.fs.GlobPattern": {
        "org.apache.hadoop.fs.GlobPattern:compiled()": "/**\n* Returns the compiled Pattern object.\n* @return compiled Pattern instance\n*/",
        "org.apache.hadoop.fs.GlobPattern:matches(java.lang.CharSequence)": "/**\n* Checks if the input sequence matches the compiled pattern.\n* @param s the character sequence to match\n* @return true if matches, false otherwise\n*/",
        "org.apache.hadoop.fs.GlobPattern:error(java.lang.String,java.lang.String,int)": "/**\n* Throws a PatternSyntaxException with a formatted error message.\n* @param message error description\n* @param pattern the regex pattern causing the error\n* @param pos position of the error in the pattern\n*/",
        "org.apache.hadoop.fs.GlobPattern:hasWildcard()": "/**\n* Checks if wildcard is enabled.\n* @return true if wildcard is enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.GlobPattern:set(java.lang.String)": "/**\n* Converts a glob pattern to a regex pattern.\n* @param glob the glob pattern to be converted\n*/",
        "org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String)": "/**\n* Initializes GlobPattern with a specified glob pattern.\n* @param globPattern the glob pattern to be converted to regex\n*/",
        "org.apache.hadoop.fs.GlobPattern:compile(java.lang.String)": "/**\n* Compiles a glob pattern into a regex Pattern.\n* @param globPattern the glob pattern to convert\n* @return compiled Pattern object\n*/"
    },
    "org.apache.hadoop.fs.FileChecksum": {
        "org.apache.hadoop.fs.FileChecksum:equals(java.lang.Object)": "/**\n* Compares this FileChecksum with another for equality.\n* @param other object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.FileChecksum:hashCode()": "/**\n* Computes the hash code using algorithm name and byte array.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.fs.FileChecksum:getChecksumOpt()": "/**\n* Retrieves the ChecksumOpt object.\n* @return ChecksumOpt instance or null if not available\n*/"
    },
    "org.apache.hadoop.fs.SafeMode": {
        "org.apache.hadoop.fs.SafeMode:setSafeMode(org.apache.hadoop.fs.SafeModeAction)": "/**\n* Sets the safe mode state based on the provided action.\n* @param action the action to set safe mode\n* @return true if successful, false otherwise\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.FileUtil": {
        "org.apache.hadoop.fs.FileUtil:fullyDeleteOnExit(java.io.File)": "/**\n* Schedules a file or directory for deletion on JVM exit.\n* @param file the file or directory to delete\n*/",
        "org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean)": "/**\n* Deletes a file and optionally logs failure if it exists.\n* @param f the file to delete\n* @param doLog flag to enable logging on failure\n* @return true if deleted, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:fullyDelete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Deletes a directory and its contents from the filesystem.\n* @param fs FileSystem instance to perform deletion\n* @param dir Path of the directory to delete\n*/",
        "org.apache.hadoop.fs.FileUtil:listFiles(java.io.File)": "/**\n* Lists files in the specified directory.\n* @param dir the directory to list files from\n* @return array of File objects or throws IOException if invalid\n*/",
        "org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File,boolean)": "/**\n* Checks if a file is regular; follows or ignores links based on allowLinks.\n* @param file the file to check\n* @param allowLinks true to follow links, false to ignore them\n* @return true if regular file, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:makeShellPath(java.lang.String)": "/**\n* Returns the shell path for the given filename.\n* @param filename the name of the file\n* @return the shell path as a String\n*/",
        "org.apache.hadoop.fs.FileUtil:getDU(java.io.File)": "/**\n* Calculates total size of files in a directory recursively.\n* @param dir directory to calculate size for\n* @return total size in bytes, or 0 if not a directory\n*/",
        "org.apache.hadoop.fs.FileUtil:addPermissions(java.util.Set,int,java.nio.file.attribute.PosixFilePermission,java.nio.file.attribute.PosixFilePermission,java.nio.file.attribute.PosixFilePermission)": "/**\n* Adds file permissions based on the given mode.\n* @param permissions set to update with permissions\n* @param mode bitwise representation of permissions\n* @param r read permission\n* @param w write permission\n* @param x execute permission\n*/",
        "org.apache.hadoop.fs.FileUtil:runCommandOnStream(java.io.InputStream,java.lang.String)": "/**\n* Executes a shell command with input/output streams.\n* @param inputStream input data for the command\n* @param command shell command to execute\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the execution is interrupted\n* @throws ExecutionException if the command execution fails\n*/",
        "org.apache.hadoop.fs.FileUtil:getCanonicalPath(java.lang.String,java.io.File)": "/**\n* Returns the canonical path of a given file.\n* @param path the file path to resolve\n* @param parentDir the parent directory for relative paths\n* @return the canonical file path as a String\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileUtil:createLocalTempFile(java.io.File,java.lang.String,boolean)": "/**\n* Creates a temporary file based on a base file.\n* @param basefile the base file for naming the temp file\n* @param prefix prefix for the temp file name\n* @param isDeleteOnExit flag to delete the temp file on JVM exit\n* @return the created temporary File\n*/",
        "org.apache.hadoop.fs.FileUtil:replaceFile(java.io.File,java.io.File)": "/**\n* Replaces a file by renaming it, handling Windows limitations.\n* @param src source file to rename\n* @param target target file name\n* @throws IOException if renaming fails after retries\n*/",
        "org.apache.hadoop.fs.FileUtil:compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)": "/**\n* Compares two FileSystem objects for equality based on URI scheme, host, and port.\n* @param srcFs source FileSystem to compare\n* @param destFs destination FileSystem to compare\n* @return true if both FileSystems are equivalent, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[])": "",
        "org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)": "",
        "org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File)": "/**\n* Checks if the given file is a regular file, following links.\n* @param file the file to check\n* @return true if regular file, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)": "/**\n* Constructs shell path from a File object.\n* @param file the File to convert\n* @param makeCanonicalPath flag to use canonical path\n* @return shell path as a String\n*/",
        "org.apache.hadoop.fs.FileUtil:permissionsFromMode(int)": "/**\n* Converts an integer mode to a set of POSIX file permissions.\n* @param mode integer representation of file permissions\n* @return Set of PosixFilePermission based on the mode\n*/",
        "org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)": "/**\n* Unpacks entries from a tar archive to a specified directory.\n* @param tis input stream of the tar archive\n* @param entry the tar archive entry to unpack\n* @param outputDir directory where entries are extracted\n* @throws IOException if an I/O error occurs or if path validation fails\n*/",
        "org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)": "/**\n* Converts FileStatus array to Path array, returning given path if input is null.\n* @param stats array of FileStatus objects\n* @param path fallback Path if stats is null\n* @return array of Paths derived from FileStatus or fallback Path\n*/",
        "org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File)": "/**\n* Creates a shell path from a File object without canonicalizing it.\n* @param file the File to convert\n* @return shell path as a String\n*/",
        "org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File)": "/****\n* Generates a secure shell path from a File object.\n* @param file the File to convert\n* @return secure shell path as a String\n*/",
        "org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)": "/**\n* Extracts files from a ZIP archive to a specified directory.\n* @param inputStream ZIP archive input stream\n* @param toDir target directory for extracted files\n* @throws IOException if an I/O error occurs during extraction\n*/",
        "org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)": "/**\n* Extracts files from a ZIP archive to a specified directory.\n* @param inFile the ZIP file to extract from\n* @param unzipDir the directory to extract files into\n* @throws IOException if an I/O error occurs during extraction\n*/",
        "org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)": "/**\n* Extracts files from a tar archive to a specified directory.\n* @param inFile input tar archive file\n* @param untarDir directory for extracted files\n* @param gzipped indicates if the archive is gzipped\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)": "/**** Unpacks a tar archive to a directory, optionally handling gzip compression. \n* @param inputStream input stream of the tar archive \n* @param untarDir directory where entries are extracted \n* @param gzipped indicates if the stream is gzipped \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.fs.FileUtil:canRead(java.io.File)": "/**\n* Checks if a file is readable.\n* @param f the file to check\n* @return true if readable, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:canWrite(java.io.File)": "/**\n* Checks if the file is writable based on the operating system.\n* @param f the file to check for write access\n* @return true if writable, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:canExecute(java.io.File)": "/**\n* Checks if a file is executable based on the OS.\n* @param f the file to check\n* @return true if executable, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])": "/**\n* Writes bytes to a specified path in the given FileContext.\n* @param fileContext context for file operations\n* @param path destination path for the data\n* @param bytes data to write\n* @return updated FileContext instance\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)": "/**\n* Writes lines to a file at the specified path with the given charset.\n* @param fileContext context for file operations\n* @param path target file path\n* @param lines lines to write\n* @param cs character set for encoding\n* @return updated FileContext\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)": "/**\n* Writes character sequence to a file at the specified path.\n* @param fs FileContext for file operations, path target file path, charseq data to write, cs charset\n* @return updated FileContext after writing\n*/",
        "org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)": "/**** Unpacks a tar file from an InputStream to a specified directory. \n* @param inputStream source of the tar file \n* @param untarDir target directory for extraction \n* @param gzipped indicates if the tar file is gzipped \n* @throws IOException on I/O errors \n* @throws InterruptedException if interrupted \n* @throws ExecutionException if command execution fails \n*/",
        "org.apache.hadoop.fs.FileUtil:list(java.io.File)": "/**\n* Lists files in a directory.\n* @param dir the directory to list files from\n* @return array of file names\n* @throws IOException if directory is invalid or not readable\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)": "/**\n* Writes character sequence to a file using UTF-8 encoding.\n* @param fileContext context for file operations, path target file, charseq data to write\n* @return updated FileContext after writing\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])": "/****\n* Writes byte array to a specified path in the given FileSystem.\n* @param fs the FileSystem instance\n* @param path the destination Path for the file\n* @param bytes the byte array to write\n* @return the FileSystem instance used\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)": "/**\n* Writes lines to a file at the specified path using the given charset.\n* @param fs file system to write to, path destination for the file, lines content to write, cs charset \n* @return the FileSystem instance used\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)": "/**\n* Writes character sequence to a file at specified path.\n* @param fs file system instance, @param path file destination, \n* @param charseq content to write, @param cs character set\n* @return the file system instance\n*/",
        "org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)": "/**\n* Extracts a tar archive to a directory, using Java or native tar based on the OS.\n* @param inputStream source of the tar archive\n* @param untarDir target directory for extraction\n* @param gzipped indicates if the stream is gzipped\n* @throws IOException on I/O errors\n* @throws InterruptedException if interrupted\n* @throws ExecutionException if command execution fails\n*/",
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)": "/**\n* Writes character sequence to a file using UTF-8 encoding.\n* @param fs file system instance, @param path file destination, @param charseq content to write\n* @return the file system instance\n*/",
        "org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)": "/**\n* Checks destination path validity, handling directories and overwriting rules.\n* @param srcName source file name or null for directory checks\n* @param dstFS destination file system\n* @param dst destination path\n* @param overwrite flag to allow overwriting existing files\n* @return validated destination path\n*/",
        "org.apache.hadoop.fs.FileUtil:readLink(java.io.File)": "/**\n* Reads the target of a symbolic link from a given file.\n* @param f the file representing the symbolic link\n* @return target path as a string or empty if null or error occurs\n*/",
        "org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])": "/**\n* Executes a shell command with a file path.\n* @param f file to be used in the command\n* @param cmd commands to execute\n* @return output of the command as a string\n*/",
        "org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)": "/**** Unzips and extracts a tar file to a specified directory.  \n* @param inFile the tar file to extract  \n* @param untarDir the directory to extract files into  \n* @param gzipped indicates if the tar file is gzipped  \n* @throws IOException if extraction fails  \n*/",
        "org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)": "/**\n* Creates a symbolic link from target to linkname.\n* @param target the file or directory to link to\n* @param linkname the name of the symbolic link\n* @return exit code indicating success or failure\n*/",
        "org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)": "/**\n* Changes file permissions and returns exit code.\n* @param filename path to the file, @param perm permission string, @param recursive applies recursively\n* @return exit code of the permission change command\n*/",
        "org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)": "/**\n* Sets the owner of a file to the specified username and group.\n* @param file the file to change ownership\n* @param username the new owner's username, or null\n* @param groupname the group name, or null\n* @throws IOException if both username and groupname are null\n*/",
        "org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions using native methods or shell commands.\n* @param f the file to modify permissions\n* @param permission the new permissions to set\n* @throws IOException if permission change fails\n*/",
        "org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])": "/**\n* Renames a file or directory using the specified FileSystem and options.\n* @param srcFs source FileSystem instance\n* @param src source path to rename\n* @param dst destination path\n* @param options rename options, e.g., overwrite\n* @throws IOException if an error occurs during renaming\n*/",
        "org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)": "/**\n* Extracts files from a tar archive to a specified directory.\n* @param inFile the tar file to extract\n* @param untarDir the directory for extracted files\n* @throws IOException if directory creation or extraction fails\n*/",
        "org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)": "/**\n* Changes file permissions using chmod.\n* @param filename path to the file, @param perm permission string\n* @return exit code of the permission change command\n*/",
        "org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)": "/**\n* Sets file readability; returns true on success, false on failure.\n* @param f file to modify, @param readable desired readability state\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)": "/**\n* Sets file writable status; returns true on success.\n* @param f file to modify, @param writable desired writable status\n* @return true if writable status set, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)": "/**** Sets executable permission for a file based on the operating system. \n* @param f file to modify, @param executable true to set executable, false to unset \n* @return true if permission is set, false otherwise \n*/",
        "org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions based on user, group, and other actions.\n* @param f the file to modify permissions\n* @param permission the new permissions to set\n* @throws IOException if permission change fails\n*/",
        "org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File)": "/**\n* Grants executable, readable, and writable permissions to the specified file.\n* @param f the file to modify\n*/",
        "org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)": "/**\n* Deletes a directory, optionally granting permissions first.\n* @param dir directory to delete\n* @param tryGrantPermissions flag to attempt permission changes\n* @return true if deletion succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File)": "/**\n* Deletes the specified directory without permission changes.\n* @param dir directory to delete\n* @return true if deletion succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)": "/**\n* Deletes all contents of a directory, optionally granting permissions first.\n* @param dir the directory to clear\n* @param tryGrantPermissions flag to enable permission changes before deletion\n* @return true if all deletions succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File)": "/**\n* Deletes all contents of a directory without granting permissions.\n* @param dir the directory to clear\n* @return true if all deletions succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Validates source and destination paths for copying in the same FileSystem.\n* @param srcFS source FileSystem, src source Path, dstFS destination FileSystem, dst destination Path\n* @throws IOException if paths are invalid for copying\n*/",
        "org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)": "/**\n* Ignores missing directory if path does not support directory listing capability.\n* @param fs the FileSystem to check the path capability\n* @param path the Path to evaluate\n* @param e the FileNotFoundException to potentially rethrow\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies a file or directory to a destination, optionally deleting the source.\n* @param src source file or directory\n* @param dstFS destination file system\n* @param dst destination path\n* @param deleteSource flag to delete source after copy\n* @return true if copy succeeded, false if failed\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies files or directories from source to destination, with optional deletion and overwriting.\n* @param srcFS source FileSystem, srcStatus source FileStatus, dstFS destination FileSystem, \n* @param dst destination path, deleteSource flag to delete source, overwrite flag to allow overwriting\n* @return true if copy succeeds, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies files or directories from source to destination.\n* @param srcFS source FileSystem, srcStatus source FileStatus, dst destination File,\n* @param deleteSource flag to delete source after copy, conf Configuration for I/O settings\n* @return true if copy succeeds, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies a file from source to destination; deletes source if specified.\n* @param srcFS source FileSystem, src source Path, dstFS destination FileSystem,\n* @param dst destination Path, deleteSource flag for source deletion, overwrite flag for overwriting\n* @return true if copy succeeds, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies a file from source to destination.\n* @param srcFS source FileSystem, src source Path, dst destination File,\n* @param deleteSource flag to delete source after copy, conf Configuration for I/O settings\n* @return true if copy succeeds, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies a file from source to destination, optionally deleting the source.\n* @param srcFS source FileSystem, src source Path, dstFS destination FileSystem,\n* @param dst destination Path, deleteSource flag for source deletion\n* @return true if copy succeeds, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Copies multiple files to a destination directory.\n* @param srcFS source FileSystem, srcs array of source Paths, dstFS destination FileSystem,\n* @param dst destination Path, deleteSource flag for source deletion, overwrite flag for overwriting\n* @return true if all copies succeed, false otherwise\n*/",
        "org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)": "/**\n* Retrieves JAR file paths from a specified directory.\n* @param path directory path with optional wildcard\n* @param useLocal flag for local filesystem usage\n* @return List of Path objects for JAR files found\n*/",
        "org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String)": "/**\n* Retrieves JAR file paths from a specified directory.\n* @param path directory path with optional wildcard\n* @return List of Path objects for JAR files found\n*/",
        "org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)": "/**\n* Creates a JAR file with classpath entries.\n* @param inputClassPath classpath entries to include\n* @param pwd working directory path\n* @param targetDir target directory for the JAR\n* @param callerEnv environment variables for replacement\n* @return array containing the JAR path and unresolved wildcards\n*/",
        "org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)": "/**\n* Creates a JAR file using specified classpath and working directory.\n* @param inputClassPath classpath entries to include\n* @param pwd working directory path\n* @param callerEnv environment variables for replacement\n* @return array containing the JAR path and unresolved wildcards\n*/"
    },
    "org.apache.hadoop.fs.permission.FsAction": {
        "org.apache.hadoop.fs.permission.FsAction:implies(org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks if this action implies the specified action.\n* @param that action to check against\n* @return true if this action implies 'that', false otherwise\n*/",
        "org.apache.hadoop.fs.permission.FsAction:or(org.apache.hadoop.fs.permission.FsAction)": "/**\n* Combines two FsAction values using a bitwise OR.\n* @param that another FsAction to combine\n* @return resulting FsAction after OR operation\n*/",
        "org.apache.hadoop.fs.permission.FsAction:not()": "/**\n* Returns the FsAction opposite of the current ordinal value.\n* @return FsAction corresponding to the negation of the current state\n*/",
        "org.apache.hadoop.fs.permission.FsAction:and(org.apache.hadoop.fs.permission.FsAction)": "/**\n* Combines this FsAction with another using bitwise AND.\n* @param that another FsAction to combine with\n* @return resulting FsAction after AND operation\n*/",
        "org.apache.hadoop.fs.permission.FsAction:getFsAction(java.lang.String)": "/**\n* Retrieves FsAction by matching permission symbol.\n* @param permission the symbol of the desired FsAction\n* @return FsAction object or null if not found\n*/"
    },
    "org.apache.hadoop.fs.FSDataOutputStream": {
        "org.apache.hadoop.fs.FSDataOutputStream:close()": "/**\n* Closes the output stream, invoking PositionCache.close().\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:toString()": "/**\n* Returns a string representation of the FSDataOutputStream object.\n* @return formatted string with wrappedStream details\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:hflush()": "/**\n* Flushes the wrapped stream, calling hflush if it's Syncable.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:hsync()": "/**\n* Synchronizes the wrapped stream, flushing if not syncable.\n* @throws IOException if an I/O error occurs during synchronization\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:setDropBehind(java.lang.Boolean)": "/**\n* Sets the drop-behind caching option for the wrapped stream.\n* @param dropBehind true to enable, false to disable caching\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:abort()": "/**\n* Attempts to abort the wrapped stream if it supports aborting.\n* @return AbortableResult indicating the result of the abort operation\n* @throws UnsupportedOperationException if the wrapped stream does not support aborting\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:getWrappedStream()": "/**\n* Returns the wrapped output stream.\n* @return OutputStream instance of the wrapped stream\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)": "",
        "org.apache.hadoop.fs.FSDataOutputStream:getPos()": "/**\n* Retrieves the current cached position value.\n* @return current position as a long\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics for the wrapped stream.\n* @return IOStatistics object or null if invalid\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Constructs FSDataOutputStream with specified OutputStream and statistics.\n* @param out OutputStream to write data\n* @param stats Statistics for the FileSystem\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String)": "/**\n* Checks if the wrapped stream has a specific capability.\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator": {
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:remove()": "/**\n* Throws UnsupportedOperationException to prevent removal in a read-only iterator.\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance()": "/**\n* Advances to the next valid path in rootDirs.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])": "/**\n* Initializes PathIterator with filesystem, path string, and root directories.\n* @param fs filesystem instance\n* @param pathStr path as a string\n* @param rootDirs array of root directory paths\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next()": "/**\n* Retrieves the next valid Path, advancing if necessary.\n* @return Path object or throws NoSuchElementException if none exists\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$11": {
        "org.apache.hadoop.fs.FileSystem$Statistics$11:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Constructs a copy of the given Statistics object.\n* @param other the Statistics object to copy from\n*/"
    },
    "org.apache.hadoop.fs.CachingGetSpaceUsed": {
        "org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(java.io.File,long,long,long)": "/**\n* Initializes caching parameters for space usage tracking.\n* @param path file path for space usage, @param interval refresh interval, \n* @param jitter random delay, @param initialUsed initial space used\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getUsed()": "/**\n* Returns the amount of used resources, ensuring it's non-negative.\n* @return long used resources, or 0 if none are used\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:incDfsUsed(long)": "/**\n* Increments the used value by the specified amount.\n* @param value amount to add to the current used value\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:running()": "/**\n* Checks if the process is currently running.\n* @return true if running, false otherwise\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:setUsed(long)": "/**\n* Sets the used value for the object.\n* @param usedValue the value to set as used\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:close()": "/**\n* Closes the resource, stopping its operation and interrupting any refresh thread.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getDirPath()": "/**\n* Retrieves the directory path as a string.\n* @return the directory path\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getJitter()": "/**\n* Retrieves the current jitter value.\n* @return the current jitter as a long\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getRefreshInterval()": "/**\n* Retrieves the refresh interval value.\n* @return the refresh interval in milliseconds\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:setShouldFirstRefresh(boolean)": "/**\n* Sets the flag for the first refresh state.\n* @param shouldFirstRefresh indicates if it's the first refresh\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean)": "/**\n* Initializes and starts a refresh thread if refreshInterval is positive.\n* @param runImmediately flag to start the thread immediately\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:init()": "/**\n* Initializes the component and starts refresh if needed.\n* Calls refresh or initRefreshThread based on conditions.\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)": "/**\n* Constructs CachingGetSpaceUsed from a builder.\n* @param builder contains parameters for initialization\n*/"
    },
    "org.apache.hadoop.fs.GetSpaceUsed$Builder": {
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getPath()": "/**\n* Retrieves the file path.\n* @return File object representing the path\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getInitialUsed()": "/**\n* Returns the initial used value or -1 if not set.\n* @return initial used value as long or -1 if null\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setPath(java.io.File)": "/**\n* Sets the file path for the builder.\n* @param path the file to set as the path\n* @return the updated Builder instance\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration and returns the builder instance.\n* @param conf configuration to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setInterval(long)": "/**\n* Sets the interval value and returns the builder instance.\n* @param interval duration in milliseconds for the interval\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setInitialUsed(long)": "/**\n* Sets the initial used value and returns the builder instance.\n* @param initialUsed the initial used value to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval()": "/**\n* Retrieves the interval value, using a default if not set or config is null.\n* @return interval as long, or default if not found\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter()": "/**\n* Retrieves the jitter value, using a default if not set.\n* @return long jitter value or default if not configured\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass()": "/**\n* Retrieves the Class for GetSpaceUsed based on OS and configuration.\n* @return Class implementing GetSpaceUsed or null if not found\n*/",
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:build()": "/**\n* Builds GetSpaceUsed instance, handling constructor errors and fallbacks.\n* @return GetSpaceUsed instance or fallback implementation\n*/"
    },
    "org.apache.hadoop.fs.DU$DUShell": {
        "org.apache.hadoop.fs.DU$DUShell:startRefresh()": "/**\n* Initiates the refresh process, executing the run method.\n* @throws IOException if an I/O error occurs during execution\n*/",
        "org.apache.hadoop.fs.DU$DUShell:toString()": "/**\n* Returns a string representation of directory usage.\n* @return formatted string with directory path and usage info\n*/",
        "org.apache.hadoop.fs.DU$DUShell:getExecString()": "/**\n* Constructs execution command for disk usage.\n* @return array of command strings for execution\n*/",
        "org.apache.hadoop.fs.DU$DUShell:parseExecResult(java.io.BufferedReader)": "/**\n* Parses execution result from BufferedReader and sets used memory.\n* @param lines input stream containing execution output\n* @throws IOException if end of stream or illegal format is encountered\n*/"
    },
    "org.apache.hadoop.fs.sftp.SFTPConnectionPool": {
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:<init>(int)": "/**\n* Initializes SFTP connection pool with a maximum connection limit.\n* @param maxConnection maximum number of connections allowed\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getFromPool(org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo)": "/**\n* Retrieves an idle ChannelSftp from the pool.\n* @param info connection details to identify the pool\n* @return ChannelSftp or null if no connection is available\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:returnToPool(com.jcraft.jsch.ChannelSftp)": "/**\n* Returns the given SFTP channel to the idle connections pool.\n* @param channel the ChannelSftp object to return\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getIdleCount()": "/**\n* Returns the number of idle connections.\n* @return count of idle connections\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getConnPoolSize()": "/**\n* Retrieves the current size of the connection pool.\n* @return number of connections in the pool\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)": "",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp)": "",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown()": "/**\n* Shuts down connections and clears connection info.\n* @return void\n*/"
    },
    "org.apache.hadoop.fs.sftp.SFTPFileSystem": {
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:checkNotClosed()": "/**\n* Checks if the resource is closed and throws IOException if it is.\n* @throws IOException if the resource is closed\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getUri()": "/**\n* Retrieves the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Throws UnsupportedOperationException for append operation in SFTPFileSystem.\n* @param f file path to append to\n* @param bufferSize size of the buffer\n* @param progress progress indicator\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory; state is not maintained.\n* @param newDir the new directory path to set\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp)": "/**\n* Disconnects the specified SFTP channel from the connection pool.\n* @param channel the SFTP channel to disconnect\n* @throws IOException if an I/O error occurs during disconnection\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:close()": "/**\n* Closes the resource and shuts down the connection pool if not already closed.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry)": "/**\n* Retrieves FsPermission from LsEntry attributes.\n* @param sftpFile the file entry to extract permissions from\n* @return FsPermission object based on file attributes\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Converts a relative Path to an absolute Path based on the working directory.\n* @param workDir the base directory to resolve against\n* @param path the Path to be converted\n* @return the absolute Path\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp)": "/**\n* Retrieves the home directory path from the SFTP channel.\n* @param channel the SFTP channel to get the current directory from\n* @return Path object or null if an error occurs\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp)": "/**\n* Retrieves the working directory, always returning the home directory.\n* @param client the SFTP channel client\n* @return Path object representing the home directory\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus for a specified file via SFTP client.\n* @param client SFTP channel client\n* @param file Path of the file to retrieve status for\n* @return FileStatus object or throws FileNotFoundException if not found\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists via SFTP.\n* @param channel SFTP channel client\n* @param file Path of the file to check\n* @return true if file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)": "/**** Retrieves FileStatus for a specified SFTP file. \n* @param channel SFTP channel client \n* @param sftpFile file entry to retrieve status for \n* @param parentPath parent directory path \n* @return FileStatus object with file attributes \n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists and is not a directory.\n* @param channel SFTP channel for file operations\n* @param file Path of the file to check\n* @return true if it's a file, false if not or if not found\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file on SFTP server if source exists and destination does not.\n* @param channel SFTP channel client\n* @param src source file path\n* @param dst destination file path\n* @return true if renamed successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses in a directory or returns single file status.\n* @param client SFTP channel client\n* @param file Path to the file or directory\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories recursively via SFTP.\n* @param client SFTP channel for operations\n* @param file Path of the directory to create\n* @param permission Permissions for the new directory\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory via SFTP.\n* @param channel SFTP channel client\n* @param file Path to the file or directory\n* @param recursive whether to delete non-empty directories\n* @return true if deletion succeeded, false if file not found\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Configures settings from a URI into a Configuration object.\n* @param uriInfo URI containing host/user info\n* @param conf Configuration object to update\n* @throws IOException if host is null\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:connect()": "/**\n* Establishes an SFTP connection and returns the ChannelSftp object.\n* @return ChannelSftp instance for the established connection\n* @throws IOException if the resource is closed or connection fails\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**** Opens a file for reading via SFTP. \n* @param f file path to open \n* @param bufferSize size of the read buffer \n* @return FSDataInputStream for the file \n* @throws IOException if an error occurs during the process \n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file via SFTP, optionally overwriting existing files.\n* @param f file path to create, @param permission permissions, \n* @param overwrite whether to overwrite, @param bufferSize buffer size,\n* @param replication replication factor, @param blockSize block size,\n* @param progress progress callback\n* @return FSDataOutputStream for writing to the new file\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file from src to dst using an SFTP connection.\n* @param src source file path\n* @param dst destination file path\n* @return true if renamed successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory via SFTP.\n* @param f Path to the file or directory\n* @param recursive whether to delete non-empty directories\n* @return true if deletion succeeded, false if file not found\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path.\n* @param f Path to the file or directory\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory()": "/**\n* Retrieves the user's home directory path.\n* @return Path object representing the home directory or null if an error occurs\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories with specified permissions.\n* @param f Path of the directory to create\n* @param permission Permissions for the new directory\n* @return true if directories were created successfully\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status via SFTP.\n* @param f path of the file to retrieve status for\n* @return FileStatus object\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/****\n* Initializes the configuration with the given URI.\n* @param uriInfo URI for configuration initialization\n* @param conf Configuration object to set up\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory()": "/**\n* Returns the working directory, which is always the home directory.\n* @return Path object representing the home directory\n*/"
    },
    "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo": {
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:<init>(java.lang.String,int,java.lang.String)": "/**\n* Initializes connection information with host, port, and user.\n* @param hst hostname, prt port number, usr username\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:equals(java.lang.Object)": "/**\n* Compares this ConnectionInfo with another object for equality.\n* @param obj the object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:hashCode()": "/**\n* Computes the hash code for the object based on host, port, and user.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:getHost()": "/**\n* Returns the host value as a string.\n* @return host string representing the host\n*/"
    },
    "org.apache.hadoop.fs.sftp.SFTPInputStream": {
        "org.apache.hadoop.fs.sftp.SFTPInputStream:close()": "/**\n* Closes the stream if not already closed.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:getPos()": "/**\n* Retrieves the current position in the stream.\n* @return the current position as a long value\n*/",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:seekToNewSource(long)": "/**\n* Seeks to a new data source at the specified position.\n* @param targetPos the position to seek to\n* @return false indicating no new source found\n*/",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal()": "",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed()": "",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long)": "/**\n* Sets the position for reading; throws EOFException for negative values.\n* @param position the new position to seek to\n* @throws IOException if the stream is closed or an I/O error occurs\n*/",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:available()": "/**\n* Returns the number of bytes available to read.\n* @return available bytes, capped at Integer.MAX_VALUE\n*/",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:read()": "/**\n* Reads a byte from the stream; returns -1 if end of stream is reached.\n* @return byte read or -1 if no more bytes are available\n*/",
        "org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Initializes SFTP input stream from given channel and path.\n* @param channel SFTP channel for file transfer\n* @param path file path to read from\n* @param stats file system statistics\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.sftp.SFTPFileSystem$1": {
        "org.apache.hadoop.fs.sftp.SFTPFileSystem$1:close()": "/**\n* Closes the resource and shuts down the connection pool if not already closed.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$4": {
        "org.apache.hadoop.fs.FileSystem$Statistics$4:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Copies data from another Statistics object to create a new instance.\n* @param other the Statistics object to copy from\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$2": {
        "org.apache.hadoop.fs.FileSystem$Statistics$2:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Constructs a copy of the given Statistics object.\n* @param other the Statistics instance to copy from\n*/"
    },
    "org.apache.hadoop.fs.FSDataOutputStream$PositionCache": {
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)": "/**\n* Initializes PositionCache with output stream, statistics, and position.\n* @param out output stream for data\n* @param stats file system statistics\n* @param pos initial position in the cache\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:close()": "/**\n* Closes the output stream if it is not null.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:getPos()": "/**\n* Returns the cached position value.\n* @return current position as a long\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int)": "/**\n* Writes a byte to the output and updates position and statistics.\n* @param b the byte to write\n*/",
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)": "/**\n* Writes bytes to output and updates position and statistics.\n* @param b byte array to write, @param off offset, @param len number of bytes to write\n*/"
    },
    "org.apache.hadoop.fs.FSOutputSummer": {
        "org.apache.hadoop.fs.FSOutputSummer:createWriteTraceScope()": "/**\n* Creates a write trace scope.\n* @return TraceScope object, currently always returns null\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:int2byte(int,byte[])": "/**\n* Converts an integer to a byte array.\n* @param integer the integer to convert\n* @param bytes the byte array to store the result\n* @return the byte array containing the integer bytes\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:getBufferedDataSize()": "/**\n* Retrieves the current size of buffered data.\n* @return the size of buffered data as an integer\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:getDataChecksum()": "/**\n* Retrieves the current data checksum.\n* @return DataChecksum object representing the checksum\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:hasCapability(java.lang.String)": "/**\n* Checks if the object has the specified capability.\n* @param capability the capability to check\n* @return false, as no capabilities are supported\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:getChecksumSize()": "",
        "org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)": "",
        "org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum)": "/**\n* Initializes FSOutputSummer with checksum data.\n* @param sum DataChecksum object for checksum calculations\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int)": "/**\n* Sets buffer size and initializes checksum array.\n* @param size new size for the buffer\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize()": "/**\n* Resets checksum buffer size based on bytes per checksum.\n* @param BUFFER_NUM_CHUNKS multiplier for buffer size calculation\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)": "/**\n* Writes checksum chunks from byte array.\n* @param b input byte array, @param off start offset, @param len length to process\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)": "/**\n* Flushes the buffer, optionally keeping or flushing partial data.\n* @param keep whether to retain unflushed data\n* @param flushPartial if true, flushes partial data\n* @return number of unflushed bytes remaining\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:flushBuffer()": "/**\n* Flushes the buffer, discarding unflushed data and ensuring all is written.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:flush()": "/**\n* Flushes the output buffer, discarding any unflushed data.\n* @throws IOException if an I/O error occurs during the flush\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:write(int)": "/**\n* Writes a byte to the buffer and flushes if full.\n* @param b byte to write\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)": "/**\n* Writes data to a local buffer and checksums if buffer is empty.\n* @param b input byte array, @param off start offset, @param len length to process\n* @return number of bytes written to the buffer\n*/",
        "org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)": "/**\n* Writes bytes to a buffer, ensuring valid indices and handling IOException.\n* @param b byte array to write, @param off start offset, @param len length to write\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference": {
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:getData()": "/**\n* Retrieves the StatisticsData object.\n* @return StatisticsData instance containing statistical information\n*/",
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp()": ""
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$6": {
        "org.apache.hadoop.fs.FileSystem$Statistics$6:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Constructs a copy of the given Statistics object.\n* @param other the Statistics instance to copy from\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$7": {
        "org.apache.hadoop.fs.FileSystem$Statistics$7:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Copies statistics from another Statistics instance.\n* @param other the Statistics object to copy from\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$9": {
        "org.apache.hadoop.fs.FileSystem$Statistics$9:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Constructs a new Statistics object by copying data from another Statistics instance.\n* @param other the Statistics instance to copy data from\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$10": {
        "org.apache.hadoop.fs.FileSystem$Statistics$10:<init>(org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Copies data from another Statistics instance.\n* @param other the Statistics instance to copy from\n*/"
    },
    "org.apache.hadoop.fs.QuotaUsage$Builder": {
        "org.apache.hadoop.fs.QuotaUsage$Builder:<init>()": "/**\n* Initializes a Builder with default quota values and storage type arrays.\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeConsumed(long[])": "/**\n* Sets the consumed types and returns the builder instance.\n* @param typeConsumed array of consumed types\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)": "/**\n* Sets the quota for a specific storage type.\n* @param type the storage type\n* @param quota the quota value to set\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)": "/**\n* Sets the consumed storage amount for a specific type.\n* @param type the storage type\n* @param consumed the amount consumed\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeQuota(long[])": "/**\n* Sets the type quota values.\n* @param typeQuota array of long values for type quotas\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:quota(long)": "/**\n* Sets the quota value and returns the builder instance.\n* @param quota the quota to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:spaceConsumed(long)": "/**\n* Sets the consumed space and returns the builder instance.\n* @param spaceConsumed amount of space consumed\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:spaceQuota(long)": "/**\n* Sets the space quota and returns the Builder instance.\n* @param spaceQuota the amount of space to allocate\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:fileAndDirectoryCount(long)": "/**\n* Sets the file and directory count.\n* @param count total number of files and directories\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.QuotaUsage$Builder:build()": ""
    },
    "org.apache.hadoop.fs.QuotaUsage": {
        "org.apache.hadoop.fs.QuotaUsage:<init>(org.apache.hadoop.fs.QuotaUsage$Builder)": "/**\n* Constructs a QuotaUsage object from a Builder.\n* @param builder contains values for initializing QuotaUsage fields\n*/",
        "org.apache.hadoop.fs.QuotaUsage:<init>()": "/**\n* Constructor for QuotaUsage class.\n*/",
        "org.apache.hadoop.fs.QuotaUsage:equals(java.lang.Object)": "/**\n* Compares this QuotaUsage object with another for equality.\n* @param obj object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.QuotaUsage:hashCode()": "/**\n* Computes hash code based on various attributes of the object.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getStorageTypeHeader(java.util.List)": "/**\n* Generates a formatted header for storage types.\n* @param storageTypes list of StorageType objects\n* @return formatted header string for storage types\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getHeader()": "/**\n* Retrieves the quota header string.\n* @return the quota header constant\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getTypeQuota(org.apache.hadoop.fs.StorageType)": "/**\n* Retrieves the quota for a specified storage type.\n* @param type the storage type to get the quota for\n* @return quota value or -1 if typeQuota is null\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getTypeConsumed(org.apache.hadoop.fs.StorageType)": "/**\n* Returns the amount consumed for the specified storage type.\n* @param type the storage type to query\n* @return long value of consumed amount or 0 if not set\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getFileAndDirectoryCount()": "/**\n* Returns the total count of files and directories.\n* @return long representing the number of files and directories\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getQuota()": "/**\n* Retrieves the current quota value.\n* @return the quota as a long value\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getSpaceConsumed()": "/**\n* Retrieves the total space consumed.\n* @return long representing the space consumed\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getSpaceQuota()": "/**\n* Retrieves the current space quota value.\n* @return long representing the space quota\n*/",
        "org.apache.hadoop.fs.QuotaUsage:setQuota(long)": "/**\n* Sets the quota value.\n* @param quota new quota value to be assigned\n*/",
        "org.apache.hadoop.fs.QuotaUsage:setSpaceConsumed(long)": "/**\n* Sets the amount of space consumed.\n* @param spaceConsumed the space amount to set\n*/",
        "org.apache.hadoop.fs.QuotaUsage:setSpaceQuota(long)": "/**\n* Sets the space quota value.\n* @param spaceQuota the new space quota to be set\n*/",
        "org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet()": "/**\n* Checks if a quota is set for any storage type.\n* @return true if any type has a quota set, otherwise false\n*/",
        "org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable()": "/**\n* Checks if any consumed type is available based on quota.\n* @return true if available, false otherwise\n*/",
        "org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)": "/**\n* Formats size as a string, either human-readable or raw.\n* @param size the size in bytes\n* @param humanReadable true for formatted string, false for raw value\n* @return formatted size string\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean)": "/**\n* Retrieves formatted quota usage and remaining space based on options.\n* @param hOption true for human-readable format\n* @return formatted quota usage string\n*/",
        "org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)": "/**\n* Generates quota usage summary for specified storage types.\n* @param hOption true for human-readable format\n* @param types list of storage types to evaluate\n* @return formatted quota usage summary string\n*/",
        "org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)": "/**\n* Returns quota usage summary based on options.\n* @param hOption true for human-readable format\n* @param tOption true to get types quota usage\n* @param types list of storage types to evaluate\n* @return formatted quota usage summary string\n*/",
        "org.apache.hadoop.fs.QuotaUsage:toString(boolean)": "/**\n* Returns string representation based on human-readable option.\n* @param hOption true for human-readable format\n* @return formatted string representation\n*/",
        "org.apache.hadoop.fs.QuotaUsage:toString()": "/**\n* Returns string representation of the object.\n* @return formatted string representation\n*/"
    },
    "org.apache.hadoop.fs.ContentSummary$Builder": {
        "org.apache.hadoop.fs.ContentSummary$Builder:length(long)": "/**\n* Sets the length and returns the Builder instance.\n* @param length the desired length to set\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.ContentSummary$Builder:fileCount(long)": "/**\n* Sets the file count and returns the builder instance.\n* @param fileCount number of files to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.ContentSummary$Builder:directoryCount(long)": "/**\n* Sets the directory count and returns the builder instance.\n* @param directoryCount number of directories to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.ContentSummary$Builder:<init>()": "",
        "org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[])": "",
        "org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)": "",
        "org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)": "",
        "org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[])": "",
        "org.apache.hadoop.fs.ContentSummary$Builder:quota(long)": "/**\n* Sets the quota and returns the builder instance for method chaining.\n* @param quota the quota to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long)": "/**\n* Sets the consumed space and returns the builder instance.\n* @param spaceConsumed amount of space consumed\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long)": "/**\n* Sets the space quota and returns the Builder for chaining.\n* @param spaceQuota the amount of space to allocate\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.ContentSummary$Builder:build()": "/**\n* Builds a ContentSummary with file and directory counts.\n* @return ContentSummary object containing the counts\n*/"
    },
    "org.apache.hadoop.fs.FileContext$Util": {
        "org.apache.hadoop.fs.FileContext$Util:listFiles(org.apache.hadoop.fs.Path,boolean)": "/**\n* Lists files in a directory, optionally recursively.\n* @param f directory path to list files from\n* @param recursive true to list files in subdirectories\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path.\n* @param f the Path to list statuses for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists file statuses for a given path filtered by a specified criteria.\n* @param results collection to store filtered FileStatus objects\n* @param f the Path to list statuses for\n* @param filter criteria to filter the file statuses\n*/",
        "org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists at the specified path.\n* @param f the Path to check\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path)": "/**\n* Retrieves content summary for a given path.\n* @param f the Path to summarize\n* @return ContentSummary object with counts and length\n*/",
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists file statuses for a given path filtered by specified criteria.\n* @param f the Path to list statuses for\n* @param filter criteria to filter the file statuses\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)": "/**\n* Lists file statuses for an array of paths filtered by the specified criteria.\n* @param files array of paths to list statuses for\n* @param filter criteria to filter the file statuses\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[])": "/**\n* Lists file statuses for an array of paths using default filtering.\n* @param files array of paths to list statuses for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file statuses matching the given path pattern.\n* @param pathPattern pattern for matching file paths\n* @return array of matching FileStatus or null if no matches found\n*/",
        "org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Retrieves file statuses matching a given path pattern and filter.\n* @param pathPattern pattern for matching paths\n* @param filter filter to apply on paths\n* @return array of matching FileStatus or null if no matches found\n*/",
        "org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)": "/**\n* Copies a file or directory from src to dst; optionally deletes source after copy.\n* @param src source Path to copy from\n* @param dst destination Path to copy to\n* @param deleteSource true to delete src after copy, false to keep it\n* @param overwrite true to overwrite dst if it exists, false to prevent\n* @return true if copy successful, false if deleteSource is false\n*/",
        "org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from src to dst without deleting the source or overwriting existing files.\n* @param src source Path to copy from\n* @param dst destination Path to copy to\n* @return true if copy successful\n*/"
    },
    "org.apache.hadoop.fs.Options$OpenFileOptions": {
        "org.apache.hadoop.fs.Options$OpenFileOptions:<init>()": "/**\n* Private constructor for OpenFileOptions class to prevent instantiation.\n*/"
    },
    "org.apache.hadoop.fs.Options$ChecksumOpt": {
        "org.apache.hadoop.fs.Options$ChecksumOpt:<init>(org.apache.hadoop.util.DataChecksum$Type,int)": "/**\n* Initializes ChecksumOpt with specified type and size.\n* @param type the checksum type\n* @param size the number of bytes per checksum\n*/",
        "org.apache.hadoop.fs.Options$ChecksumOpt:toString()": "/**\n* Returns a string representation of the checksum type and bytes per checksum.\n* @return formatted string \"checksumType:bytesPerChecksum\"\n*/",
        "org.apache.hadoop.fs.Options$ChecksumOpt:getChecksumType()": "/**\n* Retrieves the type of checksum.\n* @return DataChecksum.Type representing the checksum type\n*/",
        "org.apache.hadoop.fs.Options$ChecksumOpt:getBytesPerChecksum()": "/**\n* Retrieves the number of bytes per checksum.\n* @return int representing bytes per checksum\n*/",
        "org.apache.hadoop.fs.Options$ChecksumOpt:<init>()": "/**** Initializes ChecksumOpt with default type and unspecified size. */",
        "org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled()": "/**\n* Creates a disabled ChecksumOpt instance.\n* @return ChecksumOpt with NULL type and -1 size\n*/",
        "org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)": "/**\n* Processes checksum options based on user input and defaults.\n* @param defaultOpt default ChecksumOpt object\n* @param userOpt user-provided ChecksumOpt object\n* @param userBytesPerChecksum user-specified bytes per checksum\n* @return ChecksumOpt configured with appropriate type and size\n*/",
        "org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Processes checksum options using defaults and user inputs.\n* @param defaultOpt default ChecksumOpt object\n* @param userOpt user-provided ChecksumOpt object\n* @return configured ChecksumOpt\n*/"
    },
    "org.apache.hadoop.io.MD5Hash": {
        "org.apache.hadoop.io.MD5Hash:write(java.io.DataOutput)": "/**\n* Writes the digest to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.MD5Hash:<init>()": "/**\n* Initializes an MD5Hash instance with a byte array for the MD5 digest.\n*/",
        "org.apache.hadoop.io.MD5Hash:<init>(byte[])": "/**\n* Constructs an MD5Hash with a byte array.\n* @param digest byte array of MD5 hash, must be 16 bytes long\n*/",
        "org.apache.hadoop.io.MD5Hash:readFields(java.io.DataInput)": "/**\n* Reads data into the digest array from the specified input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.MD5Hash:set(org.apache.hadoop.io.MD5Hash)": "/**\n* Copies MD5 hash digest from the given object to this instance.\n* @param that MD5Hash object to copy from\n*/",
        "org.apache.hadoop.io.MD5Hash:getDigester()": "/**\n* Retrieves and resets a MessageDigest instance.\n* @return a fresh MessageDigest object\n*/",
        "org.apache.hadoop.io.MD5Hash:equals(java.lang.Object)": "/**\n* Compares this MD5Hash with another object for equality.\n* @param o object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.MD5Hash:quarterDigest()": "/**\n* Combines four bytes of digest into a single integer.\n* @return integer representation of the digest\n*/",
        "org.apache.hadoop.io.MD5Hash:toString()": "/**\n* Converts the MD5 digest to a hexadecimal string representation.\n* @return Hexadecimal string of the MD5 digest\n*/",
        "org.apache.hadoop.io.MD5Hash:charToNibble(char)": "/**\n* Converts a hex character to its corresponding nibble value.\n* @param c hex character (0-9, a-f, A-F)\n* @return integer nibble value (0-15)\n*/",
        "org.apache.hadoop.io.MD5Hash:read(java.io.DataInput)": "/**\n* Reads MD5Hash from DataInput stream.\n* @param in DataInput stream to read from\n* @return MD5Hash instance populated with data\n*/",
        "org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream)": "/**\n* Computes MD5 hash from input stream.\n* @param in input stream to read data from\n* @return MD5Hash object containing the hash\n*/",
        "org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)": "/**\n* Computes MD5 hash from specified byte array segment.\n* @param data byte array to hash, @param start start index, @param len length of segment\n* @return MD5Hash object containing the hash\n*/",
        "org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)": "/**\n* Computes MD5 hash from byte arrays.\n* @param dataArr input byte arrays, @param start offset, @param len length to process\n* @return MD5Hash object containing the computed hash\n*/",
        "org.apache.hadoop.io.MD5Hash:hashCode()": "/**\n* Computes the hash code using a combined digest.\n* @return integer hash code derived from the digest\n*/",
        "org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String)": "/**\n* Sets the digest from a hex string.\n* @param hex MD5 hex string (32 characters)\n*/",
        "org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash)": "/**\n* Compares this MD5Hash with another for order.\n* @param that the MD5Hash to compare against\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.MD5Hash:digest(byte[])": "/**\n* Computes MD5 hash of the entire byte array.\n* @param data byte array to hash\n* @return MD5Hash object containing the hash\n*/",
        "org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8)": "/**\n* Computes MD5 hash from a UTF8 object.\n* @param utf8 the UTF8 object to hash\n* @return MD5Hash object containing the hash\n*/",
        "org.apache.hadoop.io.MD5Hash:<init>(java.lang.String)": "/**\n* Constructs MD5Hash from a hex string.\n* @param hex MD5 hex string (32 characters)\n*/",
        "org.apache.hadoop.io.MD5Hash:digest(java.lang.String)": "/**\n* Computes MD5 hash of a UTF-8 encoded string.\n* @param string input string to hash\n* @return MD5Hash object containing the hash\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem$Store": {
        "org.apache.hadoop.fs.HarFileSystem$Store:<init>(long,long)": "/**\n* Initializes a Store with specified time range.\n* @param begin start time in milliseconds\n* @param end end time in milliseconds\n*/"
    },
    "org.apache.hadoop.HadoopIllegalArgumentException": {
        "org.apache.hadoop.HadoopIllegalArgumentException:<init>(java.lang.String)": "/**\n* Constructs a HadoopIllegalArgumentException with a specified message.\n* @param message detail message for the exception\n*/"
    },
    "org.apache.hadoop.fs.FileAlreadyExistsException": {
        "org.apache.hadoop.fs.FileAlreadyExistsException:<init>(java.lang.String)": "/**\n* Constructs a FileAlreadyExistsException with the specified detail message.\n* @param msg the detail message\n*/",
        "org.apache.hadoop.fs.FileAlreadyExistsException:<init>()": "/**\n* Constructs a new FileAlreadyExistsException with no detail message.\n*/"
    },
    "org.apache.hadoop.fs.DelegateToFileSystem": {
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file at the given path.\n* @param f the path of the file\n* @return FileStatus object representing the file's status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory at the specified path.\n* @param f path to the file or directory to delete\n* @param recursive true to delete directories recursively\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists the status of files at the given path.\n* @param f the path to check\n* @return an array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f the path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the opened file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getDelegationTokens(java.lang.String)": "/**\n* Retrieves delegation tokens for the specified renewer.\n* @param renewer the identifier for the token renewer\n* @return a list of delegation tokens\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "",
        "org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file system permissions for the specified path.\n* @param f the path to set permissions on\n* @param permission the permissions to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem)": "/**\n* Retrieves the default port from FileSystem or uses a fallback value.\n* @param theFsImpl FileSystem implementation instance\n* @return default port number, or fallback if not defined\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory()": "/**\n* Returns the initial working directory path.\n* @return Path object representing the initial directory, or null if unavailable\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the file link status, updating symlink if applicable.\n* @param f the path to the file link\n* @return FileStatus of the specified file link\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a symbolic link.\n* @param f the path of the symbolic link\n* @return Path of the link target\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates the specified file to a new length.\n* @param f the file path to truncate\n* @param newLength the new length of the file\n* @return true if successful, otherwise throws IOException\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified file path.\n* @param f the file path to set replication for\n* @param replication the desired replication factor\n* @return true if operation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a file.\n* @param f the file path to modify\n* @param mtime the new modification time\n* @param atime the new access time\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean)": "/**\n* Sets the verifyChecksum flag for file system operations.\n* @param verifyChecksum flag to enable or disable checksum verification\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks()": "/**\n* Checks if the system supports symbolic links.\n* @return true if symlinks are supported, false otherwise\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link to a target path.\n* @param target the path to the target file\n* @param link the path where the symlink will be created\n* @param createParent flag to create parent directories if needed\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort()": "/**\n* Retrieves the default port for the URI.\n* @return default port number from FileSystem\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file system status for a given path.\n* @param f the path to retrieve status for\n* @return FsStatus object with file system details\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file checksum for the specified path.\n* @param f file path\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options.\n* @param path file path to open\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus()": "/**\n* Retrieves the file system status.\n* @return FsStatus object with capacity, used, and remaining space\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Creates a file or appends to it, verifying parent directory existence.\n* @param f file path to create or append\n* @param flag creation options\n* @param absolutePermission permissions for the file\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize size of the blocks\n* @param progress progress callback\n* @param checksumOpt checksum options\n* @param createParent whether to create parent directories\n* @return FSDataOutputStream for the created file\n* @throws IOException if an error occurs during creation\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory with specified permissions.\n* @param dir path of the directory to create\n* @param permission permissions to set for the new directory\n* @param createParent indicates if parent directories should be created if missing\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory from src to dst.\n* @param src source path to rename\n* @param dst destination path\n* @throws IOException if an error occurs during renaming\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a specified file and byte range.\n* @param f file path; @param start start position; @param len range length\n* @return array of BlockLocation\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName()": "/**\n* Retrieves canonical service name from file system implementation.\n* @return canonical service name or null if absent\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)": "/**\n* Constructs DelegateToFileSystem with URI and FileSystem implementation.\n* @param theUri URI for the file system\n* @param theFsImpl FileSystem instance to delegate operations\n* @param conf configuration settings\n* @param supportedScheme expected URI scheme\n* @param authorityRequired indicates if authority is needed\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults()": "/**\n* Retrieves file storage defaults; deprecated method.\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Returns file storage defaults for the specified path.\n* @param f the file path to retrieve defaults for\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the specified path supports the given capability.\n* @param path the Path to check\n* @param capability the capability to verify\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory()": "/**\n* Retrieves the home directory path for the current user.\n* @return qualified Path object for the user's home directory\n*/"
    },
    "org.apache.hadoop.fs.ParentNotDirectoryException": {
        "org.apache.hadoop.fs.ParentNotDirectoryException:<init>(java.lang.String)": "/**\n* Constructs a ParentNotDirectoryException with the specified message.\n* @param msg detail message for the exception\n*/",
        "org.apache.hadoop.fs.ParentNotDirectoryException:<init>()": "/**\n* Constructs a ParentNotDirectoryException with no detail message.\n*/"
    },
    "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream": {
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:<init>(java.io.InputStream)": "/**\n* Constructs an HttpDataInputStream from the given InputStream.\n* @param in the InputStream to read data from\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:read(long,byte[],int,int)": "/**\n* Reads bytes into a buffer from a specified position.\n* @param position start position in the data source\n* @param buffer destination array for read bytes\n* @param offset start offset in the buffer\n* @param length number of bytes to read\n* @return number of bytes read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readFully(long,byte[],int,int)": "/**\n* Reads bytes into buffer from specified position.\n* @param position start position in the data source\n* @param buffer byte array to fill with data\n* @param offset starting index in buffer\n* @param length number of bytes to read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readFully(long,byte[])": "/**\n* Reads bytes into buffer from the specified position.\n* @param position starting position in the data source\n* @param buffer byte array to fill with data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:seek(long)": "/**\n* Seeks to a specified position in the stream.\n* @param pos the position to seek to, in bytes\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:getPos()": "/**\n* Retrieves the current position, unsupported in this implementation.\n* @return long current position\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:seekToNewSource(long)": "/**\n* Throws UnsupportedOperationException when seeking to a new source.\n* @param targetPos position to seek to\n* @return false, as the operation is unsupported\n*/"
    },
    "org.apache.hadoop.fs.http.AbstractHttpFileSystem": {
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file at the specified path with given permissions.\n* @param path file path to create\n* @param fsPermission permissions for the file\n* @param b flag for overwrite\n* @param i buffer size\n* @param i1 replication factor\n* @param l block size\n* @param progressable progress callback\n* @throws IOException if operation is not supported\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param path file path to append data\n* @param i unused parameter\n* @param progressable callback for progress updates\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory at the specified path.\n* @param path current file or directory path\n* @param path1 new file or directory path\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes the specified path.\n* @param path the path to delete\n* @param b flag indicating deletion options\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for the given path.\n* @param path the directory path to list statuses for\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates the directory specified by the path.\n* @param path directory path to create\n* @param fsPermission permissions for the new directory\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getWorkingDirectory()": "/**\n* Retrieves the current working directory path.\n* @return Path object representing the working directory\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory to the specified path.\n* @param path the new working directory path\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getUri()": "/**\n* Retrieves the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**** Opens a stream to a file at the given path. \n* @param path file location \n* @param bufferSize size of the buffer \n* @return FSDataInputStream for reading the file \n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status for the given path.\n* @param path the file path to check status\n* @return FileStatus object with default attributes\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the object with a URI and configuration.\n* @param name URI for initialization\n* @param conf configuration object\n*/",
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the path supports the specified capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if capability is supported, false otherwise\n*/"
    },
    "org.apache.hadoop.tracing.Tracer$Builder": {
        "org.apache.hadoop.tracing.Tracer$Builder:<init>(java.lang.String)": "/**\n* Initializes a Builder with the specified name.\n* @param name the name to associate with the Builder\n*/",
        "org.apache.hadoop.tracing.Tracer$Builder:conf(org.apache.hadoop.tracing.TraceConfiguration)": "/**\n* Sets the TraceConfiguration for the builder.\n* @param conf TraceConfiguration object to be set\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.tracing.Tracer$Builder:build()": "/**\n* Builds or retrieves the global Tracer instance.\n* @return Tracer instance, creates one if it doesn't exist\n*/"
    },
    "org.apache.hadoop.tracing.TraceUtils": {
        "org.apache.hadoop.tracing.TraceUtils:wrapHadoopConf(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Wraps Hadoop configuration with a specified prefix.\n* @param prefix configuration prefix\n* @param conf Hadoop Configuration object\n* @return TraceConfiguration object\n*/",
        "org.apache.hadoop.tracing.TraceUtils:byteStringToSpanContext(org.apache.hadoop.thirdparty.protobuf.ByteString)": "/**\n* Converts ByteString to SpanContext.\n* @param byteString the ByteString to convert\n* @return corresponding SpanContext object\n*/",
        "org.apache.hadoop.tracing.TraceUtils:spanContextToByteString(org.apache.hadoop.tracing.SpanContext)": "/**\n* Converts SpanContext to ByteString representation.\n* @param context SpanContext to convert\n* @return ByteString representation of the SpanContext\n*/"
    },
    "org.apache.hadoop.fs.FsTracer": {
        "org.apache.hadoop.fs.FsTracer:<init>()": "/**\n* Private constructor for FsTracer to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves or creates a global Tracer instance.\n* @param conf Hadoop Configuration object\n* @return Tracer instance\n*/"
    },
    "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException": {
        "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException:<init>()": "/**\n* Constructs a DirectoryListingStartAfterNotFoundException.\n*/",
        "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException:<init>(java.lang.String)": "/**\n* Constructs an exception with a specified message.\n* @param msg detailed error message for the exception\n*/"
    },
    "org.apache.hadoop.fs.BufferedFSInputStream": {
        "org.apache.hadoop.fs.BufferedFSInputStream:<init>(org.apache.hadoop.fs.FSInputStream,int)": "/**\n* Constructs a BufferedFSInputStream with specified input stream and buffer size.\n* @param in input stream to be buffered\n* @param size buffer size in bytes\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:getPos()": "/**\n* Retrieves the current position in the input stream.\n* @return current position offset by count and pos\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:seek(long)": "/**\n* Seeks to the specified position in the input stream.\n* @param pos the position to seek to, must be non-negative\n* @throws IOException if the stream is closed or an I/O error occurs\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:seekToNewSource(long)": "/**\n* Seeks to a new source position in the input stream.\n* @param targetPos the position to seek to\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:getFileDescriptor()": "/**\n* Retrieves the file descriptor if available.\n* @return FileDescriptor or null if not applicable\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:hasCapability(java.lang.String)": "/**\n* Checks if the input stream has the specified capability.\n* @param capability the capability to check for\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:toString()": "/**\n* Returns a string representation of the BufferedFSInputStream object.\n* @return formatted string with input stream information\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:skip(long)": "/**\n* Skips the specified number of bytes in the input stream.\n* @param n number of bytes to skip, must be positive\n* @return number of bytes skipped\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads()": "/**\n* Delegates to input stream for minimum seek size for vector reads.\n* @return minimum seek size in bytes\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads()": "/**\n* Returns the maximum read size for vector reads.\n* @return maximum read size in bytes (1 MB)\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics using the input object.\n* @return IOStatistics object or null if invalid input\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position into a buffer.\n* @param position start position in the source\n* @param buffer destination byte array\n* @param offset start index in the buffer\n* @param length number of bytes to read\n* @return number of bytes read or -1 on EOF\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)": "/**\n* Reads bytes into a buffer from a specified position in the input stream.\n* @param position start position in the source\n* @param buffer destination byte array\n* @param offset start index in the buffer\n* @param length number of bytes to read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])": "/**\n* Reads entire buffer from specified position in input stream.\n* @param position start position in the source\n* @param buffer destination byte array\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)": "/**\n* Reads data into FileRange objects using a provided allocation function.\n* @param ranges list of FileRange objects to read data into\n* @param allocate function to allocate ByteBuffer instances\n* @throws IOException if an I/O error occurs during reading\n*/"
    },
    "org.apache.hadoop.fs.PositionedReadable": {
        "org.apache.hadoop.fs.PositionedReadable:minSeekForVectorReads()": "/**\n* Returns the minimum seek size for vector reads.\n* @return minimum seek size in bytes (4096)\n*/",
        "org.apache.hadoop.fs.PositionedReadable:maxReadSizeForVectorReads()": "/**\n* Returns the maximum read size for vector reads.\n* @return maximum read size in bytes (1 MB)\n*/",
        "org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)": "/**\n* Reads data into FileRange objects using a provided allocation function.\n* @param ranges list of FileRange objects to read data into\n* @param allocate function to allocate ByteBuffer instances\n* @throws IOException if an I/O error occurs during reading\n*/"
    },
    "org.apache.hadoop.fs.BBPartHandle": {
        "org.apache.hadoop.fs.BBPartHandle:<init>(java.nio.ByteBuffer)": "/**\n* Initializes BBPartHandle with byte array from ByteBuffer.\n* @param byteBuffer source ByteBuffer to extract bytes\n*/",
        "org.apache.hadoop.fs.BBPartHandle:bytes()": "/**\n* Wraps byte array into a ByteBuffer.\n* @return ByteBuffer containing the wrapped bytes\n*/",
        "org.apache.hadoop.fs.BBPartHandle:hashCode()": "/**\n* Computes the hash code for the object based on its byte array.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer)": "/**\n* Creates a PartHandle from a ByteBuffer.\n* @param byteBuffer source ByteBuffer for PartHandle initialization\n* @return PartHandle object created from the ByteBuffer\n*/",
        "org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object)": "/**\n* Compares this PartHandle to another object for equality.\n* @param other object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.ftp.FTPException": {
        "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.String)": "/**\n* Constructs an FTPException with the specified message.\n* @param message detail message for the exception\n*/",
        "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an FTPException with a message and cause.\n* @param message error message\n* @param t cause of the exception\n*/",
        "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.Throwable)": "/**\n* Constructs an FTPException with the specified cause.\n* @param t the cause of the exception\n*/"
    },
    "org.apache.hadoop.fs.ftp.FTPInputStream": {
        "org.apache.hadoop.fs.ftp.FTPInputStream:<init>(java.io.InputStream,org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.FileSystem$Statistics)": "/**\n* Initializes FTPInputStream with an InputStream and FTP client.\n* @param stream input stream to wrap\n* @param client connected FTP client\n* @param stats file system statistics\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:seek(long)": "/**\n* Throws IOException indicating seek operation is not supported.\n* @param pos position to seek to (ignored)\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:seekToNewSource(long)": "/**\n* Throws IOException indicating seeking to a new source is not supported.\n* @param targetPos the position to seek to\n* @throws IOException if seeking is attempted\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:reset()": "/**\n* Resets the state, throwing an exception as mark is not supported.\n* @throws IOException if mark operation is not supported\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:getPos()": "/**\n* Retrieves the current position value.\n* @return current position as a long\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:close()": "/**\n* Closes the FTP client connection and handles pending commands.\n* @throws IOException if an I/O error occurs or client is not connected\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:read()": "/**\n* Reads a byte from the stream and updates position and stats.\n* @return byte read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)": "/**\n* Reads bytes into a buffer, updating position and stats if successful.\n* @param buf byte array to read into, @param off offset, @param len number of bytes to read\n* @return number of bytes read or -1 if end of stream\n*/"
    },
    "org.apache.hadoop.fs.ftp.FTPFileSystem": {
        "org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Throws exception as appending files is unsupported in FTPFileSystem.\n* @param f file path to append to\n* @param bufferSize size of the buffer for the operation\n* @param progress callback for progress updates\n* @throws IOException if append operation is attempted\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getUri()": "/**\n* Retrieves the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory; state is not maintained.\n* @param newDir the new working directory path\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient)": "/**\n* Disconnects the FTP client and handles logout.\n* @param client the FTPClient to disconnect\n* @throws IOException if an I/O error occurs during disconnection\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)": "/**\n* Retrieves FsAction based on access permissions for a given FTPFile.\n* @param accessGroup user access group ID\n* @param ftpFile the FTPFile to check permissions on\n* @return combined FsAction for READ, WRITE, EXECUTE permissions\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile)": "/**\n* Retrieves FsPermission based on FTPFile access levels.\n* @param ftpFile the FTPFile to check permissions on\n* @return FsPermission object with user, group, and other actions\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Converts a relative path to an absolute path using the specified working directory.\n* @param workDir the base directory for resolving the relative path\n* @param path the path to be converted\n* @return the absolute Path object\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)": "/****\n* Constructs FileStatus from FTPFile and parent path.\n* @param ftpFile the FTPFile to extract attributes from\n* @param parentPath the parent directory path\n* @return FileStatus object with file attributes\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus for a given file from an FTP server.\n* @param client FTP client to connect to the server\n* @param file Path of the file to retrieve status for\n* @return FileStatus object representing the file attributes\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists on the FTP server.\n* @param client FTP client for server connection\n* @param file Path of the file to check\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path on the FTP server.\n* @param client FTP client to connect to the server\n* @param file path to list statuses for\n* @return array of FileStatus objects for the files\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)": "/**\n* Checks if a given path is a file on the FTP server.\n* @param client FTP client to connect to the server\n* @param file Path of the file to check\n* @return true if it's a file, false if not or not found\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file on the FTP server.\n* @param client FTP client for server connection\n* @param src source file path\n* @param dst destination file path\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory from FTP server.\n* @param client FTP client, @param file Path to delete, @param recursive true for dir deletion\n* @return true if deleted successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories recursively on the FTP server.\n* @param client FTP client for server operations\n* @param file Path of the directory to create\n* @param permission Directory permissions to set\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration)": "/**\n* Determines the FTP transfer mode from configuration.\n* @param conf configuration object\n* @return transfer mode constant based on configuration or default value\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)": "/**\n* Sets the data connection mode for the FTP client based on configuration.\n* @param client FTPClient instance to set mode on\n* @param conf Configuration containing mode settings\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)": "/**\n* Sets the control keep-alive timeout for the FTP client.\n* @param client FTPClient instance to configure\n* @param conf Configuration object to retrieve timeout setting\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:connect()": "/**\n* Connects to an FTP server and returns an FTPClient instance.\n* @return FTPClient connected to the server\n* @throws IOException if connection or login fails\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes configuration from URI; sets host, port, user, and password.\n* @param uri the URI containing configuration information\n* @param conf the configuration object to update\n* @throws IOException if host or user/password is invalid\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file stream for reading from an FTP server.\n* @param file the path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the specified file\n* @throws IOException if an error occurs during connection or file retrieval\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file on the FTP server with specified permissions.\n* @param file path of the file to create\n* @param permission permissions to set for the file\n* @return FSDataOutputStream for writing to the file\n* @throws IOException if an error occurs during creation\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory from FTP server.\n* @param file Path to delete, @param recursive true for dir deletion\n* @return true if deleted successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses at a given path on the FTP server.\n* @param file path to list statuses for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status from an FTP server.\n* @param file Path of the file to retrieve status for\n* @return FileStatus object representing the file attributes\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories on the FTP server.\n* @param file Path of the directory to create\n* @param permission Directory permissions to set\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file on the FTP server.\n* @param src source file path\n* @param dst destination file path\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory()": "/**\n* Retrieves the home directory path from the connected FTP server.\n* @return Path of the home directory\n*/",
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory()": "/**\n* Returns the working directory, which is the home directory.\n* @return Path of the home directory\n*/"
    },
    "org.apache.hadoop.fs.PartHandle": {
        "org.apache.hadoop.fs.PartHandle:toByteArray()": "/**\n* Converts the current object to a byte array.\n* @return byte array representation of the object\n*/"
    },
    "org.apache.hadoop.util.Time": {
        "org.apache.hadoop.util.Time:now()": "/**\n* Returns the current time in milliseconds since epoch.\n* @return current time in milliseconds\n*/",
        "org.apache.hadoop.util.Time:monotonicNow()": "/**\n* Returns the current time in milliseconds since a monotonic clock.\n* @return time in milliseconds\n*/",
        "org.apache.hadoop.util.Time:formatTime(long)": "/**\n* Formats time in milliseconds to a string representation.\n* @param millis time in milliseconds\n* @return formatted time as a String\n*/",
        "org.apache.hadoop.util.Time:monotonicNowNanos()": "/**\n* Returns the current value of the system timer in nanoseconds.\n* @return current time in nanoseconds since the start of the epoch\n*/",
        "org.apache.hadoop.util.Time:getUtcTime()": "/**\n* Retrieves the current UTC time in milliseconds.\n* @return current time in milliseconds since epoch\n*/"
    },
    "org.apache.hadoop.fs.TrashPolicyDefault$Emptier": {
        "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:floor(long,long)": "/**\n* Calculates the largest multiple of interval less than or equal to time.\n* @param time the time value to be floored\n* @param interval the interval to round down to\n* @return floored time value\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)": "/**\n* Computes the smallest multiple of interval greater than time.\n* @param time the time value to be rounded up\n* @param interval the interval to round up to\n* @return adjusted time value\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run()": "/***************\n* Periodically empties trash directories based on the specified interval.\n* @throws InterruptedException if the thread is interrupted during sleep\n***************/"
    },
    "org.apache.hadoop.fs.ChecksumFileSystem": {
        "org.apache.hadoop.fs.ChecksumFileSystem:getRawFileSystem()": "/**\n* Retrieves the raw file system instance.\n* @return FileSystem object representing the raw file system\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:getBytesPerSum()": "/**\n* Returns the number of bytes per checksum.\n* @return bytesPerChecksum value\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)": "/**\n* Reports a checksum failure for a given file.\n* @param f file path to check\n* @param in input stream of the file\n* @param inPos position in the input stream\n* @param sums input stream of checksums\n* @param sumsPos position in the checksum stream\n* @return false indicating failure reporting status\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumLength(long,int)": "/**\n* Calculates the total checksum length based on file size and bytes per checksum.\n* @param size total file size in bytes\n* @param bytesPerSum number of bytes per checksum\n* @return total checksum length in bytes\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Throws an exception indicating append is unsupported for ChecksumFileSystem.\n* @param f file path to append to\n* @param bufferSize size of the buffer for output stream\n* @param progress callback for progress updates\n* @throws IOException if append operation is attempted\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Throws an exception indicating truncation is unsupported.\n* @param f the file path to truncate\n* @param newLength the desired new length of the file\n* @throws IOException if truncation is attempted\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])": "/**\n* Throws UnsupportedOperationException for unsupported concat operation.\n* @param f target file path\n* @param psrcs array of source file paths\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file system permissions for the specified path.\n* @param src path to the file or directory\n* @param permission new permissions to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner and group of the specified path.\n* @param src path to set owner and group\n* @param username new owner's username\n* @param groupname new owner's group name\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for the specified path.\n* @param src the path to set the ACL on\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for the specified path.\n* @param src the path to modify ACL entries\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL for the specified path.\n* @param src the path from which to remove the ACL\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes specified ACL entries from the given path.\n* @param src the path from which to remove ACL entries\n* @param aclSpec list of ACL entries to be removed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the default ACL for the specified path.\n* @param src path from which to remove the default ACL\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified path.\n* @param src path to the file or directory\n* @param replication desired replication factor\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)": "/**\n* Returns an iterator for file statuses in the specified path.\n* @param p the path to list file statuses\n* @return RemoteIterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Moves a temporary local file to a specified output path.\n* @param fsOutputFile destination path for the output file\n* @param tmpLocalFile source path of the temporary local file\n* @throws IOException if an I/O error occurs during the move\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:openFile(org.apache.hadoop.fs.Path)": "/**\n* Opens a file and returns a FutureDataInputStreamBuilder.\n* @param path the file path to open\n* @return FutureDataInputStreamBuilder for the opened file\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:getApproxChkSumLength(long)": "/**\n* Calculates approximate checksum length based on the size.\n* @param size the input data size in bytes\n* @return approximate checksum length as a double\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setVerifyChecksum(boolean)": "/**\n* Sets the checksum verification flag.\n* @param verifyChecksum true to enable, false to disable checksum verification\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setWriteChecksum(boolean)": "/**\n* Sets the writeChecksum flag for data integrity.\n* @param writeChecksum true to enable checksum, false to disable\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Starts local output process and returns temporary local file path.\n* @param fsOutputFile path for the output file in filesystem\n* @param tmpLocalFile temporary local file path\n* @return temporary local file path\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path)": "/**\n* Checks if the file is a checksum file based on its name.\n* @param file the Path of the file to check\n* @return true if it's a checksum file, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses in a directory.\n* @param f directory path to list files from\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options and validates mandatory keys.\n* @param path file path to open\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)": "/**\n* Computes the checksum length for a file.\n* @param file the file path\n* @param fileSize the total file size in bytes\n* @return total checksum length in bytes\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path)": "/**\n* Creates a file output stream builder with specified path.\n* @param path the file path to create the output stream for\n* @return FSDataOutputStreamBuilder instance for further configuration\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path)": "/**\n* Appends a file to the output stream builder.\n* @param path the path of the file to append\n* @return updated FSDataOutputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses from the given path.\n* @param f path to list statuses from\n* @return array of matching FileStatus objects\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem)": "/**** Initializes ChecksumFileSystem with a specified FileSystem instance. \n* @param fs the FileSystem to filter \n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path)": "/**\n* Creates directories specified by the path.\n* @param f path to the directory to create\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path)": "/**\n* Generates a checksum file path from the given file path.\n* @param file the original file path\n* @return Path for the checksum file with '.crc' extension\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory and handles its checksum file.\n* @param src source Path to rename\n* @param dst destination Path for the renamed file\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory; handles checksums for files.\n* @param f path to the file or directory to delete\n* @param recursive true to delete directories recursively\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with optional checksum verification.\n* @param f file path to open\n* @param bufferSize size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates FSDataOutputStream for the specified path with given permissions and options.\n* @param f file path, @param permission access permissions, @return FSDataOutputStream object\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates FSDataOutputStream for a specified path with permissions.\n* @param f file path, @param permission access permissions\n* @return FSDataOutputStream object\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates FSDataOutputStream for a path without recursion.\n* @param f file path, @param permission access permissions\n* @return FSDataOutputStream object\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a non-recursive FSDataOutputStream for the specified path.\n* @param f file path, @param permission access permissions, @param flags create options, \n* @param bufferSize size of the buffer, @param replication replication factor, \n* @param blockSize size of the block, @param progress progress tracker\n* @return FSDataOutputStream object\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Creates FSDataOutputStream for a specified path with permissions.\n* @param f file path, @param permission access permissions\n* @return FSDataOutputStream object\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and validates bytes per checksum.\n* @param conf configuration object to set\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)": "/**\n* Calculates the optimal buffer size based on input parameters.\n* @param bytesPerSum size of bytes per sum\n* @param bufferSize initial buffer size\n* @return maximum of calculated sizes\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if a path supports a specific capability after validation.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local to HDFS, with optional source deletion.\n* @param delSrc flag to delete the source file after copy\n* @param src source file path\n* @param dst destination file path in HDFS\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from source to local destination.\n* @param delSrc flag to delete source after copy\n* @param src source file path\n* @param dst destination local file path\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**** Copies a file or directory from HDFS to local filesystem with optional checksum. \n* @param src source Path to copy from \n* @param dst destination Path to copy to \n* @param copyCrc flag to indicate if checksum should be copied \n* @throws IOException if copy operation fails \n*/"
    },
    "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker": {
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:close()": "/**\n* Closes data streams and verifies checksum.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getChecksumFilePos(long)": "/**\n* Calculates the file position for the checksum based on data position.\n* @param dataPos position of the data in the file\n* @return calculated checksum file position\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumOffset(long,int)": "/**\n* Calculates checksum offset based on data offset and bytes per sum.\n* @param dataOffset starting data position\n* @param bytesPerSum number of bytes per checksum\n* @return computed checksum offset\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getChunkPosition(long)": "/**\n* Calculates the starting position of a data chunk.\n* @param dataPos position within the data\n* @return chunk starting position as a long\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength()": "/**\n* Retrieves the length of the file.\n* @return file length as a long, or updates if not previously set\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available()": "/**\n* Returns total bytes available to read from data source and superclass.\n* @return total available bytes, non-negative\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])": "/**\n* Reads data chunk and verifies checksums if needed.\n* @param pos position to read from; @param buf buffer for data; @param offset start index in buf; @param len length of data to read; @param checksum buffer for checksums\n* @return number of bytes read\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long)": "/**\n* Seeks to a new data source and reports checksum failure.\n* @param targetPos position to seek to\n* @return true if seeking is successful, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)": "/**\n* Validates byte data against checksums and returns the data buffer.\n* @param sumsBytes buffer containing checksums\n* @param sumsOffset offset for checksum data\n* @param data buffer to validate\n* @param dataOffset starting position in data\n* @param bytesPerSum number of bytes per checksum\n* @param file source file path\n* @return validated ByteBuffer if checksums match\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics()": "/**\n* Retrieves IOStatistics using the provided data source.\n* @return IOStatistics object or null if data is invalid\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)": "/**\n* Finds checksum ranges from given data ranges.\n* @param dataRanges list of FileRange objects to process\n* @param bytesPerSum bytes per checksum\n* @param minSeek minimum seek distance\n* @param maxSize maximum allowed size\n* @return list of CombinedFileRange objects\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String)": "/**\n* Checks if the data source has a specific capability.\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)": "/**\n* Performs vectored reading of file ranges with checksum validation.\n* @param ranges list of FileRange objects to read\n* @param allocate function to allocate ByteBuffer instances\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)": "/**\n* Initializes ChecksumFSInputChecker with file and buffer size.\n* @param fs ChecksumFileSystem instance\n* @param file Path to the input file\n* @param bufferSize size of the buffer for reading\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)": "/**** Initializes ChecksumFSInputChecker with file and default buffer size. \n* @param fs ChecksumFileSystem instance \n* @param file Path to the input file \n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position in the input file.\n* @param position the position to read from\n* @param b the buffer to store read bytes\n* @param off the offset in the buffer\n* @param len the number of bytes to read\n* @return number of bytes read\n*/"
    },
    "org.apache.hadoop.fs.impl.CombinedFileRange": {
        "org.apache.hadoop.fs.impl.CombinedFileRange:getUnderlying()": "/**\n* Retrieves the list of underlying file ranges.\n* @return List of FileRange objects\n*/",
        "org.apache.hadoop.fs.impl.CombinedFileRange:append(org.apache.hadoop.fs.FileRange)": "/**\n* Appends a FileRange to the underlying collection and updates dataSize.\n* @param range the FileRange to append\n*/",
        "org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)": "/**\n* Constructs CombinedFileRange from offset, end, and original FileRange.\n* @param offset starting position in the file\n* @param end ending position in the file\n* @param original the FileRange to append\n*/",
        "org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)": "/**\n* Merges another FileRange if within seek and size limits.\n* @param otherOffset starting offset of the other range\n* @param otherEnd ending offset of the other range\n* @param other FileRange to merge\n* @param minSeek minimum seek distance\n* @param maxSize maximum allowed size\n* @return true if merged, false if limits exceeded\n*/",
        "org.apache.hadoop.fs.impl.CombinedFileRange:toString()": "/**\n* Returns a formatted string with range count and data size details.\n* @return formatted string representation of the object\n*/"
    },
    "org.apache.hadoop.fs.Stat": {
        "org.apache.hadoop.fs.Stat:isAvailable()": "/**\n* Checks if the shell is available on supported operating systems.\n* @return true if supported OS, false otherwise\n*/",
        "org.apache.hadoop.fs.Stat:getFileStatus()": "/**\n* Retrieves the status of a file.\n* @return FileStatus object representing the file's status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.Stat:getExecString()": "/**\n* Generates command for 'stat' based on OS and dereference flag.\n* @return array of command strings for execution\n*/",
        "org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes Stat object with path, block size, and dereference flag.\n* @param path the original Path object\n* @param blockSize the size of blocks in the file system\n* @param deref flag to indicate dereferencing\n* @param fs the FileSystem instance used for qualification\n*/",
        "org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader)": "/**\n* Parses execution result from BufferedReader and updates file status.\n* @param lines BufferedReader containing execution result lines\n* @throws IOException if the input is invalid or file not found\n*/"
    },
    "org.apache.hadoop.fs.RawLocalFileSystem": {
        "org.apache.hadoop.fs.RawLocalFileSystem:getWorkingDirectory()": "/**\n* Returns the current working directory path.\n* @return Path representing the working directory\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:createOutputStreamWithMode(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates an OutputStream for a file with specified mode and permissions.\n* @param f file path to write to\n* @param append true to append, false to overwrite\n* @param permission file permissions to set\n* @return OutputStream for the specified file\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getUri()": "/**\n* Returns the URI associated with this instance.\n* @return URI constant NAME\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable()": "/**\n* Sets useDeprecatedFileStatus based on Stat availability.\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates an OutputStream for a file.\n* @param f file path to write to\n* @param append true to append, false to overwrite\n* @return OutputStream for the specified file\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:close()": "/**\n* Closes the resource, invoking the superclass's close method.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])": "/**\n* Concatenates multiple source files into a target file.\n* @param trg target file path\n* @param psrcs array of source file paths\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Creates a PathHandle from FileStatus, ensuring it's a file in the correct file system.\n* @param stat file status to create the handle from\n* @param opts optional parameters for handle options\n* @return PathHandle object for the specified file\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to an absolute Path based on the working directory.\n* @param f the Path to convert\n* @return an absolute Path\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path)": "/****** Converts a Path to a File. \n* @param path the Path to convert\n* @return File corresponding to the given Path\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory()": "/**\n* Retrieves the user's home directory as a qualified Path object.\n* @return qualified Path to the user's home directory\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory()": "/**\n* Returns the initial working directory as a qualified Path object.\n* @return qualified Path of the current user directory\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory to an absolute path.\n* @param newDir the new directory path to set\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path)": "/**\n* Checks if a file exists at the given Path.\n* @param f the Path to check for existence\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file system status for a given Path.\n* @param p the Path to check; defaults to root if null\n* @return FsStatus containing total, used, and free space\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets file modification and access times.\n* @param p file path, @param mtime modification time, @param atime access time\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:<init>()": "/**\n* Initializes RawLocalFileSystem with the current user's working directory.\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions using native or shell commands.\n* @param p Path to the file, @param permission desired permissions\n* @throws IOException if permission change fails\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates FSDataOutputStream for a file, throwing if it exists without overwrite flag.\n* @param f file path to create\n* @param permission permissions for the file\n* @param flags creation flags\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize block size\n* @param progress callback for progress updates\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)": "/**\n* Retrieves the file status of a native file link.\n* @param f the Path of the file\n* @param dereference flag to indicate dereferencing\n* @return FileStatus object representing the file's status\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner of a file specified by the Path.\n* @param p the Path of the file, @param username new owner's username, @param groupname group name\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status for a given Path.\n* @param f the Path to the file\n* @return FileStatus of the file or throws FileNotFoundException if not found\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)": "/**\n* Retrieves file link status based on deprecation flag.\n* @param f the Path of the file\n* @param dereference indicates if the link should be dereferenced\n* @return FileStatus object representing the file's link status\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status with dereferencing enabled.\n* @param f the Path of the file\n* @return FileStatus object representing the file's status\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**** Retrieves file status of a symlink with a qualified target path. \n* @param f the Path of the file \n* @return FileStatus object representing the symlink status \n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a symlink.\n* @param f the Path of the symlink\n* @return Path of the symlink target\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f the Path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)": "/**\n* Opens a file input stream with a specified buffer size.\n* @param fd file descriptor handle, must be LocalFileSystemPathHandle\n* @param bufferSize size of the buffer for the input stream\n* @return FSDataInputStream for reading the file\n* @throws IOException if file access fails\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file; throws exception if the path is a directory.\n* @param f file path to append data\n* @param bufferSize size of the buffer for output stream\n* @param progress callback for progress updates\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to a specified length.\n* @param f file path to truncate\n* @param newLength new file length\n* @return true if truncation is successful\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses in a directory or returns status for a single file.\n* @param f path to the file or directory\n* @return array of FileStatus objects\n* @throws IOException if the file does not exist or is unreadable\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status or symbolic link status for a given path.\n* @param f the path to the file or symbolic link\n* @return FileStatus object representing the file or link status\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory; throws IOException if directory is non-empty and non-recursive.\n* @param p Path to the file or directory to delete\n* @param recursive true to delete directories recursively\n* @return true if deletion succeeded, false if file/directory not found\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory with specified permissions or defaults.\n* @param p directory path, @param p2f File object, @param permission desired permissions\n* @return true if directory created successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)": "/**\n* Handles empty destination directory on Windows for renaming files.\n* @param src source path, @param srcFile source file, @param dst destination path, @param dstFile destination file\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File)": "/**\n* Creates a directory from a File object.\n* @param p2f the File object representing the directory path\n* @return true if the directory is created successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory with optional permissions.\n* @param f directory path\n* @param permission desired permissions\n* @return true if created or already a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path)": "/**\n* Creates a directory and its parent directories if needed.\n* @param f directory path\n* @return true if created or already a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory with specified permissions.\n* @param f directory path\n* @param permission desired permissions\n* @return true if created or already a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates FSDataOutputStream for a file, ensuring parent directories exist.\n* @param f file path, @param overwrite allows file replace, @param createParent ensures parent dirs\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**** Creates a symbolic link from target to link, with optional parent directory creation. \n* @param target the file or directory to link to \n* @param link the name of the symbolic link \n* @param createParent whether to create parent directories if needed \n* @throws IOException if symlinks are not supported or creation fails \n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes configuration and sets default block size.\n* @param uri resource URI for initialization\n* @param conf configuration object for settings\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates FSDataOutputStream for a file with specified parameters.\n* @param f file path, @param overwrite allows file replace, \n* @param bufferSize size of the buffer, @param replication factor, \n* @param blockSize size of a block, @param progress for monitoring\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates FSDataOutputStream for a file with specified permissions.\n* @param f file path, @param permission file permissions, @param overwrite allows file replace,\n* @param bufferSize size of buffer, @param replication number of replicas, @param blockSize size of blocks,\n* @param progress for progress reporting\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file output stream non-recursively.\n* @param f file path, @param permission file permissions, @param overwrite allows file replace\n* @param bufferSize size of the buffer, @param replication number of replicas, @param blockSize size of blocks\n* @param progress progress callback\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if a path supports a specific capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory, handling Windows-specific cases and fallback to copy.\n* @param src source Path, @param dst destination Path\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Moves a local file to a new destination.\n* @param src source Path, @param dst destination Path\n*/"
    },
    "org.apache.hadoop.fs.LocalFileSystemPathHandle": {
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:<init>(java.nio.ByteBuffer)": "/**\n* Constructs a LocalFileSystemPathHandle from a ByteBuffer.\n* @param bytes ByteBuffer containing path handle data\n* @throws IOException if bytes is null or parsing fails\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:getPath()": "/**\n* Retrieves the current path as a String.\n* @return the current path value\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:<init>(java.lang.String,java.util.Optional)": "/**\n* Constructs a LocalFileSystemPathHandle with a specified path and optional modification time.\n* @param path file system path as a string\n* @param mtime optional modification time in milliseconds\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:bytes()": "/**\n* Converts the object to a ByteBuffer representation.\n* @return ByteBuffer containing the serialized object data\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:hashCode()": "/**\n* Computes the hash code based on path and mtime fields.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:toString()": "/**\n* Returns a string representation of the LocalFileSystemPathHandle object.\n* @return formatted string with path and modification time\n*/",
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus)": "/**\n* Verifies file status and checks for modification time.\n* @param stat FileStatus object to validate\n* @throws InvalidPathHandleException if validation fails\n*/"
    },
    "org.apache.hadoop.fs.FsStatus": {
        "org.apache.hadoop.fs.FsStatus:<init>(long,long,long)": "/**\n* Initializes file system status with capacity, used, and remaining space.\n* @param capacity total storage capacity\n* @param used space currently in use\n* @param remaining available space left\n*/",
        "org.apache.hadoop.fs.FsStatus:getCapacity()": "/**\n* Retrieves the current capacity value.\n* @return current capacity as a long\n*/",
        "org.apache.hadoop.fs.FsStatus:getUsed()": "/**\n* Returns the amount of used resources.\n* @return long representing the used resource count\n*/",
        "org.apache.hadoop.fs.FsStatus:getRemaining()": "/**\n* Returns the remaining value.\n* @return long representing the remaining amount\n*/",
        "org.apache.hadoop.fs.FsStatus:write(java.io.DataOutput)": "/**\n* Writes capacity, used, and remaining values to DataOutput.\n* @param out output stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FsStatus:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput stream and updates capacity, used, and remaining values.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.Options$HandleOpt": {
        "org.apache.hadoop.fs.Options$HandleOpt:getOpt(java.lang.Class,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Retrieves an optional HandleOpt of specified type from provided options.\n* @param c class type to match, @param opt variable options to search\n* @return Optional of matching HandleOpt or empty if none found\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:<init>()": "/**\n* Default constructor for HandleOpt class.\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:resolve(java.util.function.BiFunction,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Resolves PathHandle from FileStatus using a BiFunction and optional HandleOpts.\n* @param fsr function to apply FileStatus and HandleOpts\n* @param opt optional HandleOpt parameters\n* @return Function that processes FileStatus to PathHandle\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:moved(boolean)": "/****\n* Creates a Location object based on change allowance.\n* @param allow true if changes are allowed, false otherwise\n* @return Location object with specified change setting\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:changed(boolean)": "/**\n* Creates a Data object with change allowance setting.\n* @param allow true to allow changes, false otherwise\n* @return new Data object\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:exact()": "/**\n* Creates an array of HandleOpt using changed and moved methods.\n* @return array of HandleOpt objects\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:content()": "/**\n* Creates an array of HandleOpt using changed and moved methods.\n* @return array of HandleOpt objects\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:path()": "/**\n* Creates an array of HandleOpt using change and location settings.\n* @return array of HandleOpt objects based on specified parameters\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:reference()": "/**\n* Creates an array of HandleOpt with changed and moved settings.\n* @return array of HandleOpt objects\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Resolves a PathHandle from FileStatus using a FileSystem and optional HandleOpts.\n* @param fs FileSystem to retrieve PathHandle\n* @param opt optional HandleOpt parameters\n* @return Function that processes FileStatus to PathHandle\n*/"
    },
    "org.apache.hadoop.fs.Options$HandleOpt$Location": {
        "org.apache.hadoop.fs.Options$HandleOpt$Location:allowChange()": "/**\n* Checks if changes are allowed.\n* @return true if changes are permitted, false otherwise\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt$Location:toString()": "/**\n* Returns a string representation of the object with allowChange status.\n* @return formatted string with allowChange value\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean)": "/**\n* Constructs a Location object with change allowance setting.\n* @param allowChanged true if changes are allowed, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.Options$HandleOpt$Data": {
        "org.apache.hadoop.fs.Options$HandleOpt$Data:allowChange()": "/**\n* Checks if changes are allowed.\n* @return true if changes are permitted, false otherwise\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt$Data:toString()": "/**\n* Returns a string representation of the object with allowChange status.\n* @return formatted string with allowChange value\n*/",
        "org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean)": "/**\n* Constructs a Data object with change allowance setting.\n* @param allowChanged true to allow changes, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.HardLink$LinkStats": {
        "org.apache.hadoop.fs.HardLink$LinkStats:report()": "/**\n* Generates a report of hard link statistics.\n* @return String summary of directory and link operations\n*/"
    },
    "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator": {
        "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:remove()": "/**\n* Throws UnsupportedOperationException for remove operation.\n*/",
        "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next()": ""
    },
    "org.apache.hadoop.fs.shell.CommandFactory": {
        "org.apache.hadoop.fs.shell.CommandFactory:addClass(java.lang.Class,java.lang.String[])": "/**\n* Associates command class with specified names in the class map.\n* @param cmdClass command class type\n* @param names variable names to associate with the command class\n*/",
        "org.apache.hadoop.fs.shell.CommandFactory:addObject(org.apache.hadoop.fs.shell.Command,java.lang.String[])": "/**\n* Adds a command object with associated names to maps.\n* @param cmdObject command object to add\n* @param names variable list of names for the command object\n*/",
        "org.apache.hadoop.fs.shell.CommandFactory:getNames()": "/**\n* Retrieves and sorts names from the classMap keys.\n* @return sorted array of names\n*/",
        "org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class)": "",
        "org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a CommandFactory with the provided configuration.\n* @param conf the Configuration object for initialization\n*/",
        "org.apache.hadoop.fs.shell.CommandFactory:<init>()": "/**\n* Default constructor for CommandFactory, initializes with null configuration.\n*/",
        "org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a Command instance by name and configuration.\n* @param cmdName the command name to instantiate\n* @param conf configuration for the command instance\n* @return Command object or null if not found\n*/",
        "org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String)": "/**\n* Retrieves a Command instance by command name.\n* @param cmd the command name to instantiate\n* @return Command object or null if not found\n*/"
    },
    "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction": {
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:compareTo(java.util.concurrent.Delayed)": "/**\n* Compares this RenewAction with another based on renewal time.\n* @param delayed the Delayed object to compare with\n* @return negative if this is earlier, 0 if equal, positive if later\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit)": "/**\n* Calculates delay in specified time unit until renewal time.\n* @param unit the time unit to convert the delay into\n* @return delay duration in the specified time unit\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long)": "/**** Updates the renewal time based on a delay. \n* @param delay time in milliseconds to adjust the renewal time \n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString()": "/**\n* Returns a string representation of the token renewal status.\n* @return status message including renewal delay and token info\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes RenewAction with a weak reference and updates renewal time.\n* @param fs object to retrieve renewal token from\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew()": "/***************\n* Renews a delegation token if valid.\n* @return true if token is renewed, false if not\n****************/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel()": "/**\n* Cancels the token renewal process if the filesystem is available.\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode()": "/**\n* Returns the hash code for the object using its token.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object)": "/**\n* Compares this RenewAction object with another for equality.\n* @param that object to compare\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandFormat": {
        "org.apache.hadoop.fs.shell.CommandFormat:<init>(int,int,java.lang.String[])": "/**\n* Initializes CommandFormat with min/max parameters and optional options.\n* @param min minimum parameter count\n* @param max maximum parameter count\n* @param possibleOpt variable-length list of possible options\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:getOpt(java.lang.String)": "/**\n* Retrieves the value of a specified option.\n* @param option the key of the option to retrieve\n* @return true if the option exists and is true, otherwise false\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:getOpts()": "/**\n* Retrieves a set of enabled option keys.\n* @return Set of option keys that are true in the options map\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:getOptValue(java.lang.String)": "/**\n* Retrieves the value associated with a given option.\n* @param option the key for which to get the value\n* @return the value as a String or null if option is not found\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])": "/**\n* Constructs CommandFormat with name and parameter constraints.\n* @param name command name\n* @param min minimum parameter count\n* @param max maximum parameter count\n* @param possibleOpt variable-length list of possible options\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String)": "/**\n* Adds an option with a null value if not already present.\n* @param option the option to be added\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List)": "/**\n* Parses command-line options from the provided argument list.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)": "/**\n* Parses command-line arguments starting from a specified position.\n* @param args array of command-line arguments\n* @param pos starting index for parsing\n* @return List of remaining parameters after parsing\n*/"
    },
    "org.apache.hadoop.fs.TrashPolicyDefault": {
        "org.apache.hadoop.fs.TrashPolicyDefault:<init>()": "/**\n* Constructs a new instance of TrashPolicyDefault.\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:isEnabled()": "/**\n* Checks if the feature is enabled based on deletion interval.\n* @return true if deletionInterval is greater than 0, otherwise false\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:getEmptier()": "/**\n* Returns a Runnable that empties resources at specified intervals.\n* @return Runnable instance of Emptier\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:getTimeFromCheckpoint(java.lang.String)": "/**\n* Retrieves the timestamp from a checkpoint name.\n* @param name the checkpoint name to parse\n* @return the parsed time in milliseconds\n* @throws ParseException if the name cannot be parsed\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes old checkpoints in the trash directory.\n* @param trashRoot path to the trash directory\n* @param deleteImmediately flag to delete without delay\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Creates a trash-relative Path by merging base and file paths.\n* @param basePath the base Path to merge with\n* @param rmFilePath the Path to be merged into basePath\n* @return merged Path object\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)": "/**\n* Creates a checkpoint in the trash directory.\n* @param trashRoot root path of the trash directory\n* @param date timestamp for the checkpoint naming\n* @throws IOException if checkpoint creation fails\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Initializes trash settings from configuration parameters.\n* @param conf configuration object for property retrieval\n* @param fs file system instance to associate\n* @param home home directory path for trash\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes file system settings from configuration values.\n* @param conf configuration object for property retrieval\n* @param fs file system instance to be initialized\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs TrashPolicyDefault and initializes file system settings.\n* @param fs file system instance to be initialized\n* @param conf configuration object for property retrieval\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path)": "/**\n* Moves a file to the trash if enabled and not already in trash.\n* @param path the Path to move to trash\n* @return true if moved successfully, false otherwise\n* @throws IOException if an error occurs during the operation\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir()": "/**\n* Retrieves the current trash directory Path.\n* @return Path of the current trash directory\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the current trash directory path.\n* @param path the base path to resolve the trash directory\n* @return Path object for the current trash directory\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date)": "/**\n* Creates checkpoints for all user trash roots.\n* @param date timestamp for naming checkpoints\n* @throws IOException if checkpoint creation fails\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean)": "/**\n* Deletes checkpoints from user trash directories.\n* @param deleteImmediately flag to delete without delay\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint()": "/**\n* Triggers a checkpoint creation using the current date.\n* @throws IOException if checkpoint creation fails\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint()": "/**\n* Deletes a checkpoint, triggering the deletion without immediate action.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately()": "/**\n* Deletes checkpoints immediately from user trash directories.\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.InvalidPathHandleException": {
        "org.apache.hadoop.fs.InvalidPathHandleException:<init>(java.lang.String)": "/**\n* Constructs an InvalidPathHandleException with a specified message.\n* @param str error message for the exception\n*/",
        "org.apache.hadoop.fs.InvalidPathHandleException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an InvalidPathHandleException with a message and cause.\n* @param message error message\n* @param cause underlying throwable cause\n*/"
    },
    "org.apache.hadoop.fs.impl.AbstractMultipartUploader": {
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:<init>(org.apache.hadoop.fs.Path)": "/**\n* Initializes the uploader with a specified base path.\n* @param basePath directory path for uploads, must not be null\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:close()": "/**\n* Closes the resource, releasing any associated system resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:getBasePath()": "/**\n* Retrieves the base path.\n* @return Path representing the base directory\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[])": "/**\n* Validates the upload ID for non-null and non-empty conditions.\n* @param uploadId byte array representing the upload ID\n* @throws IllegalArgumentException if uploadId is null or empty\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map)": "/**\n* Validates part handles; checks for emptiness and positive indices.\n* @param partHandles map of part handles indexed by integers\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path)": "/**\n* Validates that the path is non-null and under the specified base path.\n* @param path the Path object to check\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)": "/**\n* Validates parameters for a file upload operation.\n* @param filePath path to the file\n* @param inputStream input stream for file data\n* @param partNumber part number of the upload\n* @param uploadId unique identifier for the upload\n* @param lengthInBytes size of the file part in bytes\n*/",
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path)": "/**\n* Aborts uploads under the specified path.\n* @param path the Path object to check\n* @return CompletableFuture with abort status (-1)\n*/"
    },
    "org.apache.hadoop.util.functional.FutureIO": {
        "org.apache.hadoop.util.functional.FutureIO:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Evaluates a callable and returns a CompletableFuture.\n* @param callable a CallableRaisingIOE that may throw exceptions\n* @return CompletableFuture containing the result or exception\n*/",
        "org.apache.hadoop.util.functional.FutureIO:<init>()": "/**\n* Private constructor for FutureIO class to prevent instantiation.\n*/",
        "org.apache.hadoop.util.functional.FutureIO:unwrapInnerException(java.lang.Throwable)": "/**\n* Extracts the inner IOException from a Throwable.\n* @param e the Throwable to unwrap\n* @return the unwrapped IOException or a new IOException if none found\n*/",
        "org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException)": "/**\n* Throws the inner IOException from an ExecutionException.\n* @param e the ExecutionException to unwrap\n* @throws IOException if an inner IOException is found\n*/",
        "org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException)": "/**\n* Throws the inner IOException from a CompletionException.\n* @param e the CompletionException to unwrap\n* @throws IOException if an inner IOException is found\n*/",
        "org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future)": "/**\n* Awaits completion of a Future, handling exceptions and cancellations.\n* @param future the Future to await\n* @return result of the Future\n* @throws InterruptedIOException if interrupted\n* @throws IOException if execution fails\n* @throws CancellationException if cancelled\n*/",
        "org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)": "/**\n* Awaits a Future's result with a timeout.\n* @param future the Future to await\n* @param timeout the maximum wait time\n* @param unit the time unit of the timeout\n* @return the result of the Future\n* @throws various exceptions on failure\n*/",
        "org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection)": "/**\n* Awaits and collects results from a collection of Futures.\n* @param collection Futures to await\n* @return List of results from the Futures\n*/",
        "org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)": "/**\n* Awaits results of multiple futures with a specified timeout.\n* @param collection futures to await\n* @param duration maximum wait time\n* @return list of results from futures\n*/",
        "org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)": "/**\n* Cancels futures and waits for their completion, returning results.\n* @param collection futures to cancel and await\n* @param interruptIfRunning whether to interrupt running tasks\n* @param duration maximum wait time for completion\n* @return list of results from completed futures\n*/",
        "org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)": "/**\n* Propagates configuration options to a builder based on a prefix.\n* @param builder FSBuilder instance to modify\n* @param conf Configuration containing properties\n* @param prefix Prefix for filtering properties\n* @param mandatory If true, adds options as mandatory\n*/",
        "org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Applies configuration options to a builder with optional and mandatory prefixes.\n* @param builder FSBuilder instance to modify\n* @param conf Configuration containing properties\n* @param optionalPrefix Prefix for optional properties\n* @param mandatoryPrefix Prefix for mandatory properties\n* @return Modified FSBuilder instance\n*/"
    },
    "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl": {
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:bufferSize(int)": "/**\n* Sets the buffer size and returns the builder instance.\n* @param bufSize the desired buffer size\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:replication(short)": "/**\n* Sets the replication factor and returns the builder instance.\n* @param replica the replication factor to set\n* @return the builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:blockSize(long)": "/**\n* Sets the block size and returns the builder instance.\n* @param blkSize the new block size\n* @return the builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:create()": "/**\n* Adds a create flag and returns the builder instance.\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:overwrite(boolean)": "/**\n* Updates overwrite flag and returns the builder instance.\n* @param overwrite true to set the flag, false to remove it\n* @return the builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:append()": "/**\n* Adds an APPEND flag and returns the current builder instance.\n* @return the builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getBufferSize()": "/**\n* Retrieves the current buffer size.\n* @return the size of the buffer as an integer\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getReplication()": "/**\n* Retrieves the current replication value.\n* @return short representing the replication level\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFlags()": "/**\n* Retrieves the set of creation flags.\n* @return EnumSet of CreateFlag representing current flags\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getChecksumOpt()": "/**\n* Retrieves the current checksum options.\n* @return ChecksumOpt object representing the options\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getBlockSize()": "/**\n* Retrieves the current block size.\n* @return the size of the block as a long value\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS()": "/**\n* Retrieves the FileSystem instance, ensuring it's non-null.\n* @return FileSystem instance\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets the permission and returns the builder instance.\n* @param perm the permission to set\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Sets checksum options and returns the builder instance.\n* @param chksumOpt options for checksum\n* @return builder instance with updated options\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission()": "/**\n* Retrieves file permissions, initializing if not set.\n* @return FsPermission object representing file permissions\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)": "/**\n* Initializes MultipartUploaderBuilderImpl with file context and path.\n* @param fc FileContext for filesystem operations\n* @param p Path for the file to upload\n*/",
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Initializes MultipartUploaderBuilderImpl with file system and path.\n* @param fileSystem the file system to use\n* @param p the path for the uploader\n*/"
    },
    "org.apache.hadoop.fs.impl.FunctionsRaisingIOE": {
        "org.apache.hadoop.fs.impl.FunctionsRaisingIOE:<init>()": "/**\n* Default constructor for FunctionsRaisingIOE.\n*/"
    },
    "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl": {
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getPath()": "/**\n* Retrieves the path from an Optional<Path>.\n* @return Path object contained in the Optional\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getPathHandle()": "/**\n* Retrieves the PathHandle from an optional storage.\n* @return PathHandle object, or throws NoSuchElementException if absent\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getThisBuilder()": "/**\n* Returns the current builder instance cast to type B.\n* @return the builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getMandatoryKeys()": "/**\n* Returns an unmodifiable set of mandatory keys.\n* @return a set of mandatory key strings\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalKeys()": "/**\n* Returns an unmodifiable set of optional keys.\n* @return Set of optional key strings\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Set,java.util.Collection,java.lang.String)": "/**\n* Validates mandatory keys against known keys, throwing an exception for unknown ones.\n* @param mandatory set of mandatory keys to validate\n* @param knownKeys collection of valid known keys\n* @param extraErrorText additional error message text\n* @throws IllegalArgumentException if any mandatory key is unknown\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalPath()": "/**\n* Retrieves an optional file path.\n* @return Optional<Path> containing the file path or empty if not present\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalPathHandle()": "/**\n* Retrieves an optional PathHandle.\n* @return Optional<PathHandle> containing the PathHandle or empty if not present\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptions()": "/**\n* Retrieves the current configuration options.\n* @return Configuration object containing options\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)": "/**\n* Validates mandatory keys against known keys.\n* @param knownKeys collection of valid known keys\n* @param extraErrorText additional error message text\n* @throws IllegalArgumentException if any mandatory key is unknown\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)": "/**\n* Initializes AbstractFSBuilderImpl with optional path and path handle.\n* @param optionalPath optional file path\n* @param optionalPathHandle optional path handle\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path)": "/**\n* Constructs AbstractFSBuilderImpl with a specified file path.\n* @param path the file path to initialize the builder\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle)": "/**\n* Constructs AbstractFSBuilderImpl with a path handle.\n* @param pathHandle the path handle to be used\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)": "/**\n* Updates options with a key-value pair and returns the builder instance.\n* @param key the option key to set\n* @param value the option value to associate with the key\n* @return the builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)": "/**\n* Adds a mandatory key-value pair and returns the builder instance.\n* @param key the mandatory key to add\n* @param value the value associated with the key\n* @return the builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)": "/**\n* Updates options with a key and boolean value.\n* @param key option key to set\n* @param value option value as boolean\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)": "/**\n* Sets a long option by key and returns the builder instance.\n* @param key option key to set\n* @param value long value to associate with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)": "/**\n* Sets a double option by key.\n* @param key option key\n* @param value double value to associate with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)": "/**\n* Adds a mandatory boolean key-value pair.\n* @param key the mandatory key to add\n* @param value the boolean value associated with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)": "/**\n* Adds a mandatory long key-value pair.\n* @param key the mandatory key to add\n* @param value the long value associated with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)": "/**\n* Adds a mandatory double key-value pair.\n* @param key the mandatory key to add\n* @param value the double value associated with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])": "/**\n* Updates options with a key and values, returning the builder instance.\n* @param key the option key\n* @param values variable number of option values\n* @return the builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])": "/**\n* Sets a mandatory key with values and returns the builder instance.\n* @param key the mandatory key to set\n* @param values variable number of values for the key\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)": "/**\n* Sets an option by key with an integer value.\n* @param key option key to set\n* @param value integer value to associate with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)": "/**\n* Sets an option by key with a long value.\n* @param key option key to set\n* @param value long value to associate with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)": "/**\n* Sets a float option by key and returns the builder instance.\n* @param key option key to set\n* @param value float value to associate with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)": "/**\n* Sets a double option by key and returns the builder instance.\n* @param key option key to set\n* @param value double value to associate with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)": "/**\n* Adds a mandatory integer key-value pair.\n* @param key the mandatory key to add\n* @param value the integer value associated with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)": "/**\n* Adds a mandatory key-value pair.\n* @param key the mandatory key to add\n* @param value the long value associated with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)": "/**\n* Adds a mandatory key-value pair with a float value.\n* @param key the mandatory key to add\n* @param value the float value associated with the key\n* @return builder instance of type B\n*/",
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)": "/**\n* Adds a mandatory key-value pair with a double value.\n* @param key the mandatory key to add\n* @param value the double value associated with the key\n* @return builder instance of type B\n*/"
    },
    "org.apache.hadoop.fs.impl.BackReference": {
        "org.apache.hadoop.fs.impl.BackReference:<init>(java.lang.Object)": "/**\n* Constructs a BackReference with an optional reference object.\n* @param reference an object that may be null\n*/",
        "org.apache.hadoop.fs.impl.BackReference:toString()": "/**\n* Returns a string representation of the BackReference object.\n* @return formatted string with reference information\n*/"
    },
    "org.apache.hadoop.fs.impl.FlagSet": {
        "org.apache.hadoop.fs.impl.FlagSet:flags()": "/**\n* Returns a copy of the current flags set.\n* @return EnumSet containing the flags\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:isEmpty()": "/**\n* Checks if the flags collection is empty.\n* @return true if flags is empty, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:hasCapability(java.lang.String)": "/**\n* Checks if a specified capability is enabled.\n* @param capability the name of the capability to check\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:makeImmutable()": "/**\n* Sets the object to immutable state.\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:isImmutable()": "/**\n* Checks if the object is immutable.\n* @return true if immutable, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:toString()": "/**\n* Returns a string representation of the object's flags.\n* @return formatted string of flag names\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:equals(java.lang.Object)": "/**\n* Compares this FlagSet to another object for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:hashCode()": "/**\n* Computes the hash code based on the flags field.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:toConfigurationString()": "/**\n* Converts flags to a comma-separated string of their names.\n* @return configuration string of flag names\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:checkMutable()": "/**\n* Validates mutability of the FlagSet; throws exception if immutable.\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:pathCapabilities()": "/**\n* Retrieves a list of enabled capability names.\n* @return List of capability names that are enabled\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)": "/**\n* Constructs a FlagSet with enum class, prefix, and optional flags.\n* @param enumClass the enum type class\n* @param prefix the prefix for enum names\n* @param flags optional EnumSet of flags\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:copy()": "/**\n* Creates a copy of the FlagSet.\n* @return a new FlagSet instance with the same enum class, prefix, and flags\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)": "/**\n* Creates a FlagSet for the specified enum type with a given prefix and flags.\n* @param enumClass the enum type class\n* @param prefix the prefix for enum names\n* @param flags optional EnumSet of flags\n* @return a new FlagSet instance\n*/",
        "org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)": "/**\n* Builds a FlagSet from configuration using an enum class and key.\n* @param enumClass the enum type class\n* @param conf configuration source\n* @param key identifier for the enum values\n* @param ignoreUnknown flag to ignore unknown values\n* @return a FlagSet instance for the specified enum type\n*/"
    },
    "org.apache.hadoop.fs.impl.FSBuilderSupport": {
        "org.apache.hadoop.fs.impl.FSBuilderSupport:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FSBuilderSupport with specified configuration options.\n* @param options configuration settings for the builder\n*/",
        "org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)": "/**\n* Retrieves a long value for a key or returns a default if not found or invalid.\n* @param key property name to fetch\n* @param defVal default value if key is absent or invalid\n* @return parsed long value or defVal\n*/",
        "org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)": "/**\n* Retrieves a positive long value; returns default if negative.\n* @param key property name to fetch\n* @param defVal default value if key is absent or negative\n* @return valid positive long value\n*/"
    },
    "org.apache.hadoop.fs.store.LogExactlyOnce": {
        "org.apache.hadoop.fs.store.LogExactlyOnce:warn(java.lang.String,java.lang.Object[])": "/**\n* Logs a warning message once if not already logged.\n* @param format message format string\n* @param args values to format the message\n*/",
        "org.apache.hadoop.fs.store.LogExactlyOnce:<init>(org.slf4j.Logger)": "/**\n* Initializes LogExactlyOnce with a specified Logger instance.\n* @param log Logger to be used for logging\n*/",
        "org.apache.hadoop.fs.store.LogExactlyOnce:info(java.lang.String,java.lang.Object[])": "/**\n* Logs info message once if not already logged.\n* @param format message format string\n* @param args arguments for the format string\n*/",
        "org.apache.hadoop.fs.store.LogExactlyOnce:error(java.lang.String,java.lang.Object[])": "/**\n* Logs an error message once if not already logged.\n* @param format error message format\n* @param args message parameters\n*/",
        "org.apache.hadoop.fs.store.LogExactlyOnce:debug(java.lang.String,java.lang.Object[])": "/**\n* Logs debug message once if not already logged.\n* @param format message format string\n* @param args values for the format string\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.Retryer": {
        "org.apache.hadoop.fs.impl.prefetch.Retryer:continueRetry()": "/**\n* Determines if a retry should continue based on delay limits.\n* @return true if another retry is allowed, false if max delay is reached\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Retryer:updateStatus()": "/**\n* Checks if status should be updated based on delay and interval.\n* @return true if status update is due, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)": "/**\n* Constructs a Retryer with specified delays and interval.\n* @param perRetryDelay delay between retries\n* @param maxDelay maximum allowable delay\n* @param statusUpdateInterval interval for status updates\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool": {
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:<init>(java.util.concurrent.ExecutorService)": "/**\n* Initializes the ExecutorServiceFuturePool with a given ExecutorService.\n* @param executor the ExecutorService to manage future tasks\n*/",
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:executeFunction(java.util.function.Supplier)": "/**\n* Executes a function asynchronously.\n* @param f a Supplier that provides a Void result\n* @return Future representing the pending completion of the task\n*/",
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:executeRunnable(java.lang.Runnable)": "/**\n* Submits a Runnable task for execution and returns a Future object.\n* @param r the Runnable task to execute\n* @return Future representing the pending completion of the task\n*/",
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:toString()": "/**\n* Returns a string representation of the ExecutorServiceFuturePool.\n* @return formatted string with executor information\n*/",
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)": "/**\n* Shuts down the executor service with a timeout.\n* @param logger for logging shutdown events\n* @param timeout maximum wait time for shutdown\n* @param unit time unit for the timeout\n*/"
    },
    "org.apache.hadoop.util.concurrent.HadoopExecutors": {
        "org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit)": "/**\n* Shuts down the executor service gracefully or forcefully after a timeout.\n* @param executorService the ExecutorService to shut down\n* @param logger the Logger for logging shutdown events\n* @param timeout maximum time to wait for shutdown\n* @param unit time unit for the timeout\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadExecutor()": "/**\n* Creates a single-threaded executor for task execution.\n* @return ExecutorService instance for managing tasks\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadExecutor(java.util.concurrent.ThreadFactory)": "/**\n* Creates a single-threaded executor with the specified thread factory.\n* @param threadFactory custom thread factory for thread creation\n* @return ExecutorService instance for single-threaded execution\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadScheduledExecutor()": "/**\n* Creates a new single-threaded scheduled executor service.\n* @return ScheduledExecutorService instance\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadScheduledExecutor(java.util.concurrent.ThreadFactory)": "/**\n* Creates a ScheduledExecutorService with a custom ThreadFactory.\n* @param threadFactory factory for creating new threads\n* @return a ScheduledExecutorService instance\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:<init>()": "/**\n* Private constructor to prevent instantiation of the HadoopExecutors class.\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory)": "/**\n* Creates a cached thread pool using the specified thread factory.\n* @param threadFactory factory for creating new threads\n* @return ExecutorService instance for managing threads\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)": "/**\n* Creates a fixed thread pool with a specified number of threads.\n* @param nThreads number of threads in the pool\n* @param threadFactory factory for creating new threads\n* @return ExecutorService instance for thread management\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int)": "/**\n* Creates a fixed thread pool with a specified number of threads.\n* @param nThreads the number of threads in the pool\n* @return ExecutorService for managing threads\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int)": "/**\n* Creates a new scheduled thread pool executor.\n* @param corePoolSize number of threads to keep in the pool\n* @return ScheduledExecutorService instance\n*/",
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)": "/**\n* Creates a new ScheduledExecutorService with specified core pool size and thread factory.\n* @param corePoolSize the number of threads to keep in the pool\n* @param threadFactory factory for creating new threads\n* @return ScheduledExecutorService instance\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry": {
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:<init>(int,java.nio.file.Path,int,long)": "/**\n* Constructs an Entry with block number, path, size, and checksum.\n* @param blockNumber the block's identifier\n* @param path the file path associated with the entry\n* @param size the size of the entry in bytes\n* @param checksum the data integrity checksum\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:toString()": "/**\n* Returns a formatted string representation of the object.\n* @return String with block info including number, path, size, and checksum\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType)": "/**\n* Acquires a read or write lock based on the specified lock type.\n* @param lockType type of lock to acquire (READ or WRITE)\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:releaseLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType)": "/**\n* Releases the specified lock type (READ or WRITE).\n* @param lockType type of lock to release\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType,long,java.util.concurrent.TimeUnit)": "/**\n* Attempts to acquire a lock of specified type within a timeout.\n* @param lockType type of lock to acquire (READ or WRITE)\n* @param timeout maximum time to wait for the lock\n* @param unit time unit for the timeout\n* @return true if lock acquired, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:getPrevious()": "/**\n* Retrieves the previous Entry in a linked structure.\n* @return the previous Entry or null if none exists\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:getNext()": "/**\n* Retrieves the next Entry in the sequence.\n* @return the next Entry object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:setNext(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)": "/**\n* Sets the next entry in the linked list.\n* @param next the next Entry to link to\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:setPrevious(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)": "/**\n* Sets the previous entry in the linked structure.\n* @param previous the entry to set as previous\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation": {
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind,int)": "/**\n* Initializes an Operation with specified kind and block number.\n* @param kind type of operation\n* @param blockNumber block identifier for the operation\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getSummary(java.lang.StringBuilder)": "/**\n* Appends summary of kind and blockNumber to StringBuilder.\n* @param sb StringBuilder to append summary to\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getDebugInfo()": "/**\n* Generates debug information for the current object.\n* @return formatted debug string based on object's kind and block number\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getTimestamp()": "/**\n* Retrieves the current timestamp value.\n* @return the current timestamp as a long\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getBlockNumber()": "/**\n* Retrieves the current block number.\n* @return the current block number as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getKind()": "/**\n* Retrieves the kind of the object.\n* @return Kind object representing the type\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters": {
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getBlockData()": "/**\n* Retrieves the current block data.\n* @return BlockData object containing block information\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getBufferPoolSize()": "/**\n* Retrieves the current size of the buffer pool.\n* @return current buffer pool size as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getFuturePool()": "/**\n* Retrieves the ExecutorServiceFuturePool instance.\n* @return the current future pool instance\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getPrefetchingStatistics()": "/**\n* Retrieves the prefetching statistics.\n* @return PrefetchingStatistics object containing statistical data\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getMaxBlocksCount()": "/**\n* Returns the maximum count of blocks.\n* @return maximum number of blocks as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getTrackerFactory()": "/**\n* Retrieves the DurationTrackerFactory instance.\n* @return DurationTrackerFactory object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getLocalDirAllocator()": "/**\n* Retrieves the LocalDirAllocator instance.\n* @return LocalDirAllocator object\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockData": {
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getFileSize()": "/**\n* Retrieves the size of the file.\n* @return size of the file in bytes\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockSize()": "/**\n* Retrieves the current block size.\n* @return the size of the block as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int)": "/**\n* Validates the block number, ensuring it is within the acceptable range.\n* @param blockNumber the block number to validate\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long)": "/**\n* Validates the offset value against the allowed range.\n* @param offset the value to check for validity\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int)": "/**\n* Checks if the given block number is the last block.\n* @param blockNumber the block number to check\n* @return true if it's the last block, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int)": "/**\n* Calculates the start offset based on the block number.\n* @param blockNumber the block number to calculate the offset for\n* @return the calculated start offset as a long value\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int)": "/**\n* Retrieves the state at a specific block number.\n* @param blockNumber the block number to fetch the state from\n* @return State object at the given block number\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)": "/**\n* Sets the state for a specific block number after validation.\n* @param blockNumber the block number to set the state for\n* @param blockState the state to assign to the block\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long)": "/**\n* Calculates block number from the given offset.\n* @param offset the byte offset to convert\n* @return the corresponding block number\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int)": "/**\n* Calculates the size of a block based on its position.\n* @param blockNumber the block number to evaluate\n* @return size of the block in bytes\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)": "/**\n* Computes relative offset from a block number and given offset.\n* @param blockNumber the block number to calculate from\n* @param offset the base offset value\n* @return the calculated relative offset as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString()": "/**\n* Generates a string representation of states over blocks.\n* @return formatted string of block ranges and states\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)": "/**\n* Constructs BlockData with file and block sizes, validating inputs.\n* @param fileSize total size of the file\n* @param blockSize size of each block\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockOperations": {
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:<init>()": "/**\n* Initializes a new BlockOperations instance with an empty list of operations.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:setDebug(boolean)": "/**\n* Sets the debug mode state.\n* @param state true to enable, false to disable debug mode\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:append(java.lang.StringBuilder,java.lang.String,java.lang.Object[])": "/**\n* Appends formatted string to StringBuilder.\n* @param sb StringBuilder to append to\n* @param format String format for the output\n* @param args Arguments for the format string\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getIntList(java.lang.Iterable)": "/**\n* Converts an Iterable of integers to a comma-separated string.\n* @param nums iterable collection of integers\n* @return comma-separated string representation of integers\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)": "/**\n* Adds an operation and logs debug info if in debug mode.\n* @param op the operation to add\n* @return the added Operation object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder)": "/**\n* Analyzes operations by block number and appends results to StringBuilder.\n* @param sb StringBuilder to append analysis results\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int)": "/**\n* Retrieves a prefetched operation by block number.\n* @param blockNumber identifier for the block\n* @return the added Operation object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int)": "/**\n* Retrieves a cached operation by block number.\n* @param blockNumber identifier for the block\n* @return Operation object for the cached block\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int)": "/**\n* Retrieves a read operation for a specified block number.\n* @param blockNumber identifier for the block\n* @return added Operation object for reading\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int)": "/**\n* Releases a block by adding a RELEASE operation.\n* @param blockNumber identifier for the block to release\n* @return the added Operation object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int)": "/**\n* Requests a prefetch operation for a specified block number.\n* @param blockNumber identifier for the block to prefetch\n* @return the created Operation object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int)": "/**\n* Prefetches an operation for the specified block number.\n* @param blockNumber the block identifier to prefetch\n* @return the added Operation object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches()": "/**\n* Cancels prefetch operations and returns the added Operation.\n* @return the added Operation object for cancellation\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:close()": "/**\n* Closes an operation and returns the added Operation object.\n* @return Operation object representing the close operation\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int)": "/**\n* Requests caching for a specific block.\n* @param blockNumber identifier of the block to cache\n* @return added Operation object for caching request\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int)": "/**** Adds an operation to the cache with a specified block number. \n* @param blockNumber identifier for the block to cache \n* @return the added Operation object \n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)": "/**** Constructs and adds an End operation from the given Operation. \n* @param op the Operation to initialize from \n* @return the added End Operation object \n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String)": "/**\n* Parses a summary string to create BlockOperations.\n* @param summary formatted operation summary\n* @return BlockOperations object with parsed operations\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder)": "/**\n* Collects and formats duration statistics for operations into a StringBuilder.\n* @param sb StringBuilder to append duration information\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean)": "/**\n* Generates a summary of operations, optionally including debug info.\n* @param showDebugInfo flag to include debug information\n* @return formatted summary string\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BufferData": {
        "org.apache.hadoop.fs.impl.prefetch.BufferData:stateEqualsOneOf(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])": "/**\n* Checks if the current state matches any of the provided states.\n* @param states variable number of State objects to compare\n* @return true if current state matches any, otherwise false\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getBlockNumber()": "/**\n* Retrieves the current block number.\n* @return current block number as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getState()": "/**\n* Retrieves the current state.\n* @return the current State object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getBuffer()": "/**\n* Retrieves the current ByteBuffer instance.\n* @return ByteBuffer object representing the buffer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getActionFuture()": "/**\n* Retrieves the action future.\n* @return Future<Void> representing the action status\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getChecksum(java.nio.ByteBuffer)": "/**\n* Computes the checksum of the given ByteBuffer.\n* @param buffer input ByteBuffer to calculate checksum\n* @return checksum value as a long\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getBufferStr(java.nio.ByteBuffer)": "/**\n* Returns a formatted string representation of the ByteBuffer's state.\n* @param buf the ByteBuffer to inspect\n* @return a string describing the buffer or \"--\" if null\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getFutureStr(java.util.concurrent.Future)": "/**\n* Returns the status of a Future object as a string.\n* @param f the Future to check\n* @return \"done\", \"not done\", or \"--\" if f is null\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setDone()": "/**\n* Marks the state as DONE after verifying the checksum.\n* @throws IllegalStateException if checksum has changed\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:toString()": "/**\n* Returns a formatted string with block details including buffer and future status.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])": "/**\n* Validates the current state against provided states; throws exception if incorrect.\n* @param states expected states to validate against\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)": "/**\n* Initializes BufferData with block number and buffer.\n* @param blockNumber non-negative block identifier\n* @param buffer non-null ByteBuffer object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future)": "/**\n* Sets caching state and action future.\n* @param actionFuture future action to be executed\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])": "/**\n* Updates the state if current state matches expected states.\n* @param newState new state to set\n* @param expectedCurrentState expected current states for validation\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future)": "/**\n* Sets prefetch state and action future.\n* @param actionFuture future action to be performed during prefetch\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])": "/**\n* Prepares the buffer and sets its state to READY if checksum is not set.\n* @param expectedCurrentState states for validation during update\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask": {
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:<init>(org.apache.hadoop.fs.impl.prefetch.BufferData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager,java.time.Instant)": "/**\n* Initializes a PrefetchTask with data, block manager, and start time.\n* @param data buffer data for the task\n* @param blockManager manages cached blocks\n* @param taskQueuedStartTime time when the task was queued\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get()": "/**\n* Prefetches data block and logs errors if encountered.\n* @return always null\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BufferPool": {
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:getAll()": "/**\n* Retrieves an unmodifiable list of all allocated BufferData.\n* @return List of BufferData objects\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:numCreated()": "/**\n* Returns the number of created objects in the pool.\n* @return int representing the count of created objects\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Determines if BufferData can be released based on its state.\n* @param data the BufferData object to check\n* @return true if state is DONE or READY, otherwise false\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)": "/**\n* Calculates the absolute distance between buffer block number and given block number.\n* @param data BufferData object containing block number\n* @param blockNumber the block number to compare against\n* @return absolute distance as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int)": "/**\n* Finds BufferData by block number if not in DONE state.\n* @param blockNumber the block number to search for\n* @return BufferData object or null if not found\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:close()": "/**\n* Closes resources, cancels actions, and clears allocated buffers.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)": "/**\n* Initializes BufferPool with size and bufferSize, and tracks prefetching statistics.\n* @param size total number of buffers\n* @param bufferSize size of each buffer\n* @param prefetchingStatistics statistics for prefetching\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Releases the specified BufferData if it can be safely released.\n* @param data the BufferData to release\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int)": "/**\n* Releases the nearest 'READY' block to the given block number.\n* @param blockNumber the block number to compare against\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:toString()": "/**\n* Returns a formatted string of pool and sorted BufferData details.\n* @return formatted string representation of the object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks()": "/**\n* Releases all BufferData in DONE state.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)": "/**\n* Acquires BufferData for a block, blocking if allowed; returns null if unavailable.\n* @param blockNumber non-negative block identifier\n* @param canBlock indicates if the method can wait for buffer availability\n* @return BufferData object or null if no buffer is available\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable()": "/**\n* Returns the number of available items in the pool after releasing done blocks.\n* @return count of available items\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int)": "/**\n* Attempts to acquire BufferData without blocking.\n* @param blockNumber non-negative block identifier\n* @return BufferData object or null if unavailable\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int)": "/**\n* Acquires BufferData for a given block number with retry logic.\n* @param blockNumber non-negative block identifier\n* @return BufferData object or throws IllegalStateException if unavailable\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask": {
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:<init>(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager,java.time.Instant)": "/**\n* Initializes a CachePutTask with provided data and parameters.\n* @param data BufferData to be cached\n* @param blockFuture Future representing the block's completion\n* @param blockManager CachingBlockManager to manage caching\n* @param taskQueuedStartTime Timestamp when the task was queued\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:get()": "/**\n* Adds data to cache and releases resources.\n* @return always null\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager": {
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cachePut(int,java.nio.ByteBuffer)": "/**\n* Caches a ByteBuffer for a specified block number.\n* @param blockNumber the block identifier for caching\n* @param buffer the ByteBuffer to cache\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numCached()": "/**\n* Returns the number of cached items.\n* @return count of items in the cache\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numCachingErrors()": "/**\n* Retrieves the current count of caching errors.\n* @return number of caching errors as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numReadErrors()": "/**\n* Returns the count of read errors encountered.\n* @return number of read errors as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)": "/**\n* Creates a BlockCache instance with specified block count and tracker factory.\n* @param maxBlocksCount maximum number of blocks in the cache\n* @param trackerFactory factory for tracking durations\n* @return BlockCache object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)": "/**\n* Caches BufferData and marks it as done after processing blockFuture.\n* @param data BufferData to cache\n* @param blockFuture Future representing the block operation\n* @param taskQueuedStartTime timestamp when the task was queued\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Releases the specified BufferData if not closed.\n* @param data the BufferData to release\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters)": "/**\n* Initializes CachingBlockManager with specified parameters.\n* @param blockManagerParameters configuration settings for block management\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString()": "/** Returns a formatted string of cache and buffer pool details. */",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Requests caching for provided BufferData if not closed or disabled.\n* @param data BufferData to be cached\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches()": "/**\n* Cancels prefetch operations and caches relevant BufferData.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])": "/**\n* Reads a block of data into a buffer, handling caching and prefetching.\n* @param data the buffer data to read into\n* @param isPrefetch indicates if the operation is a prefetch\n* @param expectedState expected states for validation\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable()": "/**\n* Returns the count of available items in the buffer pool.\n* @return count of available items\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close()": "/**\n* Closes the resource, cancels prefetches, and logs operation summary.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Reads a block of data into the provided buffer.\n* @param data the buffer data to read into\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)": "/**\n* Prefetches data into the buffer and updates statistics.\n* @param data buffer data to prefetch\n* @param taskQueuedStartTime timestamp of task queuing\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int)": "/**\n* Initiates prefetching of a specified block if conditions are met.\n* @param blockNumber identifier for the block to prefetch\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int)": "/**\n* Retrieves BufferData for a specified block number.\n* @param blockNumber non-negative block identifier\n* @return BufferData object or null if unavailable\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Processes BufferData; returns true if ready to read.\n* @param data the BufferData to validate and process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int)": "/**\n* Retrieves BufferData for a specified block number with retry logic.\n* @param blockNumber non-negative block identifier\n* @return BufferData object or throws IllegalStateException if unavailable\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool": {
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquireHelper(boolean)": "/**\n* Acquires an item from the pool, blocking if necessary.\n* @param canBlock allows waiting for an item if none is available\n* @return an available item or null if not found and not blocking\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(java.lang.Object)": "/**\n* Placeholder for cleanup actions in subclasses.\n* @param item item to be closed, no action taken here\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numCreated()": "/**\n* Returns the count of created items.\n* @return number of items in createdItems collection\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire()": "/**\n* Acquires an item from the pool, blocking if necessary.\n* @return an available item or null if not found\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire()": "/**\n* Attempts to acquire an item without blocking.\n* @return an available item or null if not found\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close()": "/**\n* Closes all created items and clears associated collections.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable()": "/**\n* Calculates the number of available items.\n* @return total available items after accounting for created items\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object)": "/**\n* Releases an item to the pool after validation.\n* @param item the item to be released\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int)": "/**\n* Initializes a BoundedResourcePool with a specified size.\n* @param size maximum number of resources in the pool\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString()": "/**\n* Returns a string representation of the object's state with sizes and counts.\n* @return formatted string with size, created, in-queue, and available item counts\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.PrefetchConstants": {
        "org.apache.hadoop.fs.impl.prefetch.PrefetchConstants:<init>()": "/**\n* Private constructor to prevent instantiation of the PrefetchConstants class.\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind": {
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:fromShortName(java.lang.String)": "/**\n* Retrieves Kind enum by its short name.\n* @param shortName the short name of the Kind\n* @return corresponding Kind or null if not found\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache": {
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:containsBlock(int)": "/**\n* Checks if the specified block number exists in the collection.\n* @param blockNumber the number of the block to check\n* @return true if the block exists, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:blocks()": "/**\n* Returns an unmodifiable list of block keys.\n* @return Iterable of Integer block keys\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:size()": "/**\n* Returns the number of blocks in the collection.\n* @return the count of blocks as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:readFile(java.nio.file.Path,java.nio.ByteBuffer)": "/**\n* Reads bytes from a file into a ByteBuffer.\n* @param path file path to read from\n* @param buffer buffer to store read bytes\n* @return total number of bytes read\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToHeadOfLinkedList(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)": "/**\n* Adds an entry to the head of the linked list, updating pointers as necessary.\n* @param entry the Entry to be added\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:writeFile(java.nio.file.Path,java.nio.ByteBuffer)": "/**\n* Writes data from ByteBuffer to a file at the specified path.\n* @param path file location to write data\n* @param buffer data to be written\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getStats()": "/**\n* Returns a formatted string of statistics including entries and gets count.\n* @return formatted statistics string\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteCacheFiles()": "/**\n* Deletes cache files and logs the number of files removed.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getIntList(java.lang.Iterable)": "/**\n* Converts a list of integers into a formatted string of ranges.\n* @param nums iterable collection of integers\n* @return formatted string representation of integer ranges\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)": "",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)": "/**\n* Adds an Entry to the head of a linked list with thread safety.\n* @param entry the Entry to be added\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)": "/**\n* Validates entry size and checksum against buffer data.\n* @param entry the entry to validate\n* @param buffer the ByteBuffer to check against\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close()": "/**\n* Closes the resource and logs statistics if not already closed.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString()": "/**\n* Returns a string representation of stats and block keys.\n* @return formatted string of stats and block integers\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int)": "/**\n* Retrieves an Entry by block number, adding it to the linked list.\n* @param blockNumber non-negative index of the block\n* @return Entry associated with the block number\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)": "/**\n* Deletes a block file and evicts it from cache.\n* @param elementToPurge entry to be removed from cache\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)": "/**\n* Retrieves data into a buffer for the specified block number.\n* @param blockNumber index of the block to read\n* @param buffer buffer to store read data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)": "/**\n* Adds an entry to the list and evicts the oldest if exceeding maxBlocksCount.\n* @param entry the Entry to be added\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)": "/**\n* Generates a temporary file path with specified configuration.\n* @param conf configuration settings\n* @param localDirAllocator allocator for local directory paths\n* @return Path of the created temporary file\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)": "/**\n* Retrieves cache file path using provided configuration and allocator.\n* @param conf configuration settings\n* @param localDirAllocator allocator for local directory paths\n* @return Path of the cache file\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)": "/**\n* Checks if sufficient cache space is available for a given file size.\n* @param fileSize required space in bytes\n* @param conf configuration settings\n* @param localDirAllocator allocator for local directory paths\n* @return true if space is available, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)": "/**\n* Stores a block in cache, validating and writing data from the buffer.\n* @param blockNumber identifier for the block\n* @param buffer data to be cached\n* @param conf configuration settings\n* @param localDirAllocator allocator for local directory paths\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.Validate": {
        "org.apache.hadoop.fs.impl.prefetch.Validate:<init>()": "/**\n* Private constructor to prevent instantiation of the Validate class.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkState(boolean,java.lang.String,java.lang.Object[])": "/**\n* Validates an expression and throws an exception if false.\n* @param expression condition to check\n* @param format error message format\n* @param args message arguments\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)": "",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)": "/**\n* Validates that a string is not null and not empty.\n* @param arg string to check\n* @param argName name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)": "/**\n* Validates collection is not null and has a specific number of elements.\n* @param collection the collection to check\n* @param numElements expected number of elements\n* @param argName name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)": "/**\n* Validates if a given path exists.\n* @param path the path to check\n* @param argName name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)": "/**\n* Validates that array is not null and not empty.\n* @param array the array to check\n* @param argName the name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)": "/**\n* Validates that the array is not null and not empty.\n* @param array byte array to check\n* @param argName name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)": "/**\n* Validates that the array is not null and not empty.\n* @param array the short array to check\n* @param argName the name of the argument for error reporting\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)": "/**\n* Validates that an array is not null and not empty.\n* @param array the array to check\n* @param argName the name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)": "/**\n* Validates that the array is not null and not empty.\n* @param array the array to check\n* @param argName name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)": "/**\n* Validates that the iterable is not null and not empty.\n* @param iter the iterable to check\n* @param argName the name of the argument for error messaging\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)": "/**\n* Validates if a path exists and is a directory.\n* @param path the path to check\n* @param argName name of the argument for error messages\n*/",
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)": "/**\n* Validates if a path exists and is a regular file.\n* @param path the path to check\n* @param argName name of the argument for error messages\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics": {
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:<init>()": "/**\n* Private constructor for EmptyPrefetchingStatistics class.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockAddedToFileCache()": "/**\n* Invoked when a block is added to the file cache.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockEvictedFromFileCache()": "/**\n* Handles the event when a block is evicted from the file cache.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockRemovedFromFileCache()": "/**\n* Handles actions when a block is removed from the file cache.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:executorAcquired(java.time.Duration)": "/**\n* Handles the event when an executor is acquired after a specified queue duration.\n* @param timeInQueue duration the executor spent in the queue\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:memoryAllocated(int)": "/**\n* Allocates memory of the specified size.\n* @param size the amount of memory to allocate in bytes\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:memoryFreed(int)": "/**\n* Notifies that memory of the specified size has been freed.\n* @param size the amount of memory freed in bytes\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationCompleted()": "/**\n* Indicates that the prefetch operation has completed successfully.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted()": "/**\n* Starts prefetch operation and returns a DurationTracker instance.\n* @return DurationTracker for tracking operation durations\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.FilePosition": {
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:invalidate()": "/**\n* Resets the buffer and its offsets to invalidate the current state.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:resetReadStats()": "/**\n* Resets the read statistics to zero for bytes and read operations.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:isValid()": "/**\n* Checks if the buffer is not null.\n* @return true if buffer is valid, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer()": "/**\n* Validates that the buffer is not null; throws exception if invalid.\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer()": "/**\n* Returns the ByteBuffer after validating it.\n* @return ByteBuffer instance, or throws if invalid\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:data()": "/**\n* Returns BufferData after validating the buffer.\n* @return BufferData object\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:relative()": "/**\n* Returns the current position of the buffer.\n* @return current buffer position as an integer\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long)": "/**\n* Checks if the position is within the current buffer range.\n* @param pos position to check\n* @return true if within range, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset()": "/**\n* Returns the starting offset of the buffer.\n* @throws IllegalArgumentException if the buffer is invalid\n* @return long representing the buffer's start offset\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute()": "/**\n* Calculates absolute position in the buffer.\n* @return absolute position as a long value\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead()": "/**\n* Checks if the entire buffer has been read.\n* @return true if fully read, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long)": "/**\n* Sets buffer position if valid and within current buffer range.\n* @param pos absolute position to set\n* @return true if position set successfully, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber()": "/**\n* Retrieves the block number after validating the buffer.\n* @return block number corresponding to the buffer's start offset\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)": "/**\n* Sets buffer data and offsets after validation.\n* @param bufferData data to set, must not be null\n* @param startOffset starting position for the buffer\n* @param readOffset position to read from, must be within range\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)": "/**\n* Constructs FilePosition with validated file and block sizes.\n* @param fileSize total size of the file\n* @param blockSize size of each block\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock()": "/**\n* Checks if the current block is the last block.\n* @return true if it's the last block, false otherwise\n*/",
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:toString()": "/**\n* Returns a string representation of the object's buffer state and block information.\n* @return formatted string with buffer position and block details\n*/"
    },
    "org.apache.hadoop.fs.impl.WeakReferenceThreadMap": {
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:currentThreadId()": "/**\n* Returns the ID of the currently executing thread.\n* @return long representing the thread's unique identifier\n*/",
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread()": "/**\n* Retrieves the value associated with the current thread.\n* @return Value associated with the current thread ID\n*/",
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread()": "/**\n* Removes the value associated with the current thread.\n* @return value associated with the current thread or null if not found\n*/",
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object)": "/**\n* Sets a value for the current thread; replaces if different.\n* @param newVal value to set for the current thread\n* @return previous value or newVal if unchanged\n*/",
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)": "/**\n* Constructs a WeakReferenceThreadMap with a factory and a reference lost callback.\n* @param factory function to create values for keys\n* @param referenceLost callback for when a reference is lost\n*/"
    },
    "org.apache.hadoop.fs.impl.WeakRefMetricsSource": {
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:<init>(java.lang.String,org.apache.hadoop.metrics2.MetricsSource)": "/**\n* Constructs a WeakRefMetricsSource with a name and a weak reference to a MetricsSource.\n* @param name the name of the metrics source\n* @param source the MetricsSource to reference weakly\n*/",
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)": "/**\n* Collects metrics from the source if available.\n* @param collector collects the metrics data\n* @param all indicates whether to retrieve all metrics\n*/",
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:getSource()": "/**\n* Retrieves the MetricsSource from the weak reference.\n* @return MetricsSource object or null if not available\n*/",
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:toString()": "/**\n* Returns a string representation of the WeakRefMetricsSource object.\n* @return formatted string indicating name and weak reference status\n*/"
    },
    "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation": {
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes a bulk delete operation with the specified base path and file system.\n* @param basePath the path where the operation will be performed\n* @param fs the file system to use for the operation\n*/",
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:basePath()": "/**\n* Retrieves the base path.\n* @return Path representing the base directory\n*/",
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:pageSize()": "/**\n* Returns the default page size for pagination.\n* @return int representing the default page size\n*/",
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection)": "/**\n* Deletes specified paths and returns deletion results.\n* @param paths collection of paths to delete\n* @return List of entries with path and error message if deletion fails\n*/"
    },
    "org.apache.hadoop.util.functional.Tuples": {
        "org.apache.hadoop.util.functional.Tuples:pair(java.lang.Object,java.lang.Object)": "/**\n* Creates a map entry from a key and value.\n* @param key the entry's key\n* @param value the entry's value\n* @return a Map.Entry containing the key and value\n*/",
        "org.apache.hadoop.util.functional.Tuples:<init>()": "/**\n* Private constructor for Tuples class to prevent instantiation.\n*/"
    },
    "org.apache.hadoop.fs.impl.FutureIOSupport": {
        "org.apache.hadoop.fs.impl.FutureIOSupport:<init>()": "/**\n* Private constructor for FutureIOSupport class, preventing instantiation.\n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Evaluates a CallableRaisingIOE and returns a CompletableFuture result.\n* @param callable a CallableRaisingIOE that may throw exceptions\n* @return CompletableFuture containing the result or exception\n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException)": "/**\n* Unwraps inner IOException from an ExecutionException.\n* @param e the ExecutionException to unwrap\n* @return unwrapped IOException\n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException)": "/**\n* Unwraps and throws inner IOException from a CompletionException.\n* @param e the CompletionException to unwrap\n* @throws IOException if an inner IOException is found\n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future)": "/**\n* Awaits completion of a Future, handling exceptions.\n* @param future the Future to await\n* @return result of the Future\n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)": "/**\n* Awaits a Future's result with a timeout. \n* @param future the Future to await \n* @param timeout maximum wait time \n* @param unit time unit of the timeout \n* @return result of the Future \n* @throws exceptions on failure \n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)": "/**\n* Deprecated method to propagate options to a builder.\n* @param builder FSBuilder to modify\n* @param conf Configuration containing properties\n* @param prefix Prefix for filtering properties\n* @param mandatory If true, options are mandatory\n*/",
        "org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Applies configuration options to a builder with prefixes.\n* @param builder FSBuilder instance to modify\n* @param conf Configuration containing properties\n* @param optionalPrefix Prefix for optional properties\n* @param mandatoryPrefix Prefix for mandatory properties\n* @return Modified FSBuilder instance\n*/"
    },
    "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder": {
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getThisBuilder()": "/**\n* Returns the current instance of FileSystemMultipartUploaderBuilder.\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize()": "/**\n* Returns the current buffer size by invoking the superclass method.\n* @return the size of the buffer as an integer\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication()": "/**\n* Retrieves the current replication value from the superclass.\n* @return short representing the replication level\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags()": "/**\n* Retrieves the set of creation flags.\n* @return EnumSet of CreateFlag representing current flags\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt()": "",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize()": "/**\n* Overrides getBlockSize to return the block size from the superclass.\n* @return block size as a long value\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS()": "/**\n* Retrieves the FileSystem instance from the superclass.\n* @return FileSystem instance\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission()": "/**\n* Retrieves file permissions from the superclass.\n* @return FsPermission object representing file permissions\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build()": "/**\n* Builds a FileSystemMultipartUploader instance.\n* @return FileSystemMultipartUploader object\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Constructs a FileSystemMultipartUploaderBuilder with specified file system and path.\n* @param fileSystem the file system to use\n* @param path the path for the uploader\n*/"
    },
    "org.apache.hadoop.fs.impl.FsLinkResolution": {
        "org.apache.hadoop.fs.impl.FsLinkResolution:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)": "/**\n* Applies a function to the file system and path, returning a result.\n* @param fs the file system to operate on\n* @param p the path to apply the function to\n* @return result of the function application\n*/",
        "org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)": "/**\n* Constructs FsLinkResolution with a non-null function.\n* @param fn function to resolve, must not be null\n*/",
        "org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)": "/**\n* Resolves a path using the provided function.\n* @param fileContext context for file operations\n* @param path the path to resolve\n* @param fn function to resolve the link\n* @return resolved object of type T\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$3": {
        "org.apache.hadoop.fs.FileSystem$3:<init>()": "/**\n* Constructs a FileSystem instance with no specific configuration.\n*/"
    },
    "org.apache.hadoop.fs.shell.Ls": {
        "org.apache.hadoop.fs.shell.Ls:isOrderReverse()": "/**\n* Checks if the order is reversed.\n* @return true if order is reversed, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:isDisplayECPolicy()": "/**\n* Checks if the EC policy is set to display.\n* @return true if the EC policy is displayed, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:isOrderTime()": "/**\n* Checks if the order time is set.\n* @return true if order time is set, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:isOrderSize()": "/**\n* Checks if the order size is valid.\n* @return true if order size is valid, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:getOrderComparator()": "/**\n* Retrieves the order comparator for PathData objects.\n* @return Comparator<PathData> used for ordering\n*/",
        "org.apache.hadoop.fs.shell.Ls:isUseAtime()": "/**\n* Checks if access time is used.\n* @return true if access time is enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:isHideNonPrintable()": "/**\n* Checks if non-printable characters should be hidden.\n* @return true if hiding non-printable characters, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:maxLength(int,java.lang.Object)": "/**\n* Returns the maximum length between n and the length of value.\n* @param n an integer to compare length against\n* @param value an object whose length is evaluated\n* @return maximum length as an integer\n*/",
        "org.apache.hadoop.fs.shell.Ls:getListingGroupSize()": "/**\n* Returns the size of the listing group for output formatting.\n* @return int representing group size; 0 if paths only, otherwise 100\n*/",
        "org.apache.hadoop.fs.shell.Ls:isDirRecurse()": "/**\n* Checks if directory recursion is enabled.\n* @return true if recursion is enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:isHumanReadable()": "/**\n* Checks if the content is human-readable.\n* @return true if human-readable, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:isPathOnly()": "/**\n* Checks if the current instance is set to path-only mode.\n* @return true if path-only mode is enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with their associated names.\n* @param factory CommandFactory to register commands with\n*/",
        "org.apache.hadoop.fs.shell.Ls:isSorted()": "/**\n* Determines if the list is sorted based on order conditions.\n* @return true if sorted; false if not sorted.\n*/",
        "org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator()": "/**\n* Initializes order comparator based on time or size criteria.\n*/",
        "org.apache.hadoop.fs.shell.Ls:formatSize(long)": "/**\n* Formats size as a human-readable string or returns the raw value.\n* @param size the size in bytes\n* @return formatted size string or raw size as string\n*/",
        "org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options for file operations.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item, checking for Erasure Coding support.\n* @param item PathData object to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[])": "/**\n* Adjusts column widths based on file attributes from PathData array.\n* @param items array of PathData objects\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])": "/**\n* Processes PathData items, sorting and adjusting their widths if not pathOnly.\n* @param parent the parent PathData object\n* @param items array of PathData items to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes an Ls command with the given configuration.\n* @param conf the Configuration for the command\n*/",
        "org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item and prints its details based on configuration.\n* @param item the PathData object to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Ls:<init>()": "/**\n* Constructs an instance of Ls command.\n* No parameters or return value.\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandFormat$UnknownOptionException": {
        "org.apache.hadoop.fs.shell.CommandFormat$UnknownOptionException:<init>(java.lang.String)": "/**\n* Constructs an UnknownOptionException with a specified illegal option message.\n* @param unknownOption the illegal option that caused the exception\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Find$2": {
        "org.apache.hadoop.fs.shell.find.Find$2:<init>()": "/**\n* Initializes a Find instance with recursive search enabled.\n*/"
    },
    "org.apache.hadoop.fs.shell.find.ExpressionFactory": {
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:addClass(java.lang.Class,java.lang.String[])": "/**\n* Adds a class to the expression map with associated names.\n* @param expressionClass class type extending Expression\n* @param names variable number of names to associate with the class\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:<init>()": "/**\n* Private constructor for ExpressionFactory to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:isExpression(java.lang.String)": "/**\n* Checks if the expression exists in the map.\n* @param expressionName name of the expression to check\n* @return true if exists, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpressionFactory()": "/**\n* Retrieves the singleton instance of ExpressionFactory.\n* @return ExpressionFactory instance\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class)": "/**\n* Registers an expression class by invoking its static register method.\n* @param expressionClass the class of the expression to register\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Creates an Expression instance from a class type with provided configuration.\n* @param expressionClass class type of the Expression to create\n* @param conf configuration to apply\n* @return new Expression instance or null if class is null\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves an Expression instance by name and configuration.\n* @param expressionName name of the Expression to fetch\n* @param conf configuration for the Expression\n* @return Expression instance or null if not found\n*/",
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Creates an Expression from a class name and configuration.\n* @param expressionClassname fully qualified class name of the Expression\n* @param conf configuration to apply\n* @return new Expression instance or throws IllegalArgumentException if class not found\n*/"
    },
    "org.apache.hadoop.fs.shell.find.And": {
        "org.apache.hadoop.fs.shell.find.And:<init>()": "/**\n* Initializes And command with usage and help information.\n*/",
        "org.apache.hadoop.fs.shell.find.And:addChildren(java.util.Deque)": "/**\n* Adds children expressions from the provided deque.\n* @param expressions deque of Expression objects to add\n*/",
        "org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)": "/**\n* Registers expression classes with associated names.\n* @param factory ExpressionFactory to register classes with\n* @throws IOException if an error occurs during registration\n*/",
        "org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)": "/**\n* Applies expressions to PathData, returning combined Result of all child evaluations.\n* @param item PathData to evaluate\n* @param depth current evaluation depth (unused)\n* @return Result reflecting combined statuses of child evaluations\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Result": {
        "org.apache.hadoop.fs.shell.find.Result:isPass()": "/**\n* Checks if the operation was successful.\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Result:<init>(boolean,boolean)": "/**\n* Constructs a Result object with success status and recursion flag.\n* @param success indicates operation success\n* @param recurse  flag for recursion behavior\n*/",
        "org.apache.hadoop.fs.shell.find.Result:isDescend()": "/**\n* Checks if the current state is descending.\n* @return true if descending, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Result:equals(java.lang.Object)": "/**\n* Compares this Result object to another for equality.\n* @param obj object to compare with this Result\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result)": "/**\n* Combines two Result objects' success and descent statuses.\n* @param other Result to combine with\n* @return new Result reflecting combined statuses\n*/",
        "org.apache.hadoop.fs.shell.find.Result:negate()": "/**\n* Negates the operation result and returns a new Result object.\n* @return Result object with negated success and current descent status\n*/",
        "org.apache.hadoop.fs.shell.find.Result:toString()": "/**\n* Returns a string representation of success and recursion states.\n* @return formatted string with success and recursion status\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Print": {
        "org.apache.hadoop.fs.shell.find.Print:<init>(java.lang.String)": "/**\n* Initializes Print with a specified suffix.\n* @param suffix the string to append to printed output\n*/",
        "org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)": "/**\n* Registers expression classes with the factory.\n* @param factory ExpressionFactory to register classes with\n*/",
        "org.apache.hadoop.fs.shell.find.Print:<init>()": "/**** Initializes Print with a newline suffix. */",
        "org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)": "/**\n* Prints PathData item and returns a PASS result.\n* @param item PathData to be printed\n* @param depth current depth level in processing\n* @return Result.PASS indicating successful operation\n*/"
    },
    "org.apache.hadoop.fs.shell.find.FindOptions": {
        "org.apache.hadoop.fs.shell.find.FindOptions:getOut()": "/**\n* Returns the current PrintStream output.\n* @return PrintStream object for output operations\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setFollowLink(boolean)": "/**\n* Sets the followLink status.\n* @param followLink true to follow links, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setFollowArgLink(boolean)": "/**\n* Sets the followArgLink flag.\n* @param followArgLink true to follow argument link, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setOut(java.io.PrintStream)": "/**\n* Sets the output stream for this object.\n* @param out the PrintStream to be set as output\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setErr(java.io.PrintStream)": "/**\n* Sets the error output stream.\n* @param err PrintStream for error messages\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setIn(java.io.InputStream)": "/**\n* Sets the InputStream for processing.\n* @param in InputStream to be set\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Sets the command factory for creating commands.\n* @param factory CommandFactory instance to set\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:setConfiguration(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param configuration the Configuration to be set\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:getMaxDepth()": "/**\n* Retrieves the maximum depth value.\n* @return the maximum depth as an integer\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:isFollowLink()": "/**\n* Checks if the follow link option is enabled.\n* @return true if follow link is enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:getErr()": "/**\n* Returns the error output stream.\n* @return PrintStream for error output\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:isFollowArgLink()": "/**\n* Checks if the follow argument link is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:isDepthFirst()": "/**\n* Checks if the traversal method is depth-first.\n* @return true if depth-first, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.FindOptions:getMinDepth()": "/**\n* Retrieves the minimum depth value.\n* @return minimum depth as an integer\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Name": {
        "org.apache.hadoop.fs.shell.find.Name:setCaseSensitive(boolean)": "/**\n* Sets the case sensitivity flag.\n* @param caseSensitive true for case-sensitive, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Name:addArguments(java.util.Deque)": "/**\n* Adds arguments from the provided deque, starting from index 1.\n* @param args deque of string arguments to add\n*/",
        "org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)": "/**\n* Registers classes with the ExpressionFactory.\n* @param factory the ExpressionFactory to register classes with\n*/",
        "org.apache.hadoop.fs.shell.find.Name:<init>(boolean)": "/**\n* Initializes Name object with usage, help, and case sensitivity.\n* @param caseSensitive true for case-sensitive, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)": "/**\n* Applies glob pattern matching to the item's name.\n* @param item PathData object to extract name from\n* @param depth recursion depth\n* @return Result.PASS if matches, else Result.FAIL\n*/",
        "org.apache.hadoop.fs.shell.find.Name:<init>()": "/**\n* Default constructor initializing Name object as case-sensitive.\n*/",
        "org.apache.hadoop.fs.shell.find.Name:prepare()": "/**\n* Prepares the glob pattern from the argument.\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.find.FilterExpression": {
        "org.apache.hadoop.fs.shell.find.FilterExpression:<init>(org.apache.hadoop.fs.shell.find.Expression)": "/**\n* Initializes a FilterExpression with the provided Expression object.\n* @param expression the Expression to be set\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions)": "/**\n* Sets options for the expression if it is not null.\n* @param options configuration settings for finding\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:prepare()": "/**\n* Prepares the expression if it is not null.\n* @throws IOException if an I/O error occurs during preparation\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:apply(org.apache.hadoop.fs.shell.PathData,int)": "/**\n* Applies an expression to PathData; returns PASS if no expression is set.\n* @param item PathData object to apply the expression on\n* @param depth ignored depth parameter\n* @return Result of the expression or PASS if expression is null\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:finish()": "/**\n* Completes the expression if it is not null.\n* @throws IOException if an I/O error occurs during finish\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:getUsage()": "/**\n* Retrieves usage information from the expression.\n* @return array of usage strings or null if expression is not set\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:getHelp()": "/**\n* Retrieves help information from the expression.\n* @return array of help strings or null if expression is null\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:isAction()": "/**\n* Checks if the expression is an action.\n* @return true if expression is an action, false if null or not an action\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:isOperator()": "/**\n* Checks if the expression is an operator.\n* @return true if expression is an operator, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:getPrecedence()": "/**\n* Returns the precedence of the expression.\n* @return precedence level or -1 if expression is null\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:addChildren(java.util.Deque)": "/**\n* Adds children to the expression if it is not null.\n* @param expressions queue of Expression objects to add children to\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:addArguments(java.util.Deque)": "/**\n* Adds arguments to the expression if it is not null.\n* @param args a deque of string arguments to add\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration for the expression if it is Configurable.\n* @param conf the configuration to set\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:getConf()": "/**\n* Retrieves the configuration from a Configurable expression.\n* @return Configuration object or null if expression is not Configurable\n*/",
        "org.apache.hadoop.fs.shell.find.FilterExpression:toString()": "/**\n* Returns a string representation of the object, including its class name and expression.\n* @return String representation of the object\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Find$1": {
        "org.apache.hadoop.fs.shell.find.Find$1:<init>()": "/**\n* Initializes a Find instance with recursion enabled.\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Find": {
        "org.apache.hadoop.fs.shell.find.Find:addExpression(java.lang.Class)": "/**\n* Adds a subclass of Expression to the EXPRESSIONS collection.\n* @param clazz the class to be added as an Expression subclass\n*/",
        "org.apache.hadoop.fs.shell.find.Find:setRootExpression(org.apache.hadoop.fs.shell.find.Expression)": "/**\n* Sets the root expression for the current context.\n* @param expression the Expression to be set as root\n*/",
        "org.apache.hadoop.fs.shell.find.Find:addStop(org.apache.hadoop.fs.shell.PathData)": "/**\n* Adds a path to the stopPaths collection.\n* @param item PathData object containing the path to add\n*/",
        "org.apache.hadoop.fs.shell.find.Find:isStop(org.apache.hadoop.fs.shell.PathData)": "/**\n* Checks if the given path is in the stopPaths collection.\n* @param item PathData object containing the path to check\n* @return true if path is found, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Find:getRootExpression()": "/**\n* Retrieves the root expression of the current object.\n* @return the root Expression instance\n*/",
        "org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the CommandFactory.\n* @param factory the CommandFactory instance to register commands with\n*/",
        "org.apache.hadoop.fs.shell.find.Find:createOptions()": "/**\n* Creates and configures FindOptions with I/O and command factory settings.\n* @return configured FindOptions instance\n*/",
        "org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String)": "/**\n* Checks if the given expression exists in the map.\n* @param expressionName name of the expression to check\n* @return true if exists, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory)": "/**\n* Registers multiple expression classes with the factory.\n* @param factory the ExpressionFactory to register expressions with\n*/",
        "org.apache.hadoop.fs.shell.find.Find:getOptions()": "/**\n* Retrieves configured FindOptions, creating them if not already initialized.\n* @return FindOptions instance\n*/",
        "org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData)": "/**\n* Applies item if depth meets minimum; adds to stop if result is STOP.\n* @param item PathData object to apply\n*/",
        "org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList)": "/**\n* Processes arguments and prepares the root expression.\n* @param args list of PathData to process\n*/",
        "org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item based on traversal method; skips if depth-first.\n* @param item PathData to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes the given PathData if depth-first traversal is enabled.\n* @param item PathData object to process\n*/",
        "org.apache.hadoop.fs.shell.find.Find:<init>()": "/**\n* Constructs a Find instance and sets recursive search to true.\n*/",
        "org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory)": "/**\n* Builds a description of recognized expressions and operators.\n* @param factory generates Expression instances\n* @return formatted string of expressions and operators\n*/",
        "org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class)": "/**\n* Retrieves an Expression instance based on the provided class type.\n* @param expressionClass class type of the Expression to create\n* @return new Expression instance or null if class is null\n*/",
        "org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String)": "/**\n* Retrieves an Expression by name using the ExpressionFactory.\n* @param expressionName name of the Expression to fetch\n* @return Expression instance or null if not found\n*/",
        "org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**\n* Checks if source is an ancestor of target in the path hierarchy.\n* @param source the potential ancestor path\n* @param target the path to check against\n* @return true if source is an ancestor, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque)": "/**\n* Parses expressions from a deque of arguments.\n* @param args deque of string arguments representing expressions\n* @return the resulting Expression after parsing\n*/",
        "org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and sets root expression from remaining arguments.\n* @param args list of command-line arguments\n* @throws IOException if an input/output error occurs\n*/",
        "org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Recursively processes paths, handling symlinks and depth limits.\n* @param item PathData object to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData)": "/**\n* Determines if the given PathData item can be recursively traversed.\n* @param item the PathData object to evaluate\n* @return true if it's a directory or a valid symlink to a directory, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.shell.find.BaseExpression": {
        "org.apache.hadoop.fs.shell.find.BaseExpression:getChildren()": "/**\n* Retrieves the list of child expressions.\n* @return List of Expression objects representing children\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getOptions()": "/**\n* Retrieves current FindOptions or creates a new one if none exists.\n* @return FindOptions object\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getArguments()": "/**\n* Retrieves the list of arguments.\n* @return List of argument strings\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getArgument(int)": "/**\n* Retrieves an argument by its position.\n* @param position index of the desired argument (1-based)\n* @return the argument string\n* @throws IOException if the argument is missing or null\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:addChild(org.apache.hadoop.fs.shell.find.Expression)": "/**\n* Adds a child expression to the children stack.\n* @param expr the Expression to be added\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:addArgument(java.lang.String)": "/**\n* Adds a new argument to the arguments list.\n* @param arg the argument to be added\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getFileSystem(org.apache.hadoop.fs.shell.PathData)": "/**\n* Retrieves the FileSystem associated with the given PathData item.\n* @param item PathData object containing the FileSystem reference\n* @return FileSystem associated with the item\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque)": "/**\n* Adds arguments to the expression; no operation by default.\n* @param args a deque of string arguments to be added\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getHelp()": "/**\n* Retrieves the help information as an array of strings.\n* @return array of help strings\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Retrieves the path from the given PathData item.\n* @param item the PathData object containing the path\n* @return the Path associated with the item\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getUsage()": "/**\n* Returns an array of usage strings.\n* @return array of usage descriptions\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf Configuration object to be set\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:setHelp(java.lang.String[])": "/**\n* Sets the help text array.\n* @param help array of help text strings\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:setUsage(java.lang.String[])": "/**\n* Sets the usage information.\n* @param usage array of usage strings to be assigned\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque)": "/**\n* Adds child expressions to the current expression.\n* @param exprs a deque of Expression objects to add as children\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getPrecedence()": "/**\n* Returns the precedence level, which is always 0 for this implementation.\n* @return precedence level as an integer\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:isOperator()": "/**\n* Checks if the current instance is an operator.\n* @return false indicating not an operator\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions)": "/**\n* Sets options and updates child expressions accordingly.\n* @param options the options to be set for this instance and its children\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:prepare()": "/**\n* Prepares all child expressions for processing.\n* @throws IOException if an error occurs during preparation\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:finish()": "/**\n* Finishes all child expressions.\n* @throws IOException if an I/O error occurs during finishing\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:isAction()": "/**\n* Checks if any child expression is an action.\n* @return true if any child is an action, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:toString()": "/**\n* Returns a string representation of the object with its arguments and children.\n* @return formatted string of class name, arguments, and child expressions\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)": "/**\n* Adds specified number of child expressions from the deque.\n* @param exprs deque of Expression objects\n* @param count number of expressions to add\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)": "/**\n* Adds specified number of arguments from the deque to the list.\n* @param args deque containing arguments to add\n* @param count number of arguments to add\n*/",
        "org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)": "/**\n* Retrieves FileStatus for the given PathData, following symlinks if specified.\n* @param item PathData object to retrieve status from\n* @param depth controls symlink following behavior\n* @return FileStatus of the item\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.Delete$Rm": {
        "org.apache.hadoop.fs.shell.Delete$Rm:expandArgument(java.lang.String)": "/**\n* Expands the argument into a list of PathData, handling PathNotFoundException.\n* @param arg the argument to expand\n* @return List of PathData or empty if not found and ignored\n*/",
        "org.apache.hadoop.fs.shell.Delete$Rm:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a nonexistent path if not ignoring file not found errors.\n* @param item PathData object representing the nonexistent path\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and updates deletion settings.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData)": "/**\n* Determines if a PathData item can be safely deleted based on file count and user confirmation.\n* @param item PathData object representing the item to evaluate\n* @return true if safe to delete, false if not or user aborts\n*/",
        "org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData)": "/**\n* Moves a file to trash if not skipped.\n* @param item PathData containing file system and path\n* @return true if move is successful, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item for deletion; handles directories and trash logic.\n* @param item PathData to be processed for deletion\n* @throws IOException if deletion fails or item is a directory and not allowed\n*/"
    },
    "org.apache.hadoop.util.ToolRunner": {
        "org.apache.hadoop.util.ToolRunner:confirmPrompt(java.lang.String)": "/**\n* Prompts user for confirmation and returns true for 'Y'/'yes' or false for 'N'/'no'.\n* @param prompt message to display to the user\n* @return boolean indicating user's confirmation choice\n*/",
        "org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream)": "/**\n* Prints usage information for generic command options.\n* @param out output stream for displaying usage information\n*/",
        "org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])": "/**\n* Runs a tool with specified configuration and arguments.\n* @param conf configuration settings, or creates a new one if null\n* @param tool the tool to execute\n* @param args command-line arguments for the tool\n* @return exit status of the tool execution\n*/",
        "org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])": "/**\n* Executes a tool with provided arguments.\n* @param tool the tool to execute\n* @param args command-line arguments for the tool\n* @return exit status of the tool execution\n*/"
    },
    "org.apache.hadoop.fs.shell.PathData": {
        "org.apache.hadoop.fs.shell.PathData:checkIfSchemeInferredFromPath(java.lang.String)": "/**\n* Checks if the path string is a valid inferred scheme.\n* @param pathString the path to validate\n* @return true if scheme is inferred, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.PathData:setStat(org.apache.hadoop.fs.FileStatus)": "/**\n* Sets the file status and updates existence flag.\n* @param stat the FileStatus to set, can be null\n*/",
        "org.apache.hadoop.fs.shell.PathData:representsDirectory()": "/**\n* Checks if the URI represents a directory.\n* @return true if it's a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.PathData:removeAuthority(java.net.URI)": "/**\n* Removes authority from the given URI.\n* @param uri the original URI\n* @return URI without authority component\n*/",
        "org.apache.hadoop.fs.shell.PathData:findLongestDirPrefix(java.lang.String,java.lang.String,boolean)": "/**\n* Finds the longest directory prefix between current and given path.\n* @param cwd current working directory\n* @param path path to compare\n* @param isDir indicates if the path is a directory\n* @return index of last separator in longest prefix\n*/",
        "org.apache.hadoop.fs.shell.PathData:normalizeWindowsPath(java.lang.String)": "/**\n* Normalizes Windows file paths to a URI format.\n* @param pathString the file path to normalize\n* @return normalized file path as a URI string\n* @throws IOException if the path is invalid\n*/",
        "org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData)": "",
        "org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object)": "",
        "org.apache.hadoop.fs.shell.PathData:hashCode()": "/**\n* Returns the hash code of the URI object based on its path.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)": "/**\n* Generates a relative path from source URI to current working URI.\n* @param cwdUri current working directory URI\n* @param srcUri source URI to relativize\n* @param isDir true if source is a directory\n* @return relative path as a string\n*/",
        "org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String)": "/**\n* Converts a file path string to a URI.\n* @param pathString the file path to convert\n* @return URI representation of the path\n* @throws IOException if path normalization fails\n*/",
        "org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)": "/**\n* Converts URI to string, optionally omitting scheme for inferred paths.\n* @param uri URI to convert\n* @param inferredSchemeFromPath indicates if scheme is inferred\n* @return resulting string representation of the URI\n*/",
        "org.apache.hadoop.fs.shell.PathData:toString()": "/**\n* Converts the URI to its string representation.\n* @return string representation of the URI\n*/",
        "org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)": "/**\n* Retrieves file status from the filesystem by path.\n* @param fs the FileSystem instance\n* @param pathString the file path as a string\n* @param ignoreFNF flag to ignore FileNotFoundException\n* @return FileStatus or null if not found\n*/",
        "org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement)": "/**\n* Validates file existence and type against requirements.\n* @param typeRequirement specifies expected file type\n* @throws PathIOException if path checks fail\n*/",
        "org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path)": "/**\n* Constructs a string for the child path.\n* @param childPath the path of the child\n* @return formatted string representation of the child path\n*/",
        "org.apache.hadoop.fs.shell.PathData:refreshStatus()": "/**** Refreshes file status and handles exceptions. \n* @return FileStatus object or null if not found. \n*/",
        "org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator()": "/**\n* Retrieves an iterator for directory contents as PathData objects.\n* @return RemoteIterator of PathData\n* @throws IOException if directory check or listing fails\n*/",
        "org.apache.hadoop.fs.shell.PathData:toFile()": "/**\n* Converts the current path to a File if using LocalFileSystem.\n* @return File corresponding to the path\n* @throws IllegalArgumentException if not a local path\n*/",
        "org.apache.hadoop.fs.shell.PathData:parentExists()": "/**\n* Checks if the parent directory exists.\n* @return true if the directory or parent exists, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)": "/**\n* Initializes PathData with FileSystem, path, and status.\n* @param fs the FileSystem instance\n* @param pathString the file path as a string\n* @param stat the FileStatus to set, can be null\n*/",
        "org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)": "/**\n* Constructs PathData with FileSystem and path, retrieving file status.\n* @param fs the FileSystem instance\n* @param pathString the file path as a string\n*/",
        "org.apache.hadoop.fs.shell.PathData:getDirectoryContents()": "/**\n* Retrieves directory contents as PathData array.\n* @return array of PathData representing directory items\n* @throws IOException if directory access fails\n*/",
        "org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String)": "/**\n* Appends a suffix to the current path and returns a PathData object.\n* @param extension the suffix to add to the path\n* @return PathData instance with the new path\n*/",
        "org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData)": "/**** Retrieves PathData for a child directory. \n* @param child PathData of the child \n* @throws IOException if checks fail or path creation issues occur \n* @return PathData object for the child directory \n*/",
        "org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String)": "/**** Opens a file with specified read policy and length hint. \n* @param policy read policy for file access \n* @return FSDataInputStream for the opened file \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.fs.shell.PathData:openForSequentialIO()": "/**\n* Opens a file for sequential read I/O.\n* @return FSDataInputStream for sequential file access\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs PathData with a file path and configuration.\n* @param pathString the file path as a string\n* @param conf configuration settings for the filesystem\n* @throws IOException if filesystem initialization fails\n*/",
        "org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**** Constructs PathData with local filesystem and URI path. \n* @param localPath URI representing the local file path \n* @param conf configuration settings for the filesystem \n* @throws IOException if filesystem initialization fails \n*/",
        "org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**** Expands glob patterns to PathData array based on filesystem and configuration. \n* @param pattern the glob pattern to expand \n* @param conf configuration settings for the filesystem \n* @return array of PathData objects representing matched paths \n* @throws IOException if filesystem operations fail \n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators": {
        "org.apache.hadoop.util.functional.RemoteIterators:mappingRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Transforms a RemoteIterator<S> using a mapping function.\n* @param iterator input RemoteIterator of type S\n* @param mapper function to convert S to T\n* @return RemoteIterator of type T\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:<init>()": "/**\n* Private constructor for RemoteIterators class to prevent instantiation.\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromSingleton(java.lang.Object)": "/**\n* Creates a RemoteIterator from a singleton object.\n* @param singleton the object to iterate, may be null\n* @return RemoteIterator wrapping the singleton\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromIterator(java.util.Iterator)": "/**\n* Converts an Iterator to a RemoteIterator.\n* @param iterator the Iterator to wrap\n* @return a RemoteIterator wrapping the provided Iterator\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromIterable(java.lang.Iterable)": "/**\n* Converts an Iterable into a RemoteIterator.\n* @param iterable source Iterable to convert\n* @return RemoteIterator for the provided Iterable\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromArray(java.lang.Object[])": "/**\n* Creates a RemoteIterator from a given array.\n* @param array input array of type T\n* @return RemoteIterator for the array elements\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:typeCastingRemoteIterator(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Converts a RemoteIterator of type S to a RemoteIterator of type T.\n* @param iterator the input RemoteIterator of type S\n* @return a RemoteIterator of type T\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:filteringRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Creates a filtered RemoteIterator based on the provided filter function.\n* @param iterator the original RemoteIterator\n* @param filter function to determine if an element should be included\n* @return a new FilteringRemoteIterator\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:closingRemoteIterator(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)": "/**\n* Wraps a RemoteIterator to ensure it closes a specified resource.\n* @param iterator the RemoteIterator to wrap\n* @param toClose resource to close when iterator is done\n* @return a CloseRemoteIterator wrapping the provided iterator\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:haltableRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Creates a HaltableRemoteIterator with a control mechanism for continuation.\n* @param iterator the original RemoteIterator\n* @param continueWork a callable to determine if iteration should continue\n* @return a new HaltableRemoteIterator instance\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:rangeExcludingIterator(long,long)": "/**\n* Creates an iterator for a range excluding a specified end value.\n* @param start starting value of the range\n* @param excludedFinish value to be excluded from the range\n* @return RemoteIterator of Long values within the range\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Cleans up a RemoteIterator and logs its statistics.\n* @param source RemoteIterator to be cleaned up\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)": "/**\n* Iterates over items in a RemoteIterator, applying a consumer and counting processed items.\n* @param source RemoteIterator to iterate over\n* @param consumer function to apply to each item\n* @return number of items processed\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Converts a RemoteIterator to a List.\n* @param source RemoteIterator to convert\n* @return List containing elements from the iterator\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])": "/**\n* Converts a RemoteIterator to an array.\n* @param source RemoteIterator to convert\n* @param a array to store the elements\n* @return array containing elements from the iterator\n*/"
    },
    "org.apache.hadoop.fs.shell.Display$AvroFileInputStream": {
        "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:read()": "/**\n* Reads bytes from a buffer or file, returning -1 at EOF.\n* @return byte value or -1 if end of file is reached\n*/",
        "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:close()": "/**\n* Closes fileReader and output streams, releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus)": "/**** Initializes AvroFileInputStream with file status for reading Avro data. \n* @param status FileStatus of the Avro file \n* @throws IOException if file reading fails \n*/"
    },
    "org.apache.hadoop.fs.shell.FsUsage$TableBuilder": {
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:setRightAlign(int[])": "/**\n* Sets right alignment for specified indexes.\n* @param indexes variable number of index positions to align\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:addRow(java.lang.Object[])": "/**\n* Adds a new row of data and updates column widths.\n* @param objects values for each column in the row\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:setColumnHide(int,boolean)": "/**\n* Sets the visibility of a specified column.\n* @param columnIndex index of the column to hide/show\n* @param hideCol true to hide, false to show the column\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(int)": "/**\n* Initializes a TableBuilder with specified column count.\n* @param columns number of columns in the table\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:size()": "/**\n* Returns the number of data rows, excluding header if present.\n* @return count of data rows\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[])": "/**\n* Initializes TableBuilder with headers and adds the header row.\n* @param headers column headers for the table\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty()": "/**\n* Checks if the collection is empty.\n* @return true if no data rows are present, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream)": "/**\n* Prints formatted rows to the output stream if not empty.\n* @param out PrintStream to write formatted output\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystemOverloadScheme(org.apache.hadoop.fs.FileSystem)": "/**\n* Checks if the given FileSystem is a ViewFileSystemOverloadScheme.\n* @param fileSystem the FileSystem to check\n* @return true if it is a ViewFileSystemOverloadScheme, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:<init>()": "/**\n* Prevents instantiation of the ViewFileSystemUtil class.\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem)": "/**\n* Checks if the given file system is of type VIEWFS.\n* @param fileSystem the FileSystem to check\n* @return true if it is VIEWFS, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)": "/**** Updates the file system status for a mount point. \n* @param viewFileSystem the file system to query\n* @param mountPointMap map storing mount point statuses\n* @param mountPoint the mount point to update\n* @param path the file system path to check status\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Retrieves filesystem status for mount points based on the provided path.\n* @param fileSystem the FileSystem to check\n* @param path the path to evaluate\n* @return a map of MountPoint to FsStatus\n* @throws IOException if an I/O error occurs or filesystem is unsupported\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:getTargetFileSystemURIs()": "/**\n* Converts file system paths to URI array.\n* @return array of URIs corresponding to file system paths\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:getMountedOnPath()": "/**\n* Retrieves the path where the resource is mounted.\n* @return Path object representing the mounted location\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:<init>(org.apache.hadoop.fs.Path,java.lang.String[])": "/**\n* Constructs a MountPoint with source path and target file systems.\n* @param srcPath source path for mounting\n* @param targetFs array of target file system paths\n*/"
    },
    "org.apache.hadoop.fs.shell.FsUsage": {
        "org.apache.hadoop.fs.shell.FsUsage:setHumanReadable(boolean)": "/**\n* Sets the human-readable flag.\n* @param humanReadable true for human-readable format, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.FsUsage:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)": "/**\n* Sets the usages table for the current object.\n* @param usagesTable the TableBuilder instance to set\n*/",
        "org.apache.hadoop.fs.shell.FsUsage:getUsagesTable()": "/**\n* Retrieves the usages table builder instance.\n* @return TableBuilder object for usages\n*/",
        "org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/****\n* Registers command classes with their associated names.\n* @param factory CommandFactory instance to register classes with\n*/",
        "org.apache.hadoop.fs.shell.FsUsage:formatSize(long)": "/**\n* Formats size as a human-readable string or returns it as-is.\n* @param size the size in bytes\n* @return formatted size string or the original size\n*/"
    },
    "org.apache.hadoop.fs.shell.PrintableString": {
        "org.apache.hadoop.fs.shell.PrintableString:<init>(java.lang.String)": "/**\n* Constructs a printable string by replacing non-printable characters.\n* @param rawString input string to process\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem": {
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:close()": "/**\n* Cleans up temporary files without closing the underlying filesystem.\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes TargetFileSystem with a specified FileSystem instance.\n* @param fs the FileSystem to filter\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**\n* Renames a source path to a target path, deleting target if it exists.\n* @param src source path data\n* @param target target path data\n* @throws IOException if rename or delete operation fails\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)": "/**** Creates an FSDataOutputStream for a given path with optional lazy persistence. \n* @param item PathData object representing the file path \n* @param lazyPersist if true, enables lazy persistence \n* @return FSDataOutputStream for the created file \n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)": "/**\n* Writes input stream to a file, optionally deleting on exit.\n* @param in InputStream to read from\n* @param target PathData for the output file\n* @param lazyPersist enables lazy persistence\n* @param direct if true, skips deletion on exit\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread": {
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:setThreadCount(java.lang.String)": "/**\n* Sets the thread count based on the provided string value.\n* @param optValue string representation of the thread count\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:setThreadPoolQueueSize(java.lang.String)": "/**\n* Sets the thread pool queue size based on the provided string value.\n* @param optValue queue size as a string, defaults to DEFAULT_QUEUE_SIZE if invalid\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:initThreadPoolExecutor()": "/**\n* Initializes the ThreadPoolExecutor with specified thread count and queue size.\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:waitForCompletion()": "/**\n* Waits for executor service to complete tasks or shuts down if interrupted.\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getExecutor()": "/**\n* Retrieves the ThreadPoolExecutor instance.\n* @return the current ThreadPoolExecutor\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getThreadCount()": "/**\n* Retrieves the current thread count.\n* @return current number of threads\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getThreadPoolQueueSize()": "/**\n* Retrieves the current size of the thread pool queue.\n* @return the size of the thread pool queue\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList)": "/**\n* Checks if there are multiple source paths.\n* @param args list of PathData objects\n* @return true if more than one source path exists, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList)": "/**\n* Determines if multi-threading is needed based on thread count and source paths.\n* @param args list of PathData objects\n* @return true if multi-threading is necessary, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList)": "/**\n* Processes PathData arguments, initializing threads if needed and waiting for completion.\n* @param args list of PathData arguments\n* @throws IOException if path conditions are not met\n*/",
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**\n* Copies a file to target, using executor for async execution if available.\n* @param src source PathData object\n* @param target target PathData object\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.FsCommand": {
        "org.apache.hadoop.fs.shell.FsCommand:getCommandName()": "/**\n* Returns the command name by invoking the getName method.\n* @return command name as a String\n*/",
        "org.apache.hadoop.fs.shell.FsCommand:run(org.apache.hadoop.fs.Path)": "/**\n* Throws RuntimeException indicating unexpected method invocation.\n* @param path the file path that was processed (unused)\n* @throws IOException if an I/O error occurs (not applicable here)\n*/",
        "org.apache.hadoop.fs.shell.FsCommand:runAll()": "/**\n* Executes all tasks with provided arguments.\n* @return result code from task execution\n*/",
        "org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers multiple command classes with the given CommandFactory.\n* @param factory the CommandFactory to register commands with\n*/",
        "org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration)": "/**** Constructs an FsCommand with the provided configuration. \n* @param conf the Configuration for the command\n*/",
        "org.apache.hadoop.fs.shell.FsCommand:<init>()": "/**\n* Initializes a new instance of FsCommand.\n*/",
        "org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList)": "/**\n* Processes raw arguments and warns if fs.defaultFS is not configured.\n* @param args list of raw argument strings\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.fs.shell.Count": {
        "org.apache.hadoop.fs.shell.Count:isHumanReadable()": "/**\n* Checks if the content is human-readable.\n* @return true if content is human-readable, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the CommandFactory.\n* @param factory CommandFactory instance to register commands\n*/",
        "org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String)": "/**\n* Retrieves and validates storage types from a comma-separated string.\n* @param types storage types as a comma-separated string\n* @return List of StorageType objects or types supporting quotas if input is empty or 'all'\n*/",
        "org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options for quota and snapshot settings.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Count command with specified arguments.\n* @param cmd command arguments array\n* @param pos starting position in the array\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.fs.shell.Count:<init>()": "/**\n* Constructs a new Count instance.\n*/",
        "org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes path data and displays quota usage or content summary.\n* @param src PathData containing source path information\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException": {
        "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException:<init>(int,int)": "/**\n* Constructs an exception for incorrect number of arguments.\n* @param want expected number of arguments\n* @param got actual number of arguments received\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException:getMessage()": "/**\n* Constructs an error message comparing expected and actual values.\n* @return formatted string with expected and actual values\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute": {
        "org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute:getAttribute(char)": "/**\n* Retrieves a FileAttribute by its initial character.\n* @param symbol character representing the attribute\n* @return corresponding FileAttribute or throws NoSuchElementException if not found\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommands$Merge": {
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:writeDelimiter(org.apache.hadoop.fs.FSDataOutputStream)": "/**\n* Writes the delimiter to the output stream if not null.\n* @param out output stream to write the delimiter\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Handles nonexistent paths by setting an error flag and invoking superclass method.\n* @param item PathData representing the nonexistent path\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData)": "",
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList)": "/**\n* Processes file paths, copying non-empty files to output stream with delimiters.\n* @param items list of PathData items to process\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and initializes path data.\n* @param args command-line arguments\n* @throws IOException if URI syntax is invalid or path is a directory\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandUtils": {
        "org.apache.hadoop.fs.shell.CommandUtils:formatDescription(java.lang.String,java.lang.String[])": "/**\n* Formats a description with usage and additional lines.\n* @param usage main usage string\n* @param descriptions additional description lines\n* @return formatted description string\n*/"
    },
    "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot": {
        "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options, validating argument count and extracting names.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList)": "/**\n* Processes path arguments and renames a snapshot if no errors occurred.\n* @param items list of PathData containing snapshot information\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Validates if the given path is a directory; throws exception if not.\n* @param item PathData object to validate\n* @throws IOException if the path is not a directory\n*/"
    },
    "org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal": {
        "org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processOptions(java.util.LinkedList)": "/**\n* Throws IOException for unsupported option '-moveToLocal'.\n* @param args list of command-line arguments\n*/"
    },
    "org.apache.hadoop.fs.shell.Command": {
        "org.apache.hadoop.fs.shell.Command:run(org.apache.hadoop.fs.shell.PathData)": "/**\n* Executes a process using the specified path data.\n* @param pathData contains the path to be processed\n* @throws IOException if an I/O error occurs during execution\n*/",
        "org.apache.hadoop.fs.shell.Command:getReplacementCommand()": "/**\n* Returns a replacement command as a String.\n* @return null, indicating no command is available\n*/",
        "org.apache.hadoop.fs.shell.Command:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from the provided arguments list.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:exitCodeForError()": "/**\n* Returns the exit code for an error condition.\n* @return integer exit code, typically 1 for errors\n*/",
        "org.apache.hadoop.fs.shell.Command:getListingGroupSize()": "/**\n* Returns the size of the listing group.\n* @return integer representing the group size, always 0 in this implementation\n*/",
        "org.apache.hadoop.fs.shell.Command:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes the given PathData item.\n* @param item the PathData to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Command:postProcessPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes the given PathData item after initial handling.\n* @param item the PathData object to be processed\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:isSorted()": "/**\n* Checks if the collection is sorted.\n* @return true if sorted, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Command:getCommandField(java.lang.String)": "/**\n* Retrieves the value of a specified field using reflection.\n* @param field name of the field to retrieve\n* @return String representation of the field's value\n*/",
        "org.apache.hadoop.fs.shell.Command:setName(java.lang.String)": "/**\n* Sets the name of the object.\n* @param name the name to be assigned\n*/",
        "org.apache.hadoop.fs.shell.Command:setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Sets the command factory for creating commands.\n* @param factory CommandFactory instance to be set\n*/",
        "org.apache.hadoop.fs.shell.Command:getCommandFactory()": "/**\n* Retrieves the CommandFactory instance.\n* @return CommandFactory object\n*/",
        "org.apache.hadoop.fs.shell.Command:getDepth()": "/**\n* Retrieves the current depth value.\n* @return current depth as an integer\n*/",
        "org.apache.hadoop.fs.shell.Command:isRecursive()": "/**\n* Checks if the operation is recursive.\n* @return true if recursive, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Command:setRecursive(boolean)": "/**\n* Sets the recursive flag to enable or disable recursion.\n* @param flag true to enable recursion, false to disable\n*/",
        "org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData)": "/**\n* Checks if the given path is recursively accessible.\n* @param item PathData object containing path information\n* @return true if the path is a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Command:isDeprecated()": "/**\n* Checks if the current object is deprecated.\n* @return true if a replacement command exists, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Command:getName()": "/**\n* Retrieves the name, processing it if null or starts with '-'.\n* @return processed name as a String\n*/",
        "org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item and handles recursion if applicable.\n* @param item PathData to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Command:getDescription()": "/**\n* Returns a description, indicating if deprecated or providing command details.\n* @return String description of the command or deprecation notice\n*/",
        "org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String)": "/**\n* Displays a warning message to the error output.\n* @param message the warning message to display\n*/",
        "org.apache.hadoop.fs.shell.Command:getUsage()": "/**\n* Constructs command usage string based on deprecation status.\n* @return formatted usage string or command name if usage is empty\n*/",
        "org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Command object with the specified configuration.\n* @param conf the Configuration to initialize the Command\n*/",
        "org.apache.hadoop.fs.shell.Command:displayError(java.lang.String)": "/**\n* Increments error count and displays a warning message.\n* @param message the error message to display\n*/",
        "org.apache.hadoop.fs.shell.Command:<init>()": "/**\n* Initializes Command with standard output and error streams.\n*/",
        "org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception)": "/**\n* Handles and logs exceptions, throwing an interrupt for InterruptedIOException.\n* @param e the Exception to process\n*/",
        "org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])": "/**\n* Processes multiple PathData items, handling IOExceptions with error display.\n* @param parent the parent PathData object\n* @param items variable number of PathData items to process\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Throws PathNotFoundException for a nonexistent PathData item.\n* @param item the PathData object that does not exist\n*/",
        "org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a single PathData item, starting with no parent directory.\n* @param item the PathData item to process\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)": "/**\n* Processes PathData items from an iterator, grouping by size if needed.\n* @param parent the parent PathData object\n* @param itemsIterator iterator of PathData items\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item based on its existence.\n* @param item the PathData item to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList)": "/**\n* Processes a list of PathData items, handling IOExceptions during processing.\n* @param args list of PathData items to process\n* @throws IOException if an error occurs during argument processing\n*/",
        "org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Recursively processes directory items, handling sorting and missing directories.\n* @param item PathData object representing the directory to process\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:runAll()": "/**\n* Executes tasks on all source paths provided as arguments.\n* @return exit code indicating success (0) or failure (-1)\n*/",
        "org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String)": "/**\n* Expands a glob pattern argument into a list of PathData objects.\n* @param arg glob pattern to expand\n* @return List of PathData objects\n* @throws IOException if filesystem operations fail or no matches found\n*/",
        "org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList)": "/**\n* Expands a list of string arguments into PathData objects.\n* @param args list of string arguments to expand\n* @return LinkedList of PathData objects\n* @throws IOException if expansion fails\n*/",
        "org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList)": "/**\n* Processes raw string arguments by expanding and handling them.\n* @param args list of string arguments to process\n* @throws IOException if an error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Command:run(java.lang.String[])": "/**\n* Executes commands with provided arguments, handling deprecation and errors.\n* @param argv command-line arguments\n* @return exit code based on success or error conditions\n*/"
    },
    "org.apache.hadoop.fs.permission.AclStatus": {
        "org.apache.hadoop.fs.permission.AclStatus:getEntries()": "/**\n* Retrieves the list of access control entries.\n* @return List of AclEntry objects\n*/",
        "org.apache.hadoop.fs.permission.AclStatus:equals(java.lang.Object)": "/**\n* Compares this AclStatus instance with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.permission.AclStatus:hashCode()": "/**\n* Computes the hash code for the object based on its fields.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.permission.AclStatus:toString()": "/**\n* Returns a string representation of the object with owner, group, and ACL details.\n* @return formatted string of object properties\n*/",
        "org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Calculates effective permissions based on ACL entry and provided permissions.\n* @param entry ACL entry to evaluate\n* @param permArg fallback permissions if entry lacks them\n* @return FsAction representing effective permissions\n*/",
        "org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry)": "/**\n* Retrieves effective permissions for a given ACL entry.\n* @param entry ACL entry to evaluate\n* @return FsAction representing effective permissions\n*/",
        "org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Constructs an AclStatus object with owner, group, sticky bit, entries, and permissions.\n* @param owner the owner of the ACL\n* @param group the group associated with the ACL\n* @param stickyBit indicates if the sticky bit is set\n* @param entries ACL entries to be included\n* @param permission file system permissions\n*/"
    },
    "org.apache.hadoop.fs.permission.ScopedAclEntries": {
        "org.apache.hadoop.fs.permission.ScopedAclEntries:getAccessEntries()": "/**\n* Retrieves the list of access control entries.\n* @return List of AclEntry objects representing access permissions\n*/",
        "org.apache.hadoop.fs.permission.ScopedAclEntries:getDefaultEntries()": "/**\n* Retrieves the default access control list entries.\n* @return List of AclEntry objects\n*/",
        "org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List)": "/**\n* Finds the index of the first DEFAULT AclEntry.\n* @param aclBuilder list of AclEntry objects\n* @return index of DEFAULT entry or PIVOT_NOT_FOUND if none found\n*/",
        "org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List)": "/**\n* Initializes ScopedAclEntries from a list of AclEntry objects.\n* @param aclEntries list of AclEntry objects\n*/"
    },
    "org.apache.hadoop.fs.permission.AclUtil": {
        "org.apache.hadoop.fs.permission.AclUtil:isMinimalAcl(java.util.List)": "/**\n* Checks if the ACL entries list contains exactly three entries.\n* @param entries list of ACL entries\n* @return true if list size is three, false otherwise\n*/",
        "org.apache.hadoop.fs.permission.AclUtil:<init>()": "/**\n* Private constructor to prevent instantiation of AclUtil class.\n*/",
        "org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)": "/**\n* Constructs ACL entries from permissions and existing entries.\n* @param perm FsPermission containing user, group, and other actions\n* @param entries existing AclEntry list\n* @return List of constructed AclEntry objects\n*/",
        "org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Constructs minimal ACL entries based on user, group, and other permissions.\n* @param perm permissions for user, group, and others\n* @return list of AclEntry objects\n*/"
    },
    "org.apache.hadoop.fs.permission.AclEntry": {
        "org.apache.hadoop.fs.permission.AclEntry:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:getType()": "/**\n* Retrieves the ACL entry type.\n* @return AclEntryType representing the entry type\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:getPermission()": "/**\n* Retrieves the current file system permissions.\n* @return FsAction representing the permissions\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:getScope()": "/**\n* Retrieves the ACL entry scope.\n* @return AclEntryScope representing the current scope\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:equals(java.lang.Object)": "/**\n* Compares this AclEntry to another object for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:hashCode()": "/**\n* Computes the hash code for the object based on its fields.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:<init>(org.apache.hadoop.fs.permission.AclEntryType,java.lang.String,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.AclEntryScope)": "/**\n* Constructs an AclEntry with specified type, name, permission, and scope.\n* @param type the type of the ACL entry\n* @param name the name associated with the entry\n* @param permission the permission level granted\n* @param scope the scope of the ACL entry\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)": "/**\n* Parses ACL entry from string; validates and builds AclEntry object.\n* @param aclStr ACL specification string\n* @param includePermission flag to include permission in parsing\n* @return AclEntry instance or throws exception if invalid\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:toStringStable()": "/**\n* Constructs a stable string representation of the object.\n* @return formatted string with scope, type, name, and permission\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)": "/**\n* Parses ACL specification string into a list of AclEntry objects.\n* @param aclSpec ACL specification string\n* @param includePermission flag to include permission in parsing\n* @return List of AclEntry objects\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:toString()": "/**\n* Returns a string representation of the object, subject to change across versions.\n* @return formatted string representation\n*/",
        "org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List)": "/**\n* Converts a list of AclEntry to a comma-separated string.\n* @param aclSpec list of AclEntry objects\n* @return formatted string representation of AclEntry entries\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandWithDestination": {
        "org.apache.hadoop.fs.shell.CommandWithDestination:preserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute)": "/**\n* Adds fileAttribute to preserveStatus if not already present.\n* @param fileAttribute the file attribute to preserve\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:shouldPreserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute)": "/**\n* Checks if a file attribute should be preserved.\n* @param attribute the file attribute to check\n* @return true if the attribute is to be preserved, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:setDirectWrite(boolean)": "/**\n* Sets the direct write flag.\n* @param flag true to enable direct write, false to disable\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:setLazyPersist(boolean)": "/**\n* Sets the lazy persist flag.\n* @param flag true to enable lazy persist, false to disable\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:setOverwrite(boolean)": "/**\n* Sets the overwrite flag to determine if existing data should be replaced.\n* @param flag true to enable overwrite, false to disable it\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:setVerifyChecksum(boolean)": "/**\n* Sets the verifyChecksum flag.\n* @param flag true to enable checksum verification, false to disable it\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:setWriteChecksum(boolean)": "/**\n* Sets the write checksum flag.\n* @param flag true to enable, false to disable write checksum\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean)": "/**\n* Sets file attribute preservation status based on the preserve flag.\n* @param preserve true to preserve attributes, false to clear status\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)": "/**\n* Preserves file attributes from source to target.\n* @param src source path data, @param target target path data, @param preserveRawXAttrs flag\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Checks if source and target paths are both reserved raw paths.\n* @param src source Path object\n* @param target target Path object\n* @return true if both are reserved raw paths, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList)": "/**\n* Validates destination path based on argument count and state.\n* @param args list of PathData arguments\n* @throws IOException if path conditions are not met\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)": "/**** Copies an input stream to a target path, handling overwrites and temporary files. \n* @param in InputStream to copy from \n* @param target PathData for the output file \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Retrieves target PathData based on source PathData and directory checks.\n* @param src source PathData to derive target from\n* @return PathData for the target location\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**** Copies a file from source to target, preserving attributes if required. \n* @param src source PathData object \n* @param target target PathData object \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData object and checks for directory-related errors.\n* @param src source PathData to validate\n* @throws IOException if paths are identical or if src is a subdirectory of itself\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Recursively processes a source path, creating directories and preserving attributes.\n* @param src source PathData to process\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes source PathData and copies to destination if valid.\n* @param src source PathData object\n* @param dst target PathData object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes source PathData by copying to the target location.\n* @param src source PathData object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList)": "/**\n* Sets local destination path from arguments.\n* @param args list of arguments, last is the path or current directory if fewer than 2\n* @throws IOException if path initialization fails\n*/",
        "org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList)": "/**\n* Determines remote destination from arguments.\n* @param args list of arguments; last is the path to resolve\n* @throws IOException if path resolution fails\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader": {
        "org.apache.hadoop.io.SequenceFile$Reader:file(org.apache.hadoop.fs.Path)": "/**\n* Creates a FileOption from the given file path.\n* @param value the path to the file\n* @return an Option representing the file\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getCompressionType()": "/**\n* Determines the compression type based on decompression status.\n* @return CompressionType indicating the type of compression used\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getCompressionCodec()": "/**\n* Retrieves the current compression codec.\n* @return CompressionCodec instance being used\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:isBlockCompressed()": "/**\n* Checks if the data is block compressed.\n* @return true if block compressed, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:isCompressed()": "/**\n* Checks if the data is compressed.\n* @return true if compressed, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:createValueBytes()": "/**\n* Creates ValueBytes instance based on compression settings.\n* @return ValueBytes object, either compressed or uncompressed\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:stream(org.apache.hadoop.fs.FSDataInputStream)": "/**\n* Creates an InputStreamOption from the provided FSDataInputStream.\n* @param value the FSDataInputStream to wrap\n* @return an Option containing the InputStreamOption\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:start(long)": "/**\n* Creates a StartOption instance with the specified value.\n* @param value the value for the StartOption\n* @return a new StartOption object\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:length(long)": "/**\n* Creates a LengthOption instance from a long value.\n* @param value the length value to be encapsulated\n* @return an Option representing the length\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:bufferSize(int)": "/**\n* Creates a BufferSizeOption with the specified size.\n* @param value the size to set for the buffer\n* @return an Option representing the buffer size\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getKeyClassName()": "/**\n* Retrieves the name of the key class.\n* @return key class name as a String\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getValueClassName()": "/**\n* Retrieves the class name of the value.\n* @return class name as a String\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:deserializeValue(java.lang.Object)": "/**\n* Deserializes the given value using a value deserializer.\n* @param val the value to deserialize\n* @return the deserialized object\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:deserializeKey(java.lang.Object)": "/**\n* Deserializes the given key using the keyDeserializer.\n* @param key the key object to deserialize\n* @return the deserialized key object\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getVersion()": "/**\n* Retrieves the current version.\n* @return byte representing the version\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getSync()": "/**\n* Retrieves the sync byte array.\n* @return byte array representing the sync data\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:ignoreSync()": "/**\n* Resets the synchronization object to null.\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getMetadata()": "/**\n* Retrieves the metadata associated with the object.\n* @return Metadata object containing relevant information\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:seek(long)": "/**\n* Seeks to a specified position and triggers block read if compressed.\n* @param position the desired position in the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:readRecordLength()": "/**** Reads the record length from the input stream.\\n* @return record length or -1 if end is reached.\\n* @throws IOException if an I/O error occurs.*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getPosition()": "/**\n* Retrieves the current position of the input stream.\n* @return current position as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:sync(long)": "/**\n* Synchronizes the input stream at a specified position.\n* @param position the position to sync in the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)": "/**\n* Retrieves a Deserializer for the specified class from the SerializationFactory.\n* @param sf SerializationFactory to obtain the Deserializer\n* @param c class type for which the Deserializer is needed\n* @return Deserializer instance or null if not found\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:close()": "/**\n* Closes resources and returns decompressors to the pool.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)": "/**\n* Reads data into a buffer from an input stream.\n* @param buffer destination for read data\n* @param filter compression stream to reset state\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getKeyClass()": "/**\n* Retrieves the key class, loading it if not already cached.\n* @return Class<?> representing the key class\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getValueClass()": "/**\n* Retrieves the value class, loading it if not already cached.\n* @return Class<?> of the value or throws RuntimeException on failure\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:readBlock()": "/**\n* Reads a block of data, handling lazy decompression and verifying integrity.\n* @throws IOException if an I/O error occurs or file is corrupt\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue()": "/**\n* Seeks to the current value in a compressed or uncompressed data stream.\n* @throws IOException if an I/O error occurs during seeking\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer)": "/**\n* Reads the next raw key into the buffer.\n* @param key buffer for the raw key data\n* @return length of the key or -1 if end is reached\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable)": "/**\n* Retrieves the current value into the Writable object.\n* @param val Writable object to store the current value\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object)": "/**\n* Retrieves the current value, deserializing it if necessary.\n* @param val the value to be configured and returned\n* @return the deserialized current value\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)": "/**\n* Reads the next raw key-value pair from the input stream.\n* @param key buffer for the key data\n* @param val buffer for the value data\n* @return total length of the key-value pair or -1 if end is reached\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes)": "/**\n* Retrieves the next raw value from the stream.\n* @param val ValueBytes to store the retrieved data\n* @return Length of the retrieved value\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException)": "/**** Handles ChecksumException by skipping or throwing based on configuration. \n* @param e the ChecksumException to handle\n* @throws IOException if an I/O error occurs during sync\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:init(boolean)": "/**\n* Initializes sequence file reading and sets up deserialization parameters.\n* @param tempReader indicates if this is a temporary reader\n* @throws IOException if an I/O error occurs or version mismatch\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer)": "/**\n* Reads the next record into buffer; throws IOException for block-compressed files.\n* @param buffer output buffer for the record data\n* @return key length or -1 if end of file is reached\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Initializes file reading parameters and seeks to the specified position.\n* @param filename path to the file\n* @param in input stream for reading data\n* @param start starting position in the stream\n* @param length number of bytes to read\n* @param conf configuration settings\n* @param tempReader indicates if this is a temporary reader\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable)": "/**\n* Reads the next key into the provided Writable object.\n* @param key Writable object to populate with the next key\n* @return true if a key was read, false if end of data is reached\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object)": "/**\n* Retrieves the next key object, verifying its class and handling compression if needed.\n* @param key expected key object; must match the key class\n* @return the next key object or null if end of data is reached\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)": "/**\n* Advances to the next entry, populating key and value.\n* @param key Writable for the next key; @param val Writable for the current value\n* @return true if more entries exist, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)": "/**\n* Opens a file with specified options and returns an FSDataInputStream.\n* @param fs file system to open the file from\n* @param file path of the file to open\n* @param bufferSize size of the buffer for reading\n* @param length length of the file to read, or -1 for no limit\n* @return FSDataInputStream for the opened file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])": "/**\n* Constructs a Reader with specified options and configuration.\n* @param conf configuration settings\n* @param opts variable options for file or stream input\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a deprecated Reader from a FileSystem and Path.\n* @param fs the FileSystem to use\n* @param file the Path to the file\n* @param conf configuration settings\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Reader with input stream and specified parameters.\n* @param in input stream for reading data\n* @param buffersize size of the buffer\n* @param start starting position for reading\n* @param length number of bytes to read\n* @param conf configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.io.DataOutputBuffer": {
        "org.apache.hadoop.io.DataOutputBuffer:reset()": "/**\n* Resets the buffer and written count to zero.\n* @return this DataOutputBuffer instance\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:<init>(org.apache.hadoop.io.DataOutputBuffer$Buffer)": "/**\n* Initializes DataOutputBuffer with the specified Buffer instance.\n* @param buffer the Buffer to wrap and manage\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:writeTo(java.io.OutputStream)": "/**\n* Writes buffered data to the specified output stream.\n* @param out the output stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)": "/**\n* Writes data from input stream to buffer.\n* @param in input data stream\n* @param length number of bytes to read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:<init>()": "/**** Initializes a new DataOutputBuffer with a default Buffer instance. */",
        "org.apache.hadoop.io.DataOutputBuffer:<init>(int)": "/**\n* Constructs DataOutputBuffer with specified size.\n* @param size the size of the buffer to initialize\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:getData()": "/**\n* Retrieves byte array data from the buffer.\n* @return byte array containing the buffer data\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:getLength()": "/**\n* Retrieves the length of the buffer.\n* @return current length as an integer\n*/",
        "org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)": "/**\n* Writes an integer to a buffer at a specified offset.\n* @param v integer value to write\n* @param offset position in buffer to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.Lists": {
        "org.apache.hadoop.util.Lists:newArrayList()": "/**\n* Creates and returns a new empty ArrayList.\n* @return a new instance of ArrayList of type E\n*/",
        "org.apache.hadoop.util.Lists:<init>()": "/**\n* Private constructor for Lists class to prevent instantiation.\n*/",
        "org.apache.hadoop.util.Lists:cast(java.lang.Iterable)": "/**\n* Casts an Iterable to a Collection.\n* @param iterable the Iterable to be cast\n* @return the casted Collection\n*/",
        "org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.util.Iterator)": "/**\n* Adds all elements from an iterator to a collection.\n* @param addTo collection to add elements to\n* @param iterator source of elements to add\n* @return true if the collection was modified\n*/",
        "org.apache.hadoop.util.Lists:checkNonnegative(int,java.lang.String)": "/**\n* Validates that the value is non-negative.\n* @param value the integer to check\n* @param name the name of the value for error messaging\n* @return the original value if non-negative\n*/",
        "org.apache.hadoop.util.Lists:newLinkedList()": "/**\n* Creates and returns a new empty LinkedList.\n* @return a new instance of LinkedList\n*/",
        "org.apache.hadoop.util.Lists:saturatedCast(long)": "/**\n* Casts a long value to int with saturation.\n* @param value the long value to cast\n* @return saturated int value within Integer bounds\n*/",
        "org.apache.hadoop.util.Lists:partition(java.util.List,int)": "/**\n* Partitions a list into smaller sublists of specified size.\n* @param originalList the list to partition\n* @param pageSize size of each sublist\n* @return list of sublists\n*/",
        "org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator)": "/**\n* Creates a new ArrayList from an iterator of elements.\n* @param elements iterator providing elements to add\n* @return ArrayList containing the added elements\n*/",
        "org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)": "/**\n* Adds elements from an Iterable to a collection.\n* @param addTo collection to add elements to\n* @param elementsToAdd source of elements to add\n* @return true if the collection was modified\n*/",
        "org.apache.hadoop.util.Lists:newArrayListWithCapacity(int)": "/**\n* Creates a new ArrayList with specified initial capacity.\n* @param initialArraySize the initial size of the ArrayList\n* @return a new ArrayList instance\n*/",
        "org.apache.hadoop.util.Lists:computeArrayListCapacity(int)": "/**\n* Computes the capacity for an ArrayList based on its size.\n* @param arraySize current size of the array\n* @return calculated capacity as an int\n*/",
        "org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable)": "/**\n* Creates an ArrayList from an Iterable of elements.\n* @param elements the Iterable providing elements to add\n* @return ArrayList containing the added elements\n*/",
        "org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable)": "/**\n* Creates a LinkedList from an Iterable of elements.\n* @param elements source of elements to add\n* @return a new LinkedList containing the elements\n*/",
        "org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[])": "/**** Creates a new ArrayList from provided elements. \n* @param elements varargs of elements to add \n* @return populated ArrayList \n*/",
        "org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int)": "/**\n* Creates a new ArrayList with an estimated capacity.\n* @param estimatedSize expected size for the ArrayList\n* @return a new ArrayList with calculated capacity\n*/"
    },
    "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot": {
        "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processOptions(java.util.LinkedList)": "/**\n* Validates and processes command-line arguments for snapshot options.\n* @param args list of arguments, must contain 1 or 2 elements\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList)": "/**\n* Processes path arguments and creates a snapshot if no errors occurred.\n* @param items list of PathData to process\n* @throws IOException if snapshot creation fails\n*/",
        "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Validates if the given path is a directory; throws exception if not.\n* @param item PathData object containing path information\n* @throws PathIsNotDirectoryException if the path is not a directory\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandFormat$DuplicatedOptionException": {
        "org.apache.hadoop.fs.shell.CommandFormat$DuplicatedOptionException:<init>(java.lang.String)": "/**\n* Constructs an exception for a duplicated option.\n* @param duplicatedOption the option that is duplicated\n*/"
    },
    "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot": {
        "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and sets snapshot name.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList)": "/**\n* Processes path arguments and deletes the specified snapshot.\n* @param items list of PathData containing paths to process\n* @throws IOException if snapshot deletion fails\n*/",
        "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Validates if the given path is a directory; throws exception if not.\n* @param item PathData object containing path information\n* @throws IOException if the path is not a directory\n*/"
    },
    "org.apache.hadoop.fs.FsShellPermissions$Chown": {
        "org.apache.hadoop.fs.FsShellPermissions$Chown:parseOwnerGroup(java.lang.String)": "/**\n* Parses and validates owner and group from a string.\n* @param ownerStr formatted as [owner][:group]\n*/",
        "org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Updates the owner and group of a PathData item if specified.\n* @param item PathData containing path and its metadata\n* @throws IOException if an error occurs during ownership change\n*/",
        "org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and sets recursive flag.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.CompositeCrcFileChecksum": {
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:<init>(int,org.apache.hadoop.util.DataChecksum$Type,int)": "/**\n* Constructs a CompositeCrcFileChecksum with specified CRC values and type.\n* @param crc checksum value\n* @param crcType type of checksum\n* @param bytesPerCrc number of bytes per checksum\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getAlgorithmName()": "/**\n* Returns the algorithm name based on the CRC type.\n* @return concatenated string of \"COMPOSITE-\" and crcType name\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:readFields(java.io.DataInput)": "/**\n* Reads integer value from input stream and assigns it to crc.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:write(java.io.DataOutput)": "/**\n* Writes the CRC value to the specified DataOutput stream.\n* @param out output stream to write the CRC value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getLength()": "/**\n* Returns the predefined length value.\n* @return the length as an integer\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt()": "/**\n* Creates and returns ChecksumOpt with specified CRC type and size.\n* @return ChecksumOpt object initialized with crcType and bytesPerCrc\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:toString()": "/**\n* Returns string representation of the object with algorithm name and CRC value.\n* @return formatted string of algorithm name and CRC in hex\n*/",
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes()": "/**\n* Converts CRC value to a 4-byte array.\n* @return byte array representing the CRC value\n*/"
    },
    "org.apache.hadoop.util.Shell": {
        "org.apache.hadoop.util.Shell:<init>(long,boolean)": "/**\n* Initializes a Shell with specified interval and error stream redirection.\n* @param interval time interval for shell execution\n* @param redirectErrorStream flag to redirect error output\n*/",
        "org.apache.hadoop.util.Shell:getUsersForNetgroupCommand(java.lang.String)": "/**\n* Constructs a command array to fetch users for a specified netgroup.\n* @param netgroup the name of the netgroup\n* @return an array of command strings for execution\n*/",
        "org.apache.hadoop.util.Shell:bashQuote(java.lang.String)": "/**\n* Quotes a string for safe use in bash commands.\n* @param arg the string to be quoted\n* @return the quoted string\n*/",
        "org.apache.hadoop.util.Shell:isTimedOut()": "/**\n* Checks if the operation has timed out.\n* @return true if timed out, false otherwise\n*/",
        "org.apache.hadoop.util.Shell:getOSType()": "/**\n* Determines the operating system type.\n* @return OSType representing the detected OS\n*/",
        "org.apache.hadoop.util.Shell:getWinUtilsPath()": "/**\n* Retrieves the path for WinUtils or throws an exception if unavailable.\n* @return WinUtils path as a String\n*/",
        "org.apache.hadoop.util.Shell:appendScriptExtension(java.lang.String)": "/**\n* Appends appropriate script extension based on OS.\n* @param basename base name of the script\n* @return script name with '.cmd' or '.sh' extension\n*/",
        "org.apache.hadoop.util.Shell:checkHadoopHomeInner(java.lang.String)": "/**\n* Validates and returns the Hadoop home directory.\n* @param home path to the Hadoop home directory\n* @return File object representing the home directory\n* @throws FileNotFoundException if home is invalid or not a directory\n*/",
        "org.apache.hadoop.util.Shell:addOsText(java.lang.String)": "/**\n* Appends OS-specific text to the message if on Windows.\n* @param message the original message\n* @return modified message with OS text or original if not Windows\n*/",
        "org.apache.hadoop.util.Shell:fileNotFoundException(java.lang.String,java.lang.Exception)": "/**\n* Creates a FileNotFoundException with a cause.\n* @param text error message for the exception\n* @param ex underlying exception that caused this exception\n* @return initialized FileNotFoundException\n*/",
        "org.apache.hadoop.util.Shell:setEnvironment(java.util.Map)": "/**\n* Sets the environment map, ensuring it's not null.\n* @param env a map of environment variables\n*/",
        "org.apache.hadoop.util.Shell:joinThread(java.lang.Thread)": "/**\n* Waits for the specified thread to finish execution.\n* @param t the Thread to join\n*/",
        "org.apache.hadoop.util.Shell:getEnvironment(java.lang.String)": "/**\n* Retrieves the value of a specified environment variable.\n* @param env the name of the environment variable\n* @return the value associated with the environment variable or null if not found\n*/",
        "org.apache.hadoop.util.Shell:setTimedOut()": "/**\n* Sets the timedOut flag to true.\n*/",
        "org.apache.hadoop.util.Shell:getProcess()": "/**\n* Retrieves the current Process instance.\n* @return the Process object associated with this instance\n*/",
        "org.apache.hadoop.util.Shell:getAllShells()": "/**\n* Retrieves a set of all shells in a thread-safe manner.\n* @return Set of Shell objects from CHILD_SHELLS\n*/",
        "org.apache.hadoop.util.Shell:getMemlockLimit(java.lang.Long)": "/**\n* Returns the memory lock limit based on the operating system.\n* @param ulimit the requested memory lock limit\n* @return capped memory lock limit for Windows or original ulimit\n*/",
        "org.apache.hadoop.util.Shell:isJavaVersionAtLeast(int)": "/**\n* Checks if the current Java version is at least the specified version.\n* @param version minimum required Java version\n* @return true if current version is >= specified version, false otherwise\n*/",
        "org.apache.hadoop.util.Shell:getExitCode()": "/**\n* Retrieves the exit code of the process.\n* @return the exit code as an integer\n*/",
        "org.apache.hadoop.util.Shell:getWaitingThread()": "/**\n* Retrieves the thread currently waiting.\n* @return the waiting Thread object\n*/",
        "org.apache.hadoop.util.Shell:setWorkingDirectory(java.io.File)": "/**\n* Sets the working directory.\n* @param dir the directory to set as the working directory\n*/",
        "org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[])": "/**\n* Validates command line length; throws IOException if exceeded.\n* @param commands variable-length command strings\n*/",
        "org.apache.hadoop.util.Shell:<init>(long)": "/**\n* Constructs a Shell with a specified execution interval.\n* @param interval time interval for shell execution\n*/",
        "org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File)": "/**\n* Constructs command to run a script based on the OS.\n* @param script the script file to execute\n* @return command array for execution in Windows or bash\n*/",
        "org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String)": "/**\n* Constructs command to fetch user groups based on OS.\n* @param user the username to query groups for\n* @return command array for executing in the shell\n*/",
        "org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String)": "/**\n* Constructs command to retrieve user group IDs.\n* @param user the username to get group IDs for\n* @return command array for executing in shell\n*/",
        "org.apache.hadoop.util.Shell:getGetPermissionCommand()": "/**\n* Constructs a command array for permission listing based on OS type.\n* @return String array with command elements for Windows or Unix\n*/",
        "org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)": "/**\n* Generates chmod command for setting permissions.\n* @param perm permission string, @param recursive applies recursively if true\n* @return command as a String array\n*/",
        "org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String)": "/**\n* Generates command to set file owner based on OS type.\n* @param owner the name of the new owner\n* @return command array for setting owner\n*/",
        "org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)": "/**\n* Generates symlink command based on OS type.\n* @param target the target file or directory\n* @param link the symbolic link to create\n* @return command array for creating a symlink\n*/",
        "org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String)": "/**\n* Constructs a command array for reading a symbolic link.\n* @param link the symbolic link to read\n* @return command array for Windows or Unix systems\n*/",
        "org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)": "/**\n* Generates a command to signal a process based on the code and PID.\n* @param code signal code (0 for check alive)\n* @param pid process ID to target\n* @return command as a String array\n*/",
        "org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)": "/**\n* Creates a File with appropriate script extension.\n* @param parent directory for the script file\n* @param basename base name of the script\n* @return File object with the correct script extension\n*/",
        "org.apache.hadoop.util.Shell:checkHadoopHome()": "/**\n* Checks Hadoop home directory from system properties or environment variables.\n* @return File object representing the Hadoop home directory\n* @throws FileNotFoundException if directory is invalid\n*/",
        "org.apache.hadoop.util.Shell:getHadoopHomeDir()": "/**\n* Retrieves Hadoop home directory or throws an exception if unavailable.\n* @return File representing the Hadoop home directory\n* @throws FileNotFoundException if the directory is not found\n*/",
        "org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)": "/***************\n* Validates and retrieves the canonical file of an executable in the Hadoop bin directory.\n* @param hadoopHomeDir the Hadoop home directory\n* @param executable the name of the executable file\n* @return canonical File object of the executable\n* @throws FileNotFoundException if the bin or executable file is missing or invalid\n***************/",
        "org.apache.hadoop.util.Shell:getWinUtilsFile()": "/**\n* Retrieves the WINUTILS file or throws an exception if unavailable.\n* @return File object for WINUTILS\n* @throws FileNotFoundException if the file cannot be found\n*/",
        "org.apache.hadoop.util.Shell:destroyAllShellProcesses()": "/**\n* Destroys all shell processes and clears the CHILD_SHELLS map.\n*/",
        "org.apache.hadoop.util.Shell:<init>()": "/**\n* Default constructor for Shell, initializes with zero interval.\n*/",
        "org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)": "/**\n* Constructs chmod command with file for setting permissions.\n* @param perm permission string, @param recursive applies recursively, @param file target file\n* @return command as a String array\n*/",
        "org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String)": "/**\n* Generates command to check if a process is alive.\n* @param pid process ID to check\n* @return command as a String array\n*/",
        "org.apache.hadoop.util.Shell:getHadoopHome()": "/**\n* Retrieves the canonical path of the Hadoop home directory.\n* @return String representing the Hadoop home directory path\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String)": "/**\n* Retrieves the canonical file for a specified Hadoop executable.\n* @param executable name of the executable file\n* @return canonical File object of the executable\n* @throws FileNotFoundException if the executable is missing or invalid\n*/",
        "org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String)": "/**\n* Retrieves the canonical path of a specified Hadoop executable.\n* @param executable name of the executable file\n* @return canonical path as a String\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.Shell:runCommand()": "/**\n* Executes a shell command and handles its input/output streams.\n* @throws IOException if an I/O error occurs during execution\n*/",
        "org.apache.hadoop.util.Shell:run()": "/**\n* Executes a command if the interval has passed since the last run.\n* @throws IOException if an I/O error occurs during command execution\n*/",
        "org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)": "/**\n* Executes a shell command with specified environment and timeout.\n* @param env environment variables, @param cmd command to execute, @param timeout execution timeout\n* @return output of the command as a string\n*/",
        "org.apache.hadoop.util.Shell:execCommand(java.lang.String[])": "/**\n* Executes a shell command with no environment variables or timeout.\n* @param cmd commands to execute\n* @return output of the command as a string\n*/",
        "org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])": "/**\n* Executes a shell command with given environment variables.\n* @param env environment variables, @param cmd command to execute\n* @return output of the command as a string\n*/",
        "org.apache.hadoop.util.Shell:checkIsBashSupported()": "/**\n* Checks if Bash is supported on the current OS.\n* @return true if supported, false otherwise; throws InterruptedIOException on interruption\n*/",
        "org.apache.hadoop.util.Shell:isSetsidSupported()": "/**\n* Checks if setsid command is supported on the current system.\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.Globber$GlobBuilder": {
        "org.apache.hadoop.fs.Globber$GlobBuilder:build()": "/**\n* Constructs a Globber instance based on the file system or file context.\n* @return Globber object configured with provided parameters\n*/",
        "org.apache.hadoop.fs.Globber$GlobBuilder:withPathPattern(org.apache.hadoop.fs.Path)": "/**\n* Sets the path pattern for the GlobBuilder.\n* @param pattern the Path pattern to set\n* @return the updated GlobBuilder instance\n*/",
        "org.apache.hadoop.fs.Globber$GlobBuilder:withPathFiltern(org.apache.hadoop.fs.PathFilter)": "/**\n* Sets the path filter for the GlobBuilder.\n* @param pathFilter the filter to apply to paths\n* @return the updated GlobBuilder instance\n*/",
        "org.apache.hadoop.fs.Globber$GlobBuilder:withResolveSymlinks(boolean)": "/**\n* Sets whether to resolve symbolic links.\n* @param resolve true to resolve symlinks, false otherwise\n* @return this GlobBuilder instance\n*/",
        "org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext)": "/**\n* Constructs a GlobBuilder with a FileContext.\n* @param fc the FileContext to be used, must not be null\n*/",
        "org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes a GlobBuilder with a non-null FileSystem.\n* @param fs the FileSystem to be used, must not be null\n*/"
    },
    "org.apache.hadoop.fs.RawPathHandle": {
        "org.apache.hadoop.fs.RawPathHandle:<init>(java.nio.ByteBuffer)": "/**\n* Initializes RawPathHandle with a read-only ByteBuffer.\n* @param fd input ByteBuffer, allocates empty if null\n*/",
        "org.apache.hadoop.fs.RawPathHandle:<init>(org.apache.hadoop.fs.PathHandle)": "/**\n* Constructs RawPathHandle from a given PathHandle.\n* @param handle PathHandle to extract bytes from, or null for empty buffer\n*/",
        "org.apache.hadoop.fs.RawPathHandle:bytes()": "/**\n* Returns a read-only ByteBuffer from the file descriptor.\n* @return read-only ByteBuffer representation\n*/",
        "org.apache.hadoop.fs.RawPathHandle:writeObject(java.io.ObjectOutputStream)": "/**\n* Serializes the object state to the output stream.\n* @param out ObjectOutputStream to write the object data\n* @throws IOException if an I/O error occurs during serialization\n*/",
        "org.apache.hadoop.fs.RawPathHandle:readObject(java.io.ObjectInputStream)": "/**\n* Custom deserialization method for reading object state.\n* @param in input stream for object data\n* @throws IOException if an I/O error occurs or buffer length is illegal\n* @throws ClassNotFoundException if the class of a serialized object cannot be found\n*/",
        "org.apache.hadoop.fs.RawPathHandle:readObjectNoData()": "/**\n* Throws exception if no data is available during object deserialization.\n* @throws ObjectStreamException when stream data is missing\n*/",
        "org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object)": "/**\n* Compares this PathHandle to another for equality.\n* @param other object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.RawPathHandle:hashCode()": "/**\n* Computes the hash code based on the file's byte representation.\n* @return hash code of the file's byte data\n*/",
        "org.apache.hadoop.fs.RawPathHandle:toString()": "/**\n* Returns the string representation of the ByteBuffer from the file descriptor.\n* @return String representation of the ByteBuffer\n*/"
    },
    "org.apache.hadoop.io.Text": {
        "org.apache.hadoop.io.Text:<init>()": "/**\n* Default constructor for the Text class.\n*/",
        "org.apache.hadoop.io.Text:clear()": "/**\n* Resets the length and textLength to initial values.\n*/",
        "org.apache.hadoop.io.Text:copyBytes()": "/**\n* Creates a copy of the byte array up to the specified length.\n* @return a new byte array containing the copied bytes\n*/",
        "org.apache.hadoop.io.Text:bytesToCodePoint(java.nio.ByteBuffer)": "/**\n* Converts UTF-8 bytes in ByteBuffer to a Unicode code point.\n* @param bytes ByteBuffer containing UTF-8 encoded bytes\n* @return Unicode code point or -1 for invalid encoding\n*/",
        "org.apache.hadoop.io.Text:encode(java.lang.String,boolean)": "/**\n* Encodes a string to ByteBuffer, replacing invalid characters if specified.\n* @param string the input string to encode\n* @param replace flag to replace malformed/unmappable characters\n* @return ByteBuffer containing encoded bytes\n*/",
        "org.apache.hadoop.io.Text:getBytes()": "/**\n* Returns the byte array stored in the object.\n* @return byte array representing the object's data\n*/",
        "org.apache.hadoop.io.Text:getLength()": "/**\n* Returns the length of the object.\n* @return the length as an integer\n*/",
        "org.apache.hadoop.io.Text:ensureCapacity(int)": "/**\n* Ensures the backing array has at least the specified capacity.\n* @param capacity minimum required capacity\n* @return true if the array was expanded, false otherwise\n*/",
        "org.apache.hadoop.io.Text:decode(java.nio.ByteBuffer,boolean)": "/**\n* Decodes UTF-8 ByteBuffer to String, optionally replacing errors.\n* @param utf8 ByteBuffer containing UTF-8 encoded data\n* @param replace flag to replace malformed/unmappable characters\n* @return decoded String\n*/",
        "org.apache.hadoop.io.Text:validateUTF8(byte[],int,int)": "/**\n* Validates UTF-8 byte sequence.\n* @param utf8 byte array to validate\n* @param start starting index of the validation\n* @param len length of the byte sequence to validate\n* @throws MalformedInputException if the sequence is invalid\n*/",
        "org.apache.hadoop.io.Text:utf8Length(java.lang.String)": "/**\n* Calculates the UTF-8 byte length of a given string.\n* @param string input string to measure\n* @return int byte length of the UTF-8 encoding\n*/",
        "org.apache.hadoop.io.Text:charAt(int)": "/**\n* Retrieves the Unicode code point at the specified position.\n* @param position index of the character\n* @return code point or -1 if out of bounds\n*/",
        "org.apache.hadoop.io.Text:set(java.lang.String)": "/**\n* Sets byte array and lengths from encoded string.\n* @param string input string to encode\n*/",
        "org.apache.hadoop.io.Text:encode(java.lang.String)": "/**\n* Encodes a string to ByteBuffer, replacing invalid characters.\n* @param string the input string to encode\n* @return ByteBuffer containing encoded bytes\n*/",
        "org.apache.hadoop.io.Text:set(byte[],int,int)": "/**\n* Sets a portion of a byte array to internal storage.\n* @param utf8 source byte array, @param start start index, @param len length to copy\n*/",
        "org.apache.hadoop.io.Text:append(byte[],int,int)": "/**\n* Appends a byte array to the existing bytes.\n* @param utf8 byte array to append\n* @param start starting index in utf8\n* @param len number of bytes to append\n*/",
        "org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)": "/**\n* Reads specified length of data into the byte array.\n* @param in DataInput stream to read from\n* @param len number of bytes to read\n*/",
        "org.apache.hadoop.io.Text:decode(byte[])": "/**\n* Decodes UTF-8 byte array to String, replacing errors if necessary.\n* @param utf8 byte array containing UTF-8 encoded data\n* @return decoded String\n*/",
        "org.apache.hadoop.io.Text:decode(byte[],int,int)": "/**\n* Decodes a UTF-8 byte array to a String.\n* @param utf8 byte array containing UTF-8 encoded data\n* @param start starting index of the byte array\n* @param length number of bytes to decode\n* @return decoded String\n*/",
        "org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)": "/**\n* Decodes a byte array to a String using UTF-8 encoding.\n* @param utf8 byte array containing UTF-8 data\n* @param start starting index for decoding\n* @param length number of bytes to decode\n* @param replace flag to replace malformed characters\n* @return decoded String\n*/",
        "org.apache.hadoop.io.Text:validateUTF8(byte[])": "/**\n* Validates the entire UTF-8 byte array.\n* @param utf8 byte array to validate\n* @throws MalformedInputException if the sequence is invalid\n*/",
        "org.apache.hadoop.io.Text:write(java.io.DataOutput)": "/**\n* Writes data to output stream, including length and byte array.\n* @param out output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.Text:write(java.io.DataOutput,int)": "/**\n* Writes data to output stream if within maxLength limit.\n* @param out output stream for writing data\n* @param maxLength maximum allowed length of data\n* @throws IOException if data exceeds maxLength or I/O error occurs\n*/",
        "org.apache.hadoop.io.Text:<init>(java.lang.String)": "/**\n* Constructs a Text object from an encoded string.\n* @param string input string to encode\n*/",
        "org.apache.hadoop.io.Text:find(java.lang.String,int)": "/**** \n* Finds the position of a substring starting from a given index.\n* @param what substring to search for\n* @param start index to begin the search\n* @return position of substring or -1 if not found\n*/",
        "org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)": "/**\n* Writes a string to output stream, returning its byte length.\n* @param out output stream to write to\n* @param s string to encode and write\n* @return length of the written byte array\n*/",
        "org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)": "/**\n* Writes a string to output, ensuring it does not exceed maxLength.\n* @param out output stream, @param s string to write, @param maxLength max allowed length\n* @return actual length of the written string\n*/",
        "org.apache.hadoop.io.Text:set(byte[])": "/**\n* Sets internal byte array from provided UTF-8 data.\n* @param utf8 source byte array\n*/",
        "org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text)": "/**\n* Copies bytes from another Text object and updates text length.\n* @param other the Text object to copy from\n*/",
        "org.apache.hadoop.io.Text:toString()": "/**\n* Converts byte array to String; handles decoding errors.\n* @return decoded String representation of byte array\n*/",
        "org.apache.hadoop.io.Text:hashCode()": "/**\n* Returns the hash code for this object.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.io.Text:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream; decodes length and reads data accordingly.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)": "/**\n* Reads fields from input stream with length checks.\n* @param in DataInput stream to read from\n* @param maxLength maximum allowed bytes to read\n* @throws IOException if length conditions are violated\n*/",
        "org.apache.hadoop.io.Text:skip(java.io.DataInput)": "/**\n* Skips a specified number of bytes from the input stream.\n* @param in DataInput stream to skip bytes from\n* @throws IOException if unable to read or skip bytes\n*/",
        "org.apache.hadoop.io.Text:readString(java.io.DataInput,int)": "/**\n* Reads a UTF-8 string from input with a maximum specified length.\n* @param in input stream to read from\n* @param maxLength maximum length of the string\n* @return decoded UTF-8 string\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.Text:find(java.lang.String)": "/**\n* Initiates substring search from the start.\n* @param what substring to search for\n* @return position of substring or -1 if not found\n*/",
        "org.apache.hadoop.io.Text:<init>(byte[])": "/**\n* Initializes Text object with UTF-8 byte array.\n* @param utf8 source byte array for text content\n*/",
        "org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text)": "/**\n* Constructs a Text object by copying from another Text.\n* @param utf8 the Text object to copy from\n*/",
        "org.apache.hadoop.io.Text:getTextLength()": "/**\n* Returns the length of the text; computes it if not previously calculated.\n* @return length of the text as an integer\n*/",
        "org.apache.hadoop.io.Text:equals(java.lang.Object)": "/**\n* Compares this Text object with another for equality.\n* @param o object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.Text:readString(java.io.DataInput)": "/**\n* Reads a UTF-8 string from input without length limit.\n* @param in input stream to read from\n* @return decoded UTF-8 string\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.Globber": {
        "org.apache.hadoop.fs.Globber:unescapePathComponent(java.lang.String)": "/**\n* Unescapes a path component by replacing backslash escapes.\n* @param name the escaped path component\n* @return the unescaped string\n*/",
        "org.apache.hadoop.fs.Globber:getPathComponents(java.lang.String)": "/**\n* Splits a file path into its components.\n* @param path the file path to split\n* @return a list of non-empty path components\n*/",
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Initializes a Globber with context, path pattern, and filter.\n* @param fc FileContext for filesystem operations\n* @param pathPattern pattern for matching paths\n* @param filter filter to apply on paths\n*/",
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)": "/**\n* Initializes a Globber instance with context, path, filter, and symlink resolution.\n* @param fc FileContext for file operations, pathPattern the path to match, filter PathFilter, \n* @param resolveSymlinks indicates if symlinks should be resolved\n*/",
        "org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext)": "/**\n* Creates a GlobBuilder instance using the provided FileContext.\n* @param fileContext the FileContext to be used, must not be null\n* @return a new GlobBuilder object\n*/",
        "org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem)": "/**\n* Creates a GlobBuilder instance using the provided FileSystem.\n* @param filesystem the FileSystem to be used, must not be null\n* @return a new GlobBuilder object\n*/",
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Initializes Globber with filesystem, path pattern, and filter.\n* @param fs FileSystem instance\n* @param pathPattern pattern for file paths\n* @param filter filter to apply on paths\n*/",
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)": "/**\n* Initializes a Globber for path pattern matching.\n* @param fs FileSystem instance to operate on\n* @param pathPattern pattern for matching paths\n* @param filter filter to apply on matched paths\n* @param resolveSymlinks flag to resolve symbolic links\n*/",
        "org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path)": "/**\n* Resolves and returns the absolute path from a potentially relative Path input.\n* @param path the Path to resolve\n* @return resolved Path object\n*/",
        "org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path)": "/**\n* Extracts the scheme from a given Path.\n* @param path the Path to retrieve the scheme from\n* @return scheme as a String, or null if not found\n*/",
        "org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path)": "/**\n* Extracts authority from the given Path object.\n* @param path the Path to extract authority from\n* @return authority string or null if not available\n*/",
        "org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path, returning an empty array on failure.\n* @param path the Path to list statuses for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status for a given path.\n* @param path the Path to check\n* @return FileStatus or null if not found\n*/",
        "org.apache.hadoop.fs.Globber:doGlob()": "/**\n* Expands glob patterns and retrieves matching file statuses.\n* @return array of matching FileStatus or null if no matches found\n*/",
        "org.apache.hadoop.fs.Globber:glob()": "/**\n* Retrieves file statuses matching the glob pattern.\n* @return array of matching FileStatus or null if no matches found\n*/"
    },
    "org.apache.hadoop.tracing.Tracer": {
        "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String)": "/**\n* Creates a new TraceScope with the given description.\n* @param description brief details of the scope\n* @return a TraceScope object\n*/",
        "org.apache.hadoop.tracing.Tracer:close()": "/**\n* Closes the resource, releasing any associated system resources.\n*/",
        "org.apache.hadoop.tracing.Tracer:curThreadTracer()": "/**\n* Retrieves the tracer for the current thread.\n* @return Tracer instance associated with the current thread\n*/",
        "org.apache.hadoop.tracing.Tracer:activateSpan(org.apache.hadoop.tracing.Span)": "/**\n* Activates a given span and returns a TraceScope.\n* @param span the Span to activate\n* @return a TraceScope object\n*/",
        "org.apache.hadoop.tracing.Tracer:getCurrentSpan()": "/**\n* Retrieves the current Span object.\n* @return Span object or null if no current span exists\n*/",
        "org.apache.hadoop.tracing.Tracer:<init>(java.lang.String)": "/**\n* Initializes a Tracer with a specified name.\n* @param name the name of the tracer\n*/",
        "org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "/**\n* Creates a new Span with a given description and context.\n* @param description brief info about the span\n* @param spanCtx context for the span\n* @return a new Span object\n*/"
    },
    "org.apache.hadoop.tracing.TraceScope": {
        "org.apache.hadoop.tracing.TraceScope:addKVAnnotation(java.lang.String,java.lang.String)": "/**\n* Adds a key-value annotation.\n* @param key the annotation key\n* @param value the annotation value\n*/",
        "org.apache.hadoop.tracing.TraceScope:getSpan()": "/**\n* Retrieves the current Span object.\n* @return Span object representing the current span\n*/",
        "org.apache.hadoop.tracing.TraceScope:addTimelineAnnotation(java.lang.String)": "/**\n* Adds a timeline annotation with the specified message.\n* @param msg the message to annotate in the timeline\n*/",
        "org.apache.hadoop.tracing.TraceScope:<init>(org.apache.hadoop.tracing.Span)": "/**\n* Initializes TraceScope with the given Span.\n* @param span the Span object to associate with this TraceScope\n*/",
        "org.apache.hadoop.tracing.TraceScope:addKVAnnotation(java.lang.String,java.lang.Number)": "/**\n* Adds a key-value annotation.\n* @param key the annotation key\n* @param value the annotation value as a Number\n*/",
        "org.apache.hadoop.tracing.TraceScope:detach()": "/**\n* Detaches the current object from its associated context.\n*/",
        "org.apache.hadoop.tracing.TraceScope:reattach()": "/**\n* Reattaches the current object to its parent or context.\n*/",
        "org.apache.hadoop.tracing.TraceScope:span()": "/**\n* Returns the current Span object.\n* @return Span instance associated with the method\n*/",
        "org.apache.hadoop.tracing.TraceScope:close()": "/**\n* Closes the span if it is not null to release resources.\n*/"
    },
    "org.apache.hadoop.util.DurationInfo": {
        "org.apache.hadoop.util.DurationInfo:close()": "/**\n* Closes the resource and logs its status based on log level.\n*/",
        "org.apache.hadoop.util.DurationInfo:getFormattedText()": "/**\n* Retrieves formatted text, initializing it if null.\n* @return formatted text string\n*/",
        "org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])": "/**\n* Initializes DurationInfo and logs start message based on log level.\n* @param log Logger instance for logging messages\n* @param logAtInfo flag to log at INFO level\n* @param format message format string\n* @param args arguments for the format string\n*/",
        "org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])": "/**\n* Constructs DurationInfo with logging enabled.\n* @param log Logger instance for logging messages\n* @param format message format string\n* @param args arguments for the format string\n*/",
        "org.apache.hadoop.util.DurationInfo:toString()": "/**\n* Returns a string representation including formatted text and duration.\n* @return formatted string of duration information\n*/"
    },
    "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator": {
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:getIter()": "/**\n* Retrieves the next LongStatistic iterator or null if exhausted.\n* @return Iterator of LongStatistic or null if no more available\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:remove()": "/**\n* Throws UnsupportedOperationException when remove is called.\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext()": "/**\n* Checks if the iterator has more elements.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next()": "/**\n* Retrieves the next LongStatistic from the iterator.\n* @return LongStatistic object\n* @throws NoSuchElementException if no more elements are available\n*/"
    },
    "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context": {
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(int)": "/**\n* Updates and retrieves the last accessed directory index.\n* @param delta increment value for directory index\n* @return previous directory index before update\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed()": "/**\n* Retrieves and increments the last accessed directory index.\n* @return previous directory index before update\n*/"
    },
    "org.apache.hadoop.fs.ClusterStorageCapacityExceededException": {
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>()": "/**\n* Constructs a ClusterStorageCapacityExceededException with no detail message.\n*/",
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.String)": "/**\n* Constructs an exception with a specified error message.\n* @param message detailed error message\n*/",
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an exception with a message and cause.\n* @param message detail message of the exception\n* @param cause the cause of the exception\n*/",
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.Throwable)": "/**\n* Constructs an exception with the specified cause.\n* @param cause the underlying reason for the exception\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation": {
        "org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedExceptionAction)": "/**\n* Executes a privileged action with the current subject.\n* @param action the privileged action to execute\n* @return result of the action\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.security.UserGroupInformation:hashCode()": "/**\n* Returns the hash code based on the object's identity.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.security.UserGroupInformation:equals(java.lang.Object)": "/**\n* Compares this object to another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:setShouldRenewImmediatelyForTests(boolean)": "/**\n* Sets the flag for immediate renewal in tests.\n* @param immediate true to renew immediately, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isInitialized()": "/**\n* Checks if the configuration is initialized.\n* @return true if initialized, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:setLoginUser(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Sets the current login user.\n* @param ugi UserGroupInformation for the new login user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getOsPrincipalClass()": "/**\n* Retrieves the OS-specific Principal class.\n* @return Class of the Principal or null if not found\n*/",
        "org.apache.hadoop.security.UserGroupInformation:trimLoginMethod(java.lang.String)": "/**\n* Trims username to remove any trailing spaces.\n* @param userName the input username\n* @return trimmed username without spaces\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getRefreshTime(javax.security.auth.kerberos.KerberosTicket)": "/**\n* Calculates the refresh time for a Kerberos ticket.\n* @param tgt the KerberosTicket to evaluate\n* @return calculated refresh time in milliseconds\n*/",
        "org.apache.hadoop.security.UserGroupInformation:executeAutoRenewalTask(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable)": "/**\n* Executes an auto-renewal task for the specified user.\n* @param userName the name of the user for whom the task is executed\n* @param task the task to be executed for auto-renewal\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getSubject()": "/**\n* Retrieves the current Subject instance.\n* @return Subject object representing the current subject\n*/",
        "org.apache.hadoop.security.UserGroupInformation:addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier)": "/**\n* Adds a token identifier to the public credentials.\n* @param tokenId the token identifier to add\n* @return true if added successfully, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getTokenIdentifiers()": "/**\n* Retrieves a set of public TokenIdentifiers for the subject.\n* @return Set of TokenIdentifier objects\n*/",
        "org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedAction)": "/**\n* Executes a privileged action with the current subject.\n* @param action the privileged action to execute\n* @return result of the action execution\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getOSLoginModuleName()": "/**\n* Determines the OS-specific login module name.\n* @return String representing the login module name based on the OS\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Calculates the next target renewal time based on end time and current time.\n* @param tgtEndTime target end time in milliseconds\n* @param now current time in milliseconds\n* @param rp retry policy to determine delay\n* @return the computed next renewal time in milliseconds\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal()": "/**\n* Retrieves or creates Credentials for the subject.\n* @return Credentials object associated with the subject\n*/",
        "org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)": "/**** Creates a new HadoopLoginContext for authentication. \n* @param appName application name \n* @param subject security subject \n* @param loginConf Hadoop configuration \n* @return HadoopLoginContext instance \n* @throws LoginException if authentication fails \n*/",
        "org.apache.hadoop.security.UserGroupInformation:reset()": "/**\n* Resets authentication and configuration settings to their defaults.\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getLogin()": "/**\n* Retrieves HadoopLoginContext from the user's LoginContext.\n* @return HadoopLoginContext or null if not applicable\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isLoginSuccess()": "/**\n* Checks if the user is logged in based on their LoginContext.\n* @return true if logged in, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext)": "/**\n* Sets the login context for the user.\n* @param login LoginContext to be set for the user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:setLastLogin(long)": "/**\n* Updates the user's last login timestamp.\n* @param loginTime timestamp of the last login in milliseconds\n*/",
        "org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject)": "/**\n* Initializes UserGroupInformation with the given Subject.\n* @param subject the Subject containing user principals\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getUserName()": "/**\n* Retrieves the user's name.\n* @return the user's full name as a String\n*/",
        "org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials()": "/**\n* Checks if the user has Kerberos authentication credentials.\n* @return true if Kerberos credentials are present, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod()": "/**\n* Retrieves the current authentication method.\n* @return AuthenticationMethod object from the user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder()": "/**** \n* Cleans up invalid Kerberos tickets from the current Subject's credentials.\n*/",
        "org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long)": "/**\n* Checks if sufficient time has elapsed since the last login.\n* @param now current timestamp in milliseconds\n* @return true if enough time has passed, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getRealUser()": "/**** Retrieves the real user from the current subject's principals. \n* @return UserGroupInformation of the real user or null if not found \n*/",
        "org.apache.hadoop.security.UserGroupInformation:getShortUserName()": "/**** Retrieves the short username from the user object. \n* @return short username as a String \n*/",
        "org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)": "/**\n* Sets the authentication method for the user.\n* @param authMethod the authentication method to set\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getTokens()": "/**\n* Retrieves an unmodifiable collection of tokens for the subject.\n* @return Collection of Token objects associated with the subject\n*/",
        "org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Sets the user's authentication method.\n* @param authMethod the authentication method to set\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getKeytab()": "/**** Retrieves the keytab from the HadoopLoginContext. \n* @return keytab string or null if login context is unavailable \n*/",
        "org.apache.hadoop.security.UserGroupInformation:isHadoopLogin()": "/**\n* Checks if Hadoop login context is managing the user group information.\n* @return true if managing, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Creates a proxy UserGroupInformation for a specified user and real user.\n* @param user the proxy user's name\n* @param realUser the actual UserGroupInformation of the real user\n* @return UserGroupInformation object for the proxy user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Returns the real user or the given user if not found.\n* @param user UserGroupInformation to check\n* @return UserGroupInformation of real user or the input user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:toString()": "/**\n* Returns a string representation of the user with auth method and real user info.\n* @return formatted user details including name, auth method, and real user if available\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod()": "/**\n* Retrieves the authentication method for the real user.\n* @return AuthenticationMethod object or null if user not found\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Retrieves the authentication method for the given user.\n* @param ugi user group information\n* @return AuthenticationMethod object for the user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getTGT()": "/**\n* Retrieves the original TGT from the subject's Kerberos tickets.\n* @return KerberosTicket or null if none found\n*/",
        "org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)": "/**\n* Performs a relogin for the user, updating last login time and handling exceptions.\n* @param login HadoopLoginContext for authentication\n* @param ignoreLastLoginTime flag to bypass last login checks\n* @throws IOException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**** Creates UserGroupInformation for a remote user. \n* @param user the username; must not be null or empty\n* @param authMethod the authentication method to set\n* @return UserGroupInformation instance\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isFromKeytab()": "/**\n* Checks if the user is authenticated via a keytab.\n* @return true if authenticated, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isFromTicket()": "/**\n* Checks if the user is authenticated via ticket without a keytab.\n* @return true if using Kerberos and no keytab, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:shouldRelogin()": "/**\n* Determines if a relogin is required based on authentication status.\n* @return true if both Kerberos credentials and Hadoop login are valid\n*/",
        "org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)": "/**\n* Performs an atomic relogin for the user.\n* @param login HadoopLoginContext for authentication\n* @param ignoreLastLoginTime flag to bypass last login checks\n* @throws IOException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:reattachMetrics()": "/**\n* Reattaches metrics by invoking UgiMetrics.reattach().\n*/",
        "org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String)": "/**\n* Creates UserGroupInformation for a remote user with SIMPLE auth.\n* @param user the username; must not be null or empty\n* @return UserGroupInformation instance\n*/",
        "org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab()": "/**\n* Spawns a renewal thread for Kerberos credentials if relogin is not needed.\n*/",
        "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)": "/**\n* Relogs in using a keytab if conditions are met.\n* @param checkTGT flag to verify TGT validity\n* @param ignoreLastLoginTime flag to skip last login checks\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean)": "/**\n* Relogs user if conditions are met.\n* @param ignoreLastLoginTime skips last login checks\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)": "/**\n* Adds a token associated with an alias.\n* @param alias identifier for the token\n* @param token token to be added, ignored if null\n* @return true if the token was added successfully\n*/",
        "org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab()": "/**\n* Forces relogin using a keytab, ignoring TGT validity checks.\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean)": "/**\n* Relogs in using a keytab, verifying TGT if specified.\n* @param checkTGT flag to verify TGT validity\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache()": "/**\n* Forces relogin from ticket cache, ignoring last login checks.\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache()": "/**\n* Relogs user from ticket cache without last login checks.\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token)": "/**\n* Adds a token if not null.\n* @param token token to be added\n* @return true if added successfully, false if token is null\n*/",
        "org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab()": "/**\n* Checks TGT validity and relogs in using a keytab.\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab()": "/**\n* Relogs in using a keytab without TGT verification.\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials)": "/**\n* Adds credentials to the subject in a thread-safe manner.\n* @param credentials Credentials to be added\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getCredentials()": "/**\n* Retrieves Credentials, removing private tokens from the collection.\n* @return Credentials object with non-private tokens\n*/",
        "org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Logs user information and credentials if debug logging is enabled.\n* @param log Logger instance for logging messages\n* @param caption Message prefix for the log\n* @param ugi UserGroupInformation containing user credentials\n*/",
        "org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean)": "/**\n* Spawns a renewal thread for user credentials if conditions are met.\n* @param force if true, forces thread spawn regardless of conditions\n*/",
        "org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Initializes security configuration and metrics based on provided settings.\n* @param conf Hadoop configuration object\n* @param overrideNameRules flag to override name rules\n*/",
        "org.apache.hadoop.security.UserGroupInformation:ensureInitialized()": "/**\n* Ensures the initialization of UserGroupInformation.\n* Initializes if not already done, using default config.\n*/",
        "org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the Hadoop configuration.\n* @param conf Hadoop configuration object\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)": "/**\n* Checks if a specific authentication method is enabled.\n* @param method the authentication method to check\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled()": "/**\n* Checks if Kerberos key tab login renewal is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor()": "/**\n* Retrieves the Kerberos login renewal executor.\n* @return Optional ExecutorService for renewal or empty if not initialized\n*/",
        "org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])": "/**** Creates a UserGroupInformation for testing. \n* @param user the username; must not be null or empty \n* @param userGroups array of user group names \n* @return UserGroupInformation instance \n*/",
        "org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])": "/**\n* Creates a proxy UserGroupInformation for testing.\n* @param user proxy user's name\n* @param realUser actual UserGroupInformation of the real user\n* @param userGroups array of user groups for the proxy user\n* @return UserGroupInformation object for the proxy user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getGroups()": "/**\n* Retrieves user groups; returns empty list if an error occurs.\n* @return List of group names for the current user\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getGroupsSet()": "/**\n* Retrieves user group names as an unmodifiable set.\n* @return Set of group names or empty set if an error occurs\n*/",
        "org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)": "/**\n* Performs login using a Subject and LoginParams.\n* @param subject the security subject; can be null\n* @param params login parameters; defaults if both are null\n* @return UserGroupInformation for the logged-in user\n* @throws IOException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled()": "/**\n* Checks if security is enabled by verifying authentication method.\n* @return true if security is enabled, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab()": "/**\n* Logs out the user from the keytab if Kerberos credentials are present.\n* @throws IOException if logout fails or user is not logged in\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName()": "/**\n* Retrieves the primary group name.\n* @return primary group name or throws IOException if no groups exist\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getGroupNames()": "/**\n* Retrieves user group names as an array.\n* @return Array of group names or empty array if none found\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject)": "/**\n* Retrieves UserGroupInformation from a Subject.\n* @param subject security subject; must not be null and must contain a principal\n* @return UserGroupInformation for the subject\n* @throws IOException if login fails or subject is invalid\n*/",
        "org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject)": "/**\n* Creates a UserGroupInformation object for login, handling proxy users and loading tokens.\n* @param subject security subject for login\n* @return UserGroupInformation for the logged-in user\n* @throws IOException if login or token loading fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:print()": "/**\n* Prints user name and group names to the console.\n* @throws IOException if an I/O error occurs during output\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getLoginUser()": "/**\n* Retrieves the current UserGroupInformation, initializing if necessary.\n* @return UserGroupInformation for the logged-in user\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject)": "/**\n* Logs in a user based on the provided security Subject.\n* @param subject security Subject for login\n* @throws IOException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getCurrentUser()": "/**\n* Retrieves current UserGroupInformation, initializing if necessary.\n* @return UserGroupInformation for the logged-in user\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased()": "/**\n* Checks if the logged-in user is authenticated via a keytab.\n* @return true if authenticated, false otherwise\n* @throws IOException if user retrieval fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased()": "/**\n* Checks if the login is based on a ticket.\n* @return true if the user is authenticated via ticket, false otherwise\n* @throws IOException if retrieving the login user fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)": "/**\n* Retrieves the best UserGroupInformation based on ticket cache or username.\n* @param ticketCachePath path to the ticket cache; can be null\n* @param user username for remote user; can be null\n* @return UserGroupInformation instance\n* @throws IOException if ticket cache retrieval fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)": "/**** Logs in a user from a keytab and returns UserGroupInformation.\n* @param user the principal name of the user\n* @param path the path to the keytab file\n* @return UserGroupInformation for the logged-in user\n* @throws IOException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Logs user information if debug logging is enabled.\n* @param log Logger instance for logging messages\n* @param ugi UserGroupInformation containing user credentials\n* @throws IOException if logging fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)": "/**\n* Retrieves UserGroupInformation from a ticket cache or username.\n* @param ticketCache path to the ticket cache\n* @param user username for remote user\n* @return UserGroupInformation instance\n* @throws IOException if retrieval fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)": "/**\n* Logs in a user from a keytab file.\n* @param user the principal name of the user\n* @param path the path to the keytab file\n* @throws IOException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Logs user information using provided UserGroupInformation.\n* @param ugi UserGroupInformation containing user credentials\n* @throws IOException if logging fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[])": "/**\n* Main method to display UserGroupInformation and authenticate via keytab if provided.\n* @param args command line arguments for keytab authentication\n* @throws Exception if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Cache": {
        "org.apache.hadoop.fs.FileSystem$Cache:remove(org.apache.hadoop.fs.FileSystem$Cache$Key,org.apache.hadoop.fs.FileSystem)": "/**\n* Removes a key from the cache and updates auto-close list if necessary.\n* @param key the key to remove\n* @param fs the FileSystem to check against the cached value\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:getDiscardedInstances()": "/**\n* Retrieves the count of discarded instances.\n* @return number of discarded instances as a long\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean)": "/**\n* Closes all file systems, logging exceptions if any occur.\n* @param onlyAutomatic if true, only automatic file systems are closed\n* @throws IOException if multiple I/O errors occur during closing\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Closes all FileSystems for the given UserGroupInformation.\n* @param ugi user group information for access control\n* @throws IOException if closing fails for any FileSystem\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:closeAll()": "/**\n* Closes all file systems, logging exceptions if they occur.\n* @throws IOException if multiple I/O errors occur during closing\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes cache with a semaphore for parallel creation permits.\n* @param conf configuration with permits setting\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)": "/**\n* Retrieves or creates a FileSystem instance for the given URI and configuration.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @param key the key used to store/retrieve the filesystem\n* @return the FileSystem instance\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a unique FileSystem instance for the given URI.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if an error occurs while fetching the filesystem\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a FileSystem instance for the given URI and configuration.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n*/"
    },
    "org.apache.hadoop.security.AccessControlException": {
        "org.apache.hadoop.security.AccessControlException:<init>(java.lang.String)": "/**\n* Constructs an AccessControlException with the specified detail message.\n* @param s detail message for the exception\n*/",
        "org.apache.hadoop.security.AccessControlException:<init>()": "/**\n* Constructs an AccessControlException with a default message \"Permission denied.\"\n*/",
        "org.apache.hadoop.security.AccessControlException:<init>(java.lang.Throwable)": "/**\n* Constructs an AccessControlException with a specified cause.\n* @param cause the underlying reason for the exception\n*/"
    },
    "org.apache.hadoop.fs.UnsupportedFileSystemException": {
        "org.apache.hadoop.fs.UnsupportedFileSystemException:<init>(java.lang.String)": "/**\n* Constructs an UnsupportedFileSystemException with a specified message.\n* @param message error message describing the exception\n*/"
    },
    "org.apache.hadoop.util.LambdaUtils": {
        "org.apache.hadoop.util.LambdaUtils:eval(java.util.concurrent.CompletableFuture,java.util.concurrent.Callable)": "/**\n* Evaluates a Callable and completes the CompletableFuture with its result or exception.\n* @param result CompletableFuture to complete\n* @param call Callable to execute for the result\n* @return completed CompletableFuture with the result or exception\n*/",
        "org.apache.hadoop.util.LambdaUtils:<init>()": "/**\n* Private constructor to prevent instantiation of the LambdaUtils class.\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystem": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:fsGetter()": "/**\n* Creates and returns a new instance of FsGetter.\n* @return a new FsGetter object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getScheme()": "/**\n* Returns the scheme constant for ViewFS.\n* @return String representing the ViewFS scheme\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:supportAutoAddingFallbackOnNoMounts()": "/**\n* Checks if auto-adding fallback is supported when no mounts exist.\n* @return false indicating fallback is not supported\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:verifyRenameStrategy(java.net.URI,java.net.URI,boolean,org.apache.hadoop.fs.viewfs.ViewFileSystem$RenameStrategy)": "/**\n* Verifies rename strategy for URIs; throws IOException if invalid.\n* @param srcUri source URI, dstUri destination URI, isSrcDestSame flag, renameStrategy type\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUri()": "/**\n* Retrieves the URI associated with this object.\n* @return the URI of this object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getWorkingDirectory()": "/**\n* Returns the current working directory path.\n* @return Path representing the working directory\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setVerifyChecksum(boolean)": "/**\n* Sets checksum verification flag; does nothing for ViewFileSystem.\n* @param verifyChecksum flag to enable or disable checksum verification\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setWriteChecksum(boolean)": "/**\n* Sets the write checksum flag for the file system.\n* @param writeChecksum true to enable, false to disable checksum writing\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)": "/**\n* Creates an AccessControlException for a readonly operation.\n* @param operation the operation attempted\n* @param p the path where the operation was attempted\n* @return AccessControlException with a detailed message\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List)": "/**\n* Initializes mounted FileSystems from provided mount points.\n* @param mountPoints list of mount points to initialize\n* @return map of source paths to their corresponding FileSystems\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize()": "/**\n* Retrieves the default block size; throws exception if not in mountpoint.\n* @throws NotInMountpointException if called outside of a valid mountpoint\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication()": "/**\n* Throws NotInMountpointException for default replication access.\n* @throws NotInMountpointException if method is called outside a valid mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults()": "/**\n* Retrieves server default settings.\n* @throws IOException if not in a valid mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems()": "/**\n* Retrieves child FileSystem instances from mounted points.\n* @return array of FileSystem objects\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)": "/**\n* Creates an AccessControlException for a readonly operation.\n* @param operation the operation attempted\n* @param p the path where the operation was attempted\n* @return AccessControlException with a detailed message\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies()": "/**\n* Retrieves all storage policies from child file systems.\n* @return Collection of BlockStoragePolicySpi objects\n* @throws IOException if retrieval fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to absolute if it's not already.\n* @param f the Path to be converted\n* @return absolute Path object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory()": "/**\n* Returns the user's home directory path.\n* @return Path object representing the home directory\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints()": "/**\n* Retrieves an array of MountPoint objects from the file system state.\n* @return array of MountPoint instances\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the URI path from a given Path.\n* @param p the Path to be converted \n* @return the URI path as a String\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory after validating the path.\n* @param new_dir the new directory Path to set as working directory\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param f the file path to append to\n* @param bufferSize size of the buffer for writing\n* @param progress callback for progress updates\n* @return FSDataOutputStream for the file\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a non-recursive FSDataOutputStream for the specified path.\n* @param f file path, @param permission file permissions, @param flags creation flags,\n* @param bufferSize size of buffer, @param replication replication factor,\n* @param blockSize block size, @param progress progress tracker\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file at the specified path with given permissions and options.\n* @param f file path to create; @param permission file permissions; @return FSDataOutputStream\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory; throws exceptions for access issues.\n* @param f path of the file/directory to delete\n* @param recursive flag for recursive deletion\n* @return true if deletion succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file checksum for the specified path.\n* @param f file path\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)": "/**\n* Retrieves the checksum of a specified file.\n* @param f file path; @param length expected file length\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists located file statuses in a directory.\n* @param f directory path; @param filter criteria for filtering files\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path)": "/**\n* Creates directories for the specified path.\n* @param dir path to the directory to create\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories at the specified path with given permissions.\n* @param dir the directory path to create\n* @param permission the permissions to set for the new directory\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f the file path to open; @param bufferSize size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to the specified length.\n* @param f file path to truncate\n* @param newLength the new length of the file\n* @return true if truncation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner of a specified path to the given username and groupname.\n* @param f path to set the owner for\n* @param username new owner's username\n* @param groupname new owner's group name\n* @throws AccessControlException, FileNotFoundException, IOException on errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file system permissions for the specified path.\n* @param f the path to set permissions on\n* @param permission the permissions to apply\n* @throws AccessControlException if access is denied\n* @throws FileNotFoundException if the path does not exist\n* @throws IOException for other I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified file path.\n* @param f the file path to set replication for\n* @param replication the desired replication factor\n* @return true if operation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a specified file path.\n* @param f the file path to modify\n* @param mtime the new modification time\n* @param atime the new access time\n* @throws AccessControlException if access is denied\n* @throws FileNotFoundException if the file does not exist\n* @throws IOException for I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for a specified path.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n* @throws IOException if ACL modification fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes ACL entries from the specified path.\n* @param path the file system path; @param aclSpec list of ACL entries to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the default ACL for a specified path.\n* @param path the path for which to remove the default ACL\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL from the specified file path.\n* @param path the file path whose ACL is to be removed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for a specified file path.\n* @param path the file path for which ACL status is requested\n* @return AclStatus object containing ACL details\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a file path.\n* @param path file path to set the attribute on\n* @param name name of the extended attribute\n* @param value byte array of the attribute value\n* @param flag set flags for attribute modification\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attribute by path and name.\n* @param path the file path; @param name the attribute name\n* @return byte array of the attribute value\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a given file system path.\n* @param path the file system path to query\n* @return map of attribute names to their byte values\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a given file path.\n* @param path the file path to query\n* @param names list of attribute names to retrieve\n* @return map of attribute names to their byte values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Lists extended attributes for the specified path.\n* @param path the file or directory path\n* @return List of extended attribute names\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes the specified extended attribute from the given file path.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the default replication factor for a given path.\n* @param f the path to check for replication\n* @return short representing the default replication factor\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ContentSummary for a specified file or directory.\n* @param f Path of the file or directory\n* @return ContentSummary object with aggregated counts and length\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)": "/**\n* Retrieves quota usage for the specified file or directory.\n* @param f Path of the file or directory\n* @return QuotaUsage object based on content summary\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**** Creates a snapshot of the specified path.  \n* @param path the path to create a snapshot for  \n* @param snapshotName the name for the created snapshot  \n* @throws IOException if snapshot creation fails  \n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot in the file system.\n* @param path path of the snapshot; @param snapshotOldName current name; @param snapshotNewName desired name\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot by name at the specified path.\n* @param path location of the snapshot; @param snapshotName name of the snapshot to delete\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Applies storage policy to the specified path.\n* @param src the path to apply the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a given path.\n* @param src source path to set policy for\n* @param policyName name of the storage policy\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for a specified path.\n* @param src the path for which to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves storage policy for a given path.\n* @param src source path for policy retrieval\n* @return BlockStoragePolicySpi object for the path\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file system status for the specified path.\n* @param p the path for which the status is requested\n* @return FsStatus object with capacity, used, and remaining space\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed()": "/**\n* Returns the used length of content; throws if in a mount point.\n* @return used content length as a long\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a symbolic link.\n* @param path the path of the symbolic link\n* @return resolved Path of the link target\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)": "/**\n* Resolves and returns a chrooted Path based on the filesystem type.\n* @param res resolution result containing filesystem info\n* @param status file status to derive path suffix\n* @param f base Path to qualify\n* @return qualified Path object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path)": "/**\n* Deletes a file or directory with recursive option.\n* @param f path of the file/directory to delete\n* @return true if deletion succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus()": "/**\n* Retrieves file system status for the root path.\n* @return FsStatus object with capacity, used, and remaining space\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)": "/**\n* Retrieves block locations for a file within a specified byte range.\n* @param fs FileStatus of the target file\n* @param start starting byte position\n* @param len length of the range\n* @return array of BlockLocation or empty if no blocks found\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)": "/**** Wraps a local FileStatus with a qualified Path. \n* @param orig original FileStatus object \n* @param qualified Path to modify the file location \n* @return Wrapped FileStatus instance \n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)": "/**\n* Adjusts FileStatus path for viewfs URI and returns modified instance.\n* @param orig original FileStatus object\n* @param qualified new qualified Path to set\n* @return modified FileStatus instance\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus for a given path.\n* @param f the path to get the FileStatus for\n* @return FileStatus object for the specified path\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path.\n* @param f the path to list statuses for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem)": "/**\n* Closes child FileSystem instances if caching is disabled.\n* @param fs parent FileSystem to retrieve children from\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:close()": "/**\n* Closes resources, caches, and child file systems based on cache settings.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes configuration and sets up inner cache with given URI and settings.\n* @param theUri the URI for initialization\n* @param conf configuration object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the default block size for a given file path.\n* @param f the file path to get the block size for\n* @return default block size in bytes\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves the given Path and returns the filesystem Path or original if internal directory.\n* @param f the Path to resolve\n* @return resolved Path object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file storage defaults for the specified path.\n* @param f the path to retrieve defaults for\n* @return FsServerDefaults object containing configuration parameters\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the specified path supports the given capability.\n* @param path the path to check\n* @param capability the capability to verify\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the enclosing root Path for a given Path.\n* @param path the Path to qualify\n* @return qualified enclosing root Path\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory from src to dst.\n* @param src source Path to rename; @param dst destination Path\n* @return true if rename is successful, false otherwise\n* @throws IOException if rename operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>()": "/**\n* Initializes ViewFileSystem with current user info and creation time.\n* @throws IOException if user info retrieval fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs ViewFileSystem and initializes with given URI and configuration.\n* @param theUri the URI for initialization\n* @param conf configuration object\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the trash root path for a given file system path.\n* @param path the path to resolve for trash root\n* @return Path of the trash root or throws NotInMountpointException\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean)": "/**\n* Retrieves trash roots for users.\n* @param allUsers flag to include all users' trash\n* @return collection of FileStatus for trash directories\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a given path and action mode.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs ViewFileSystem with the given configuration.\n* @param conf configuration object\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ChRootedFs": {
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFsStatus()": "/**\n* Retrieves the file system status.\n* @return FsStatus object representing the current file system status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults()": "/**\n* Retrieves server default settings.\n* @return FsServerDefaults object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getUriDefaultPort()": "/**\n* Retrieves the default port for the URI.\n* @return default port as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setVerifyChecksum(boolean)": "/**\n* Sets the checksum verification flag.\n* @param verifyChecksum true to enable, false to disable checksum verification\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getMyFs()": "/**\n* Retrieves the current file system instance.\n* @return AbstractFileSystem instance\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the storage policy for a given file path.\n* @param src path of the source file\n* @return BlockStoragePolicySpi object representing the storage policy\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Checks and applies the storage policy for a given file system path.\n* @param path the file system path to check\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies()": "/**\n* Retrieves all storage policies.\n* @return collection of BlockStoragePolicySpi or throws IOException\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks()": "/**\n* Indicates if the file system supports symbolic links.\n* @return true if symlinks are supported, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String)": "/**\n* Retrieves delegation tokens for the specified renewer.\n* @param renewer the identifier for the token renewer\n* @return list of tokens associated with the renewer\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)": "/**\n* Initializes a chrooted filesystem with a specified root path.\n* @param fs the AbstractFileSystem instance\n* @param theRoot the Path representing the root directory\n* @throws URISyntaxException if the URI is malformed\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)": "/**\n* Resolves and qualifies a given Path.\n* @param f the Path to be resolved\n* @return a qualified Path object\n* @throws FileNotFoundException if the Path is not found\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path)": "/**\n* Constructs full path by validating input and combining with root path.\n* @param path the Path object to validate and combine\n* @return new Path object representing the full path\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path)": "/**\n* Strips root path from the given Path object.\n* @param p the Path to process\n* @return the path without the root part\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String)": "/**\n* Validates a name by checking it against filesystem rules.\n* @param src name to validate\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Creates a file in the filesystem with specified parameters.\n* @param f file path, flags, permissions, buffer size, replication, block size, and progress\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory at the specified path.\n* @param f the Path object to delete\n* @param recursive true for recursive deletion, false otherwise\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a specified file range.\n* @param f file path, @param start offset, @param len length of bytes to read\n* @return array of BlockLocation objects\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the checksum of a file.\n* @param f Path of the file to check\n* @return FileChecksum object for the file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file status for the given path.\n* @param f the Path object for which to get the file status\n* @return FileStatus object representing the file's status\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file link status for a given path.\n* @param f the file path to check status for\n* @return FileStatus object representing the file's status\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server default settings for the specified file path.\n* @param f file path to get defaults for\n* @return FsServerDefaults object\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file status for the given path.\n* @param f the Path object to list status for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path)": "/**\n* Returns an iterator for file statuses in the specified path.\n* @param f the path to list file statuses\n* @return RemoteIterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses for a given path.\n* @param f the path to list file statuses\n* @return an iterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory with specified permissions.\n* @param dir directory path to create\n* @param permission permissions for the new directory\n* @param createParent whether to create parent directories if they don't exist\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f the Path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to a specified length.\n* @param f path to the file to truncate\n* @param newLength desired new length of the file\n* @return true if truncation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file from src to dst within the same file system.\n* @param src source Path to rename\n* @param dst destination Path for the renamed file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Renames a file or directory within the same filesystem.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @param overwrite flag to allow overwriting existing destination\n* @throws IOException if an I/O error occurs\n* @throws UnresolvedLinkException if the path is a symlink\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner and group for the specified file path.\n* @param f file path to set ownership for\n* @param username new owner's username\n* @param groupname new owner's group name\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions for the specified path.\n* @param f the Path object of the file\n* @param permission the FsPermission object to apply\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified file.\n* @param f file path to set replication for\n* @param replication new replication factor\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a file.\n* @param f file Path, @param mtime modification time, @param atime access time\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for a specified file path.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes specified ACL entries from the given path.\n* @param path the path from which to remove ACL entries\n* @param aclSpec list of ACL entries to be removed\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes default ACL for the specified path.\n* @param path the path for which default ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL for the specified path.\n* @param path the Path object for which ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to set\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for the specified file path.\n* @param path the file path for which ACL status is requested\n* @return AclStatus object\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a specified file path.\n* @param path target file path\n* @param name attribute name\n* @param value value to set\n* @param flag options for setting the attribute\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves the extended attribute of a file.\n* @param path the file system path\n* @param name the attribute name to retrieve\n* @return byte array of attribute value\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for the specified file path.\n* @param path the file path to retrieve extended attributes\n* @return a map of attribute names and their corresponding byte values\n* @throws IOException if the operation is unsupported\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a given file path.\n* @param path the file path to get attributes from\n* @param names list of attribute names to retrieve\n* @return map of attribute names and their byte values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Lists extended attributes for the specified path.\n* @param path directory path to list attributes for\n* @return List of attribute names\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes an extended attribute from a specified file path.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot at the specified path with the given name.\n* @param path the path for the snapshot\n* @param name the name of the snapshot\n* @return Path object of the created snapshot\n* @throws IOException if snapshot creation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot at the given path.\n* @param path the file path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot by directory and name.\n* @param snapshotDir directory of the snapshot\n* @param snapshotName name of the snapshot to delete\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a specified file system path.\n* @param path the file system path to set the policy for\n* @param policyName the name of the storage policy to apply\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for the specified path.\n* @param src the path to the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link at the specified path.\n* @param target the target file path\n* @param link the symlink path to create\n* @param createParent whether to create parent directories\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Resolves the link target for a given path.\n* @param f the path to resolve\n* @return resolved Path object\n* @throws IOException if link resolution fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory()": "/**\n* Retrieves the home directory path for the current user.\n* @return Path object representing the user's home directory\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a specified path.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$MountPoint": {
        "org.apache.hadoop.fs.viewfs.InodeTree$MountPoint:<init>(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)": "/**\n* Initializes a MountPoint with source path and link to target inode.\n* @param srcPath the source path for the mount point\n* @param mountLink the link to the target inode\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs": {
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:checkPathIsSlash(org.apache.hadoop.fs.Path)": "/**\n* Validates if the given path is the root path.\n* @param f the Path object to check\n* @throws IOException if the path is not the root path\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getUriDefaultPort()": "/**\n* Returns the default port for the URI, always -1 indicating no default port.\n* @return default port number\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])": "",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus()": "/**\n* Returns the file system status with zero capacity, used, and remaining space.\n* @return FsStatus object with initialized values\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the checksum of a file at the given path.\n* @param f the Path object representing the file\n* @throws FileNotFoundException if the path points to a directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading.\n* @param f the Path to the file\n* @param bufferSize the size of the buffer to use\n* @throws FileNotFoundException if the path points to a directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attributes for a path.\n* @param path the path to access attributes\n* @param name the name of the attribute\n* @throws IOException if access is outside a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the path to fetch attributes from\n* @throws IOException if an error occurs or path is not in mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the path to access attributes\n* @param names list of attribute names to retrieve\n* @throws IOException if the path is not in a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Lists extended attributes for a given path.\n* @param path the path to check for extended attributes\n* @return throws NotInMountpointException if path is outside a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean)": "/**\n* Throws AccessControlException for setting checksum verification in read-only context.\n* @param verifyChecksum indicates if checksum verification should be enabled\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory; throws exception if read-only.\n* @param f Path to delete\n* @param recursive true to delete directories recursively\n* @return false, as deletion is not performed\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file at the specified path to a new length.\n* @param f the Path of the file to truncate\n* @param newLength the new length of the file\n* @return false, always throws an exception\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory, enforcing read-only restrictions.\n* @param src source Path to rename\n* @param dst destination Path for the new name\n* @throws AccessControlException if operation is not permitted\n* @throws IOException for I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symlink at the specified path if allowed.\n* @param target the target path for the symlink\n* @param link the link path to create\n* @param createParent indicates if parent directories should be created\n* @throws AccessControlException if operation is not permitted\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner of the given path.\n* @param f the Path to set ownership for\n* @param username the new owner's username\n* @param groupname the new owner's group name\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets permissions for a given path after validating it.\n* @param f the Path to set permissions on\n* @param permission the desired FsPermission to apply\n* @throws AccessControlException if the operation is not allowed\n* @throws IOException for I/O issues\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets replication factor for a path.\n* @param f the Path to set replication for\n* @param replication the new replication factor\n* @throws AccessControlException if the operation is not permitted\n* @throws IOException for I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets the modification and access times of a path.\n* @param f the Path to update, mtime modification time, atime access time\n* @throws AccessControlException if the operation is not permitted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for the specified path.\n* @param path the Path to modify ACL entries for\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if path validation fails or operation is read-only\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes ACL entries for the specified path.\n* @param path the Path object to modify\n* @param aclSpec list of AclEntry objects to remove\n* @throws IOException if path is invalid or read-only\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes default ACL for the given path.\n* @param path the Path object to process\n* @throws IOException if an error occurs during ACL removal\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL for the specified path.\n* @param path the Path object to modify\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets ACL for the specified path, throwing an exception if not allowed.\n* @param path the Path to set ACL on\n* @param aclSpec list of AclEntry specifications\n* @throws IOException if ACL setting fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute on a specified path.\n* @param path the target file path\n* @param name the attribute name\n* @param value the attribute value\n* @param flag the set flags\n* @throws IOException if path is invalid or operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes an extended attribute from a path.\n* @param path the Path object to modify\n* @param name the name of the attribute to remove\n* @throws IOException if the operation is not permitted\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot at the specified path.\n* @param path the directory path for the snapshot\n* @param snapshotName the name of the snapshot\n* @throws IOException if path is not valid or read-only\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot at the specified path.\n* @param path the Path object for the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName new name for the snapshot\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot at the specified path.\n* @param path the Path object for the snapshot location\n* @param snapshotName the name of the snapshot to delete\n* @throws IOException if the operation is read-only\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Throws an AccessControlException for a read-only storage policy operation.\n* @param path the path involved in the operation\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a given path, throwing an exception if read-only.\n* @param path the path to set the storage policy for\n* @param policyName the name of the storage policy to apply\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults()": "/**\n* Retrieves default filesystem server configuration settings.\n* @return FsServerDefaults object with server configuration values\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server configuration settings.\n* @param f filesystem path (unused in this implementation)\n* @return FsServerDefaults object with server configuration values\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)": "/**** Initializes InternalDirOfViewFs with directory info and configuration parameters.  \n* @param dir directory inode structure  \n* @param cTime creation time in milliseconds  \n* @param ugi user group information  \n* @param uri URI for the filesystem  \n* @param fsState filesystem state  \n* @param conf configuration settings  \n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Creates a file at the specified path with given parameters.\n* @param f path to create the file\n* @param flag creation flags\n* @param absolutePermission permissions for the file\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize size of the blocks\n* @param progress progress indicator\n* @param checksumOpt checksum options\n* @param createParent whether to create parent directories\n* @return FSDataOutputStream for the created file\n* @throws various exceptions for file creation errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a file, checking fallback if necessary.\n* @param f file path, @param start start offset, @param len length of the block\n* @return array of BlockLocation objects\n* @throws FileNotFoundException if the path is a directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory with specified permissions, optionally creating parent directories.\n* @param dir directory path to create\n* @param permission desired permissions for the new directory\n* @param createParent whether to create parent directories if they don't exist\n* @throws IOException if an I/O error occurs or if the operation is not permitted\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the enclosing root Path for the specified Path.\n* @param path the Path to analyze\n* @return the enclosing root Path\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus for a given path.\n* @param f the Path object to check\n* @return FileStatus object with file attributes\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/***************************************\n* Retrieves the FileStatus of a file link.\n* @param f the Path of the file link\n* @return FileStatus object representing the link or a default status if not found\n****************************************/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves AclStatus for the given path.\n* @param path the Path object to check\n* @return AclStatus instance with owner, group, and permissions\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a file link.\n* @param f the Path of the file link\n* @return Path of the link target\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink()": "/**\n* Lists file statuses for the root fallback link if available.\n* @return array of FileStatus or empty if no fallback link\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path, including links and internal directories.\n* @param f the Path to list statuses for\n* @return array of FileStatus objects for the specified path\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir": {
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getChildren()": "/**\n* Retrieves an unmodifiable view of the children map.\n* @return a map of child nodes indexed by their names\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isRoot()": "/**\n* Checks if the current user has root privileges.\n* @return true if user is root, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addFallbackLink(org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)": "/**\n* Sets a fallback link if the current node is root.\n* @param link the fallback link to be added\n* @throws IOException if not called on root node\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:resolveInternal(java.lang.String)": "/**\n* Resolves and retrieves child node by path component.\n* @param pathComponent the key to locate the child node\n* @return INode corresponding to the pathComponent or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDirLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink)": "/**\n* Adds a directory link to the children map.\n* @param pathComponent key for the directory link\n* @param dirLink directory link to be added\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:setInternalDirFs(java.lang.Object)": "/**\n* Sets the internal directory filesystem.\n* @param internalDirFs the filesystem to be set\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isInternalDir()": "/**\n* Indicates if the directory is internal.\n* @return true if the directory is internal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:setRoot(boolean)": "/**\n* Sets the root status of the object.\n* @param root true if the object is a root, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getInternalDirFs()": "/**\n* Retrieves the internal directory file system.\n* @return internalDirFs of type T\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getFallbackLink()": "/**\n* Retrieves the fallback link.\n* @return INodeLink<T> representing the fallback link\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**** Constructs an INodeDir with the specified path and user group information. */",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)": "/**\n* Adds a link at the specified path component.\n* @param pathComponent the path where the link is added\n* @param link the INodeLink to be added\n* @throws FileAlreadyExistsException if the path already exists\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Adds a directory if it doesn't exist.\n* @param pathComponent name of the directory to add\n* @param aUgi user group information for the new directory\n* @return the created INodeDir object\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink": {
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetFileSystem()": "/**\n* Retrieves the target FileSystem, initializing it if necessary.\n* @return Target FileSystem instance\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetFileSystemForClose()": "/**\n* Retrieves the target file system for closure.\n* @return target file system of type T\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:isInternalDir()": "/**\n* Checks if the directory is internal.\n* @return false, indicating it's not an internal directory\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])": "/**\n* Constructs an INodeLink with specified path, user info, target FS, and directory links.\n* @param pathToNode the full path to the node\n* @param aUgi user group information for access control\n* @param targetMergeFs target file system for the link\n* @param aTargetDirLinkList array of target directory links\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)": "/**\n* Initializes an INodeLink with path, user info, and target directory.\n* @param pathToNode the full path to the node\n* @param aUgi user group information for access control\n* @param createFileSystemMethod function to create a file system\n* @param aTargetDirLink target directory link as a string\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink()": "/**\n* Constructs a Path from concatenated target directory links.\n* @return Path object representing the merged target links\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$INode": {
        "org.apache.hadoop.fs.viewfs.InodeTree$INode:isLink()": "/**\n* Checks if the current object is a link (not an internal directory).\n* @return true if it is a link, false if it is an internal directory\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INode:getLink()": "/**\n* Retrieves the link of type INodeLink<T>.\n* @return INodeLink<T> instance or null if no link exists\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INode:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Constructs an INode with the specified path and user group information.\n* @param pathToNode the full path to the node\n* @param aUgi user group information for access control\n*/"
    },
    "org.apache.hadoop.fs.permission.AclStatus$Builder": {
        "org.apache.hadoop.fs.permission.AclStatus$Builder:owner(java.lang.String)": "/**\n* Sets the owner and returns the Builder instance.\n* @param owner the owner name to set\n* @return the Builder for method chaining\n*/",
        "org.apache.hadoop.fs.permission.AclStatus$Builder:group(java.lang.String)": "/**\n* Sets the group name for the builder.\n* @param group the name of the group\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.permission.AclStatus$Builder:addEntries(java.lang.Iterable)": "/**\n* Adds multiple ACL entries to the builder.\n* @param entries collection of ACL entries to add\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.permission.AclStatus$Builder:stickyBit(boolean)": "/**\n* Sets the sticky bit flag and returns the builder instance.\n* @param stickyBit true to enable, false to disable sticky bit\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.permission.AclStatus$Builder:build()": "/**\n* Constructs an AclStatus object with specified attributes.\n* @return AclStatus instance with owner, group, and permissions\n*/",
        "org.apache.hadoop.fs.permission.AclStatus$Builder:addEntry(org.apache.hadoop.fs.permission.AclEntry)": "/**\n* Adds an AclEntry to the builder and returns the builder instance.\n* @param e the AclEntry to add\n* @return the current Builder instance\n*/"
    },
    "org.apache.hadoop.fs.viewfs.NotInMountpointException": {
        "org.apache.hadoop.fs.viewfs.NotInMountpointException:<init>(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Constructs an exception with a message about an operation outside a mount point.\n* @param path the path being accessed\n* @param operation the operation attempted on the path\n*/",
        "org.apache.hadoop.fs.viewfs.NotInMountpointException:<init>(java.lang.String)": "/**\n* Constructs an exception with a message for invalid operations on an empty path.\n* @param operation the invalid operation attempted\n*/"
    },
    "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType": {
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:get(java.lang.String)": "/**\n* Retrieves the interceptor type by configuration name.\n* @param configName the name of the configuration\n* @return RegexMountPointInterceptorType associated with the configName or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:getConfigName()": "/**\n* Retrieves the configuration name.\n* @return the name of the configuration as a String\n*/"
    },
    "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode": {
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:nflyStatus()": "/**\n* Retrieves the status of the Nfly file system.\n* @return FileStatus object representing the current Nfly status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode)": "",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object)": "/**\n* Checks equality of MRNflyNode instances.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode()": "/**\n* Returns the object's hash code, satisfying FindBugs requirements.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode)": "/**\n* Constructs an MRNflyNode from an existing NflyNode.\n* @param n source NflyNode to copy properties from\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus()": "/**\n* Clones the current FileStatus object.\n* @return a new FileStatus instance with the same attributes\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Updates file status based on the given path.\n* @param f the file path to update status for\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory": {
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:<init>()": "/**\n* Private constructor for RegexMountPointInterceptorFactory.\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String)": "/**\n* Creates a RegexMountPointInterceptor from settings string.\n* @param interceptorSettingsString settings for the interceptor\n* @return RegexMountPointInterceptor or null if invalid settings\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:<init>(org.apache.hadoop.fs.viewfs.FsGetter)": "/**\n* Initializes InnerCache with a FsGetter instance.\n* @param fsCreator instance of FsGetter for cache operations\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:clear()": "/**\n* Clears the map while ensuring exclusive write access.\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll()": "/**\n* Closes all file systems in the map, logging failures.\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves or creates a FileSystem instance for the given URI and configuration.\n* @param uri the URI of the filesystem\n* @param config configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if an error occurs during retrieval or creation\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult": {
        "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isInternalDir()": "/**\n* Checks if the current result is an internal directory.\n* @return true if it's an internal directory, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isLastInternalDirLink()": "/**\n* Checks if the current link is the last internal directory link.\n* @return true if it is the last link, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:<init>(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.Object,java.lang.String,org.apache.hadoop.fs.Path,boolean)": "/**\n* Constructs a ResolveResult with specified parameters.\n* @param k ResultKind indicating the type of result\n* @param targetFs target file system of type T\n* @param resolveP resolved path as a String\n* @param remainingP remaining path as a Path object\n* @param isLastIntenalDirLink flag for last internal directory link\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree": {
        "org.apache.hadoop.fs.viewfs.InodeTree:getHomeDirPrefixValue()": "/**\n* Retrieves the home directory prefix value.\n* @return String representing the home directory prefix\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:getMountPoints()": "/**\n* Retrieves the list of mount points.\n* @return List of MountPoint objects\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:isRootInternalDir()": "/**\n* Checks if the root directory is an internal directory.\n* @return true if root is internal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:breakIntoPathComponents(java.lang.String)": "/**\n* Splits a file path into its components.\n* @param path the file path to split\n* @return array of path components or null if path is null\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:checkMntEntryKeyEqualsTarget(java.lang.String,java.lang.String)": "/**\n* Validates if the mount entry key matches the target key.\n* @param mntEntryKey the key to check\n* @param targetMntEntryKey the expected key\n* @throws IOException if keys do not match\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:getLinkEntries(java.util.List)": "/**\n* Sorts and returns unique LinkEntry objects by their source.\n* @param linkEntries list of LinkEntry objects\n* @return sorted collection of unique LinkEntry objects\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:hasFallbackLink()": "/**\n* Checks if a fallback link is present.\n* @return true if rootFallbackLink is not null, otherwise false\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)": "/**\n* Builds a ResolveResult for a regex mount point.\n* @param resultKind type of result; @param resolvedPathStr resolved path as a String;\n* @param targetOfResolvedPathStr target path as a String; @param remainingPath remaining path\n* @return ResolveResult object or null if initialization fails\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)": "/**\n* Constructs a LinkEntry from configuration and mount entry details.\n* @param config configuration settings\n* @param ugi user group information\n* @param mntEntryStrippedKey key for mount entry\n* @param mntEntryValue value for mount entry\n* @return LinkEntry object based on provided parameters\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:getRootDir()": "/**\n* Retrieves the root directory after validating its internal status.\n* @return INodeDir<T> root directory casted to INodeDir type\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:getRootLink()": "/**\n* Retrieves the root link of the node.\n* @return INodeLink<T> representing the root node\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink()": "/**\n* Returns the root fallback link if the root is an internal directory.\n* @return INodeLink<T> root fallback link\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry)": "/**\n* Adds a regex mount entry from LinkEntry.\n* @param le LinkEntry containing source, target, and settings\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a link from src to target with specified linkType and settings.\n* @param src source path for the link\n* @param target target path for the link\n* @param linkType type of link to create\n* @param settings additional settings for link creation\n* @param aUgi user group information\n* @param config configuration settings\n* @throws URISyntaxException if the target URI is malformed\n* @throws IOException if an I/O error occurs\n* @throws FileAlreadyExistsException if the link already exists\n* @throws UnsupportedFileSystemException if the file system is unsupported\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)": "/**\n* Constructs a Path from the remaining path segments starting at index.\n* @param path array of path segments\n* @param startIndex index to start from\n* @return Path object representing the remaining path\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)": "/**\n* Attempts to resolve a path in regex mount points.\n* @param srcPath path to resolve; @param resolveLastComponent flag for last component inclusion\n* @return ResolveResult object or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)": "/**\n* Resolves a path and returns the corresponding ResolveResult.\n* @param p path string to resolve; @param resolveLastComponent flag for last component inclusion\n* @return ResolveResult object containing resolution details\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)": "/**\n* Initializes InodeTree with configuration and URI, setting up mount table and links.\n* @param config configuration settings\n* @param viewName name of the view for the mount table\n* @param theUri URI for fallback link if no mounts are found\n* @param initingUriAsFallbackOnNoMounts flag for fallback initialization\n* @throws UnsupportedFileSystemException if the filesystem is unsupported\n* @throws URISyntaxException if the URI is malformed\n* @throws FileAlreadyExistsException if the link already exists\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus": {
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:stripRoot()": "/**\n* Returns the stripped root string.\n* @return stripped root as a String\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime()": "",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath()": "/**\n* Retrieves the current path from the realStatus object.\n* @return Path object representing the current path\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication()": "/**\n* Returns the block replication factor.\n* @return short representing the replication factor\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize()": "",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime()": "",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission()": "/**\n* Retrieves the file system permissions.\n* @return FsPermission object representing the permissions\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner()": "/**\n* Returns the owner's name from the realStatus object.\n* @return String representing the owner's name\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup()": "/**\n* Returns the group associated with the instance.\n* @return the group as a String\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory()": "/**\n* Checks if the current object is a directory.\n* @return true if it is a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink()": "",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen()": "/**\n* Retrieves the length value from realStatus.\n* @return the length as a long\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path)": "/**\n* Sets the symlink path using the provided Path object.\n* @param p Path to be set as the symlink\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path)": "/**\n* Sets the path for the real status object.\n* @param p the Path object to be set\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile()": "/**\n* Checks if the current status represents a file.\n* @return true if it's a file, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink()": "/**\n* Retrieves the symbolic link path.\n* @return Path of the symlink from realStatus\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object)": "/**\n* Compares this FileStatus object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode()": "/**\n* Returns the hash code of the object based on its realStatus.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString()": "/**\n* Returns string representation of the object's realStatus.\n* @return formatted string of realStatus properties\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)": "/**\n* Constructs NflyStatus with file system and status.\n* @param realFs the file system to process\n* @param realStatus the file status to initialize from\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ChRootedFileSystem": {
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getMyFs()": "/**\n* Retrieves the raw file system instance.\n* @return FileSystem object representing the raw file system\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getUri()": "/**\n* Retrieves the URI associated with this instance.\n* @return URI object representing the instance's URI\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getWorkingDirectory()": "/**\n* Returns the current working directory path.\n* @return Path representing the working directory\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory, resolving relative paths against the current directory.\n* @param new_dir the new directory to set as working directory\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)": "/**\n* Initializes ChRootedFileSystem with a URI and sets up the root path.\n* @param fs the underlying FileSystem\n* @param uri the URI defining the chroot environment\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)": "/**\n* Resolves and qualifies a given Path.\n* @param f the Path to resolve\n* @return qualified Path object\n* @throws FileNotFoundException if the path is invalid\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path)": "/**\n* Strips root path from a given Path object.\n* @param p the Path to process\n* @return the path without the root part\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the filesystem with URI and configuration settings.\n* @param name URI for filesystem initialization\n* @param conf configuration object for settings\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path)": "/**\n* Constructs full path based on input and current directory.\n* @param path the input Path to resolve\n* @return the resolved full Path\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file with specified permissions and options.\n* @param f file path to create\n* @param permission file permissions\n* @param overwrite flag to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @param progress callback for progress updates\n* @return FSDataOutputStream for writing to the file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "/**** Creates a non-recursive file output stream. \n* @param f the Path to create\n* @param permission file permissions\n* @param flags creation flags\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @param progress progress callback\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory, optionally recursively.\n* @param f path to the file or directory to delete\n* @param recursive true to delete contents recursively\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)": "/**\n* Retrieves block locations for a file within a specified byte range.\n* @param fs FileStatus object representing file details\n* @param start starting byte position\n* @param len length of the range\n* @return array of BlockLocation or empty if no blocks found\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file checksum for the specified path.\n* @param f file path\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)": "/**\n* Retrieves the checksum of a file at the specified path.\n* @param f file path to check\n* @param length expected file length\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file at the specified path.\n* @param f the file path to check\n* @return FileStatus object representing the file's status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a symbolic link.\n* @param f the path of the symbolic link\n* @return target Path of the symbolic link\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves filesystem status for the specified path.\n* @param p the path to check status for\n* @return FsStatus object containing filesystem details\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for the given path.\n* @param f the path to list file statuses\n* @return an array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses in the specified directory.\n* @param f directory path to list files from\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory and its parents with specified permissions.\n* @param f path of the directory to create\n* @param permission permissions for the new directory\n* @return true if created, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path)": "/**\n* Creates directories specified by the given path.\n* @param f path to the directory to create\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f the path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param f file path to append data to\n* @param bufferSize size of the buffer for the operation\n* @param progress callback for progress updates\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory from src to dst in the same FileSystem.\n* @param src source path to rename\n* @param dst destination path\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner of the specified file or directory.\n* @param f the Path of the file or directory\n* @param username the new owner's username\n* @param groupname the new owner's group name\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file system permissions for the given path.\n* @param f the path to set permissions on\n* @param permission the permissions to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified file path.\n* @param f the source path to set replication for\n* @param replication the desired replication factor\n* @return true if operation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets file modification and access times using the resolved full path.\n* @param f the file path to modify\n* @param mtime the new modification time\n* @param atime the new access time\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for a specified file path.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes specified ACL entries from the given file system path.\n* @param path the file system path\n* @param aclSpec list of ACL entries to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the default ACL for the specified path.\n* @param path the input Path to modify\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the ACL from the specified file path.\n* @param path the file path whose ACL is removed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for a specified file path.\n* @param path the file path for which ACL status is requested\n* @return AclStatus object\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a specified file path.\n* @param path file path to set the attribute on\n* @param name name of the extended attribute\n* @param value byte array of the attribute value\n* @param flag set flags for attribute modification\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attribute of a file.\n* @param path the file path\n* @param name the attribute name\n* @return byte array of attribute value\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a specified file system path.\n* @param path the file system path to query\n* @return map of attribute names to their byte values\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a specified path.\n* @param path the file path to query\n* @param names list of attribute names to retrieve\n* @return map of attribute names and their byte values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to the specified length using its resolved full path.\n* @param path the input file path to truncate\n* @param newLength the new length of the file\n* @return true if truncation is successful\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path)": "/**** \n* Lists extended attributes for a given file or directory path.\n* @param path the file or directory path\n* @return List of extended attribute names\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes an extended attribute from a specified file path.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot at the specified path with given name.\n* @param path the path to snapshot\n* @param name the name of the snapshot\n* @return Path of the created snapshot\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot in the file system.\n* @param path path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n* @throws IOException if the rename operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot by name at the specified directory path.\n* @param snapshotDir directory containing the snapshot\n* @param snapshotName name of the snapshot to delete\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves the filesystem path from the given Path object.\n* @param p the Path object to resolve\n* @return resolved Path object\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path)": "/**\n* Retrieves content summary for a specified file or directory.\n* @param f Path of the file or directory\n* @return ContentSummary object with aggregated counts and length\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)": "/**\n* Retrieves quota usage for the specified file or directory.\n* @param f Path of the file or directory\n* @return QuotaUsage object based on content summary\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)": "/**\n* Returns the default block size for the given file path.\n* @param f the file path to get the block size for\n* @return default block size in bytes\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)": "/**\n* Returns the default replication factor for a given file path.\n* @param f the file path to check replication for\n* @return short representing the default replication factor\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves storage policy for a given path.\n* @param src the source path for which the policy is requested\n* @return BlockStoragePolicySpi object or throws IOException if unsupported\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Applies storage policy to the specified path.\n* @param src the path to apply storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a specified path.\n* @param src source path to set policy for\n* @param policyName name of the storage policy\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for the specified path.\n* @param src the path for which to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path)": "/**\n* Creates a file output stream builder at the specified path.\n* @param path destination Path for output stream\n* @return FSDataOutputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file storage defaults for a specified path.\n* @param f the file path to retrieve defaults for\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if a path supports a specific capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path)": "/**\n* Deletes a file or directory recursively.\n* @param f path to the file or directory to delete\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize()": "/**\n* Retrieves the default block size for the root path.\n* @return default block size in bytes\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication()": "/**\n* Retrieves the default replication factor for the root path.\n* @return short representing the default replication factor\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults()": "/**\n* Retrieves server defaults using the resolved root path.\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path)": "/**\n* Opens a file using the resolved full path.\n* @param path the input Path to resolve\n* @return FutureDataInputStreamBuilder for the file\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a specified path and action mode.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n*/",
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs ChRootedFileSystem with the specified URI and configuration.\n* @param uri the URI defining the chroot environment\n* @param conf configuration settings for the filesystem\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry": {
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getSrc()": "/**\n* Retrieves the source string.\n* @return source string value\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a LinkEntry with source, target, link type, settings, user info, and configuration.\n* @param src source identifier\n* @param target target identifier\n* @param linkType type of the link\n* @param settings configuration settings\n* @param ugi user group information\n* @param config configuration details\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getLinkType()": "/**\n* Retrieves the current link type.\n* @return LinkType representing the current link type\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getTarget()": "/**\n* Retrieves the target string.\n* @return the target string value\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getSettings()": "/**\n* Retrieves the current settings as a String.\n* @return settings configuration string\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getUgi()": "/**\n* Retrieves the UserGroupInformation object.\n* @return UserGroupInformation instance\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getConfig()": "/**\n* Retrieves the current configuration object.\n* @return Configuration instance representing current settings\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:<init>(java.lang.String)": "/**\n* Initializes ChildFsGetter with the specified root scheme.\n* @param rootScheme the root scheme to be set\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:newInstance(java.lang.Class,java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a new instance of the specified class.\n* @param theClass class type to instantiate\n* @param uri unused URI parameter\n* @param conf unused Configuration parameter\n* @return new instance of theClass\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates and initializes a FileSystem instance.\n* @param uri the URI for the file system\n* @param conf configuration settings for the file system\n* @return initialized FileSystem object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Returns a FileSystem instance based on URI and configuration.\n* @param uri the URI for the file system\n* @param conf configuration settings for the file system\n* @return initialized FileSystem object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a FileSystem based on the URI and configuration.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if filesystem initialization fails\n*/"
    },
    "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader": {
        "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:logInvalidFileNameFormat(java.lang.String)": "/**\n* Logs a warning for invalid file name format of the mount-table version file.\n* @param cur the current file name being validated\n*/",
        "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Loads mount table configuration from the specified path.\n* @param mountTableConfigPath path to the mount table file\n* @param conf configuration to add loaded properties\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getScheme()": "/**\n* Retrieves the scheme component of the URI.\n* @return scheme as a String, or null if not present\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:canonicalizeUri(java.net.URI)": "/**\n* Canonicalizes the given URI.\n* @param uri the URI to be canonicalized\n* @return the canonicalized URI\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter()": "/**\n* Creates an instance of ChildFsGetter using the scheme from the URI.\n* @return FsGetter instance initialized with the scheme\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem()": "/**\n* Retrieves the fallback FileSystem if available.\n* @return FileSystem instance or null if not present or on error\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves MountTableConfigLoader instance from configuration.\n* @param conf configuration containing class details\n* @return MountTableConfigLoader instance\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ViewFileSystemOverloadScheme with URI and configuration settings.\n* @param theUri URI for initialization\n* @param conf configuration object containing settings\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>()": "/**\n* Constructs ViewFileSystemOverloadScheme, initializing with user info.\n* @throws IOException if user info retrieval fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the raw FileSystem for a given path and configuration.\n* @param path the path to resolve\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance or throws NotInMountpointException if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves mount path info for a given path and configuration.\n* @param path the path to resolve\n* @param conf configuration settings for the filesystem\n* @return MountPathInfo containing resolved path and FileSystem instance\n* @throws IOException if an error occurs during resolution\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$MountPathInfo": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$MountPathInfo:<init>(org.apache.hadoop.fs.Path,java.lang.Object)": "/**\n* Initializes MountPathInfo with target path and filesystem.\n* @param pathOnTarget the path on the target filesystem\n* @param targetFs the target filesystem object\n*/"
    },
    "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream": {
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:osException(int,java.lang.String,java.lang.Throwable,java.util.List)": "/**\n* Handles OS exceptions for a specific operation.\n* @param i index of the operation\n* @param op operation name\n* @param t throwable encountered\n* @param ioExceptions list to collect IOExceptions\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List)": "/**\n* Throws IOException if opSet's cardinality is below minReplication.\n* @param ioExceptions list of IOException instances\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int)": "/**\n* Writes an integer to multiple output streams, handling exceptions.\n* @param d integer to write\n* @throws IOException if any IOExceptions are encountered\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)": "/**\n* Writes byte array to output streams, collecting IOExceptions.\n* @param bytes data to write, @param offset start position, @param len number of bytes to write\n* @throws IOException if any IOExceptions are encountered\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush()": "/**\n* Flushes output streams and collects IOExceptions.\n* @throws IOException if any IOExceptions occurred during flushing\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles()": "/**\n* Cleans up all temporary files from output streams.\n* @throws IOException if file deletion fails\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit()": "/**\n* Commits file operations, handling IOExceptions and setting timestamps.\n* @throws IOException if multiple IOExceptions occur\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close()": "/**\n* Closes output streams, handling exceptions and ensuring minimum replication.\n* @throws IOException if replication fails or I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.FilterFileSystem": {
        "org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file at the specified path with given permissions and options.\n* @param f file path to create\n* @param permission file permissions\n* @param overwrite flag to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @param progress callback for progress updates\n* @return FSDataOutputStream for writing to the file\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory.\n* @param f path to the file or directory to delete\n* @param recursive true to delete directory contents recursively\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file at the given path.\n* @param f the file path to check\n* @return FileStatus object representing the file's status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists the status of files in the specified path.\n* @param f the path to list file statuses\n* @return an array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory and any necessary parent directories.\n* @param f path of the directory to create\n* @param permission permissions for the new directory\n* @return true if the directory was created, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f the path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param f the file path to append data to\n* @param bufferSize size of the buffer for the operation\n* @param progress callback for progress updates\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory from src to dst.\n* @param src source path to rename\n* @param dst destination path\n* @return true if rename succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getUri()": "/**\n* Retrieves the URI from the file system.\n* @return URI of the file system\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory for the file system.\n* @param newDir the new directory path to set\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getWorkingDirectory()": "/**\n* Retrieves the current working directory path.\n* @return Path of the current working directory\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getConf()": "/**\n* Retrieves the configuration settings for the file system.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getChildFileSystems()": "/**\n* Returns an array of child file systems.\n* @return array containing child FileSystem objects\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getRawFileSystem()": "/**\n* Retrieves the raw file system instance.\n* @return FileSystem object representing the raw file system\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:msync()": "/**\n* Synchronizes file system changes to storage.\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if msync is not supported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication()": "",
        "org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Creates a file with specified parameters.\n* @param f file path, @param permission access rights,\n* @param flags creation options, @param bufferSize size of buffer,\n* @param replication number of replicas, @param blockSize size of blocks,\n* @param progress callback for progress, @param checksumOpt checksum options\n* @return FSDataOutputStream for writing to the file\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)": "",
        "org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "",
        "org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)": "/**\n* Returns an iterator for file statuses in the specified directory.\n* @param f directory path\n* @return RemoteIterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file system permissions for the specified path.\n* @param p the path to set permissions on\n* @param permission the permissions to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory()": "/**\n* Returns the initial working directory path.\n* @return Path object or null if unavailable\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file link.\n* @param f the path to the file\n* @return FileStatus of the specified file\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target of a symbolic link.\n* @param f the path of the symbolic link\n* @return the target Path of the symbolic link\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to the specified length.\n* @param f the file path to truncate\n* @param newLength the new length of the file\n* @return true if truncation is successful\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified source path.\n* @param src the source path to set replication for\n* @param replication the desired replication factor\n* @return true if operation succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a specified file path.\n* @param p the file path to modify\n* @param mtime the new modification time\n* @param atime the new access time\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean)": "/**\n* Sets the verifyChecksum flag for file system operations.\n* @param verifyChecksum flag to enable or disable checksum verification\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks()": "/**\n* Checks if the filesystem supports symbolic links.\n* @return true if symlinks are supported, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/****\n* Creates a symbolic link to a target path.\n* @param target the path to the target file\n* @param link the path where the symlink will be created\n* @param createParent flag to create parent directories if needed\n* @throws IOException for I/O errors during link creation\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])": "/**\n* Concatenates source files into a target file.\n* @param f target file path\n* @param psrcs array of source file paths\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes an extended attribute from a specified file path.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a specified file system path.\n* @param path the file system path to query\n* @return map of attribute names to their byte values\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves the extended attribute of a file.\n* @param path the file path\n* @param name the attribute name\n* @return byte array of attribute value\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot in the file system.\n* @param path path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n* @throws IOException if the rename operation fails\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for a specified file path.\n* @param path the file path for which ACL status is requested\n* @return AclStatus object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the ACL from the specified file path.\n* @param path the file path whose ACL is removed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for a specified file path.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes specified ACL entries from the given file system path.\n* @param path the file system path\n* @param aclSpec list of ACL entries to remove\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot of the specified path.\n* @param path the path to snapshot\n* @param snapshotName the name of the snapshot\n* @return Path of the created snapshot\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot at the specified path by name.\n* @param path location of the snapshot\n* @param snapshotName name of the snapshot to delete\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)": "/**\n* Opens a file stream with specified buffer size.\n* @param fd file descriptor handle\n* @param bufferSize size of the buffer for the stream\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory with specified permissions.\n* @param f path of the directory to create\n* @param absolutePermission permissions for the directory\n* @return true if created, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)": "/**\n* Lists corrupt file blocks in the specified directory.\n* @param path directory path to check for corrupt files\n* @return RemoteIterator of Path objects representing corrupt file blocks\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)": "/**\n* Lists located file statuses in a directory.\n* @param f directory path to list files from\n* @param filter filter to apply on listed files\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path)": "/**\n* Resolves a symbolic link for the given path.\n* @param f the path to resolve\n* @return resolved Path object\n* @throws IOException if an I/O error occurs or symlinks are unsupported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)": "/**\n* Retrieves the checksum of a file.\n* @param f file path\n* @param length expected file length\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a specified file path.\n* @param path file path to set the attribute on\n* @param name name of the extended attribute\n* @param value byte array of the attribute value\n* @param flag set flags for attribute modification\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a specified path.\n* @param path the file path to query\n* @param names list of attribute names to retrieve\n* @return map of attribute names and their byte values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Lists extended attributes for a given file or directory path.\n* @param path the file or directory path\n* @return List of extended attribute names\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Applies storage policy to the specified path. \n* @param src the path to apply storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a specified path.\n* @param src source path to set policy for\n* @param policyName name of the storage policy\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for a specified path.\n* @param src the path for which to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the storage policy for a given path.\n* @param src the source path for which the policy is requested\n* @return BlockStoragePolicySpi object or throws IOException if unsupported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies()": "/**\n* Retrieves all storage policies.\n* @return collection of BlockStoragePolicySpi or throws IOException if unsupported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Initiates local output process for specified files.\n* @param fsOutputFile path for output file in file system\n* @param tmpLocalFile path for temporary local file\n* @return temporary local file path\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean)": "/**\n* Sets the writeChecksum flag for the filesystem.\n* @param writeChecksum indicates if checksum should be written\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the default replication factor for a given file path.\n* @param f the file path to check replication for\n* @return short representing the default replication factor\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI)": "/**\n* Canonicalizes the given URI using the filesystem's method.\n* @param uri the URI to canonicalize\n* @return modified URI with default port, if applicable\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file system status for a given path.\n* @param p the path to check status for\n* @return FsStatus object containing filesystem details\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses in the specified directory.\n* @param f directory path to list files from\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file checksum for the specified path.\n* @param f file path\n* @return FileChecksum object or null if not found\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])": "/**\n* Sets an extended attribute for a specified file path.\n* @param path file path to set the attribute on\n* @param name name of the extended attribute\n* @param value byte array of the attribute value\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options.\n* @param path file path to open\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options.\n* @param pathHandle file path handle\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:close()": "/**\n* Closes the file system and releases resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri()": "/**\n* Retrieves the canonical URI from the filesystem.\n* @return canonicalized URI or null if invalid\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:<init>()": "/**\n* Constructs a FilterFileSystem instance.\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Initializes FilterFileSystem with a given FileSystem instance.\n* @param fs the FileSystem to filter\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path)": "/**\n* Creates a file output stream builder at the specified path.\n* @param path destination Path for output stream\n* @return FSDataOutputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path)": "/**\n* Appends a file at the specified path.\n* @param path the destination Path for the output stream\n* @return FSDataOutputStreamBuilder for further configuration\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Creates a file or appends to it using specified parameters.\n* @param f file path to create or append\n* @param absolutePermission permissions for the file\n* @param flag creation options\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize size of the blocks\n* @param progress progress callback\n* @param checksumOpt checksum options\n* @return FSDataOutputStream for the created file\n* @throws IOException if an error occurs during creation\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the used length of content at the specified path.\n* @param path location of the file or directory\n* @return length of used content as a long value\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])": "/**\n* Creates a PathHandle from FileStatus with optional handle options.\n* @param stat file status information\n* @param opts optional handle options\n* @return PathHandle object based on provided parameters\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path)": "/**\n* Creates directories specified by the path.\n* @param f path to the directory to create\n* @return true if directories were created, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getUsed()": "/**\n* Retrieves the used length of content at the root path.\n* @return length of used content as a long value\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)": "/**\n* Retrieves block locations for a specified file range.\n* @param file the file to retrieve locations from\n* @param start starting byte position\n* @param len length of the range\n* @return array of BlockLocation or empty if no blocks found\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])": "/**\n* Renames a file or directory using specified options.\n* @param src source path to rename\n* @param dst destination path\n* @param options rename options, e.g., overwrite\n* @throws IOException if an error occurs during renaming\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize()": "/**\n* Retrieves the default block size in bytes from the file system.\n* @return default block size, defaulting to 32MB if not configured\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the filesystem with URI and configuration.\n* @param name URI for filesystem initialization\n* @param conf configuration object for settings\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path)": "/**\n* Validates the given filesystem path.\n* @param path the Path object to check\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getServerDefaults()": "/**\n* Retrieves file storage defaults.\n* @return FsServerDefaults object from the filesystem\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the default block size for the specified file path.\n* @param f the file path to get the block size for\n* @return default block size in bytes\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path)": "/**\n* Returns a qualified Path, optionally swapping its scheme.\n* @param path the Path to qualify\n* @return a qualified Path with potentially updated scheme\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves the filesystem path from the given Path object.\n* @param p the Path object to resolve\n* @return resolved Path object\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file storage defaults for a given path.\n* @param f the file path to retrieve defaults for\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Returns the enclosing root Path for a given input Path.\n* @param path the Path to qualify\n* @return qualified root Path\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if a path supports a specific capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path)": "/**\n* Opens a file and returns a FutureDataInputStreamBuilder.\n* @param path the path of the file to open\n* @return FutureDataInputStreamBuilder for the file\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle)": "/**** Opens a file for reading using the given path handle. \n* @param pathHandle the handle of the file to open \n* @return FutureDataInputStreamBuilder for file operations \n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory()": "/**\n* Returns the home directory path for the current user.\n* @return qualified Path object for the user's home directory\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the user's trash directory path.\n* @param path unused in this method\n* @return qualified Path for the trash directory\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean)": "/**\n* Retrieves trash roots for users.\n* @param allUsers flag to include all users' trash\n* @return collection of FileStatus for trash directories\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a given path and action mode.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)": "/**\n* Copies files from local filesystem to a destination.\n* @param delSrc flag to delete source files after copy\n* @param overwrite flag to overwrite existing files\n* @param srcs array of source Paths\n* @param dst destination Path\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local to HDFS.\n* @param delSrc delete source after copy if true\n* @param overwrite overwrite destination if true\n* @param src source file path\n* @param dst destination file path\n* @throws IOException if copy operation fails\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local to destination in HDFS.\n* @param delSrc flag to delete source after copy\n* @param src source Path\n* @param dst destination Path\n* @throws IOException if copy fails\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from HDFS to local filesystem.\n* @param delSrc flag to delete source after copy\n* @param src source Path\n* @param dst destination Path\n* @throws IOException if copy operation fails\n*/",
        "org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Completes local output by moving a temporary file to its final destination.\n* @param fsOutputFile destination Path for the output file\n* @param tmpLocalFile source Path of the temporary local file\n* @throws IOException if the move operation fails\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus": {
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getLen()": "/**\n* Retrieves the length of the file system.\n* @return length of the file system as a long value\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isFile()": "/**\n* Checks if the current instance represents a file.\n* @return true if it is a file, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isDirectory()": "/**\n* Checks if the current file system object is a directory.\n* @return true if it is a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isSymlink()": "/**\n* Checks if the current file is a symbolic link.\n* @return true if it is a symlink, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockSize()": "/**\n* Retrieves the block size of the filesystem.\n* @return long representing the block size in bytes\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getReplication()": "/**\n* Retrieves the replication factor from the file system.\n* @return short representing the replication factor\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getModificationTime()": "/**\n* Retrieves the last modification time of the file system.\n* @return long representing the modification time in milliseconds\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getAccessTime()": "/**\n* Retrieves the access time from the file system.\n* @return access time in milliseconds since epoch\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getPermission()": "/**\n* Retrieves the file system permissions.\n* @return FsPermission object representing the current permissions\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getOwner()": "/**\n* Retrieves the owner of the file system.\n* @return String representing the owner's name\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getGroup()": "/**\n* Retrieves the group associated with the file system.\n* @return String representing the group name\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getSymlink()": "/**\n* Retrieves the symbolic link path.\n* @return Path of the symbolic link\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:setSymlink(org.apache.hadoop.fs.Path)": "/**\n* Sets a symbolic link at the specified path.\n* @param p the path where the symlink will be created\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations()": "/**\n* Retrieves an array of block locations from the filesystem.\n* @return array of BlockLocation objects\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus)": "/**\n* Compares this FileStatus with another for order.\n* @param o the FileStatus to compare to\n* @return comparison result as an int\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode()": "/**\n* Returns the hash code of the current object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)": "/**\n* Constructs ViewFsLocatedFileStatus with file status and path.\n* @param locatedFileStatus file status information\n* @param path path associated with the file\n*/"
    },
    "org.apache.hadoop.fs.LocatedFileStatus": {
        "org.apache.hadoop.fs.LocatedFileStatus:getBlockLocations()": "/**\n* Retrieves an array of block locations.\n* @return array of BlockLocation objects\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:setBlockLocations(org.apache.hadoop.fs.BlockLocation[])": "/**\n* Sets the block locations.\n* @param locations array of BlockLocation objects\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus)": "/**\n* Compares this FileStatus with another for order.\n* @param o the FileStatus to compare to\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object)": "/**\n* Compares this FileStatus object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:hashCode()": "/**\n* Returns the hash code of the current object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])": "/**\n* Constructs a LocatedFileStatus with file attributes and block locations.\n* @param locations array of block location information\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])": "/**\n* Constructs a LocatedFileStatus with file attributes and block locations.\n* @param length file length, isdir indicates if it's a directory, locations array of block location info\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])": "/**\n* Constructs LocatedFileStatus from FileStatus and block locations.\n* @param stat FileStatus object for file attributes\n* @param locations array of block location info\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])": "/**\n* Constructs a LocatedFileStatus with file attributes and block locations.\n* @param length file length, isdir indicates if it's a directory, locations array of block location info\n*/",
        "org.apache.hadoop.fs.LocatedFileStatus:<init>()": "/**\n* Default constructor for LocatedFileStatus initializing with default values.\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFs$MountPoint": {
        "org.apache.hadoop.fs.viewfs.ViewFs$MountPoint:<init>(org.apache.hadoop.fs.Path,java.lang.String[])": "/**\n* Constructs a MountPoint with source path and target URIs.\n* @param srcPath the source path for the mount point\n* @param targetURIs array of target URIs to be mounted\n*/"
    },
    "org.apache.hadoop.fs.viewfs.NflyFSystem": {
        "org.apache.hadoop.fs.viewfs.NflyFSystem:workSet()": "/**\n* Creates a copy of MRNflyNode array from existing nodes.\n* @return array of MRNflyNode objects\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getRack(java.lang.String)": "/**\n* Returns the rack string or a default value if null.\n* @param rackString the input rack string\n* @return the rack string or default if null\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:mayThrowFileNotFound(java.util.List,int)": "/**\n* Throws FileNotFoundException if all nodes are not found.\n* @param ioExceptions list of IOExceptions encountered\n* @param numNotFounds count of nodes not found\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param f file path to append data\n* @param bufferSize size of the buffer for appending\n* @param progress callback for progress updates\n* @return FSDataOutputStream for writing, or null if failed\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getUri()": "/**\n* Retrieves the URI associated with this instance.\n* @return URI object representing the instance's URI\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])": "/**\n* Processes a Throwable and adds IOException to the list if provided.\n* @param nflyNode node context for the operation\n* @param op operation description\n* @param t throwable to process\n* @param ioExceptions list to collect IOExceptions\n* @param f file paths involved in the operation\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory()": "/**\n* Retrieves the working directory from the first node's filesystem.\n* @return Path representing the working directory\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses in the specified directory.\n* @param f directory path to list files from\n* @return RemoteIterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the file status for a given path, handling IOExceptions.\n* @param f the file path to check status for\n* @throws IOException if file not found or multiple IOExceptions occur\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates an FSDataOutputStream for the specified file path.\n* @param f file path, @param permission file permissions, @param overwrite allows overwriting,\n* @param bufferSize size of the buffer, @param replication number of replicas, \n* @param blockSize size of the block, @param progress callback for progress updates\n* @return FSDataOutputStream for writing data\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory for all NflyNodes.\n* @param newDir the new directory to set as working directory\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path)": "/**\n* Constructs a temporary Path based on the parent and name of the given Path.\n* @param f the original Path\n* @return new Path for temporary storage\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path)": "/**\n* Creates a FileStatus indicating a file was not found.\n* @param f the path of the file\n* @return FileStatus with default \"not found\" values\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for the given path, handling IOExceptions.\n* @param f the path to list file statuses\n* @return an array of FileStatus objects\n* @throws FileNotFoundException if the path is not found\n* @throws IOException if an IO error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories for all nodes with specified permissions.\n* @param f path of the directory to create\n* @param permission permissions for the new directory\n* @return true if all directories created, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory; throws IOException on failure.\n* @param src source path, @param dst destination path\n* @return true if all renames succeeded, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory across nodes.\n* @param f path to the file or directory; @param recursive if deletion should be recursive\n* @return true if successful, false if any node failed\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)": "/**\n* Repairs and opens a file from MRNflyNodes.\n* @param mrNodes array of MRNflyNode objects\n* @param f path of the file to open\n* @param bufferSize size of the buffer for reading\n* @return FSDataInputStream for the opened file or null if failed\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f path of the file to open\n* @param bufferSize size of the buffer for reading\n* @return FSDataInputStream for the opened file\n* @throws IOException if file not found or other I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)": "/**\n* Initializes NflyFSystem with URIs and configuration for replication.\n* @param uris array of URIs for file systems\n* @param conf configuration settings\n* @param minReplication minimum required replicas\n* @param nflyFlags flags for Nfly behavior\n* @param fsGetter retrieves file system instances\n* @throws IOException if replication requirements are not met\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)": "/**\n* Constructs NflyFSystem with URIs and configuration for replication.\n* @param uris array of URIs for file systems\n* @param conf configuration settings\n* @param minReplication minimum required replicas\n* @param nflyFlags flags for Nfly behavior\n* @throws IOException if replication requirements are not met\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)": "/**\n* Creates a FileSystem instance with specified URIs and configuration.\n* @param uris array of URIs for file systems\n* @param conf configuration settings\n* @param settings key-value pairs for additional settings\n* @param fsGetter retrieves file system instances\n* @return FileSystem object\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFs": {
        "org.apache.hadoop.fs.viewfs.ViewFs:getType()": "/**\n* Returns the type of the file system.\n* @return String representing the file system type\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a specified file segment.\n* @param f file path to locate blocks for\n* @param start starting byte offset in the file\n* @param len length in bytes to retrieve block locations\n* @return array of BlockLocation objects for the file segment\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the checksum of the specified file.\n* @param f the file path to check\n* @return FileChecksum object of the file\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory with specified permissions.\n* @param dir path of the directory to create\n* @param permission permissions for the new directory\n* @param createParent whether to create parent directories\n* @throws AccessControlException if access is denied\n* @throws FileAlreadyExistsException if the directory already exists\n* @throws FileNotFoundException if the path is invalid\n* @throws UnresolvedLinkException if the path contains unresolved links\n* @throws IOException for general I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with a specified buffer size.\n* @param f file path to open\n* @param bufferSize size of the read buffer\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner and group of the specified file path.\n* @param f file path to update ownership\n* @param username new owner's username\n* @param groupname new owner's group name\n* @throws AccessControlException if access is denied\n* @throws FileNotFoundException if the file does not exist\n* @throws UnresolvedLinkException if the path contains unresolved links\n* @throws IOException for general I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets the permission for a specified file path.\n* @param f file path to set permissions on\n* @param permission new permissions to apply\n* @throws AccessControlException if access is denied\n* @throws FileNotFoundException if the file does not exist\n* @throws UnresolvedLinkException if a symbolic link cannot be resolved\n* @throws IOException for other I/O errors\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a specified file.\n* @param f file path to set replication on\n* @param replication desired replication factor\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a specified file.\n* @param f file path to update times\n* @param mtime new modification time\n* @param atime new access time\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getUriDefaultPort()": "/**\n* Returns the default port for the URI, which is always -1 indicating no default port.\n* @return default port number\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setVerifyChecksum(boolean)": "/**\n* Sets the checksum verification flag for file operations.\n* @param verifyChecksum true to enable verification, false to disable\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the storage policy for a given file path.\n* @param src the path of the source file\n* @return BlockStoragePolicySpi object for the file's storage policy\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a specified file path.\n* @param path target file path\n* @param name attribute name\n* @param value attribute value\n* @param flag options for setting the attribute\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL for the specified path.\n* @param path the path for which ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for the specified file path.\n* @param path the file path for ACL status retrieval\n* @return AclStatus object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a file.\n* @param path the file path to get attributes from\n* @param names list of attribute names to retrieve\n* @return map of attribute names to their byte values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes an extended attribute from the specified file path.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n* @throws IOException if the removal operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot from the specified path.\n* @param path directory of the snapshot\n* @param snapshotName name of the snapshot to delete\n* @throws IOException if the delete operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to a specified length.\n* @param f path to the file to truncate\n* @param newLength desired new length of the file\n* @return true if truncation is successful\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a specified file system path.\n* @param path the file system path to set the policy for\n* @param policyName the name of the storage policy to apply\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attribute by name from the specified file system path.\n* @param path the file system path\n* @param name the attribute name to retrieve\n* @return byte array of attribute value\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus()": "/**\n* Retrieves file system status with zero capacity, used, and remaining space.\n* @return FsStatus object representing the file system status\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to set\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for the specified file path.\n* @param path the file path to retrieve extended attributes\n* @return a map of attribute names and their byte values\n* @throws IOException if the operation is unsupported\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot of the specified path.\n* @param path the path to snapshot\n* @param snapshotName the name for the created snapshot\n* @return Path of the created snapshot\n* @throws IOException if snapshot creation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)": "/** Constructs an AccessControlException for a read-only operation. @param operation the attempted operation @param p the path involved */",
        "org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for the specified file path.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n* @throws IOException if ACL modification fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes specified ACL entries from the given path.\n* @param path the path from which to remove ACL entries\n* @param aclSpec list of ACL entries to be removed\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the default ACL from the specified path.\n* @param path the path for which default ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot at the specified path.\n* @param path the file path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n* @throws IOException if renaming fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Validates and applies storage policy for the specified file system path.\n* @param path the file system path to check\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for a given path.\n* @param src the path to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Resolves and retrieves the target path of a symbolic link.\n* @param f the path of the link to resolve\n* @return Path of the target or null if not found\n* @throws IOException if link resolution fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String)": "/**\n* Retrieves delegation tokens for the specified renewer from all mounted file systems.\n* @param renewer the identifier for the token renewer\n* @return list of delegation tokens or empty if none found\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory, throwing exceptions for access issues or non-existent files.\n* @param f path to the file or directory to delete\n* @param recursive true to delete directories recursively\n* @return true if deletion is successful\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses in the specified path.\n* @param f the path to list file statuses\n* @return RemoteIterator of FileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses for a given path.\n* @param f the path to list file statuses\n* @return iterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)": "/**\n* Constructs an AccessControlException for a read-only operation.\n* @param operation the attempted operation\n* @param p the path involved\n* @return AccessControlException for the specified operation\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Creates a file and returns its output stream.\n* @param f file path, @param flag create options, @param absolutePermission permissions,\n* @param bufferSize size of buffer, @param replication number of replications,\n* @param blockSize block size, @param progress progress callback, \n* @param checksumOpt checksum options, @param createParent whether to create parent dirs\n* @return FSDataOutputStream for the created file\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link at the specified path.\n* @param target path to the target file\n* @param link path where the symlink will be created\n* @param createParent whether to create parent directories if they don't exist\n* @throws IOException if symlinks are not supported\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults()": "/**\n* Retrieves deprecated filesystem server configuration settings.\n* @return FsServerDefaults object with server configuration values\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server default settings for the specified file path.\n* @param f file path to resolve defaults\n* @return FsServerDefaults object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory()": "/**\n* Retrieves the user's home directory path.\n* @return Path object representing the home directory\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints()": "/**\n* Retrieves an array of MountPoint objects from the filesystem state.\n* @return array of MountPoint instances\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves a Path object, returning it if internal, or resolving with target file system.\n* @param f the Path to resolve\n* @return resolved Path object\n* @throws FileNotFoundException, AccessControlException, UnresolvedLinkException, IOException\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Renames a file or directory, handling internal directory restrictions.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @param overwrite flag to allow overwriting existing destination\n* @throws IOException if rename fails due to file state issues\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory without overwriting.\n* @param src source path to rename\n* @param dst destination path for the rename\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus for a given path, converting it to ViewFsFileStatus.\n* @param f the path to the file\n* @return ViewFsFileStatus object representing the file status\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path)": "/**** Lists file statuses for a given path. \n* @param f the path to list file statuses for \n* @return array of FileStatus objects \n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ViewFs with URI and configuration, setting up user info and mount links.\n* @param theUri URI for the file system\n* @param conf configuration settings\n* @throws IOException if an I/O error occurs\n* @throws URISyntaxException if the URI syntax is incorrect\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs ViewFs with the specified configuration.\n* @param conf configuration settings for ViewFs\n* @throws IOException if an I/O error occurs\n* @throws URISyntaxException if the URI syntax is incorrect\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks and enforces access permissions for a specified file path.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n* @throws AccessControlException if access denied\n* @throws IOException for I/O issues\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator": {
        "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:hasNext()": "/**\n* Checks if there are more elements in the iterator.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next()": "/**\n* Retrieves the next file status with resolved path.\n* @return FileStatus object with updated path information\n*/"
    },
    "org.apache.hadoop.fs.viewfs.RegexMountPoint": {
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:<init>(org.apache.hadoop.fs.viewfs.InodeTree,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Initializes RegexMountPoint with inode tree and path settings.\n* @param inodeTree tree structure for inodes\n* @param sourcePathRegex regex for source path matching\n* @param destPath destination path\n* @param settingsStr configuration settings for interceptors\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getVarListInString(java.lang.String)": "/**\n* Extracts variable names and their occurrences from a string.\n* @param input the input string to parse for variables\n* @return a map of variable names to their occurrences as sets\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getSrcPathRegex()": "/**\n* Retrieves the source path regex.\n* @return the source path regex as a String\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getSrcPattern()": "/**\n* Retrieves the source pattern.\n* @return Pattern object representing the source pattern\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getDstPath()": "/**\n* Retrieves the destination path.\n* @return destination path as a String\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getVarInDestPathMap()": "/**\n* Retrieves a map of variable names to their destination paths.\n* @return Map with variable names as keys and sets of destination paths as values\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getRegexGroupValueFromMather(java.util.regex.Matcher,java.lang.String)": "/**\n* Retrieves regex group value by index or name from a Matcher.\n* @param srcMatcher the Matcher to extract group from\n* @param regexGroupNameOrIndexStr group name or index as a string\n* @return the matched group value or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)": "/**\n* Replaces placeholders in path with regex capture group values.\n* @param parsedDestPath initial path with placeholders\n* @param srcMatcher regex matcher for capturing groups\n* @param regexGroupNameOrIndexStr group name or index\n* @param groupRepresentationStrSetInDest placeholders to replace\n* @return updated path with replacements\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)": "/**\n* Retrieves path excluding the last component if specified.\n* @param srcPath the source path to process\n* @param resolveLastComponent flag to include last path component\n* @return modified path or null if no slashes found\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors()": "/**\n* Initializes interceptors from settings string.\n* @throws IOException if settings are invalid or initialization fails\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize()": "/**\n* Initializes the mount point with regex and variable mappings.\n* @throws IOException if regex is invalid or initialization fails\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)": "/**\n* Constructs a Path object from the remaining substring of srcPath.\n* @param srcPath the original path string\n* @param resolvedPathStr the resolved path string to subtract\n* @return Path object representing the remaining path\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)": "/**\n* Resolves the source path and returns a ResolveResult object.\n* @param srcPath path to resolve; @param resolveLastComponent flag for last component inclusion\n* @return ResolveResult or null if no matches found\n*/"
    },
    "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink": {
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:getLink()": "/**\n* Retrieves the link associated with the current node.\n* @return INodeLink<T> the link of the node\n*/",
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)": "/**\n* Constructs an INodeDirLink with specified path, user info, and link.\n* @param pathToNode path to the node\n* @param aUgi user group information\n* @param link INodeLink object associated with the directory\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ConfigUtil": {
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(java.lang.String)": "/**\n* Constructs a configuration view file system prefix.\n* @param mountTableName name of the mount table\n* @return concatenated prefix string\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix()": "/**\n* Retrieves the configuration view file system prefix.\n* @return concatenated prefix string from default mount table\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)": "/**\n* Adds a link to the configuration with a specified source and target URI.\n* @param conf configuration object to modify\n* @param mountTableName name of the mount table\n* @param src source link name\n* @param target URI to link to\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)": "/**\n* Adds a link merge slash configuration to the provided Configuration.\n* @param conf the Configuration object to modify\n* @param mountTableName name of the mount table\n* @param target URI to set as the link merge slash\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)": "/**\n* Adds a link fallback to the configuration.\n* @param conf configuration object to modify\n* @param mountTableName name of the mount table\n* @param target URI to set as the link fallback\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])": "/**** Adds link merge configuration for given mount table and target URIs. \n* @param conf configuration object \n* @param mountTableName name of the mount table \n* @param targets array of target URIs \n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Adds a link configuration for a specified mount table.\n* @param conf configuration object\n* @param mountTableName name of the mount table\n* @param src source identifier\n* @param settings configuration settings\n* @param targets target values to set\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Adds a link regex to the configuration with a specified prefix.\n* @param conf configuration object to update\n* @param mountTableName name of the mount table\n* @param srcRegex source regex pattern\n* @param targetStr target string to associate with the regex\n* @param interceptorSettings optional settings for interceptors\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Validates and sets home directory configuration.\n* @param conf configuration object\n* @param mountTableName name of the mount table\n* @param homedir home directory path\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)": "/****\n* Retrieves home directory value from configuration.\n* @param conf configuration object\n* @param mountTableName name of the mount table\n* @return home directory value or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the default mount table name from configuration.\n* @param conf configuration object\n* @return default mount table name or a predefined constant\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])": "/**\n* Adds a link configuration for a specified mount table.\n* @param conf configuration object\n* @param mountTableName name of the mount table\n* @param src source identifier\n* @param settings configuration settings\n* @param targets target URIs\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Sets nested mount point support configuration.\n* @param conf configuration object to update\n* @param isNestedMountPointSupported true to support, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)": "/**\n* Adds a link to the default mount table in the configuration.\n* @param conf configuration to modify\n* @param src source link name\n* @param target URI to link to\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)": "/**\n* Adds a link merge slash using default mount table from configuration.\n* @param conf the Configuration object to modify\n* @param target URI to set as the link merge slash\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)": "/**\n* Adds a link fallback to the configuration using the default mount table name.\n* @param conf configuration object to modify\n* @param target URI to set as the link fallback\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])": "/**\n* Adds link merge configuration using default mount table and target URIs.\n* @param conf configuration object\n* @param targets array of target URIs\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Sets home directory configuration using default mount table name.\n* @param conf configuration object\n* @param homedir home directory path\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves home directory value from configuration using default mount table name.\n* @param conf configuration object\n* @return home directory value or null if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])": "/**\n* Adds a link configuration using default mount table and source identifier.\n* @param conf configuration object\n* @param src source identifier\n* @param targets target URIs\n*/",
        "org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration)": "/**\n* Checks if nested mount point support is enabled.\n* @param conf configuration object\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:<init>(java.net.URI)": "/**\n* Initializes Key object with URI scheme and authority in lowercase.\n* @param uri the URI to extract scheme and authority from\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:hashCode()": "/**\n* Computes hash code based on scheme and authority fields.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:equals(java.lang.Object)": "/**\n* Compares this Key object with another for equality.\n* @param obj object to compare with this Key\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs": {
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:checkPathIsSlash(org.apache.hadoop.fs.Path)": "/**\n* Validates if the given path is the root slash path.\n* @param f the Path to check\n* @throws IOException if the path is not the root slash\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getWorkingDirectory()": "/**\n* Throws an exception indicating improper invocation of getWorkingDirectory.\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Throws an exception indicating an internal implementation error.\n* @param new_dir the new working directory path (not used)\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getUri()": "/**\n* Retrieves the URI associated with this instance.\n* @return the URI object\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])": "",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies()": "/**\n* Retrieves all block storage policies from child file systems.\n* @return Collection of BlockStoragePolicySpi objects\n* @throws IOException if an error occurs during retrieval\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server default settings for a specified path.\n* @param f the path for which defaults are requested\n* @throws IOException if an error occurs during retrieval\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path)": "/**\n* Throws NotInMountpointException for accessing block size outside mount point.\n* @param f the path for which the block size is requested\n* @throws NotInMountpointException if the path is not in a valid mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path)": "/**\n* Throws NotInMountpointException when accessing default replication.\n* @param f the path being accessed\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the path to access attributes\n* @param name the name of the attribute\n* @throws IOException if an error occurs or path is not in mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the path for which to get extended attributes\n* @throws IOException if the path is not within a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a given path.\n* @param path the path to access attributes\n* @param names list of attribute names to retrieve\n* @throws IOException if an error occurs during retrieval\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path)": "/****\n* Lists extended attributes for a given path.\n* @param path the path to list attributes for\n* @throws IOException if an error occurs or path is not in a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path)": "/**\n* Retrieves quota usage for a given path.\n* @param f the path for which quota usage is requested\n* @throws IOException if the path is not within a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the storage policy for a given path.\n* @param src the path to check the storage policy for\n* @throws IOException if the path is not in a mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file checksum for the given path.\n* @param f the Path to check\n* @throws FileNotFoundException if path is a directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading.\n* @param f the Path to the file\n* @param bufferSize the size of the buffer for reading\n* @throws IOException if the path is invalid or points to a directory\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file, throwing an exception for read-only access.\n* @param f the file path to append to\n* @param bufferSize the size of the buffer to use\n* @param progress callback for progress updates\n* @throws IOException if the operation is not permitted\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory, checking path and permissions.\n* @param f the Path to delete\n* @param recursive whether to delete directories recursively\n* @return false (always throws exception)\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file from src to dst, validating paths and enforcing readonly constraints.\n* @param src source file path\n* @param dst destination file path\n* @return always throws AccessControlException\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to a specified length.\n* @param f the file path to truncate\n* @param newLength the new length of the file\n* @return always throws an exception\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets file owner and group, validating the path first.\n* @param f the Path to set owner for\n* @param username the new owner's username\n* @param groupname the new owner's group name\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets permissions for a specified path; throws exception if path is invalid.\n* @param f the Path to set permissions on\n* @param permission the desired FsPermission to apply\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a file path.\n* @param f the Path to set replication for\n* @param replication the desired replication factor\n* @return always throws an exception\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets access times for a file; throws exception if path is not valid.\n* @param f the Path to modify\n* @param mtime modified time\n* @param atime access time\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for a specified path.\n* @param path the target Path to modify ACLs\n* @param aclSpec list of AclEntry specifications\n* @throws IOException if path is invalid or readonly\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes ACL entries at the specified path.\n* @param path the Path where ACL entries are to be removed\n* @param aclSpec list of ACL entries to remove\n* @throws IOException if path is invalid or access is denied\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes default ACL for the given path.\n* @param path the Path to check and remove ACL from\n* @throws IOException if the path is invalid or readonly operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes ACL for the specified path.\n* @param path the Path for which ACL is to be removed\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets ACL for a given path; throws exception if path is invalid.\n* @param path the path to set ACLs on\n* @param aclSpec list of ACL entries to apply\n* @throws IOException if path is not valid or ACL operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute on a given path.\n* @param path the target path for the attribute\n* @param name the name of the attribute\n* @param value the value of the attribute\n* @param flag options for setting the attribute\n* @throws IOException if path is invalid or read-only\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes extended attribute; throws exception if path is not root or readonly.\n* @param path the Path to check and operate on\n* @param name the name of the extended attribute to remove\n* @throws IOException if the path is not valid or operation is not permitted\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot of the specified path.\n* @param path the path to snapshot\n* @param snapshotName the name for the snapshot\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot at the given path.\n* @param path path to the snapshot\n* @param snapshotOldName current snapshot name\n* @param snapshotNewName new snapshot name\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot at the specified path.\n* @param path the path of the snapshot\n* @param snapshotName the name of the snapshot to delete\n* @throws IOException if the path is invalid or read-only\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Validates path and throws exception for storage policy violation.\n* @param src the Path to validate\n* @throws IOException if the path is invalid or readonly operation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets storage policy for the specified path.\n* @param src the path to apply the policy\n* @param policyName the name of the storage policy\n* @throws IOException if the path check fails or policy application errors occur\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for a given path.\n* @param src the path to unset the storage policy\n* @throws IOException if the path is invalid or readonly\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path)": "/**\n* Deletes a file or directory, checking path and permissions.\n* @param f the Path to delete\n* @return result of the delete operation\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file and handles fallback if the path already exists.\n* @param f file path to create\n* @param permission file permissions\n* @param overwrite whether to overwrite existing file\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize size of the block\n* @param progress progress indicator\n* @return FSDataOutputStream for the created file\n* @throws IOException if file creation fails\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink()": "/**\n* Lists file statuses for the fallback link if available.\n* @return array of FileStatus or empty if no fallback link\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates a directory with specified permissions; throws if already exists or read-only.\n* @param dir directory path to create\n* @param permission permissions for the new directory\n* @return true if directory exists or was created successfully\n* @throws IOException on failure to create directory\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)": "/**\n* Retrieves block locations for a file, checking fallback if applicable.\n* @param fs FileStatus of the target file; @param start start byte; @param len byte range\n* @return array of BlockLocation or throws exceptions if not found\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)": "/**\n* Initializes InternalDirOfViewFs with directory, creation time, user info, and configuration.\n* @param dir directory inode\n* @param cTime creation time in milliseconds\n* @param ugi user group information\n* @param uri URI for the filesystem\n* @param config configuration settings\n* @param fsState filesystem state\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the enclosing root Path for the given Path.\n* @param path the Path to qualify\n* @return the qualified enclosing root Path\n* @throws IOException if the path is not found in the mount point\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves FileStatus for a given path.\n* @param f the Path to check\n* @return FileStatus object with file attributes\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for a given path, including links and internal directories.\n* @param f the path to list statuses for\n* @return array of FileStatus objects for the specified path\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves ACL status for a given path.\n* @param path the Path to check\n* @return AclStatus object containing owner, group, and permissions\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path)": "/****\n* Computes ContentSummary for a given path.\n* @param f Path of the file or directory\n* @return ContentSummary object with aggregated counts and length\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file system status for the specified path.\n* @param p the path to retrieve status for\n* @return FsStatus summarizing capacity, used, and remaining space\n*/"
    },
    "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor": {
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:<init>(java.lang.String,java.lang.String)": "/**\n* Initializes interceptor with source regex and replacement string.\n* @param srcRegex pattern to match source paths\n* @param replaceString string to replace matched paths\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:initialize()": "/**\n* Initializes regex pattern from source string; throws IOException on failure.\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptResolvedDestPathStr(java.lang.String)": "/**\n* Replaces matched patterns in destination path string.\n* @param parsedDestPathStr input string to process\n* @return modified string with replacements applied\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:getType()": "/**\n* Returns the interceptor type for regex mount point.\n* @return RegexMountPointInterceptorType constant\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptRemainingPath(org.apache.hadoop.fs.Path)": "/**\n* Returns the remaining path without modification.\n* @param remainingPath the path to be returned\n* @return the unchanged remainingPath\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptSource(java.lang.String)": "/**\n* Returns the input source string unchanged.\n* @param source the string to be intercepted\n* @return the same source string provided\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString()": "/**\n* Serializes object to string format.\n* @return concatenated string of config name, source regex, and replace string\n*/",
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String)": "/**\n* Deserializes a string to create a RegexMountPointResolvedDstPathReplaceInterceptor.\n* @param serializedString input string in format \"replaceresolvedpath:wordToReplace:replaceString\"\n* @return RegexMountPointResolvedDstPathReplaceInterceptor or null if format is invalid\n*/"
    },
    "org.apache.hadoop.fs.BulkDeleteUtils": {
        "org.apache.hadoop.fs.BulkDeleteUtils:<init>()": "/**\n* Private constructor to prevent instantiation of the BulkDeleteUtils class.\n*/",
        "org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Checks if the path is under the specified parent directory.\n* @param p path to validate\n* @param basePath parent directory path\n* @return true if p is under basePath, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer": {
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:writeChunk(byte[],int,int,byte[],int,int)": "/**\n* Writes a data chunk and its checksum to output streams.\n* @param b byte array of data, offset and length specify the portion to write\n* @param checksum byte array of checksum, ckoff and cklen specify the portion to write\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:checkClosed()": "/**\n* Checks if the channel is closed; throws exception if it is.\n* @throws IOException if the channel is closed\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close()": "",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics()": "/**\n* Retrieves IOStatistics from the specified data source.\n* @return IOStatistics object or null if invalid source\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String)": "/**\n* Checks if the stream has a specific capability.\n* @param capability the capability name to verify\n* @return true if capability exists, false if syncable probe fails\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Initializes ChecksumFSOutputSummer with file and checksum settings.\n* @param fs FileSystem for checksum operations\n* @param file Path of the file to create\n* @param overwrite flag to overwrite if file exists\n* @param bufferSize size of the buffer for file operations\n* @param replication replication factor for file\n* @param blockSize size of each block in the file\n* @param progress Progressable for tracking progress\n* @param permission file permissions\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.DF": {
        "org.apache.hadoop.fs.DF:verifyExitCode()": "/**\n* Validates the exit code and throws IOException if it's non-zero.\n* @throws IOException with error details if exit code is not 0\n*/",
        "org.apache.hadoop.fs.DF:parseOutput()": "/**\n* Parses filesystem output and initializes properties.\n* @throws IOException if output format is invalid or insufficient\n*/",
        "org.apache.hadoop.fs.DF:getCapacity()": "/**\n* Retrieves the total capacity of the directory file.\n* @return total capacity in bytes\n*/",
        "org.apache.hadoop.fs.DF:getUsed()": "/**\n* Calculates the used disk space in bytes.\n* @return long representing the used space\n*/",
        "org.apache.hadoop.fs.DF:getAvailable()": "/**\n* Retrieves the available disk space in bytes.\n* @return long representing the available space\n*/",
        "org.apache.hadoop.fs.DF:getExecString()": "/**\n* Generates execution command for 'df' in a Unix environment.\n* @return String array containing the command to execute\n*/",
        "org.apache.hadoop.fs.DF:parseExecResult(java.io.BufferedReader)": "/**\n* Parses execution results from a BufferedReader and stores them in output.\n* @param lines BufferedReader containing lines to parse\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DF:getDirPath()": "/**\n* Returns the directory path as a string.\n* @return the directory path\n*/",
        "org.apache.hadoop.fs.DF:getFilesystem()": "/**\n* Retrieves the filesystem path based on OS type.\n* @return Filesystem path as a String\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.DF:getMount()": "/**\n* Retrieves the mount point from the specified directory.\n* @return mount point as a String\n* @throws IOException if the path does not exist or output parsing fails\n*/",
        "org.apache.hadoop.fs.DF:getPercentUsed()": "/**\n* Calculates the percentage of used disk space.\n* @return percentage of used space as an integer\n*/",
        "org.apache.hadoop.fs.DF:<init>(java.io.File,long)": "/**\n* Initializes a DF object with a file path and execution interval.\n* @param path file path for the DF object\n* @param dfInterval time interval for execution\n*/",
        "org.apache.hadoop.fs.DF:toString()": "/**\n* Returns a formatted string of disk usage statistics.\n* @return formatted disk usage details including capacity, used, available, and percentage\n*/",
        "org.apache.hadoop.fs.DF:main(java.lang.String[])": "/**\n* Main method to execute the disk usage reporting with an optional file path argument.\n* @param args command-line arguments; first argument sets the file path\n*/",
        "org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)": "/**** Initializes a DF object with a file path and configuration settings. \n* @param path file path for the DF object \n* @param conf configuration settings for retrieving execution interval */"
    },
    "org.apache.hadoop.fs.HardLink$HardLinkCGUnix": {
        "org.apache.hadoop.fs.HardLink$HardLinkCGUnix:setLinkCountCmdTemplate(java.lang.String[])": "/**\n* Sets the link count command template.\n* @param template array of command template strings\n*/",
        "org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File)": "/**\n* Prepares command array for link counting with a file's shell path.\n* @param file the File to process\n* @return command array as String[]\n*/"
    },
    "org.apache.hadoop.fs.FsShell$Usage": {
        "org.apache.hadoop.fs.FsShell$Usage:processRawArguments(java.util.LinkedList)": "/**\n* Processes raw arguments and prints usage information.\n* @param args list of argument strings to process\n*/"
    },
    "org.apache.hadoop.fs.MultipartUploaderBuilder": {
        "org.apache.hadoop.fs.MultipartUploaderBuilder:build()": "/**\n* Builds and returns an object of type S.\n* @return an instance of S\n* @throws IllegalArgumentException if input parameters are invalid\n* @throws IOException if an I/O error occurs during build\n*/"
    },
    "org.apache.hadoop.fs.InvalidRequestException": {
        "org.apache.hadoop.fs.InvalidRequestException:<init>(java.lang.String)": "/**\n* Constructs an InvalidRequestException with the specified detail message.\n* @param str the detail message\n*/",
        "org.apache.hadoop.fs.InvalidRequestException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an InvalidRequestException with a message and cause.\n* @param message error message\n* @param cause underlying Throwable cause\n*/"
    },
    "org.apache.hadoop.fs.FsShellPermissions$Chgrp": {
        "org.apache.hadoop.fs.FsShellPermissions$Chgrp:parseOwnerGroup(java.lang.String)": "/**\n* Parses owner group from a string and validates its format.\n* @param groupStr input string representing the owner group\n*/"
    },
    "org.apache.hadoop.fs.EmptyStorageStatistics": {
        "org.apache.hadoop.fs.EmptyStorageStatistics:getLongStatistics()": "/**\n* Returns an empty iterator for LongStatistic objects.\n* @return an empty Iterator<LongStatistic>\n*/",
        "org.apache.hadoop.fs.EmptyStorageStatistics:getLong(java.lang.String)": "/**\n* Retrieves a Long value associated with the given key.\n* @param key the identifier for the Long value\n* @return Long value or null if not found\n*/",
        "org.apache.hadoop.fs.EmptyStorageStatistics:isTracked(java.lang.String)": "/**\n* Checks if the specified key is being tracked.\n* @param key the identifier to check tracking status\n* @return false, indicating the key is not tracked\n*/",
        "org.apache.hadoop.fs.EmptyStorageStatistics:reset()": "/**\n* Resets the state of the object to its initial configuration.\n*/",
        "org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String)": "/**\n* Constructs EmptyStorageStatistics with a specified name.\n* @param name the name of the storage statistics\n*/"
    },
    "org.apache.hadoop.fs.PathHandle": {
        "org.apache.hadoop.fs.PathHandle:toByteArray()": "/**\n* Converts the current object to a byte array.\n* @return byte array representation of the object\n*/"
    },
    "org.apache.hadoop.fs.audit.CommonAuditContext$GlobalIterable": {
        "org.apache.hadoop.fs.audit.CommonAuditContext$GlobalIterable:iterator()": "/**\n* Returns an iterator over the entries of the global context map.\n* @return Iterator of Map.Entry<String, String> from GLOBAL_CONTEXT_MAP\n*/"
    },
    "org.apache.hadoop.fs.audit.CommonAuditContext": {
        "org.apache.hadoop.fs.audit.CommonAuditContext:<init>()": "/**\n* Private constructor for CommonAuditContext to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.lang.String)": "/**\n* Adds or removes a key-value pair in evaluatedEntries.\n* @param key the entry key; @param value the entry value or null to remove the key\n* @return Supplier<String> associated with the key or null if removed\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.util.function.Supplier)": "/**\n* Adds a key-value pair to evaluatedEntries.\n* @param key unique context key\n* @param value Supplier for the associated string value\n* @return previous Supplier associated with the key, or null if absent\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:remove(java.lang.String)": "/**\n* Removes an entry from the context by its key.\n* @param key the identifier of the entry to remove\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:get(java.lang.String)": "/**\n* Retrieves the value associated with the given key.\n* @param key the key to look up\n* @return the corresponding value or null if not found\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:currentThreadID()": "/**\n* Retrieves the current thread's ID as a string.\n* @return String representation of the current thread's ID\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:containsKey(java.lang.String)": "/**\n* Checks if the specified key exists in the evaluated entries.\n* @param key the key to check for existence\n* @return true if the key is found, false otherwise\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:currentAuditContext()": "/**\n* Retrieves the current audit context.\n* @return CommonAuditContext associated with the current thread\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:setGlobalContextEntry(java.lang.String,java.lang.String)": "/**\n* Sets a key-value pair in the global context map.\n* @param key the context key\n* @param value the context value\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:getGlobalContextEntry(java.lang.String)": "/**\n* Retrieves a global context entry by its key.\n* @param key the identifier for the context entry\n* @return the associated value or null if not found\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:removeGlobalContextEntry(java.lang.String)": "/**\n* Removes the entry associated with the specified key from the global context map.\n* @param key the key of the entry to be removed\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:getGlobalContextEntries()": "/**\n* Retrieves global context entries as an iterable collection.\n* @return Iterable of Map.Entry with key-value pairs of global context\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:init()": "/**\n* Initializes dynamic thread context by storing current thread ID.\n* @param PARAM_THREAD1 unique key for thread context\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object)": "/**\n* Records the entry point of a tool in the global context if not already present.\n* @param tool the tool object to extract the class name from\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:reset()": "/**\n* Resets evaluated entries and reinitializes thread context.\n*/",
        "org.apache.hadoop.fs.audit.CommonAuditContext:createInstance()": "/**\n* Creates and initializes a CommonAuditContext instance.\n* @return initialized CommonAuditContext object\n*/"
    },
    "org.apache.hadoop.fs.audit.AuditConstants": {
        "org.apache.hadoop.fs.audit.AuditConstants:<init>()": "/**\n* Private constructor to prevent instantiation of the AuditConstants class.\n*/"
    },
    "org.apache.hadoop.fs.audit.AuditStatisticNames": {
        "org.apache.hadoop.fs.audit.AuditStatisticNames:<init>()": "/**\n* Private constructor for AuditStatisticNames; prevents instantiation of this utility class.\n*/"
    },
    "org.apache.hadoop.fs.FSBuilder": {
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,boolean)": "/**\n* Retrieves an optional value by key, converting boolean to string.\n* @param key the identifier for the value\n* @param value the boolean to convert\n* @return the optional value of type B\n*/",
        "org.apache.hadoop.fs.FSBuilder:optLong(java.lang.String,long)": "/**\n* Retrieves an optional value as a String for the given key, defaulting to provided long value.\n* @param key the key to look up\n* @param value the default long value if key is absent\n* @return the optional value as B type\n*/",
        "org.apache.hadoop.fs.FSBuilder:optDouble(java.lang.String,double)": "/**\n* Retrieves optional double value by key, defaults to provided value if not found.\n* @param key the key to look up\n* @param value the default double value if key is absent\n* @return optional B representation of the double value\n*/",
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,boolean)": "/**\n* Converts boolean to string and calls must method.\n* @param key identifier for the operation\n* @param value boolean to be converted to string\n* @return B result from must method\n*/",
        "org.apache.hadoop.fs.FSBuilder:mustLong(java.lang.String,long)": "/**\n* Ensures the key is present with the specified long value.\n* @param key the key to check\n* @param value the long value to associate with the key\n* @return result of the must method\n*/",
        "org.apache.hadoop.fs.FSBuilder:mustDouble(java.lang.String,double)": "/**\n* Converts a double to String and validates the key.\n* @param key the key to validate\n* @param value the double value to convert\n* @return result of must method with validated key and value\n*/",
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)": "/**\n* Retrieves an optional value for the given key, defaulting to an integer.\n* @param key the key to look up\n* @param value the default integer value if key is absent\n* @return optional value as B type\n*/",
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)": "/**\n* Retrieves an optional value as B type for the given key, using float as default.\n* @param key the key to look up\n* @param value the default float value if key is absent\n*/",
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)": "/**\n* Retrieves an optional value for the given key, defaulting to a long value.\n* @param key the key to look up\n* @param value the default long value if key is absent\n* @return optional value as B type\n*/",
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)": "/**\n* Retrieves an optional value as B type for the given key, using a double as default.\n* @param key the key to look up\n* @param value the default double value if key is absent\n* @return optional value as B type\n*/",
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)": "/**\n* Validates key and associates it with an integer value.\n* @param key the key to check\n* @param value the integer value to associate with the key\n* @return result from mustLong method\n*/",
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)": "/**\n* Deprecated method ensuring key presence with a float value.\n* @param key the key to check\n* @param value the float value to associate with the key\n* @return result of the mustLong method\n*/",
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)": "/**\n* Deprecated method to ensure key presence with a long value.\n* @param key the key to check\n* @param value the long value to associate with the key\n* @return result of mustLong method\n*/",
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)": "/**\n* Deprecated method to ensure key presence with a double value.\n* @param key key to check\n* @param value double value to associate with the key\n* @return result of mustLong method\n*/"
    },
    "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream": {
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seek(long)": "/**\n* Seeks to a specified position in the input stream.\n* @param pos the position to seek to (must be non-negative)\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:available()": "/**\n* Returns the number of bytes that can be read without blocking.\n* @return number of available bytes in the input stream\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:close()": "/**\n* Closes file input stream and optional async channel, aggregating IO statistics.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getFileDescriptor()": "/**\n* Retrieves the file descriptor from the input stream.\n* @return FileDescriptor associated with the input stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:hasCapability(java.lang.String)": "/**\n* Checks if the specified capability is supported.\n* @param capability the capability to check\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getAsyncChannel()": "/**\n* Retrieves an asynchronous file channel for reading.\n* @return AsynchronousFileChannel instance\n* @throws IOException if the channel cannot be opened\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getPos()": "/**\n* Retrieves the current position value.\n* @return current position as a long\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seekToNewSource(long)": "/**\n* Seeks to a new source at the specified position.\n* @param targetPos position to seek to\n* @return false as a placeholder for actual implementation\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long)": "/**\n* Skips n bytes in the input stream and updates position.\n* @param n number of bytes to skip\n* @return actual bytes skipped, may be less than n\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)": "/**\n* Reads data from specified file ranges into allocated byte buffers.\n* @param ranges list of file ranges to read\n* @param allocate function to allocate byte buffers\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read()": "/****************************************************************************** \n* Reads a byte from the input stream and updates statistics.\n* @return byte read or -1 if end of stream; throws IOException on error\n*******************************************************************************/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)": "/**\n* Reads bytes into an array and updates read statistics.\n* @param b byte array to read into, @param off offset, @param len number of bytes to read\n* @return number of bytes read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position into a buffer.\n* @param position the start position to read from\n* @param b the buffer to store read bytes\n* @param off the offset in the buffer\n* @param len the number of bytes to read\n* @return number of bytes read or 0 if none\n*/"
    },
    "org.apache.hadoop.fs.UnionStorageStatistics": {
        "org.apache.hadoop.fs.UnionStorageStatistics:getLongStatistics()": "/**\n* Returns an iterator for long statistics.\n* @return Iterator of LongStatistic objects\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics:getLong(java.lang.String)": "/**\n* Retrieves the first non-null Long value associated with the given key.\n* @param key the key to search for\n* @return the Long value or null if not found\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics:isTracked(java.lang.String)": "/**\n* Checks if the specified key is tracked in the storage statistics.\n* @param key the key to check for tracking\n* @return true if the key is tracked, false otherwise\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics:reset()": "/**\n* Resets all storage statistics to their initial state.\n*/",
        "org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])": "/****\n* Constructs UnionStorageStatistics with a name and array of StorageStatistics.\n* @param name non-null name for the statistics\n* @param stats non-null array of StorageStatistics, must not contain null elements\n*/"
    },
    "org.apache.hadoop.fs.FsShell": {
        "org.apache.hadoop.fs.FsShell:getHelp()": "/**\n* Retrieves and initializes Help object if not already created.\n* @return Help instance\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FsShell:getUsagePrefix()": "/**\n* Retrieves the usage prefix string.\n* @return usagePrefix string value\n*/",
        "org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers commands for FsShell if the current class matches.\n* @param factory CommandFactory for registering commands\n*/",
        "org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs an FsShell instance with the specified Hadoop configuration.\n* @param conf the Configuration to initialize the shell\n*/",
        "org.apache.hadoop.fs.FsShell:close()": "/**\n* Closes the file system and nullifies the reference.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)": "/**\n* Prints command usage to the specified output stream.\n* @param out PrintStream to display the usage\n* @param instance Command instance to retrieve usage information\n*/",
        "org.apache.hadoop.fs.FsShell:<init>()": "/**\n* Initializes an FsShell instance with default configuration.\n*/",
        "org.apache.hadoop.fs.FsShell:newShellInstance()": "/**\n* Creates a new instance of FsShell.\n* @return a newly initialized FsShell object\n*/",
        "org.apache.hadoop.fs.FsShell:createOptionTableListing()": "/**\n* Creates a TableListing with specified fields and wrap width.\n* @return configured TableListing instance\n*/",
        "org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)": "/**\n* Prints help information for a command instance.\n* @param out output stream for printing help details\n* @param instance command instance to display help for\n*/",
        "org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)": "/**\n* Prints command info or help based on cmd; outputs to specified PrintStream.\n* @param out output stream for printing information\n* @param cmd command name for specific help or usage\n* @param showHelp true to display help, false for usage details\n*/",
        "org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)": "/**\n* Displays error messages with command suggestions.\n* @param cmd the command that triggered the error\n* @param message the error message to display\n*/",
        "org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream)": "/**\n* Prints command usage details to the specified output stream.\n* @param out output stream for printing usage information\n*/",
        "org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)": "/**\n* Prints command usage details to the specified output stream.\n* @param out output stream for printing information\n* @param cmd command name for usage details\n*/",
        "org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream)": "/**\n* Prints help information to the specified output stream.\n* @param out output stream for printing help details\n*/",
        "org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)": "/**\n* Prints help information for a specific command.\n* @param out output stream for printing help info\n* @param cmd command name for help details\n*/",
        "org.apache.hadoop.fs.FsShell:getFS()": "/**\n* Retrieves the FileSystem instance, initializing if necessary.\n* @return FileSystem object\n*/",
        "org.apache.hadoop.fs.FsShell:getTrash()": "/**\n* Retrieves the Trash instance, initializing it if necessary.\n* @return Trash object representing the trash system\n* @throws IOException if Trash initialization fails\n*/",
        "org.apache.hadoop.fs.FsShell:getCurrentTrashDir()": "/**\n* Retrieves the current trash directory.\n* @return Path to the current trash directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the current trash directory for the specified path.\n* @param path base path to derive the trash directory\n* @return Path object representing the current trash directory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FsShell:init()": "/**\n* Initializes command factory and sets quiet mode for Hadoop configuration.\n*/",
        "org.apache.hadoop.fs.FsShell:run(java.lang.String[])": "/**\n* Executes a command with arguments and handles errors.\n* @param argv command-line arguments\n* @return exit code based on command execution result\n*/",
        "org.apache.hadoop.fs.FsShell:main(java.lang.String[])": "/**\n* Main method to execute FsShell with provided arguments.\n* @param argv command-line arguments for the tool\n* @throws Exception if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.conf.Configuration": {
        "org.apache.hadoop.conf.Configuration:setQuietMode(boolean)": "/**\n* Sets the quiet mode state.\n* @param quietmode true to enable quiet mode, false to disable it\n*/",
        "org.apache.hadoop.conf.Configuration:getResource(java.lang.String)": "/**\n* Retrieves a resource URL by its name.\n* @param name the name of the resource to fetch\n* @return URL of the resource or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getClassByNameOrNull(java.lang.String)": "/**\n* Retrieves Class by name or returns null if not found.\n* @param name fully qualified class name\n* @return Class object or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getClassLoader()": "/**\n* Returns the ClassLoader instance used by this context.\n* @return ClassLoader associated with this context\n*/",
        "org.apache.hadoop.conf.Configuration:setClassLoader(java.lang.ClassLoader)": "/**\n* Sets the ClassLoader for the current instance.\n* @param classLoader the ClassLoader to be set\n*/",
        "org.apache.hadoop.conf.Configuration:getOverlay()": "/**\n* Retrieves the overlay properties, initializing if null.\n* @return Properties object containing overlay settings\n*/",
        "org.apache.hadoop.conf.Configuration:logDeprecation(java.lang.String)": "/**\n* Logs a deprecation warning message.\n* @param message the deprecation message to log\n*/",
        "org.apache.hadoop.conf.Configuration:getQuietMode()": "/**\n* Retrieves the current quiet mode status.\n* @return true if quiet mode is enabled, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration:reloadConfiguration()": "/**\n* Reloads the configuration by resetting properties and clearing parameters.\n*/",
        "org.apache.hadoop.conf.Configuration:findSubVariable(java.lang.String)": "/**\n* Finds the start and end indices of a variable in the eval string.\n* @param eval input string to search for variable patterns\n* @return array containing start and end indices, or {-1, -1} if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getenv(java.lang.String)": "/**\n* Retrieves environment variable by name if not restricted.\n* @param name the name of the environment variable\n* @return the value of the environment variable or null if restricted\n*/",
        "org.apache.hadoop.conf.Configuration:getProperty(java.lang.String)": "/**\n* Retrieves system property by key if not restricted.\n* @param key property name to retrieve\n* @return property value or null if restricted\n*/",
        "org.apache.hadoop.conf.Configuration:putIntoUpdatingResource(java.lang.String,java.lang.String[])": "/**\n* Adds a key-value pair to the updating resource map.\n* @param key unique identifier for the resource\n* @param value array of string values associated with the key\n*/",
        "org.apache.hadoop.conf.Configuration:getHexDigits(java.lang.String)": "/**\n* Extracts hex digits from a string, returning null if not in hex format.\n* @param value input string potentially containing hex digits\n* @return hex string or null if not valid hex\n*/",
        "org.apache.hadoop.conf.Configuration:convertStorageUnit(double,org.apache.hadoop.conf.StorageUnit,org.apache.hadoop.conf.StorageUnit)": "/**\n* Converts a value from one storage unit to another.\n* @param value the amount in the source unit\n* @param sourceUnit the unit to convert from\n* @param targetUnit the unit to convert to\n* @return the converted value in the target unit\n*/",
        "org.apache.hadoop.conf.Configuration:getFinalParameters()": "/**\n* Retrieves a set of final parameters.\n* @return a Set of final parameter strings\n*/",
        "org.apache.hadoop.conf.Configuration:parse(java.io.InputStream,java.lang.String,boolean)": "/**\n* Parses an InputStream into an XMLStreamReader.\n* @param is input stream to parse\n* @param systemIdStr system identifier for XML\n* @param restricted flag to enable/disable DTD support\n* @return XMLStreamReader for the parsed XML\n*/",
        "org.apache.hadoop.conf.Configuration:addTags(java.util.Properties)": "/**\n* Adds system and custom tags from properties to the TAGS collection.\n* @param prop properties containing tag configurations\n*/",
        "org.apache.hadoop.conf.Configuration:overlay(java.util.Properties,java.util.Properties)": "/**\n* Overlays properties from 'from' onto 'to' in a synchronized manner.\n* @param to properties to be updated\n* @param from properties to overlay from\n*/",
        "org.apache.hadoop.conf.Configuration:readTagFromConfig(java.lang.String,java.lang.String,java.lang.String,java.lang.String[])": "/**\n* Reads tags from configuration and updates properties.\n* @param attributeValue comma-separated tags\n* @param confName property name to set\n* @param confValue value to associate with the property\n* @param confSource source of the configuration\n*/",
        "org.apache.hadoop.conf.Configuration:checkForOverride(java.util.Properties,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Checks for overriding final parameters in properties.\n* @param properties configuration properties\n* @param name identifier for logging\n* @param attr parameter name to check\n* @param value new value to compare against\n*/",
        "org.apache.hadoop.conf.Configuration:toString(java.util.List,java.lang.StringBuilder)": "/**\n* Appends string representations of list elements to StringBuilder.\n* @param resources list of items to convert to string\n* @param sb StringBuilder to append results\n*/",
        "org.apache.hadoop.conf.Configuration:getAllPropertiesByTag(java.lang.String)": "/**\n* Retrieves all properties associated with a specific tag.\n* @param tag the identifier for the properties\n* @return Properties object containing properties for the tag\n*/",
        "org.apache.hadoop.conf.Configuration:isPropertyTag(java.lang.String)": "/**\n* Checks if the given tag string is a valid property tag.\n* @param tagStr the tag string to check\n* @return true if tagStr is a property tag, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration:setAllowNullValueProperties(boolean)": "/**\n* Sets the flag for allowing null value properties.\n* @param val true to allow null values, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration:setRestrictSystemProperties(boolean)": "/**\n* Sets the flag to restrict system properties.\n* @param val true to restrict, false to allow\n*/",
        "org.apache.hadoop.conf.Configuration:setRestrictSystemProps(boolean)": "/**\n* Sets the restrictSystemProps flag.\n* @param val true to restrict system properties, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String)": "/**\n* Retrieves configuration resource as InputStream.\n* @param name the name of the resource to fetch\n* @return InputStream of the resource or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String)": "/**\n* Retrieves a resource as a Reader by its name.\n* @param name the name of the resource\n* @return Reader for the resource or null if not found or an error occurs\n*/",
        "org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String)": "/**\n* Retrieves Class by name, throws exception if not found.\n* @param name fully qualified class name\n* @return Class object\n* @throws ClassNotFoundException if class is not found\n*/",
        "org.apache.hadoop.conf.Configuration:<init>(boolean)": "/**\n* Initializes Configuration with optional default loading.\n* @param loadDefaults flag to load default settings\n*/",
        "org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String)": "/**\n* Checks if a key is deprecated.\n* @param key the key to check for deprecation\n* @return true if the key is deprecated, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String)": "/**\n* Retrieves DeprecatedKeyInfo for a specified key.\n* @param key the key whose deprecated info is to be fetched\n* @return DeprecatedKeyInfo object or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys()": "/**\n* Prints deprecated keys and their associated new keys to the console.\n*/",
        "org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String)": "/**\n* Checks if a deprecation warning has been issued for a given key name.\n* @param name the key name to check for warnings\n* @return true if warned, otherwise false\n*/",
        "org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String)": "/**\n* Retrieves the deprecated key for a given key.\n* @param key the original key to find its deprecated counterpart\n* @return the deprecated key or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:reloadExistingConfigurations()": "/**\n* Reloads all existing configurations from the registry.\n*/",
        "org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String)": "/**\n* Adds a resource name to defaults and reloads configurations if needed.\n* @param name resource name to add\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)": "/**\n* Converts a time duration string to a specified TimeUnit.\n* @param name identifier for logging, vStr duration string, defaultUnit fallback unit, returnUnit target unit\n* @return converted duration in the specified TimeUnit\n*/",
        "org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)": "/****\n* Parses a URL into an XMLStreamReader.\n* @param url the URL to parse\n* @param restricted flag to enable/disable DTD support\n* @return XMLStreamReader for the parsed XML\n*/",
        "org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])": "/**\n* Loads property values into a Properties object and manages final parameters.\n* @param properties target properties to update\n* @param name identifier for logging\n* @param attr property attribute to set\n* @param value value to assign to the attribute\n* @param finalParameter indicates if the attribute is final\n* @param source source array for updating resource tracking\n*/",
        "org.apache.hadoop.conf.Configuration:toString()": "/**\n* Converts configuration to string format.\n* @return String representation of configuration details\n*/",
        "org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List)": "/**\n* Retrieves properties for multiple tags.\n* @param tagList list of tag identifiers\n* @return Properties object containing all properties for the tags\n*/",
        "org.apache.hadoop.conf.Configuration:<init>()": "/***************\n* Initializes Configuration with default loading enabled.\n* @param loadDefaults flag to load default settings\n***************/",
        "org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)": "/**\n* Logs deprecation warning if accessed status is false.\n* @param name the name of the deprecated key\n* @param source source context for the warning message\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)": "/**\n* Retrieves time duration in specified TimeUnit.\n* @param name identifier for logging, vStr duration string, unit target TimeUnit\n* @return duration in the specified TimeUnit\n*/",
        "org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)": "/**\n* Retrieves an XMLStreamReader2 from various resource types.\n* @param wrapper resource wrapper containing resource info\n* @param quiet suppresses logging if true\n* @return XMLStreamReader2 for the parsed resource or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)": "/**\n* Appends XML property to document if property name is valid.\n* @param doc XML document to append to\n* @param conf parent element to append property to\n* @param propertyName name of the property to retrieve\n* @param redactor ConfigRedactor for sensitive value handling\n*/",
        "org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[])": "/**\n* Adds deprecation changes to the current context.\n* @param deltas array of deprecation changes to apply\n*/",
        "org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)": "/**\n* Retrieves CredentialEntry by name or its deprecated equivalents.\n* @param provider credential provider to fetch entries\n* @param name the name of the credential to retrieve\n* @return CredentialEntry or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)": "/**\n* Loads a resource and overlays properties if applicable.\n* @param properties target properties to update\n* @param wrapper resource wrapper containing resource info\n* @param quiet suppresses logging if true\n* @return Resource object or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)": "/**\n* Adds a deprecation entry with a key, new identifiers, and an optional message.\n* @param key unique identifier for deprecation\n* @param newKeys array of new identifiers\n* @param customMessage optional message for deprecation\n*/",
        "org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)": "/**\n* Loads resources from properties and updates the resources list.\n* @param properties configuration properties\n* @param resources list of Resource objects to update\n* @param startIdx index to start loading resources\n* @param fullReload flag to load defaults if true\n* @param quiet suppresses logging if true\n*/",
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Adds a deprecation entry with a key and a new identifier.\n* @param key unique identifier for deprecation\n* @param newKey new identifier for deprecation\n* @param customMessage optional message for deprecation\n*/",
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])": "/**\n* Marks a key as deprecated with new identifiers.\n* @param key unique identifier for deprecation\n* @param newKeys array of new identifiers\n*/",
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)": "/**\n* Adds a deprecation entry with a key and a new identifier.\n* @param key unique identifier for deprecation\n* @param newKey new identifier for deprecation\n*/",
        "org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)": "/**** Loads properties and updates resources based on provided parameters. \n* @param props configuration properties \n* @param startIdx index to start loading resources \n* @param fullReload flag to load defaults if true \n*/",
        "org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource)": "/**\n* Adds a resource object and updates system properties.\n* @param resource the Resource object to add\n*/",
        "org.apache.hadoop.conf.Configuration:getProps()": "/**\n* Retrieves properties, loading them if not already initialized.\n* @return Properties object containing configuration settings\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)": "/**\n* Adds a resource by name with optional restricted parser usage.\n* @param name resource name\n* @param restrictedParser flag to restrict parser usage\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)": "/**\n* Adds a resource from a URL, optionally using a restricted parser.\n* @param url the resource URL\n* @param restrictedParser flag for restricted parser usage\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)": "/**\n* Adds a resource from a file path with optional restricted parser usage.\n* @param file path to the resource file\n* @param restrictedParser flag to enable restricted parsing\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)": "/**\n* Adds a resource from an InputStream, optionally using a restricted parser.\n* @param in InputStream to create the resource from\n* @param restrictedParser flag to restrict parser usage\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)": "/**\n* Adds a resource using an input stream and name.\n* @param in input stream for resource data\n* @param name resource name\n* @param restrictedParser flag for parser restriction\n*/",
        "org.apache.hadoop.conf.Configuration:setDeprecatedProperties()": "/**\n* Updates deprecated properties based on new keys from the deprecation context.\n*/",
        "org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])": "/**\n* Updates properties with values from deprecated keys.\n* @param deprecations context containing deprecated key mappings\n* @param newNames array of new property names to update\n*/",
        "org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Clones a Configuration object, copying its resources and properties.\n* @param other the Configuration to clone from\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration)": "/**\n* Adds a resource using the provided configuration.\n* @param conf configuration settings for resource creation\n*/",
        "org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String)": "/**\n* Retrieves alternative names for a deprecated key.\n* @param name the name of the deprecated key\n* @return array of alternative names or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String)": "/**\n* Retrieves property sources by name.\n* @param name the name of the property\n* @return array of property sources or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:size()": "/**\n* Returns the size of the properties object.\n* @return number of properties in the configuration settings\n*/",
        "org.apache.hadoop.conf.Configuration:clear()": "/**\n* Clears properties and overlay settings.\n* @return void\n*/",
        "org.apache.hadoop.conf.Configuration:iterator()": "/**\n* Returns an iterator over string key-value pairs from properties.\n* @return Iterator of Map.Entry<String, String>\n*/",
        "org.apache.hadoop.conf.Configuration:write(java.io.DataOutput)": "/**\n* Serializes properties to a DataOutput stream.\n* @param out output stream for writing serialized data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String)": "/**\n* Retrieves key-value pairs matching a regex from properties.\n* @param regex the regex pattern to match keys\n* @return a map of matching keys and their substituted values\n*/",
        "org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)": "/**\n* Handles deprecation by updating names and logging warnings if applicable.\n* @param deprecations context with deprecated key mappings\n* @param name original property name\n* @return updated names array, possibly with deprecated keys replaced\n*/",
        "org.apache.hadoop.conf.Configuration:handleDeprecation()": "/**\n* Manages deprecation for all configuration properties.\n*/",
        "org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String)": "/**\n* Checks if only a specific key exists in properties.\n* @param name the property name to check\n* @return true if the key exists; false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration:getRaw(java.lang.String)": "/**\n* Retrieves raw property value by name, handling deprecation if necessary.\n* @param name the original property name\n* @return raw property value or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Sets a property value, handling deprecation and alternative names.\n* @param name property name\n* @param value property value\n* @param source origin of the property setting\n*/",
        "org.apache.hadoop.conf.Configuration:unset(java.lang.String)": "/**\n* Removes properties by name, handling deprecation if necessary.\n* @param name property name to unset\n*/",
        "org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)": "/**\n* Creates an XML document with configuration properties.\n* @param propertyName optional specific property to include\n* @param redactor ConfigRedactor for sensitive value handling\n* @return XML Document containing configuration properties\n*/",
        "org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)": "/**\n* Substitutes variables in the expression with their values.\n* @param expr input expression containing variables\n* @return modified expression with substituted variable values\n*/",
        "org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)": "/**\n* Sets a property value using a default source.\n* @param name property name\n* @param value property value\n*/",
        "org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String)": "/**\n* Substitutes common variables in the expression.\n* @param expr input expression with variables\n* @return modified expression with substituted values\n*/",
        "org.apache.hadoop.conf.Configuration:get(java.lang.String)": "/**\n* Retrieves the property value by name, handling deprecations if necessary.\n* @param name the property name to fetch\n* @return the property value or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)": "/**\n* Retrieves property value by name, handling deprecations and substitutions.\n* @param name property name to retrieve\n* @param defaultValue value returned if property is not found\n* @return the property value or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)": "/****\n* Sets an integer property value as a string.\n* @param name property name\n* @param value integer value to set\n*/",
        "org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)": "/**\n* Sets a long property value as a string.\n* @param name property name\n* @param value long value to set\n*/",
        "org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)": "/**\n* Sets a float property value as a string.\n* @param name property name\n* @param value float value to set\n*/",
        "org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)": "/**\n* Sets a property value as a string representation of a double.\n* @param name property name\n* @param value double value to be set\n*/",
        "org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)": "/**\n* Sets a boolean property value as a string.\n* @param name property name\n* @param value boolean value to set\n*/",
        "org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)": "/**\n* Sets time duration with a formatted suffix.\n* @param name property name\n* @param value duration value\n* @param unit time unit for the duration\n*/",
        "org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)": "/**\n* Sets storage size with unit suffix.\n* @param name property name\n* @param value size value\n* @param unit storage unit type\n*/",
        "org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)": "/**\n* Sets a pattern value for the specified property name.\n* @param name property name\n* @param pattern regex pattern to set\n*/",
        "org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])": "/**\n* Sets a property with a name and a comma-separated string of values.\n* @param name property name\n* @param values variable number of string values\n*/",
        "org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)": "/**\n* Sets socket address property using name and formatted host:port string.\n* @param name property name\n* @param addr InetSocketAddress containing host and port information\n*/",
        "org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)": "/**\n* Validates and sets class name property.\n* @param name property name\n* @param theClass class to validate\n* @param xface interface the class must implement\n*/",
        "org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and updates resource properties.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String)": "/**\n* Retrieves and trims the property value by name.\n* @param name the property name to fetch\n* @return trimmed property value or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)": "/**\n* Sets property value if not already set.\n* @param name property name to check and set\n* @param value value to set if name is unset\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)": "/**\n* Retrieves time duration from property name or returns default value.\n* @param name property name, @param defaultValue fallback duration, \n* @param defaultUnit unit of default value, @param returnUnit target unit for return value\n* @return duration in the specified returnUnit\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)": "/**\n* Retrieves time duration by name, converting to specified TimeUnit.\n* @param name property name, @param defaultValue fallback duration string\n* @param defaultUnit fallback TimeUnit, @param returnUnit target TimeUnit\n* @return duration in returnUnit or converted from defaultValue\n*/",
        "org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)": "/**\n* Retrieves storage size in target unit from a property name or default value.\n* @param name property name to fetch size\n* @param defaultValue default size if property is blank\n* @param targetUnit unit to convert the size into\n* @return converted storage size as double\n*/",
        "org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)": "/**\n* Retrieves storage size by name, converting to target unit.\n* @param name property name to fetch size\n* @param defaultValue default value if property is not found\n* @param targetUnit unit to convert size to\n* @return converted storage size as double\n*/",
        "org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)": "/**\n* Retrieves a compiled regex pattern by name or returns a default if not valid.\n* @param name the property name for the regex\n* @param defaultValue the default Pattern to return if not found or invalid\n* @return compiled Pattern or defaultValue if not valid\n*/",
        "org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String)": "/**\n* Retrieves a collection of substrings from a property value by name.\n* @param name the property name to fetch\n* @return Collection of substrings or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getStrings(java.lang.String)": "/**\n* Retrieves substrings from a property value by name.\n* @param name the property name to fetch\n* @return array of substrings or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])": "/**\n* Retrieves strings by name or returns default values if not found.\n* @param name property name to fetch\n* @param defaultValue fallback strings if property is absent\n* @return array of strings or default values\n*/",
        "org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String)": "/**\n* Retrieves a collection of trimmed strings from a property value by name.\n* @param name the property name to fetch\n* @return Collection of trimmed strings or an empty collection if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String)": "/**\n* Retrieves trimmed substrings from a property value by name.\n* @param name the property name to fetch\n* @return array of trimmed substrings or empty array if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])": "/**\n* Retrieves trimmed strings from a property or returns default values.\n* @param name property name to fetch\n* @param defaultValue fallback array if property is not found\n* @return array of trimmed substrings or default values\n*/",
        "org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String)": "/**\n* Retrieves properties starting with a specified prefix.\n* @param confPrefix the prefix to filter property names\n* @return a map of filtered properties without the prefix\n*/",
        "org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)": "/**\n* Appends a JSON property with redacted value and metadata.\n* @param jsonGen JSON generator for output\n* @param config Configuration containing property details\n* @param name Property name to append\n* @param redactor Redactor for sensitive value handling\n*/",
        "org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)": "/**\n* Retrieves an EnumSet from a configuration key.\n* @param key identifier for the enum values\n* @param enumClass class of the enum type\n* @param ignoreUnknown flag to ignore unknown values\n* @return EnumSet of the specified enum type\n*/",
        "org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)": "/**\n* Retrieves integer ranges based on property name.\n* @param name property name to fetch ranges\n* @param defaultValue fallback value if property is absent\n* @return IntegerRanges object initialized with the ranges\n*/",
        "org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)": "/**\n* Updates and returns the resolved connection address.\n* @param name property name for the address\n* @param addr input socket address to resolve\n* @return resolved InetSocketAddress\n*/",
        "org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)": "/**\n* Retrieves trimmed property value or returns default if not found.\n* @param name property name to fetch\n* @param defaultValue value to return if property is null\n* @return trimmed property value or defaultValue\n*/",
        "org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)": "/**\n* Retrieves an integer from a property name, using default if not found.\n* @param name property name to fetch\n* @param defaultValue value to return if property is absent\n* @return parsed integer value\n*/",
        "org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)": "/**\n* Retrieves a long value from a property name, using default if not found.\n* @param name property name to fetch\n* @param defaultValue value to return if property is absent\n* @return parsed long value or defaultValue if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)": "/**\n* Retrieves long value from property; returns default if not found.\n* @param name property name to fetch\n* @param defaultValue value to return if property is missing\n* @return long value of the property or defaultValue\n*/",
        "org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)": "/**\n* Retrieves a float property value by name or returns default if not found.\n* @param name property name to fetch\n* @param defaultValue value to return if property is missing\n* @return parsed float value or defaultValue\n*/",
        "org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)": "/**\n* Retrieves a double property value by name or returns default if not found.\n* @param name property name to fetch\n* @param defaultValue value to return if property is absent\n* @return parsed double value or defaultValue\n*/",
        "org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)": "/**\n* Retrieves a boolean value from a property name or returns default if invalid.\n* @param name property name to fetch\n* @param defaultValue value to return if property is invalid\n* @return boolean value or defaultValue if not valid\n*/",
        "org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)": "/**\n* Retrieves Class by name or returns default if not found.\n* @param name class name to fetch, @param defaultValue fallback Class object\n* @return Class object or defaultValue if name is null\n*/",
        "org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)": "/**\n* Sets boolean property if not already set.\n* @param name property name to check and set\n* @param value boolean value to set if name is unset\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)": "/**\n* Retrieves time duration based on property name or returns default value.\n* @param name property name, @param defaultValue fallback duration, @param unit unit of default value\n* @return duration in the specified unit\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)": "/**\n* Retrieves time duration by name, converting to specified TimeUnit.\n* @param name property name, @param defaultValue fallback duration string, @param unit target TimeUnit\n* @return duration in target TimeUnit\n*/",
        "org.apache.hadoop.conf.Configuration:getInts(java.lang.String)": "/**\n* Converts trimmed substrings to integers.\n* @param name property name for fetching substrings\n* @return array of parsed integers\n*/",
        "org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)": "/**\n* Retrieves time durations as long array from trimmed property values.\n* @param name property name to fetch durations\n* @param unit target TimeUnit for the durations\n* @return array of durations in the specified TimeUnit\n*/",
        "org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])": "/**\n* Retrieves an array of Class objects from a property name.\n* @param name the property name to fetch classes from\n* @param defaultValue fallback classes if property value is null\n* @return array of Class objects or defaultValue if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)": "/**\n* Retrieves a File object based on directory property and path.\n* @param dirsProp property name for directory paths\n* @param path file name to locate\n* @return File object if found, throws IOException if not\n*/",
        "org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)": "/**\n* Writes XML document to output stream.\n* @param propertyName optional property to include\n* @param out Writer to output the XML\n* @param config Configuration object for sensitive key handling\n*/",
        "org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)": "/**\n* Dumps configuration properties as JSON.\n* @param config Configuration object to extract properties\n* @param out Writer to output the JSON data\n*/",
        "org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)": "/**\n* Retrieves InetSocketAddress from a trimmed property or defaults.\n* @param name property name for address lookup\n* @param defaultAddress fallback address if not found\n* @param defaultPort port to use if not specified\n* @return InetSocketAddress created from address and port\n*/",
        "org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)": "/**\n* Updates connection address using host and address properties.\n* @param hostProperty name of the host property\n* @param addressProperty name of the address property\n* @param defaultAddressValue fallback address if not found\n* @param addr original InetSocketAddress\n* @return updated InetSocketAddress\n*/",
        "org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String)": "/**\n* Retrieves the password as a char array from config if clear text fallback is enabled.\n* @param name the property name to fetch\n* @return char array of password or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)": "/**\n* Retrieves Class by name, ensuring it implements a specified interface.\n* @param name class name, @param defaultValue fallback Class, @param xface interface to check\n* @return Class implementing xface or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String)": "/**\n* Retrieves password as char[] from credential providers by name.\n* @param name the name of the credential to fetch\n* @return char[] representing the credential or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)": "/**\n* Retrieves instances of classes implementing a specified interface.\n* @param name property name to fetch classes\n* @param xface interface class type to match\n* @return list of instances implementing the interface\n*/",
        "org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)": "/**\n* Writes XML document to output stream.\n* @param propertyName optional property to include\n* @param out Writer to output the XML\n*/",
        "org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)": "/**\n* Dumps specified configuration property as JSON.\n* @param config configuration object\n* @param propertyName name of the property to dump\n* @param out Writer for JSON output\n*/",
        "org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)": "/**\n* Retrieves InetSocketAddress using host and address properties with defaults.\n* @param hostProperty property name for host lookup\n* @param addressProperty property name for address lookup\n* @param defaultAddressValue fallback address if not found\n* @param defaultPort port to use if not specified\n* @return InetSocketAddress created from host or defaults\n*/",
        "org.apache.hadoop.conf.Configuration:getPassword(java.lang.String)": "/**\n* Retrieves password as char[] from providers or config.\n* @param name the credential name to fetch\n* @return char[] password or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer)": "/**\n* Writes XML document to output stream.\n* @param out Writer to output the XML\n*/",
        "org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream)": "/**\n* Writes XML data to the specified output stream.\n* @param out OutputStream to write the XML data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.conf.Configuration:main(java.lang.String[])": "/**\n* Initializes Configuration and writes XML to standard output.\n* @param args command-line arguments\n* @throws Exception if an error occurs during XML writing\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)": "/**\n* Adds a resource from the input stream with a specified name.\n* @param in InputStream to read resource data\n* @param name Name of the resource to be added\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.lang.String)": "/**** Adds a resource by name. \n* @param name the name of the resource to add \n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.net.URL)": "/**\n* Adds a resource from the specified URL.\n* @param url the URL to create a Resource from\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path)": "/**\n* Adds a resource from the specified file path.\n* @param file path to the resource file\n*/",
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream)": "/**\n* Adds a resource from an InputStream.\n* @param in InputStream to create a Resource object\n*/",
        "org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)": "/**\n* Creates a local Path for the given file in specified directories.\n* @param dirsProp comma-separated directory properties\n* @param path the file path to create\n* @throws IOException if no valid local directories are found\n*/"
    },
    "org.apache.hadoop.tools.TableListing": {
        "org.apache.hadoop.tools.TableListing:addRow(java.lang.String[])": "/**\n* Adds a new row of data to the table.\n* @param row variable-length array of column values\n* @throws RuntimeException if row length does not match column count\n*/",
        "org.apache.hadoop.tools.TableListing:<init>(org.apache.hadoop.tools.TableListing$Column[],boolean,int)": "/**\n* Initializes a TableListing with specified columns and display options.\n* @param columns Array of Column objects for the table\n* @param showHeader Flag to display table header\n* @param wrapWidth Maximum width for text wrapping in the table\n*/",
        "org.apache.hadoop.tools.TableListing:toString()": "/**\n* Generates a formatted string representation of the table.\n* @return formatted table as a string\n*/"
    },
    "org.apache.hadoop.tools.TableListing$Builder": {
        "org.apache.hadoop.tools.TableListing$Builder:<init>()": "/**\n* Constructs a new Builder instance.\n*/",
        "org.apache.hadoop.tools.TableListing$Builder:wrapWidth(int)": "/**\n* Sets the wrap width and returns the Builder instance.\n* @param width the wrap width to set\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.tools.TableListing$Builder:build()": "/**\n* Constructs a TableListing from specified columns and options.\n* @return TableListing instance with configured columns and display settings\n*/",
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)": "/**\n* Adds a field to the builder with specified title, justification, and wrap settings.\n* @param title column title for the new field\n* @param justification alignment of the column content\n* @param wrap determines if content should wrap\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String)": "/**** Adds a field with left justification and no wrapping. \n* @param title column title for the new field \n* @return Builder instance for method chaining \n*/",
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)": "/**\n* Adds a field to the builder with a title and justification.\n* @param title column title for the new field\n* @param justification alignment of the column content\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)": "/**\n* Adds a field with left justification and wrap option.\n* @param title column title for the new field\n* @param wrap determines if content should wrap\n* @return Builder instance for method chaining\n*/"
    },
    "org.apache.hadoop.tracing.Span": {
        "org.apache.hadoop.tracing.Span:addKVAnnotation(java.lang.String,java.lang.String)": "/**\n* Adds a key-value annotation to the span.\n* @param key the annotation key\n* @param value the annotation value\n* @return the updated Span object\n*/",
        "org.apache.hadoop.tracing.Span:addTimelineAnnotation(java.lang.String)": "/**\n* Adds a timeline annotation message.\n* @param msg the annotation message to be added\n* @return the current Span instance\n*/",
        "org.apache.hadoop.tracing.Span:<init>()": "/** Initializes a new Span object. */",
        "org.apache.hadoop.tracing.Span:close()": "/**\n* Closes resources or connections held by this object.\n*/",
        "org.apache.hadoop.tracing.Span:getContext()": "/**\n* Retrieves the current SpanContext.\n* @return SpanContext object, or null if not available\n*/"
    },
    "org.apache.hadoop.fs.BBUploadHandle": {
        "org.apache.hadoop.fs.BBUploadHandle:<init>(java.nio.ByteBuffer)": "/**\n* Initializes BBUploadHandle with byte array from ByteBuffer.\n* @param byteBuffer source ByteBuffer containing data\n*/",
        "org.apache.hadoop.fs.BBUploadHandle:hashCode()": "/**\n* Returns the hash code for the object based on its byte array.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.BBUploadHandle:bytes()": "/**\n* Wraps byte array into a ByteBuffer.\n* @return ByteBuffer containing the byte array\n*/",
        "org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer)": "/**\n* Creates UploadHandle from ByteBuffer.\n* @param byteBuffer source ByteBuffer containing data\n* @return new BBUploadHandle instance\n*/",
        "org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object)": "/**\n* Compares this UploadHandle with another for equality.\n* @param other object to compare\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries": {
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:<init>(java.util.List,boolean)": "/**\n* Constructs a BatchedListEntries object with specified entries and availability status.\n* @param entries list of entries\n* @param hasMore indicates if more entries are available\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:get(int)": "/**\n* Retrieves the element at the specified index.\n* @param i index of the element to retrieve\n* @return element at index i\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:size()": "/**\n* Returns the number of entries in the collection.\n* @return the size of the entries collection\n*/",
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:hasMore()": "/**\n* Checks if there are more items available.\n* @return true if more items exist, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.permission.PermissionStatus$1": {
        "org.apache.hadoop.fs.permission.PermissionStatus$1:<init>()": "/**\n* Private constructor for PermissionStatus class; prevents instantiation.\n*/"
    },
    "org.apache.hadoop.fs.permission.RawParser": {
        "org.apache.hadoop.fs.permission.RawParser:getPermission()": "/**\n* Retrieves the current permission level.\n* @return short representing the permission level\n*/",
        "org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String)": "/**\n* Initializes RawParser with permission mode string.\n* @param modeStr permission mode string\n*/"
    },
    "org.apache.hadoop.fs.permission.UmaskParser": {
        "org.apache.hadoop.fs.permission.UmaskParser:getUMask()": "/**\n* Retrieves the umask value.\n* @return Complement of umask in symbolic mode or raw umaskMode value\n*/",
        "org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String)": "/**\n* Initializes UmaskParser with permission mode string.\n* @param modeStr permission mode string\n*/"
    },
    "org.apache.hadoop.fs.permission.FsPermission$1": {
        "org.apache.hadoop.fs.permission.FsPermission$1:<init>()": "/**\n* Private constructor for FsPermission to prevent instantiation.\n*/"
    },
    "org.apache.hadoop.fs.permission.AclEntry$Builder": {
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setScope(org.apache.hadoop.fs.permission.AclEntryScope)": "/**\n* Sets the scope for the ACL entry.\n* @param scope the AclEntryScope to set\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setType(org.apache.hadoop.fs.permission.AclEntryType)": "/**\n* Sets the ACL entry type and returns the builder instance.\n* @param type the ACL entry type to set\n* @return the builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setPermission(org.apache.hadoop.fs.permission.FsAction)": "/**\n* Sets the permission for the builder.\n* @param permission the FsAction permission to set\n* @return the updated Builder instance\n*/",
        "org.apache.hadoop.fs.permission.AclEntry$Builder:build()": "/**\n* Constructs an AclEntry with specified attributes.\n* @return AclEntry instance created from type, name, permission, and scope\n*/",
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setName(java.lang.String)": "/**\n* Sets the name if it's non-null and non-empty.\n* @param name the name to set\n* @return the Builder instance for chaining\n*/"
    },
    "org.apache.hadoop.fs.permission.PermissionStatus": {
        "org.apache.hadoop.fs.permission.PermissionStatus:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Constructs a PermissionStatus with user, group, and permission details.\n* @param user username associated with the permission\n* @param group group name associated with the permission\n* @param permission FsPermission object defining the access rights\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:<init>()": "/**\n* Private constructor for PermissionStatus; prevents instantiation of this class.\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:toString()": "/**\n* Returns a string representation of the object in \"username:groupname:permission\" format.\n* @return formatted string of username, groupname, and permission\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:getGroupName()": "/**\n* Retrieves the name of the group.\n* @return the name of the group as a String\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:getPermission()": "/**\n* Retrieves the file system permission.\n* @return FsPermission object representing current permissions\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:getUserName()": "/**\n* Retrieves the username of the current user.\n* @return the username as a String\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates an immutable PermissionStatus with specified user, group, and permissions.\n* @param user username, @param group group name, @param permission access rights\n* @return PermissionStatus object\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Writes username, groupname, and permissions to output.\n* @param out output stream, @param username user name, @param groupname group name, @param permission permissions\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream: username, groupname, and permission.\n* @param in input stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput)": "/**\n* Writes user data to output stream.\n* @param out output stream for user data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput)": "/**\n* Reads PermissionStatus from input stream.\n* @param in input stream to read from\n* @return PermissionStatus object\n*/"
    },
    "org.apache.hadoop.fs.permission.PermissionStatus$2": {
        "org.apache.hadoop.fs.permission.PermissionStatus$2:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput stream into class variables.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Constructs a PermissionStatus with user, group, and permission details.\n* @param user username associated with the permission\n* @param group group name associated with the permission\n* @param permission FsPermission object defining the access rights\n*/"
    },
    "org.apache.hadoop.fs.permission.FsCreateModes": {
        "org.apache.hadoop.fs.permission.FsCreateModes:getUnmasked()": "/**\n* Returns the unmasked file system permissions.\n* @return FsPermission object representing unmasked permissions\n*/",
        "org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Constructs FsCreateModes with masked and unmasked permissions.\n* @param masked the masked FsPermission object\n* @param unmasked the unmasked FsPermission object\n*/",
        "org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object)": "/**\n* Compares this FsCreateModes with another object for equality.\n* @param o the object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.permission.FsCreateModes:hashCode()": "/**\n* Computes hash code using superclass and unmasked permissions.\n* @return int hash code based on the object's state\n*/",
        "org.apache.hadoop.fs.permission.FsCreateModes:toString()": "/**\n* Returns a formatted string with masked and unmasked permissions.\n* @return formatted string representation of the object\n*/",
        "org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates FsCreateModes with specified masked and unmasked permissions.\n* @param masked masked FsPermission object\n* @param unmasked unmasked FsPermission object\n* @return FsCreateModes instance\n*/",
        "org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Applies a umask to FsPermission if unmasked permissions are not present.\n* @param mode initial FsPermission to apply umask to\n* @param umask FsPermission to apply as a mask\n* @return updated FsPermission with applied umask\n*/"
    },
    "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission": {
        "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:readFields(java.io.DataInput)": "/**\n* Reads fields from the input stream; not supported in this implementation.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short)": "/**\n* Constructs an ImmutableFsPermission with specified permission mode.\n* @param permission short encoding for permissions and sticky bit\n*/"
    },
    "org.apache.hadoop.fs.permission.AclEntryType": {
        "org.apache.hadoop.fs.permission.AclEntryType:toStringStable()": "/**\n* Returns a stable string representation using enum value names.\n* @return stable string representation of the enum\n*/",
        "org.apache.hadoop.fs.permission.AclEntryType:toString()": "/**\n* Returns a string representation of the object, may change across versions.\n* @return stable string representation of the enum\n*/"
    },
    "org.apache.hadoop.fs.permission.PermissionParser": {
        "org.apache.hadoop.fs.permission.PermissionParser:applyNormalPattern(java.lang.String,java.util.regex.Matcher)": "/**\n* Applies permission changes from a symbolic mode string using a regex matcher.\n* @param modeStr the symbolic mode string to parse\n* @param matcher the regex matcher for extracting permissions\n*/",
        "org.apache.hadoop.fs.permission.PermissionParser:applyOctalPattern(java.util.regex.Matcher)": "/**\n* Applies octal permission patterns from a Matcher.\n* @param matcher contains octal values for sticky and user/group/others modes\n*/",
        "org.apache.hadoop.fs.permission.PermissionParser:combineModeSegments(char,int,int,boolean)": "/**\n* Combines mode segments based on type and conditions.\n* @param type operation type ('+', '-', '=')\n* @param mode initial mode value\n* @param existing current mode state\n* @param exeOk execution status\n* @return updated mode after combination\n*/",
        "org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)": "/**\n* Parses modeStr for permissions using symbolic or octal patterns.\n* @param modeStr permission mode string\n* @param symbolic regex pattern for symbolic permissions\n* @param octal regex pattern for octal permissions\n*/",
        "org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)": "/**\n* Combines multiple mode segments into a single integer value.\n* @param existing current mode state\n* @param exeOk execution status\n* @return combined mode value\n*/"
    },
    "org.apache.hadoop.fs.LocalFileSystem": {
        "org.apache.hadoop.fs.LocalFileSystem:getRaw()": "/**\n* Retrieves the raw file system instance.\n* @return FileSystem object representing the raw file system\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the status of a file link.\n* @param f the path to the file\n* @return FileStatus of the specified file\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the target path of a symbolic link.\n* @param f the path of the symbolic link\n* @return target Path of the link\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link to a target path.\n* @param target the path to the target file\n* @param link the path where the symlink will be created\n* @param createParent flag to create parent directories if needed\n* @throws IOException for I/O errors during link creation\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Constructs LocalFileSystem with a specified raw FileSystem.\n* @param rawLocalFileSystem the FileSystem to wrap\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a File using RawLocalFileSystem.\n* @param path the Path to convert\n* @return File corresponding to the given Path\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:<init>()": "/**\n* Initializes LocalFileSystem with a new RawLocalFileSystem instance.\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)": "/**\n* Moves a file to the highest writable parent directory after a checksum failure.\n* @param p path of the file to move\n* @param in input stream of the file\n* @param inPos position in the input stream\n* @param sums checksum input stream\n* @param sumsPos position in the checksum stream\n* @return false if an error occurs during the process\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes filesystem configuration with URI and adjusts scheme if necessary.\n* @param name URI for filesystem initialization\n* @param conf configuration object for filesystem\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from local to HDFS, optionally deleting the source.\n* @param delSrc flag to delete the source file after copy\n* @param src source Path in local filesystem\n* @param dst destination Path in HDFS\n*/",
        "org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Copies a file from HDFS to local filesystem.\n* @param delSrc flag to delete source after copy, @param src source Path, @param dst destination Path\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$BlockFactory": {
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a BlockFactory with specified buffer directory and configuration.\n* @param keyToBufferDir directory for buffer files\n* @param conf configuration settings for the factory\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:close()": "/**\n* Closes the resource, releasing any associated system resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:getKeyToBufferDir()": "/**\n* Retrieves the key for the buffer directory.\n* @return String representing the buffer directory key\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory": {
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)": "/**\n* Creates a ByteBufferBlock with specified index, limit, and upload statistics.\n* @param index the starting index of the block\n* @param limit the maximum size of the block\n* @param statistics statistics for the block upload\n* @return a new ByteBufferBlock instance\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:getOutstandingBufferCount()": "/**\n* Retrieves the count of outstanding buffers.\n* @return the number of outstanding buffers\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:toString()": "/**\n* Returns a string representation of the ByteBufferBlockFactory object.\n* @return formatted string with buffersOutstanding count\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a ByteBufferBlockFactory with specified buffer directory and configuration.\n* @param keyToBufferDir directory for buffer files\n* @param conf configuration settings for the factory\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int)": "/**\n* Requests a ByteBuffer of specified limit size.\n* @param limit the desired size of the ByteBuffer\n* @return a ByteBuffer instance from the pool\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer)": "/**\n* Releases a ByteBuffer back to the pool and decrements outstanding buffer count.\n* @param buffer the ByteBuffer to be released\n*/"
    },
    "org.apache.hadoop.util.DirectBufferPool": {
        "org.apache.hadoop.util.DirectBufferPool:getBuffer(int)": "/**\n* Retrieves a ByteBuffer of specified size, reusing if available.\n* @param size the desired ByteBuffer size\n* @return a ByteBuffer instance\n*/",
        "org.apache.hadoop.util.DirectBufferPool:returnBuffer(java.nio.ByteBuffer)": "/**\n* Resets and stores a ByteBuffer in a concurrent queue by its capacity size.\n* @param buf the ByteBuffer to be returned\n*/",
        "org.apache.hadoop.util.DirectBufferPool:countBuffersOfSize(int)": "/**\n* Counts the number of ByteBuffers of a specified size.\n* @param size the size of the ByteBuffers to count\n* @return the count of ByteBuffers of the given size\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream": {
        "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream:<init>(int)": "/**\n* Constructs a DataBlockByteArrayOutputStream with specified initial size.\n* @param size initial buffer size\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream:getInputStream()": "/**\n* Creates an InputStream from the buffer and resets it.\n* @return ByteArrayInputStream containing buffer data\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$DataBlock": {
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:<init>(long,org.apache.hadoop.fs.store.BlockUploadStatistics)": "/**\n* Initializes a DataBlock with an index and upload statistics.\n* @param index unique identifier for the data block\n* @param statistics statistics related to block upload\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:verifyState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)": "/**\n* Verifies the current state against the expected state.\n* @param expected the state expected for validation\n* @throws IllegalStateException if the current state does not match the expected state\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:hasData()": "/**\n* Checks if there is any data present.\n* @return true if data size is greater than zero, otherwise false\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:innerClose()": "/**\n* Closes resources and handles any I/O exceptions.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:blockAllocated()": "/**\n* Increments block allocation statistics if available.\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:blockReleased()": "/**\n* Notifies that a block has been released, updating statistics if available.\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getIndex()": "/**\n* Retrieves the current index value.\n* @return the current index as a long\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getState()": "/**\n* Retrieves the current state.\n* @return DestState representing the current state\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getStatistics()": "/**\n* Retrieves the current upload statistics.\n* @return BlockUploadStatistics object containing upload data\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)": "/**** \n* Changes the state if the current state is valid.\n* @param current current state to verify\n* @param next new state to enter\n* @throws IllegalStateException if current state is invalid\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)": "/**\n* Writes data from buffer starting at offset for specified length.\n* @param buffer data source; cannot be null\n* @param offset start position in buffer; must be non-negative\n* @param length amount of data to write; must be non-negative\n* @return number of bytes written\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush()": "/**\n* Flushes the current state after verifying it is 'Writing'.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload()": "/**** Starts the upload process of a data block. \n* @return null as no data is returned during upload initiation \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState()": "/**\n* Transitions to the closed state if not already in that state.\n* @return true if state changed, false if already closed\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:close()": "/**\n* Closes the resource if not already closed, logging the action.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$DiskBlock": {
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:dataSize()": "/**\n* Returns the size of data written in bytes.\n* @return number of bytes written\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:remainingCapacity()": "/**\n* Calculates the remaining capacity based on limit and bytes written.\n* @return remaining capacity as an integer\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:closeBlock()": "/**\n* Closes the block, releasing resources and deleting the buffer file if not already closed.\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)": "/**\n* Constructs a DiskBlock with file buffer, limit, and upload statistics.\n* @param bufferFile file for block storage\n* @param limit maximum size for the block\n* @param index unique block identifier\n* @param statistics block upload statistics\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long)": "/**\n* Checks if adding specified bytes exceeds capacity limit.\n* @param bytes amount of data to add\n* @return true if within limit, false otherwise\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString()": "/**\n* Returns a string representation of the FileBlock object.\n* @return formatted string with index, destFile, state, dataSize, and limit\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose()": "/**\n* Closes the block based on its state and handles buffer file deletion.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)": "/**\n* Writes data to output from the buffer with specified offset and length.\n* @param b byte array data source\n* @param offset start position in the array\n* @param len number of bytes to write\n* @return number of bytes actually written\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush()": "/**\n* Flushes output streams and verifies writing state.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload()": "/**\n* Initiates block upload and returns BlockUploadData.\n* @return BlockUploadData object containing uploaded data\n* @throws IOException if an I/O error occurs during upload\n*/"
    },
    "org.apache.hadoop.fs.store.ByteBufferInputStream": {
        "org.apache.hadoop.fs.store.ByteBufferInputStream:<init>(int,java.nio.ByteBuffer)": "/**\n* Initializes ByteBufferInputStream with specified size and ByteBuffer.\n* @param size the size of the input stream\n* @param byteBuffer the ByteBuffer to read data from\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:close()": "/**\n* Closes the ByteBufferInputStream and clears the byte buffer.\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:verifyOpen()": "/**\n* Verifies if the stream is open; throws IOException if closed.\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:isOpen()": "/**\n* Checks if the byte buffer is initialized.\n* @return true if byteBuffer is not null, otherwise false\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:toString()": "/**\n* Returns a string representation of the ByteBufferInputStream object.\n* @return formatted string with size and byteBuffer details\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState()": "/**\n* Validates if the stream is open; throws exception if closed.\n* @throws IllegalStateException if the stream is closed\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:available()": "/**\n* Returns the number of remaining bytes in the buffer.\n* @return count of available bytes in the byte buffer\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:position()": "/**\n* Returns the current position of the byte buffer.\n* @return current position as an integer\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining()": "/**\n* Checks if the buffer has remaining bytes.\n* @return true if bytes are available, false otherwise\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:reset()": "/**\n* Resets the byte buffer after validating the stream's open state.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:read()": "/**\n* Reads a byte from the buffer if available.\n* @return byte value (0-255) or -1 if no bytes are left\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long)": "/**\n* Skips bytes in the stream by a given offset.\n* @param offset number of bytes to skip\n* @return new position after skipping bytes\n* @throws IOException if stream is closed or offset is invalid\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int)": "/**\n* Marks the current position in the buffer for future reset.\n* @param readlimit maximum number of bytes to read before the mark becomes invalid\n*/",
        "org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)": "/**\n* Reads bytes from the buffer into the provided array.\n* @param b destination array for bytes, @param offset start position, @param length bytes to read\n* @return number of bytes read or -1 if no bytes are available\n*/"
    },
    "org.apache.hadoop.fs.store.EtagChecksum": {
        "org.apache.hadoop.fs.store.EtagChecksum:<init>()": "/**\n* Constructs an EtagChecksum instance.\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:<init>(java.lang.String)": "/**\n* Constructs an EtagChecksum with the specified eTag value.\n* @param eTag the entity tag string\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:getLength()": "/**\n* Returns the length of the eTag string in bytes using UTF-8 encoding.\n* @return length of the eTag in bytes\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:getBytes()": "/**\n* Returns the UTF-8 encoded bytes of the eTag or an empty byte array if eTag is null.\n* @return byte array of eTag bytes or empty array if eTag is null\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:write(java.io.DataOutput)": "/**\n* Writes the eTag to the output stream.\n* @param out DataOutput stream to write the eTag\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:readFields(java.io.DataInput)": "/**\n* Reads eTag from DataInput stream.\n* @param in input stream to read data from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:toString()": "/**\n* Returns a string representation of the object with its eTag value.\n* @return formatted string containing the eTag\n*/",
        "org.apache.hadoop.fs.store.EtagChecksum:getAlgorithmName()": "/**\n* Returns the name of the algorithm used.\n* @return String representing the algorithm name\n*/"
    },
    "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder": {
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:build()": "/**\n* Constructs a new HttpReferrerAuditHeader instance.\n* @return a new HttpReferrerAuditHeader object\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withAttributes(java.util.Map)": "/**\n* Adds attributes from the provided map to the builder.\n* @param value map of attributes to add\n* @return the updated Builder instance\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withAttribute(java.lang.String,java.lang.String)": "/**\n* Adds an attribute to the builder.\n* @param key the attribute name\n* @param value the attribute value\n* @return the builder instance\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withEvaluated(java.util.Map)": "/**\n* Adds evaluated values to the builder.\n* @param value map of string keys to string suppliers\n* @return the Builder instance for chaining\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withEvaluated(java.lang.String,java.util.function.Supplier)": "/**\n* Adds a key-value pair to evaluated map and returns the builder instance.\n* @param key the identifier for the value\n* @param value a supplier providing the value associated with the key\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withFilter(java.util.Collection)": "/**\n* Sets filter fields for the builder.\n* @param fields collection of field names to filter\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>()": "/**\n* Private constructor for Builder class to prevent instantiation.\n*/"
    },
    "org.apache.hadoop.fs.store.audit.AuditSpan": {
        "org.apache.hadoop.fs.store.audit.AuditSpan:close()": "/**\n* Closes the resource by deactivating it.\n*/"
    },
    "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader": {
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:addAttribute(java.lang.String,java.lang.String)": "/**\n* Adds a non-empty attribute to the attributes map.\n* @param key the attribute key\n* @param value the attribute value to be added\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:toString()": "/**\n* Returns a string representation of the HttpReferrerAuditHeader object.\n* @return formatted string with initialHeader value\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:escapeToPathElement(java.lang.CharSequence)": "/**\n* Converts special characters in source to path-safe elements.\n* @param source input string to escape\n* @return escaped string with '/' and '@' replaced by '+'\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:maybeStripWrappedQuotes(java.lang.String)": "/**\n* Removes leading and trailing quotes from the given string.\n* @param header the input string potentially wrapped in quotes\n* @return the stripped string without leading/trailing quotes\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:builder()": "/**\n* Creates a new instance of the Builder class.\n* @return a new Builder object\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer()": "/**\n* Builds an HTTP referrer URI from attributes and returns it.\n* @return Referrer URI as a String, or empty if an error occurs\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)": "/**\n* Sets an attribute by adding it to the attributes map.\n* @param key non-null attribute key\n* @param value attribute value to be added\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String)": "/**\n* Extracts query parameters from a URI string.\n* @param header URI string potentially wrapped in quotes\n* @return Map of query parameter names and values\n*/",
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder)": "/**\n* Constructs HttpReferrerAuditHeader from Builder parameters.\n* @param builder contains necessary attributes for initialization\n*/"
    },
    "org.apache.hadoop.fs.store.audit.AuditingFunctions": {
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:<init>()": "/**\n* Private constructor to prevent instantiation of the AuditingFunctions class.\n*/",
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Executes an operation within an audit span if provided.\n* @param auditSpan optional audit span for tracking\n* @param operation callable operation to execute\n* @return a callable that executes the operation with audit span\n*/",
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.InvocationRaisingIOE)": "/**\n* Returns an operation wrapped in an audit span if provided.\n* @param auditSpan optional audit span to activate\n* @param operation the operation to be executed\n* @return wrapped operation that activates the audit span\n*/",
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Wraps an operation in an audit span if provided.\n* @param auditSpan optional audit span for tracking\n* @param operation function to execute within the span\n* @return wrapped function that activates the span\n*/",
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:callableWithinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,java.util.concurrent.Callable)": "/**\n* Wraps a Callable in an AuditSpan for tracking.\n* @param auditSpan optional AuditSpan for activation\n* @param operation Callable operation to execute\n* @return Callable that activates AuditSpan if present\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock": {
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:dataSize()": "/**\n* Returns the size of the data.\n* @return size of data or buffer if dataSize is null\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:innerClose()": "/**\n* Closes the inner resources by nullifying the buffer and releasing the block.\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:toString()": "/**\n* Returns a string representation of the ByteArrayBlock object.\n* @return formatted string with index, state, limit, and dataSize\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)": "/**\n* Initializes ByteArrayBlock with index, limit, and statistics.\n* @param index block index, @param limit buffer size limit, @param statistics upload stats\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long)": "/**\n* Checks if there is enough capacity for additional bytes.\n* @param bytes amount of bytes to check for capacity\n* @return true if capacity is available, false otherwise\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity()": "/**\n* Calculates remaining capacity based on limit and current data size.\n* @return remaining capacity as an integer\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)": "/**\n* Writes data to buffer with respect to available capacity.\n* @param b byte array to write from; cannot be null\n* @param offset start position in array; must be non-negative\n* @param len number of bytes to write; must be non-negative\n* @return number of bytes actually written\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload()": "/**\n* Initiates block upload and returns BlockUploadData object.\n* @return BlockUploadData containing upload stream data\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData": {
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.InputStream)": "/**\n* Initializes BlockUploadData with an input stream for upload.\n* @param uploadStream stream containing upload data\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(byte[])": "/**\n* Initializes BlockUploadData with a byte array.\n* @param byteArray data to be uploaded, must not be null\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File)": "",
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close()": "/**\n* Closes resources and logs exceptions; deletes file if present.\n* @throws IOException if an I/O error occurs during cleanup\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray()": "/**\n* Converts the object to a byte array.\n* @return byte array representation of the object\n* @throws IOException if an I/O error occurs during conversion\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock": {
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:bufferCapacityUsed()": "/**\n* Calculates the used capacity of the block buffer.\n* @return the number of bytes used in the buffer\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:remainingCapacity()": "/**\n* Returns the remaining capacity of the block buffer.\n* @return remaining capacity as an integer, or 0 if blockBuffer is null\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:innerClose()": "/**\n* Closes the inner resources and releases the block buffer if it's not null.\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize()": "/**\n* Returns the size of data; uses buffer capacity if dataSize is null.\n* @return size of data in bytes\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)": "/**\n* Checks if the block buffer can accommodate the specified byte size.\n* @param bytes size in bytes to check against capacity\n* @return true if capacity is sufficient, false otherwise\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)": "/**\n* Writes data from byte array to block buffer.\n* @param b byte array data source; cannot be null\n* @param offset start position in array; must be non-negative\n* @param len amount of data to write; must be non-negative\n* @return number of bytes written to the buffer\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString()": "/**\n* Returns a string representation of the ByteBufferBlock object.\n* @return formatted string with index, state, dataSize, limit, and remaining capacity\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload()": "/**\n* Initiates data block upload and prepares buffer for reading.\n* @return BlockUploadData containing upload data stream\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks": {
        "org.apache.hadoop.fs.store.DataBlocks:<init>()": "/**\n* Private constructor for DataBlocks class to prevent instantiation.\n*/",
        "org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)": "/**\n* Validates write arguments for a byte array.\n* @param b byte array to validate\n* @param off starting offset\n* @param len length to write\n* @throws IOException if validation fails\n*/",
        "org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Creates a BlockFactory based on the specified type.\n* @param keyToBufferDir directory for buffer files\n* @param configuration configuration settings\n* @param name type of block factory to create\n* @return BlockFactory instance\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Cache$Key": {
        "org.apache.hadoop.fs.FileSystem$Cache$Key:isEqual(java.lang.Object,java.lang.Object)": "/**\n* Checks if two objects are equal.\n* @param a first object to compare\n* @param b second object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode()": "/**\n* Computes the hash code for the object based on its components.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object)": "/**\n* Compares this Key object with another for equality.\n* @param obj object to compare with this Key\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache$Key:toString()": "/**\n* Returns a formatted string representation of the object.\n* @return String with user info, scheme, and authority details\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)": "/**\n* Initializes a Key object with URI scheme, authority, and unique identifier.\n* @param uri the URI containing scheme and authority\n* @param conf configuration settings (not used here)\n* @param unique unique identifier for the Key\n* @throws IOException if fetching current user fails\n*/",
        "org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Key object from URI and configuration.\n* @param uri the URI containing scheme and authority\n* @param conf configuration settings (not used here)\n*/"
    },
    "org.apache.hadoop.util.ShutdownHookManager": {
        "org.apache.hadoop.util.ShutdownHookManager:get()": "/**\n* Retrieves the singleton instance of ShutdownHookManager.\n* @return ShutdownHookManager instance\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:isShutdownInProgress()": "/**\n* Checks if a shutdown process is currently in progress.\n* @return true if shutdown is in progress, false otherwise\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:getShutdownHooksInOrder()": "/**\n* Retrieves sorted shutdown hooks by priority.\n* @return List of HookEntry sorted in descending order of priority\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:<init>()": "/**\n* Constructor for ShutdownHookManager, intended for internal use and testing.\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:clearShutdownHooks()": "/**\n* Clears all registered shutdown hooks.\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)": "/**\n* Adds a shutdown hook with specified priority and timeout.\n* @param shutdownHook the Runnable to execute on shutdown\n* @param priority execution priority of the hook\n* @param timeout duration before timeout occurs\n* @param unit the time unit for the timeout\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable)": "/**\n* Removes a shutdown hook if not in progress.\n* @param shutdownHook the Runnable to remove\n* @return true if removed, false if not found\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable)": "/**** Checks if a shutdown hook is registered.  \n* @param shutdownHook the Runnable to check  \n* @return true if the hook is registered, false otherwise  \n*/",
        "org.apache.hadoop.util.ShutdownHookManager:executeShutdown()": "/**\n* Executes shutdown hooks and counts timeouts.\n* @return number of hooks that timed out during execution\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves shutdown timeout duration from configuration.\n* @param conf configuration object\n* @return timeout duration in milliseconds, at least TIMEOUT_MINIMUM\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration)": "/**\n* Shuts down the executor with a timeout from configuration.\n* @param conf configuration object for timeout settings\n*/",
        "org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)": "/**** Adds a shutdown hook with specified priority. \n* @param shutdownHook the Runnable to execute on shutdown\n* @param priority the execution priority of the hook\n*/"
    },
    "org.apache.hadoop.fs.ZeroCopyUnavailableException": {
        "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.String)": "/**\n* Constructs a ZeroCopyUnavailableException with a specified error message.\n* @param message the detail message for the exception\n*/",
        "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.String,java.lang.Exception)": "/**\n* Constructs a ZeroCopyUnavailableException with a message and cause.\n* @param message error message\n* @param e underlying exception that caused this exception\n*/",
        "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.Exception)": "/**\n* Constructs a ZeroCopyUnavailableException with the specified cause.\n* @param e the underlying exception that caused this exception\n*/"
    },
    "org.apache.hadoop.fs.CommonPathCapabilities": {
        "org.apache.hadoop.fs.CommonPathCapabilities:<init>()": "/**\n* Private constructor for CommonPathCapabilities class.\n*/"
    },
    "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer": {
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:writeChunk(byte[],int,int,byte[],int,int)": "/**\n* Writes a data chunk and its checksum to the output.\n* @param b byte array containing data to write\n* @param offset starting position in data array\n* @param len number of bytes to write from data array\n* @param checksum byte array containing checksum data\n* @param ckoff starting position in checksum array\n* @param cklen number of bytes to write from checksum array\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:checkClosed()": "/**\n* Checks if the channel is closed and throws an exception if it is.\n* @throws IOException if the channel is closed\n*/",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close()": "",
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Initializes ChecksumFSOutputSummer with checksum data and creates files.\n* @param fs the ChecksumFs instance, file the target Path, createFlag flags for creation,\n*        absolutePermission permissions, bufferSize size of the buffer, replication factor,\n*        blockSize size of the block, progress progress indicator, checksumOpt options,\n*        createParent whether to create parent directories\n*/"
    },
    "org.apache.hadoop.fs.UnsupportedMultipartUploaderException": {
        "org.apache.hadoop.fs.UnsupportedMultipartUploaderException:<init>(java.lang.String)": "/**\n* Constructs an UnsupportedMultipartUploaderException with a specified message.\n* @param message detail message for the exception\n*/"
    },
    "org.apache.hadoop.fs.LocalDirAllocator": {
        "org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String,org.apache.hadoop.util.DiskValidator)": "/**\n* Constructs a LocalDirAllocator with specified context and disk validator.\n* @param contextCfgItemName configuration item name for context\n* @param diskValidator validator for disk space checks\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:isContextValid(java.lang.String)": "/**\n* Checks if the given context configuration item name is valid.\n* @param contextCfgItemName name of the context configuration item\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:removeContext(java.lang.String)": "/**\n* Removes a context configuration item by name.\n* @param contextCfgItemName name of the context item to remove\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String)": "/**\n* Retrieves or creates an AllocatorPerContext by configuration item name.\n* @param contextCfgItemName configuration item name\n* @return AllocatorPerContext instance\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex()": "/**\n* Retrieves the index of the current directory.\n* @return index of the current directory from context\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Checks if a file exists using a specified path and configuration.\n* @param pathStr the file path as a String\n* @param conf configuration settings\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String)": "/**\n* Initializes LocalDirAllocator with a context configuration item name.\n* @param contextCfgItemName name for context configuration\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Retrieves a writable local path based on size and configuration.\n* @param pathStr desired path string\n* @param size required size for the path\n* @param conf configuration settings\n* @param checkWrite flag to validate write permissions\n* @return Path object for writing\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves local file path using context and configuration.\n* @param pathStr the file path as a string\n* @param conf configuration settings\n* @return Path object\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves all local file paths to read based on the given path and configuration.\n* @param pathStr the path string to process\n* @param conf configuration settings\n* @return iterable of Path objects\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a writable local path based on size and configuration.\n* @param pathStr desired path string\n* @param size required size for the path\n* @param conf configuration settings\n* @return Path object for writing\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a temporary writable file in a specified directory.\n* @param pathStr desired path string\n* @param size required size for the file\n* @param conf configuration settings\n* @return temporary File object\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Gets a writable local path based on provided path string and configuration.\n* @param pathStr desired path string\n* @param conf configuration settings\n* @return Path object for writing\n*/"
    },
    "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext": {
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:<init>(java.lang.String,org.apache.hadoop.util.DiskValidator)": "/**\n* Constructs AllocatorPerContext with configuration item name and disk validator.\n* @param contextCfgItemName name of the context configuration item\n* @param diskValidator instance for validating disk operations\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getCurrentDirectoryIndex()": "/**\n* Retrieves the index of the last accessed directory.\n* @return index of the last accessed directory\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Checks if a file exists in local directories.\n* @param pathStr the file path as a String\n* @param conf configuration settings\n* @return true if the file exists, false otherwise\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)": "/**\n* Creates a Path object, checking write permissions if specified.\n* @param dir parent directory Path\n* @param path child path as a String\n* @param checkWrite flag to validate write permissions\n* @return Path object or null if write check fails\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration)": "/**\n* Updates context with new configuration and local directories.\n* @param conf configuration settings\n* @return updated Context object\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Retrieves a writable local path based on size and configuration.\n* @param pathStr desired path string\n* @param size required size for the path\n* @param conf configuration settings\n* @param checkWrite flag to validate write permissions\n* @return Path object for writing or throws DiskErrorException if unavailable\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a local file path based on the provided string and configuration.\n* @param pathStr the file path as a string\n* @param conf configuration settings\n* @return Path object if found, throws DiskErrorException if not\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves all local file paths to read.\n* @param pathStr the path string to process\n* @param conf configuration settings\n* @return iterable of Path objects\n*/",
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a temporary writable file in a specified directory.\n* @param pathStr desired path string\n* @param size required size for the file\n* @param conf configuration settings\n* @return temporary File object\n*/"
    },
    "org.apache.hadoop.fs.FilterFs": {
        "org.apache.hadoop.fs.FilterFs:getFsStatus()": "/**\n* Retrieves the file system status.\n* @return FsStatus object representing current file system status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:getServerDefaults()": "/**\n* Retrieves filesystem server defaults.\n* @return FsServerDefaults object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:getUriDefaultPort()": "/**\n* Retrieves the default port for the URI.\n* @return default port number as an integer\n*/",
        "org.apache.hadoop.fs.FilterFs:setVerifyChecksum(boolean)": "/**\n* Sets the checksum verification flag for the file system.\n* @param verifyChecksum true to enable, false to disable verification\n* @throws IOException if an I/O error occurs\n* @throws UnresolvedLinkException if a symbolic link cannot be resolved\n*/",
        "org.apache.hadoop.fs.FilterFs:getMyFs()": "/**\n* Retrieves the current file system instance.\n* @return AbstractFileSystem object representing the current file system\n*/",
        "org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the storage policy for a given file path.\n* @param src path of the source file\n* @return BlockStoragePolicySpi object\n*/",
        "org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)": "/**\n* Sets an extended attribute for a specified path.\n* @param path target file path\n* @param name attribute name\n* @param value value to set\n* @param flag options for setting the attribute\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FilterFs:getUri()": "/**\n* Retrieves the URI from the file system instance.\n* @return URI object from the file system\n*/",
        "org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path)": "",
        "org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Retrieves extended attributes for a specified file path.\n* @param path the file path to get attributes from\n* @param names list of attribute names to retrieve\n* @return map of attribute names to their byte values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Removes an extended attribute from a specified file.\n* @param path the file path from which to remove the attribute\n* @param name the name of the extended attribute to remove\n* @throws IOException if removal is unsupported\n*/",
        "org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves filesystem status for the specified path.\n* @param f the path to check the filesystem status\n* @return FsStatus of the specified path\n*/",
        "org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Deletes a snapshot by directory and name.\n* @param path directory of the snapshot\n* @param snapshotName name of the snapshot to delete\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Sets the storage policy for a specified file system path.\n* @param path the file system path to set the policy for\n* @param policyName the name of the storage policy to apply\n*/",
        "org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Retrieves extended attribute by path and name.\n* @param path the file system path\n* @param name the attribute name to retrieve\n* @return byte array of the attribute value\n* @throws IOException if the operation is not supported\n*/",
        "org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server default settings.\n* @param f file path (unused in this implementation)\n* @return FsServerDefaults object\n*/",
        "org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Sets the ACL for a specified path.\n* @param path the file or directory path\n* @param aclSpec list of ACL entries to set\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path)": "/**\n* Retrieves extended attributes from the specified file path.\n* @param path the file path for which to get extended attributes\n* @return a map of attribute names and values as byte arrays\n* @throws IOException if the operation is unsupported\n*/",
        "org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Creates a snapshot of the specified path.\n* @param path the path for which to create a snapshot\n* @param snapshotName the name of the snapshot\n* @return Path of the created snapshot\n* @throws IOException if snapshot creation fails\n*/",
        "org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String)": "/**\n* Validates the given name by delegating to myFs.\n* @param src the name to validate\n* @return true if valid, false if contains prohibited elements\n*/",
        "org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Modifies ACL entries for a specified file path.\n* @param path the file path for ACL modification\n* @param aclSpec list of ACL entries to modify\n*/",
        "org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)": "/**\n* Removes specified ACL entries from the given path.\n* @param path the path from which to remove ACL entries\n* @param aclSpec list of ACL entries to be removed\n*/",
        "org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path)": "/**\n* Removes the default ACL for the specified path.\n* @param path the path for which default ACL removal is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Renames a snapshot at the specified path.\n* @param path the file path of the snapshot\n* @param snapshotOldName current name of the snapshot\n* @param snapshotNewName desired new name for the snapshot\n* @throws IOException if renaming fails\n*/",
        "org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Validates storage policy for a given file system path.\n* @param path the file system path to check\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)": "/**\n* Unsets the storage policy for a specified path.\n* @param src the path to unset the storage policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:getAllStoragePolicies()": "/**\n* Retrieves all block storage policies.\n* @return Collection of BlockStoragePolicySpi or throws IOException on failure\n*/",
        "org.apache.hadoop.fs.FilterFs:supportsSymlinks()": "/**\n* Checks if the file system supports symbolic links.\n* @return true if symlinks are supported, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Creates a symbolic link at the specified path.\n* @param target path to the target file\n* @param link path for the symlink\n* @param createParent whether to create parent directories\n* @throws IOException if symlinks are not supported\n*/",
        "org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path)": "/**\n* Resolves the target path for a given link.\n* @param f the path to resolve\n* @return the resolved Path object\n* @throws IOException if link resolution fails\n*/",
        "org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String)": "/**\n* Retrieves delegation tokens for the specified renewer.\n* @param renewer the identifier for the token renewer\n* @return list of tokens or an empty list if none found\n*/",
        "org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory()": "/**\n* Returns the initial working directory from the file system.\n* @return Path object representing the initial working directory, or null if not set\n*/",
        "org.apache.hadoop.fs.FilterFs:msync()": "/**\n* Synchronizes file system changes to storage.\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if msync is unsupported\n*/",
        "org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path)": "/**\n* Lists corrupt file blocks for the specified path.\n* @param path the file path to check for corrupt blocks\n* @return an iterator of corrupt file block paths\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.fs.FilterFs:getStatistics()": "/**\n* Retrieves current statistics from the file system.\n* @return Statistics object containing statistical data\n*/",
        "org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])": "/**\n* Sets an extended attribute for a specified path.\n* @param path the target file path\n* @param name the name of the attribute\n* @param value the value to set for the attribute\n*/",
        "org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)": "/**\n* Opens a file with specified options.\n* @param path file path to open\n* @param parameters options for opening the file\n* @return CompletableFuture of FSDataInputStream\n*/",
        "org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path)": "/**\n* Creates a MultipartUploaderBuilder for the specified base path.\n* @param basePath directory path for uploads\n* @return MultipartUploaderBuilder instance\n*/",
        "org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem)": "/****\n* Initializes FilterFs with an AbstractFileSystem instance.\n* @param fs the AbstractFileSystem to wrap\n* @throws URISyntaxException if the URI is invalid\n*/",
        "org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path)": "/**\n* Validates the specified path using the file system's checkPath method.\n* @param path the Path object to validate\n*/",
        "org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the URI path from a given Path object.\n* @param p the Path to extract the URI from\n* @return the URI path as a String\n*/",
        "org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path)": "/**\n* Resolves and returns the path of a given Path object.\n* @param p the Path to resolve\n* @return resolved Path object\n* @throws exceptions related to file access and resolution\n*/",
        "org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)": "/**\n* Creates a file with specified parameters.\n* @param f file path, @param flag creation flags, @return FSDataOutputStream for the file\n*/",
        "org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes a file or directory, optionally recursively.\n* @param f the Path to delete\n* @param recursive whether to delete directories recursively\n* @return true if deletion was successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)": "/**\n* Retrieves block locations for a file within specified range.\n* @param f file path, @param start start offset, @param len length of the range\n* @return array of BlockLocation objects\n*/",
        "org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the checksum of a specified file.\n* @param f the Path object representing the file\n* @return FileChecksum of the file\n*/",
        "org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the file status for a given path.\n* @param f the Path object representing the file\n* @return FileStatus of the specified path\n*/",
        "org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path)": "/**\n* Retrieves file link status after validating the path.\n* @param f the file path to check status for\n* @return FileStatus object representing the file's status\n*/",
        "org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file status for the given path.\n* @param f the Path to list file statuses for\n* @return array of FileStatus objects\n*/",
        "org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists located file statuses for a given path after validating it.\n* @param f the path to list file statuses\n* @return iterator of LocatedFileStatus objects\n*/",
        "org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)": "/**\n* Creates a directory with specified permissions.\n* @param dir directory path to create\n* @param permission permissions for the new directory\n* @param createParent flag to create parent directories if needed\n*/",
        "org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path)": "/**\n* Opens a file stream after validating the path.\n* @param f the file path to open\n* @return FSDataInputStream for the file\n*/",
        "org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a file for reading with specified buffer size.\n* @param f the Path of the file to open\n* @param bufferSize the size of the buffer for reading\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)": "/**\n* Truncates a file to a specified length after validating its path.\n* @param f path to the file to truncate\n* @param newLength desired new length of the file\n* @return true if truncation is successful\n*/",
        "org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)": "/**\n* Sets the owner and group of a specified file.\n* @param f Path of the file to modify\n* @param username new owner's username\n* @param groupname new owner's group name\n*/",
        "org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets file permissions for the specified path.\n* @param f the Path to set permissions for\n* @param permission the FsPermission object to apply\n*/",
        "org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)": "/**\n* Sets the replication factor for a file.\n* @param f the Path of the file to modify\n* @param replication the new replication factor\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)": "/**\n* Sets modification and access times for a file.\n* @param f file path, @param mtime modification time, @param atime access time\n*/",
        "org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path)": "/**\n* Converts a Path to a qualified URI Path.\n* @param path the Path to be qualified\n* @return a qualified Path object\n*/",
        "org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the specified path supports a capability.\n* @param path the Path object to check\n* @param capability the capability string to verify\n* @return true if capability is supported, false otherwise\n*/",
        "org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path)": "/**\n* Retrieves the qualified enclosing root Path for the specified Path.\n* @param path the Path to qualify\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)": "/**\n* Renames a file or directory, delegating to myFs.\n* @param src source path to rename, @param dst destination path, @param overwrite allows overwriting\n*/",
        "org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Validates and renames a file or directory from src to dst.\n* @param src source path to rename\n* @param dst destination path for the rename\n* @throws IOException if an I/O error occurs\n* @throws UnresolvedLinkException if a link cannot be resolved\n*/",
        "org.apache.hadoop.fs.FilterFs:getCanonicalServiceName()": "/**\n* Returns the canonical service name from the underlying file system.\n* @return formatted service name or null if authority is absent\n*/",
        "org.apache.hadoop.fs.FilterFs:getHomeDirectory()": "/**\n* Retrieves the home directory path for the current user.\n* @return Path object representing the user's home directory\n*/",
        "org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Checks access permissions for a specified path.\n* @param path the file path to check permissions for\n* @param mode the action to verify permissions against\n*/"
    },
    "org.apache.hadoop.util.DiskChecker$DiskErrorException": {
        "org.apache.hadoop.util.DiskChecker$DiskErrorException:<init>(java.lang.String)": "/**\n* Constructs a DiskErrorException with a specified message.\n* @param msg the detail message for the exception\n*/",
        "org.apache.hadoop.util.DiskChecker$DiskErrorException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a DiskErrorException with a message and a cause.\n* @param msg descriptive error message\n* @param cause underlying throwable cause of the exception\n*/"
    },
    "org.apache.hadoop.service.LoggingStateChangeListener": {
        "org.apache.hadoop.service.LoggingStateChangeListener:<init>(org.slf4j.Logger)": "/**\n* Initializes LoggingStateChangeListener with a Logger.\n* @param log Logger instance for logging state changes\n*/",
        "org.apache.hadoop.service.LoggingStateChangeListener:stateChanged(org.apache.hadoop.service.Service)": "/**\n* Logs the state change of a service.\n* @param service the Service object whose state has changed\n*/",
        "org.apache.hadoop.service.LoggingStateChangeListener:<init>()": "/**\n* Constructs LoggingStateChangeListener with a default logger.\n*/"
    },
    "org.apache.hadoop.service.ServiceStateException": {
        "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a ServiceStateException with a message and cause.\n* @param message error message\n* @param cause underlying Throwable cause, may provide exit code\n*/",
        "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.Throwable)": "/**\n* Constructs a ServiceStateException with the specified cause.\n* @param cause the underlying reason for the exception\n*/",
        "org.apache.hadoop.service.ServiceStateException:getExitCode()": "",
        "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String)": "/**\n* Constructs a ServiceStateException with an error message.\n* @param message error message for the exception\n*/",
        "org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)": "/**\n* Constructs a ServiceStateException with exit code, message, and cause.\n* @param exitCode error exit code\n* @param message error message\n* @param cause underlying Throwable cause\n*/",
        "org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)": "/**\n* Converts a Throwable to a RuntimeException or creates a ServiceStateException.\n* @param text error message for the exception\n* @param fault original Throwable to convert\n* @return RuntimeException based on the input fault\n*/",
        "org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable)": "/**\n* Converts Throwable to RuntimeException or ServiceStateException.\n* @param fault the Throwable to convert\n* @return RuntimeException or a new ServiceStateException\n*/"
    },
    "org.apache.hadoop.service.ServiceStateModel": {
        "org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String,org.apache.hadoop.service.Service$STATE)": "/**\n* Constructs a ServiceStateModel with a name and state.\n* @param name service name\n* @param state current state of the service\n*/",
        "org.apache.hadoop.service.ServiceStateModel:isInState(org.apache.hadoop.service.Service$STATE)": "/**\n* Checks if the current state matches the proposed state.\n* @param proposed the state to compare with the current state\n* @return true if states match, false otherwise\n*/",
        "org.apache.hadoop.service.ServiceStateModel:getState()": "/**\n* Retrieves the current state of the service.\n* @return Service.STATE representing the current state\n*/",
        "org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String)": "/**\n* Initializes ServiceStateModel with a name and default NOTINITED state.\n* @param name service name\n*/",
        "org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)": "/**\n* Checks if a state transition is valid.\n* @param current current state\n* @param proposed proposed state\n* @return true if transition is valid, false otherwise\n*/",
        "org.apache.hadoop.service.ServiceStateModel:toString()": "/**\n* Returns a string representation of the object including name and state.\n* @return formatted string of name and state\n*/",
        "org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE)": "/**\n* Validates current service state against the expected state.\n* @param expectedState the state that must be matched\n*/",
        "org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)": "/**\n* Validates state transition and throws exception if invalid.\n* @param name service name, @param state current state, @param proposed desired state\n*/",
        "org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE)": "/**\n* Updates service state and returns the previous state.\n* @param proposed desired new state\n* @return oldState previous state before transition\n*/"
    },
    "org.apache.hadoop.service.Service$STATE": {
        "org.apache.hadoop.service.Service$STATE:getValue()": "/**\n* Returns the current value of the instance.\n* @return the integer value of the instance\n*/",
        "org.apache.hadoop.service.Service$STATE:toString()": "/**\n* Returns the string representation of the object.\n* @return the state name as a String\n*/"
    },
    "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler": {
        "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(java.lang.Thread$UncaughtExceptionHandler)": "/**\n* Initializes the handler with a delegate for uncaught exceptions.\n* @param delegate the handler to delegate uncaught exceptions to\n*/",
        "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>()": "/**\n* Constructs a HadoopUncaughtExceptionHandler with no delegate handler.\n*/",
        "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)": "/**** Handles uncaught exceptions in threads, logging errors and managing shutdown behavior. */"
    },
    "org.apache.hadoop.util.ExitUtil": {
        "org.apache.hadoop.util.ExitUtil:haltOnOutOfMemory(java.lang.OutOfMemoryError)": "/**\n* Halts the JVM on an OutOfMemoryError without cleanup.\n* @param oome the OutOfMemoryError caught\n*/",
        "org.apache.hadoop.util.ExitUtil:<init>()": "/**\n* Private constructor to prevent instantiation of ExitUtil class.\n*/",
        "org.apache.hadoop.util.ExitUtil:terminateCalled()": "/**\n* Checks if termination was initiated by an exception.\n* @return true if termination was called, false otherwise\n*/",
        "org.apache.hadoop.util.ExitUtil:haltCalled()": "/**\n* Checks if halt was triggered by an exception.\n* @return true if halt was called; false otherwise\n*/",
        "org.apache.hadoop.util.ExitUtil:getFirstExitException()": "/**\n* Retrieves the first ExitException from a thread-local variable.\n* @return ExitException instance stored in thread-local or null\n*/",
        "org.apache.hadoop.util.ExitUtil:getFirstHaltException()": "/**\n* Retrieves the first HaltException instance.\n* @return HaltException object from FIRST_HALT_EXCEPTION\n*/",
        "org.apache.hadoop.util.ExitUtil:resetFirstExitException()": "/**\n* Resets the FIRST_EXIT_EXCEPTION to null.\n*/",
        "org.apache.hadoop.util.ExitUtil:resetFirstHaltException()": "/**\n* Resets the FIRST_HALT_EXCEPTION to null.\n*/",
        "org.apache.hadoop.util.ExitUtil:addSuppressed(java.lang.Throwable,java.lang.Throwable)": "/**\n* Adds a suppressed exception to a suppressor.\n* @param suppressor the primary exception\n* @param suppressed the exception to be added\n* @return the suppressor exception\n*/",
        "org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException)": "/**\n* Terminates the application based on ExitException status.\n* @param ee the ExitException containing exit code and message\n* @throws ExitException if an error occurs during termination\n*/",
        "org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException)": "/**\n* Halts the VM or logs an error based on HaltException status.\n* @param he HaltException containing exit code and message\n* @throws HaltException rethrown if not handled by a higher priority error\n*/",
        "org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)": "/**\n* Terminates the application with a status code and an optional throwable cause.\n* @param status exit status code\n* @param t throwable cause for termination\n* @throws ExitException if an error occurs during termination\n*/",
        "org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)": "/**\n* Terminates the application with a given status and message.\n* @param status exit status code\n* @param msg detailed termination message\n*/",
        "org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)": "/**\n* Halts execution with a status or rethrows HaltException.\n* @param status HTTP status code\n* @param t underlying throwable cause\n*/",
        "org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)": "/**\n* Halts execution with a specified status and message.\n* @param status HTTP status code\n* @param message error message for the HaltException\n* @throws HaltException rethrown if not handled\n*/",
        "org.apache.hadoop.util.ExitUtil:terminate(int)": "/**\n* Terminates the application with a specified exit status.\n* @param status exit status code\n* @throws ExitException if termination fails\n*/",
        "org.apache.hadoop.util.ExitUtil:halt(int)": "/**\n* Halts execution with a specified HTTP status code.\n* @param status HTTP status code\n* @throws HaltException rethrown if not handled\n*/"
    },
    "org.apache.hadoop.service.launcher.IrqHandler$InterruptData": {
        "org.apache.hadoop.service.launcher.IrqHandler$InterruptData:<init>(java.lang.String,int)": "/**\n* Constructs InterruptData with a name and number.\n* @param name identifier for the interrupt\n* @param number unique interrupt number\n*/",
        "org.apache.hadoop.service.launcher.IrqHandler$InterruptData:toString()": "/**\n* Returns a string representation of the signal with its name and number.\n* @return formatted string of the signal\n*/"
    },
    "org.apache.hadoop.service.launcher.InterruptEscalator": {
        "org.apache.hadoop.service.launcher.InterruptEscalator:getOwner()": "/**\n* Retrieves the ServiceLauncher instance referenced by ownerRef.\n* @return ServiceLauncher object or null if not set\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:isSignalAlreadyReceived()": "/**\n* Checks if the signal has already been received.\n* @return true if the signal is received, false otherwise\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)": "/**\n* Initializes InterruptEscalator with owner and shutdown time.\n* @param owner ServiceLauncher instance; must not be null\n* @param shutdownTimeMillis time in milliseconds before shutdown\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:getService()": "/**\n* Retrieves the associated service from the owner.\n* @return Service object or null if owner is not set\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String)": "/**\n* Retrieves an IrqHandler by its signal name.\n* @param signalName the name of the signal to look up\n* @return the corresponding IrqHandler or null if not found\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String)": "/**\n* Registers a signal handler for the specified signal name.\n* @param signalName name of the signal to register\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:toString()": "/**\n* Returns a string representation of the InterruptEscalator object.\n* @return formatted details including signal, owner, and shutdown status\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData)": "/**\n* Handles service interruption and initiates forced shutdown if necessary.\n* @param interruptData data related to the interrupt event\n*/"
    },
    "org.apache.hadoop.service.launcher.ServiceLauncher": {
        "org.apache.hadoop.service.launcher.ServiceLauncher:getService()": "/**\n* Retrieves the service instance.\n* @return the service of type S\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a ServiceLauncher with a service name and class name.\n* @param serviceName name of the service\n* @param serviceClassName fully qualified class name of the service\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:isClassnameDefined()": "/**\n* Checks if the service class name is defined and not empty.\n* @return true if defined, false otherwise\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:getUsageMessage()": "/**\n* Retrieves the usage message for command options.\n* @return formatted usage message string\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:createOptions()": "/**\n* Creates command-line options for application configuration.\n* @return Options object containing defined command-line options\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:getConfigurationsToCreate()": "/**\n* Returns a list of configuration class names to create.\n* @return List of configuration class names\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:getClassLoader()": "/**\n* Retrieves the ClassLoader for the current class.\n* @return ClassLoader instance associated with this class\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:getServiceName()": "/**\n* Retrieves the service name or defaults to a predefined name.\n* @return formatted service name string\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:warn(java.lang.String)": "/**\n* Logs a warning message or prints to stderr if logging is disabled.\n* @param text the warning message to log\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:error(java.lang.String,java.lang.Throwable)": "/**\n* Logs an error message with an optional throwable.\n* @param message error description\n* @param thrown optional exception to log\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String)": "/****\n* Initializes ServiceLauncher with service name and class name.\n* @param serviceClassName fully qualified class name of the service\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:toString()": "/**\n* Returns a string representation of the ServiceLauncher object.\n* @return formatted service details including name and class name if defined\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException)": "/**\n* Logs details of an ExitException if its exit code is non-zero.\n* @param exitException the exception containing exit code and cause\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions()": "/**\n* Binds command-line options for the application.\n* @return void\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses()": "/**** Loads configuration classes and returns the count of successfully loaded instances. */",
        "org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception)": "/**\n* Creates a ServiceLaunchException for service creation failure.\n* @param exception the cause of the failure\n* @return ServiceLaunchException with exit code and cause\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[])": "/**\n* Checks existence of configuration files and throws an exception if not found.\n* @param filenames array of configuration file names\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable)": "/**\n* Converts a Throwable to an ExitException with an exit code and message.\n* @param thrown the Throwable to convert\n* @return ExitException constructed from the Throwable\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException)": "/**\n* Terminates the application using the provided ExitException.\n* @param ee the ExitException containing exit code and message\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)": "/**\n* Terminates application with a status and message.\n* @param status exit status code\n* @param message termination message\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling()": "/**\n* Registers failure handling and uncaught exception handler.\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration)": "/**** Instantiates a service from the given configuration. \n* @param conf service configuration\n* @return Service instance created from serviceClassName\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)": "/**\n* Logs uncaught exceptions and terminates the application.\n* @param thread the thread where the exception occurred\n* @param exception the uncaught Throwable exception\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage()": "/**\n* Exits application with usage message.\n* Calls exitWithMessage with predefined exit code and message.\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)": "/**\n* Exits the application with a specified code and message.\n* @param exitCode status code for termination\n* @param message detailed exit message\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)": "/**\n* Generates a startup/shutdown message using class name and host info.\n* @param classname name of the class starting up\n* @param args additional startup arguments\n* @return formatted startup/shutdown message as a string\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration()": "/**\n* Creates a new Configuration instance.\n* @return a newly initialized Configuration object\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)": "/**\n* Launches a service with optional shutdown hook and execution control.\n* @param conf service configuration\n* @param instance existing service instance or null to instantiate\n* @param processedArgs arguments for the service\n* @param addShutdownHook whether to register a shutdown hook\n* @param execute whether to execute the service\n* @return exit code from service execution\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)": "/**\n* Launches a service and returns an ExitException on failure.\n* @param conf service configuration\n* @param instance existing service instance or null\n* @param processedArgs arguments for the service\n* @param addShutdownHook whether to add a shutdown hook\n* @param execute whether to execute the service\n* @return ExitException with exit code and message\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)": "/**\n* Launches a service with given configuration and arguments.\n* @param conf service configuration\n* @param processedArgs arguments for the service\n* @param addShutdownHook whether to add a shutdown hook\n* @param execute whether to execute the service\n* @return ExitException on failure\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])": "/**\n* Creates a GenericOptionsParser instance.\n* @param conf configuration settings\n* @param argArray command-line arguments\n* @return GenericOptionsParser object\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)": "/**\n* Parses command-line arguments and updates configuration settings.\n* @param conf configuration settings\n* @param args list of command-line arguments\n* @return remaining command-line arguments\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)": "/**\n* Extracts command options from arguments.\n* @param conf configuration settings\n* @param args list of command-line arguments\n* @return remaining command options or empty list if insufficient args\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List)": "/**\n* Launches a service with arguments and handles exit conditions.\n* @param args command-line arguments for the service\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List)": "/**\n* Manages service execution based on command-line arguments.\n* @param argsList list of command-line arguments for the service\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[])": "/**\n* Entry point for the application, initiates service execution.\n* @param args command-line arguments for the service\n*/",
        "org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[])": "/**\n* Entry point for service execution with variable arguments.\n* @param args command-line arguments for the service\n*/"
    },
    "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown": {
        "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:run()": "/**\n* Stops the service and waits for it to shut down.\n* Sets serviceWasShutdown to true if service is null or stopped.\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:getServiceWasShutdown()": "/**\n* Checks if the service was shut down.\n* @return true if the service is shut down, false otherwise\n*/",
        "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)": "/**\n* Initializes a forced shutdown for the specified service.\n* @param service the service to be shut down\n* @param shutdownTimeMillis time in milliseconds before shutdown\n*/"
    },
    "org.apache.hadoop.service.launcher.IrqHandler": {
        "org.apache.hadoop.service.launcher.IrqHandler:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.service.launcher.IrqHandler:raise()": "/**\n* Raises a specified signal.\n* @param signal the signal to be raised\n*/",
        "org.apache.hadoop.service.launcher.IrqHandler:toString()": "/**\n* Returns a string representation of the IrqHandler with its signal name.\n* @return String description of the IrqHandler\n*/",
        "org.apache.hadoop.service.launcher.IrqHandler:getSignalCount()": "/**\n* Retrieves the current count of signals.\n* @return the number of signals as an integer\n*/",
        "org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)": "/**\n* Constructs an IrqHandler with a name and an interrupted handler.\n* @param name identifier for the handler, must not be null\n* @param handler interrupted handler, must not be null\n*/",
        "org.apache.hadoop.service.launcher.IrqHandler:bind()": "/**** Binds a signal handler; throws if already bound. \n* @throws IllegalArgumentException if signal binding fails. \n*/",
        "org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal)": "/**** Handles incoming signals, logs interruption, and notifies the handler. \n* @param s the Signal object containing interrupt details \n*/"
    },
    "org.apache.hadoop.service.launcher.AbstractLaunchableService": {
        "org.apache.hadoop.service.launcher.AbstractLaunchableService:bindArgs(org.apache.hadoop.conf.Configuration,java.util.List)": "/**\n* Binds arguments to the given configuration.\n* @param config Configuration object to bind args to\n* @param args List of arguments to be bound\n* @return Updated Configuration object\n*/",
        "org.apache.hadoop.service.launcher.AbstractLaunchableService:execute()": "",
        "org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String)": "/**\n* Initializes AbstractLaunchableService with a specified name.\n* @param name the service name\n*/"
    },
    "org.apache.hadoop.util.ExitUtil$ExitException": {
        "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.Throwable)": "/**\n* Constructs an ExitException with a status code and a cause.\n* @param status exit status code\n* @param cause the throwable cause of the exception\n*/",
        "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.String)": "/**\n* Constructs an ExitException with a status code and message.\n* @param status exit status code\n* @param msg detailed exception message\n*/",
        "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.String,java.lang.Throwable)": "/**\n* Constructs an ExitException with status, message, and cause.\n* @param status exit status code\n* @param message error message\n* @param cause underlying throwable cause\n*/",
        "org.apache.hadoop.util.ExitUtil$ExitException:getExitCode()": "/**\n* Retrieves the exit code of the process.\n* @return the exit status as an integer\n*/",
        "org.apache.hadoop.util.ExitUtil$ExitException:toString()": "/**\n* Returns string representation of the object with status and message.\n* @return formatted string with status and message or default toString\n*/"
    },
    "org.apache.hadoop.service.launcher.ServiceShutdownHook": {
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:shutdown()": "/**\n* Stops the service and clears its reference.\n* @return true if service stopped successfully, false otherwise\n*/",
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:run()": "/**\n* Executes the run method to shutdown the service.\n*/",
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service)": "/**\n* Initializes a ServiceShutdownHook with a weak reference to the given service.\n* @param service the service to be referenced weakly\n*/",
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister()": "/**\n* Unregisters the current instance as a shutdown hook.\n* @throws IllegalStateException if removal fails\n*/",
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int)": "/**\n* Registers the current instance as a shutdown hook with specified priority.\n* @param priority the execution priority of the hook\n*/"
    },
    "org.apache.hadoop.util.GenericOptionsParser": {
        "org.apache.hadoop.util.GenericOptionsParser:isParseSuccessful()": "/**\n* Checks if the parsing was successful.\n* @return true if parsing succeeded, false otherwise\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:getCommandLine()": "/**\n* Retrieves the CommandLine object.\n* @return CommandLine instance associated with this context\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:getRemainingArgs()": "/**\n* Retrieves remaining command line arguments.\n* @return array of arguments or empty array if none exist\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:printGenericCommandUsage(java.io.PrintStream)": "/**\n* Prints usage information for generic command options.\n* @param out output stream for displaying usage information\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:buildGeneralOptions(org.apache.commons.cli.Options)": "/**\n* Builds general options for application configuration.\n* @param opts existing Options object to modify\n* @return modified Options object with new options added\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:matchesCurrentDirectory(java.lang.String)": "/**\n* Checks if the given path matches the current directory.\n* @param path directory path to check\n* @return true if path is empty or matches current directory\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:preProcessForWindows(java.lang.String[])": "/**\n* Processes command-line arguments for Windows compatibility.\n* @param args input arguments array\n* @return modified arguments array or null if input is null\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:getConfiguration()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves an array of URLs for library JARs specified in the configuration.\n* @param conf configuration settings\n* @return array of URLs or null if no JARs are specified\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)": "/**\n* Expands wildcard paths for JARs in a directory.\n* @param finalPaths list to store qualified JAR paths\n* @param path directory path to search for JARs\n* @param fs filesystem instance for operations\n* @throws IOException if directory not found or access fails\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)": "/**\n* Validates file paths and expands wildcards if specified.\n* @param files comma-separated file paths\n* @param expandWildcard flag to expand wildcard paths\n* @return concatenated valid file paths\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String)": "/**\n* Validates file paths without expanding wildcards.\n* @param files comma-separated file paths\n* @return concatenated valid file paths\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine)": "/**\n* Processes command line options to configure FileSystem and job settings.\n* @param line command line options\n* @throws IOException if file system or reading operations fail\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])": "/**\n* Parses command-line options and returns parsing success status.\n* @param opts options to parse against\n* @param args command-line arguments\n* @return true if parsing succeeded, false otherwise\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])": "/**\n* Initializes GenericOptionsParser and parses command-line options.\n* @param conf configuration settings\n* @param options options to parse against\n* @param args command-line arguments\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])": "/**\n* Initializes GenericOptionsParser with provided options and arguments.\n* @param opts options to parse against\n* @param args command-line arguments\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[])": "/**\n* Constructs GenericOptionsParser with default configuration and options.\n* @param args command-line arguments to parse\n*/",
        "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])": "/**\n* Initializes GenericOptionsParser with configuration and command-line arguments.\n* @param conf configuration settings\n* @param args command-line arguments\n*/"
    },
    "org.apache.hadoop.net.NetUtils": {
        "org.apache.hadoop.net.NetUtils:getHostname()": "/**\n* Retrieves the local hostname as a String.\n* @return Hostname or exception message if not found\n*/",
        "org.apache.hadoop.net.NetUtils:createURI(java.lang.String,boolean,java.lang.String,boolean)": "/**\n* Creates a URI from target string, using cache if enabled.\n* @param target the URI string to create\n* @param hasScheme indicates if target has a URI scheme\n* @param helpText additional context for errors\n* @param useCacheIfPresent whether to use cached URI\n* @return the created URI\n*/",
        "org.apache.hadoop.net.NetUtils:getStaticResolution(java.lang.String)": "/**\n* Retrieves the resolved IP address for a given host.\n* @param host hostname to resolve\n* @return resolved IP address as a String or null if not found\n*/",
        "org.apache.hadoop.net.NetUtils:addStaticResolution(java.lang.String,java.lang.String)": "/**\n* Adds a static resolution for a host.\n* @param host the hostname to resolve\n* @param resolvedName the resolved name for the host\n*/",
        "org.apache.hadoop.net.NetUtils:getAllStaticResolutions()": "/**\n* Retrieves all static host-resolution pairs.\n* @return List of String arrays containing host-value pairs or null if none exist\n*/",
        "org.apache.hadoop.net.NetUtils:normalizeHostName(java.lang.String)": "/**\n* Normalizes a host name to its IP address.\n* @param name the host name to normalize\n* @return the IP address or the original name if unknown\n*/",
        "org.apache.hadoop.net.NetUtils:verifyHostnames(java.lang.String[])": "/**\n* Validates hostnames in the array.\n* @param names array of hostname strings\n* @throws UnknownHostException if any hostname is invalid or null\n*/",
        "org.apache.hadoop.net.NetUtils:getHostNameOfIP(java.lang.String)": "/**\n* Retrieves the host name from a given IP address or IP:port string.\n* @param ipPort IP address or IP:port string\n* @return Host name or null if invalid or not found\n*/",
        "org.apache.hadoop.net.NetUtils:getHostPortString(java.net.InetSocketAddress)": "/**\n* Returns a string representing the host and port from the given InetSocketAddress.\n* @param addr InetSocketAddress containing host and port information\n* @return formatted string \"hostname:port\"\n*/",
        "org.apache.hadoop.net.NetUtils:getLocalHostname()": "/**\n* Retrieves the local hostname of the machine.\n* @return hostname as a String or error message if not found\n*/",
        "org.apache.hadoop.net.NetUtils:getPortFromHostPortString(java.lang.String)": "/**\n* Extracts port number from a host:port formatted string.\n* @param addr host:port string\n* @return port number as an integer\n* @throws IllegalArgumentException if format is invalid\n*/",
        "org.apache.hadoop.net.NetUtils:isLocalAddress(java.net.InetAddress)": "/**\n* Determines if the given InetAddress is local or loopback.\n* @param addr the InetAddress to check\n* @return true if local, otherwise false\n*/",
        "org.apache.hadoop.net.NetUtils:see(java.lang.String)": "/**\n* Constructs a detail URL by appending entry to a base URL.\n* @param entry specific detail to append\n* @return complete URL string\n*/",
        "org.apache.hadoop.net.NetUtils:wrapWithMessage(java.io.IOException,java.lang.String)": "/**\n* Wraps an IOException with a custom message.\n* @param exception the original IOException\n* @param msg custom message for the new exception\n* @return the wrapped IOException\n*/",
        "org.apache.hadoop.net.NetUtils:quoteHost(java.lang.String)": "/**\n* Quotes the hostname or returns a placeholder if null.\n* @param hostname the host name to quote\n* @return quoted hostname or UNKNOWN_HOST if null\n*/",
        "org.apache.hadoop.net.NetUtils:isValidSubnet(java.lang.String)": "/**\n* Validates if the given subnet string is a valid subnet format.\n* @param subnet the subnet string to validate\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.net.NetUtils:addMatchingAddrs(java.net.NetworkInterface,org.apache.commons.net.util.SubnetUtils$SubnetInfo,java.util.List)": "/**\n* Adds matching IP addresses from a network interface to a list based on subnet range.\n* @param nif NetworkInterface to retrieve addresses from\n* @param subnetInfo SubnetInfo to check address range\n* @param addrs List to store matching InetAddress objects\n*/",
        "org.apache.hadoop.net.NetUtils:getFreeSocketPort()": "/**\n* Retrieves an available socket port.\n* @return an available port number or 0 if unsuccessful\n*/",
        "org.apache.hadoop.net.NetUtils:bindToLocalAddress(java.net.InetAddress,boolean)": "/**\n* Binds to a local address or returns null for wildcard binding.\n* @param localAddr the local InetAddress to bind\n* @param bindWildCardAddress flag for wildcard binding\n* @return localAddr or null if wildcard binding is enabled\n*/",
        "org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection)": "/**\n* Normalizes a collection of host names.\n* @param names collection of host names to normalize\n* @return list of normalized host names\n*/",
        "org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)": "/**\n* Constructs a string with local and destination host details.\n* @param destHost destination hostname\n* @param destPort destination port number\n* @param localHost local hostname\n* @return formatted string of host details\n*/",
        "org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)": "/**\n* Retrieves IP addresses within a specified subnet.\n* @param subnet subnet in CIDR notation\n* @param returnSubinterfaces flag to include subinterfaces\n* @return List of matching InetAddress objects\n*/",
        "org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int)": "/**\n* Retrieves a set of free socket ports.\n* @param numOfPorts number of ports to acquire (1-25)\n* @return Set of available port numbers\n*/",
        "org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)": "/**\n* Wraps IOException with detailed messages based on the exception type.\n* @param destHost destination hostname, localHost local hostname, \n* @param destPort destination port, localPort local port, \n* @param exception the original IOException\n* @return wrapped IOException with custom message\n*/",
        "org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)": "/**\n* Creates a SocketInputWrapper for the given socket with a specified timeout.\n* @param socket the socket to wrap\n* @param timeout duration in milliseconds for input stream timeout\n* @return SocketInputWrapper instance\n*/",
        "org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)": "/**\n* Retrieves an OutputStream from a Socket with an optional timeout.\n* @param socket the Socket to get the OutputStream from\n* @param timeout the timeout duration in milliseconds\n* @return OutputStream associated with the Socket\n*/",
        "org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)": "/**\n* Connects a socket to a remote address with optional local binding and timeout.\n* @param socket the socket to connect\n* @param endpoint the remote address to connect to\n* @param localAddr optional local address to bind\n* @param timeout maximum wait time in milliseconds\n* @throws IOException if connection fails or times out\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)": "/**\n* Creates an InetSocketAddress for a given host and port.\n* @param host hostname to resolve\n* @param port port number for the socket\n* @return InetSocketAddress for the resolved host and port\n*/",
        "org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String)": "/**\n* Canonicalizes a host name using a cache.\n* @param host the host name to canonicalize\n* @return canonical host name or original if unresolved\n*/",
        "org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String)": "/****\n* Retrieves local InetAddress for a specified hostname.\n* @param host the hostname to resolve\n* @return InetAddress if local, otherwise null\n*/",
        "org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)": "/*******************************************************************************  \n* Retrieves a SocketFactory instance based on configuration property.  \n* @param conf configuration settings  \n* @param propValue property name for class lookup  \n* @return SocketFactory instance  \n*******************************************************************************/",
        "org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket)": "/**\n* Retrieves a SocketInputWrapper for the given socket.\n* @param socket the socket to wrap\n* @return SocketInputWrapper instance\n*/",
        "org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket)": "/**\n* Retrieves an OutputStream from a Socket without a timeout.\n* @param socket the Socket to get the OutputStream from\n* @return OutputStream associated with the Socket\n*/",
        "org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)": "/**\n* Connects a socket to a remote address with a timeout.\n* @param socket the socket to connect\n* @param address the remote address to connect to\n* @param timeout maximum wait time in milliseconds\n* @throws IOException if connection fails or times out\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)": "/**\n* Creates an InetSocketAddress from a target string, using default port if needed.\n* @param target the address string to parse\n* @param defaultPort the port to use if not specified\n* @param configName optional config name for error context\n* @param useCacheIfPresent whether to use cached URI\n* @param isResolved indicates if the address should be resolved\n* @return InetSocketAddress for the parsed host and port\n*/",
        "org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress)": "/**\n* Resolves the connect address for a given InetSocketAddress.\n* @param addr input socket address to resolve\n* @return resolved InetSocketAddress\n*/",
        "org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)": "/**\n* Returns a canonical URI with a valid host and port.\n* @param uri the original URI\n* @param defaultPort the port to use if none is specified\n* @return canonical URI or original if no authority\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String)": "/**\n* Creates an unresolved InetSocketAddress from a target string.\n* @param target address string to parse\n* @return InetSocketAddress for the parsed host with unresolved port\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)": "/**\n* Creates an InetSocketAddress from a target string with a default port.\n* @param target the address string to parse\n* @param defaultPort the port to use if not specified\n* @param configName optional config name for error context\n* @param useCacheIfPresent whether to use cached URI\n* @return InetSocketAddress for the parsed host and port\n*/",
        "org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server)": "/**\n* Resolves the connect address for a given server.\n* @param server the server to retrieve the connect address from\n* @return resolved InetSocketAddress of the server's connection\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)": "/**\n* Creates an InetSocketAddress from a target string with a default port.\n* @param target the address string to parse\n* @param defaultPort the port to use if not specified\n* @param configName optional config name for error context\n* @return InetSocketAddress for the parsed host and port\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)": "/**\n* Creates an InetSocketAddress from a target string with a default port.\n* @param target the address string to parse\n* @param defaultPort the port to use if not specified\n* @return InetSocketAddress for the parsed host and port\n*/",
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String)": "/**\n* Creates an InetSocketAddress from a target string with default port -1.\n* @param target the address string to parse\n* @return InetSocketAddress for the parsed host and port\n*/",
        "org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String)": "/**\n* Normalizes IP:port string to hostname format.\n* @param ipPort input IP:port string\n* @return hostname:port string or original input if invalid\n*/",
        "org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the default SocketFactory based on configuration settings.\n* @param conf configuration settings\n* @return SocketFactory instance or default if not specified\n*/",
        "org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)": "/**\n* Retrieves a SocketFactory based on configuration or defaults.\n* @param conf configuration settings\n* @param clazz class type for property lookup\n* @return SocketFactory instance or null if not found\n*/"
    },
    "org.apache.hadoop.service.ServiceOperations$ServiceListeners": {
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:add(org.apache.hadoop.service.ServiceStateChangeListener)": "/**\n* Adds a ServiceStateChangeListener if not already present.\n* @param l listener to be added\n*/",
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:remove(org.apache.hadoop.service.ServiceStateChangeListener)": "/**\n* Removes a ServiceStateChangeListener from the listeners list.\n* @param l the listener to be removed\n* @return true if removed, false if not found\n*/",
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:reset()": "/**\n* Clears all registered listeners.\n*/",
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners(org.apache.hadoop.service.Service)": "/**\n* Notifies all registered listeners of a service state change.\n* @param service the service whose state has changed\n*/"
    },
    "org.apache.hadoop.service.AbstractService": {
        "org.apache.hadoop.service.AbstractService:getName()": "/**\n* Returns the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.service.AbstractService:setConfig(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object for the current instance.\n* @param conf the Configuration object to be set\n*/",
        "org.apache.hadoop.service.AbstractService:serviceStart()": "/**\n* Initiates the service startup process.\n* @throws Exception if an error occurs during startup\n*/",
        "org.apache.hadoop.service.AbstractService:serviceStop()": "/**\n* Stops the service and handles any exceptions that may occur.\n* @throws Exception if an error occurs during service stop\n*/",
        "org.apache.hadoop.service.AbstractService:waitForServiceToStop(long)": "/**\n* Waits for the service to stop within a specified timeout.\n* @param timeout maximum wait time in milliseconds\n* @return true if service stopped, false otherwise\n*/",
        "org.apache.hadoop.service.AbstractService:getLifecycleHistory()": "/**\n* Retrieves an unmodifiable snapshot of the lifecycle event history.\n* @return List of LifecycleEvent objects\n*/",
        "org.apache.hadoop.service.AbstractService:toString()": "/**\n* Returns a string representation of the service's name and state.\n* @return formatted service description\n*/",
        "org.apache.hadoop.service.AbstractService:putBlocker(java.lang.String,java.lang.String)": "/**\n* Adds a blocker entry to the blockerMap.\n* @param name unique identifier for the blocker\n* @param details information related to the blocker\n*/",
        "org.apache.hadoop.service.AbstractService:removeBlocker(java.lang.String)": "/**\n* Removes a blocker from the blockerMap by name.\n* @param name the name of the blocker to remove\n*/",
        "org.apache.hadoop.service.AbstractService:getBlockers()": "/**\n* Returns an unmodifiable copy of the current blockers map.\n* @return a map of blockers as key-value pairs\n*/",
        "org.apache.hadoop.service.AbstractService:getConfig()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current config\n*/",
        "org.apache.hadoop.service.AbstractService:getFailureCause()": "/**\n* Retrieves the cause of failure.\n* @return Throwable representing the failure cause\n*/",
        "org.apache.hadoop.service.AbstractService:getFailureState()": "/**\n* Retrieves the current failure state.\n* @return the current STATE of failure\n*/",
        "org.apache.hadoop.service.AbstractService:getStartTime()": "/**\n* Retrieves the start time value.\n* @return long representing the start time in milliseconds\n*/",
        "org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE)": "/**\n* Checks if the service is in the expected state.\n* @param expected the state to check against the current state\n* @return true if in expected state, false otherwise\n*/",
        "org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)": "/**\n* Registers a ServiceStateChangeListener.\n* @param l listener to be registered\n*/",
        "org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)": "/** Registers a global listener for service state changes. \n* @param l listener to be registered \n*/",
        "org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)": "/**\n* Unregisters a service state change listener.\n* @param l the listener to be removed from the listeners list\n*/",
        "org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)": "/**\n* Unregisters a global listener.\n* @param l the listener to be removed\n* @return true if removed, false if not found\n*/",
        "org.apache.hadoop.service.AbstractService:resetGlobalListeners()": "/**\n* Resets all global event listeners to their initial state.\n*/",
        "org.apache.hadoop.service.AbstractService:notifyListeners()": "/**\n* Notifies all registered listeners of the current service state change.\n*/",
        "org.apache.hadoop.service.AbstractService:getServiceState()": "/**\n* Retrieves the current service state.\n* @return Service.STATE representing the current state\n*/",
        "org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the service with a given configuration.\n* @param conf the Configuration object to set if different from current config\n*/",
        "org.apache.hadoop.service.AbstractService:<init>(java.lang.String)": "/**\n* Constructs AbstractService with a name and initializes its state model.\n* @param name the service name\n*/",
        "org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception)": "/**\n* Logs and records the first failure encountered.\n* @param exception the failure exception to note\n*/",
        "org.apache.hadoop.service.AbstractService:recordLifecycleEvent()": "/**\n* Records a lifecycle event with current time and service state.\n*/",
        "org.apache.hadoop.service.AbstractService:start()": "/**\n* Starts the service if not already started, notifying listeners on success.\n*/",
        "org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE)": "/**\n* Changes the service state and logs the transition.\n* @param newState desired new state\n* @return oldState previous state before transition\n*/",
        "org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the service with the provided configuration.\n* @param conf the Configuration object for initialization\n*/",
        "org.apache.hadoop.service.AbstractService:stop()": "/**\n* Stops the service if not already stopped, handling exceptions and notifying listeners.\n*/",
        "org.apache.hadoop.service.AbstractService:close()": "/**\n* Closes the service by stopping it.\n* @throws IOException if an I/O error occurs during stopping\n*/"
    },
    "org.apache.hadoop.service.CompositeService": {
        "org.apache.hadoop.service.CompositeService:getServices()": "/**\n* Returns an unmodifiable list of services.\n* @return List of Service objects\n*/",
        "org.apache.hadoop.service.CompositeService:addService(org.apache.hadoop.service.Service)": "/**\n* Adds a service to the service list with debug logging.\n* @param service the Service object to be added\n*/",
        "org.apache.hadoop.service.CompositeService:removeService(org.apache.hadoop.service.Service)": "/**\n* Removes a service from the service list.\n* @param service the Service object to be removed\n* @return true if removed, false if not found\n*/",
        "org.apache.hadoop.service.CompositeService:serviceStart()": "/**\n* Starts all services and logs the process.\n* @throws Exception if a service fails to start\n*/",
        "org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object)": "/**\n* Adds a Service object to the list if valid.\n* @param object potential Service to add\n* @return true if added, false otherwise\n*/",
        "org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes services with the provided configuration.\n* @param conf the Configuration object for service initialization\n*/",
        "org.apache.hadoop.service.CompositeService:stop(int,boolean)": "/**** Stops services in reverse order, logging exceptions if any occur. \n* @param numOfServicesStarted total services to stop, \n* @param stopOnlyStartedServices flag to stop only started services. \n*/",
        "org.apache.hadoop.service.CompositeService:<init>(java.lang.String)": "/**\n* Constructs CompositeService with a name.\n* @param name the service name\n*/",
        "org.apache.hadoop.service.CompositeService:serviceStop()": "/**\n* Stops all started services and logs the operation.\n* @throws Exception if an error occurs during service stop\n*/"
    },
    "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook": {
        "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:<init>(org.apache.hadoop.service.CompositeService)": "/**\n* Initializes the shutdown hook for the given composite service.\n* @param compositeService the service to manage during shutdown\n*/",
        "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run()": "/**\n* Stops the composite service quietly.\n* @param compositeService service to be stopped\n*/"
    },
    "org.apache.hadoop.service.ServiceOperations": {
        "org.apache.hadoop.service.ServiceOperations:<init>()": "/**\n* Private constructor for ServiceOperations to prevent instantiation.\n*/",
        "org.apache.hadoop.service.ServiceOperations:stop(org.apache.hadoop.service.Service)": "/**\n* Stops the given service if it is not null.\n* @param service the Service instance to be stopped\n*/",
        "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)": "/**\n* Stops a service quietly, logging warnings on failure.\n* @param log logger for warning messages\n* @param service the Service instance to stop\n* @return Exception if stopping fails, null otherwise\n*/",
        "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)": "/**\n* Stops the service quietly, logging any exceptions encountered.\n* @param log Logger to log warnings, @param service Service to be stopped\n* @return Exception if stopping fails, null otherwise\n*/",
        "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service)": "/**\n* Stops the given service quietly, logging any exceptions.\n* @param service Service to be stopped\n* @return Exception if stopping fails, null otherwise\n*/"
    },
    "org.apache.hadoop.util.Options$ProgressableOption": {
        "org.apache.hadoop.util.Options$ProgressableOption:<init>(org.apache.hadoop.util.Progressable)": "/**\n* Initializes ProgressableOption with a given Progressable value.\n* @param value the Progressable object to set\n*/",
        "org.apache.hadoop.util.Options$ProgressableOption:getValue()": "/**\n* Retrieves the current Progressable value.\n* @return Progressable object representing the current value\n*/"
    },
    "org.apache.hadoop.io.ShortWritable": {
        "org.apache.hadoop.io.ShortWritable:<init>()": "/**\n* Default constructor for ShortWritable, initializes a new instance.\n*/",
        "org.apache.hadoop.io.ShortWritable:set(short)": "/**\n* Sets the internal value to the specified short.\n* @param value the short value to set\n*/",
        "org.apache.hadoop.io.ShortWritable:readFields(java.io.DataInput)": "/**\n* Reads a short value from the input stream.\n* @param in the DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ShortWritable:write(java.io.DataOutput)": "/**\n* Writes a short value to the output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ShortWritable:toString()": "/**\n* Returns the string representation of the short value.\n* @return string representation of the short value\n*/",
        "org.apache.hadoop.io.ShortWritable:compareTo(org.apache.hadoop.io.ShortWritable)": "/**\n* Compares this ShortWritable with another for order.\n* @param o ShortWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.ShortWritable:<init>(short)": "/**\n* Constructs a ShortWritable with the specified short value.\n* @param value the short value to set\n*/"
    },
    "org.apache.hadoop.io.WritableComparator": {
        "org.apache.hadoop.io.WritableComparator:define(java.lang.Class,org.apache.hadoop.io.WritableComparator)": "/**\n* Associates a class with its WritableComparator.\n* @param c class to associate\n* @param comparator WritableComparator for the class\n*/",
        "org.apache.hadoop.io.WritableComparator:forceInit(java.lang.Class)": "/**\n* Forces initialization of the specified class.\n* @param cls the class to initialize\n*/",
        "org.apache.hadoop.io.WritableComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable)": "/**\n* Compares two WritableComparable objects.\n* @param a first object to compare\n* @param b second object to compare\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int,int)": "/**\n* Computes a hash value for a byte array segment.\n* @param bytes byte array to hash\n* @param offset starting index in the array\n* @param length number of bytes to include in the hash\n* @return computed hash value as an integer\n*/",
        "org.apache.hadoop.io.WritableComparator:readInt(byte[],int)": "/**\n* Reads a 4-byte integer from a byte array starting at a given index.\n* @param bytes byte array containing the integer\n* @param start index to start reading from\n* @return the integer value read from the byte array\n*/",
        "org.apache.hadoop.io.WritableComparator:readVLong(byte[],int)": "/**\n* Reads a variable-length long from a byte array starting at a specified index.\n* @param bytes byte array containing encoded long\n* @param start starting index for reading\n* @return decoded long value\n* @throws IOException if there are not enough bytes to read\n*/",
        "org.apache.hadoop.io.WritableComparator:getKeyClass()": "/**\n* Returns the class type of the key used in the WritableComparable.\n* @return Class type of the key\n*/",
        "org.apache.hadoop.io.WritableComparator:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.io.WritableComparator:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration instance to set\n*/",
        "org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)": "/**\n* Compares two objects as WritableComparable types.\n* @param a first object to compare\n* @param b second object to compare\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays lexicographically.\n* @param b1 first byte array, b2 second byte array\n* @return negative, zero, or positive integer based on comparison\n*/",
        "org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)": "/**\n* Computes hash value for a byte array segment.\n* @param bytes byte array to hash\n* @param length number of bytes to include in the hash\n* @return computed hash value as an integer\n*/",
        "org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)": "/**\n* Converts bytes to a float starting from a given index.\n* @param bytes byte array containing the float data\n* @param start index to start reading from\n* @return float value converted from the byte array\n*/",
        "org.apache.hadoop.io.WritableComparator:readLong(byte[],int)": "/**\n* Reads a 64-bit long from a byte array starting at a given index.\n* @param bytes byte array containing the long value\n* @param start index to start reading from\n* @return the long value read from the byte array\n*/",
        "org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)": "/**\n* Reads a variable-length integer from a byte array.\n* @param bytes byte array containing encoded integer\n* @param start starting index for reading\n* @return decoded integer value\n* @throws IOException if there are not enough bytes to read\n*/",
        "org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)": "/**\n* Converts bytes to a double using a long representation.\n* @param bytes byte array containing the double value\n* @param start index to start reading from\n* @return double value converted from the byte array\n*/",
        "org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays as keys after parsing.\n* @param b1 first byte array, s1 start index, l1 length\n* @param b2 second byte array, s2 start index, l2 length\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.WritableComparator:newKey()": "/**\n* Creates a new key instance of the specified class.\n* @return WritableComparable object for the key\n*/",
        "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Initializes WritableComparator with key class and configuration.\n* @param keyClass class type of the key\n* @param conf configuration settings, defaults if null\n* @param createInstances flag to create key instances\n*/",
        "org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves WritableComparator for a given class and configuration.\n* @param c class type extending WritableComparable\n* @param conf configuration settings\n* @return WritableComparator instance\n*/",
        "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class)": "/**\n* Constructs WritableComparator with specified key class.\n* @param keyClass class type of the key\n*/",
        "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)": "/**\n* Constructs WritableComparator with specified key class.\n* @param keyClass class type of the key\n* @param createInstances flag to create key instances\n*/",
        "org.apache.hadoop.io.WritableComparator:get(java.lang.Class)": "/**\n* Retrieves WritableComparator for a given class.\n* @param c class type extending WritableComparable\n* @return WritableComparator instance\n*/",
        "org.apache.hadoop.io.WritableComparator:<init>()": "/**\n* Default constructor for WritableComparator, initializes with null key class.\n*/"
    },
    "org.apache.hadoop.util.Options$LongOption": {
        "org.apache.hadoop.util.Options$LongOption:<init>(long)": "/**\n* Initializes LongOption with a specified long value.\n* @param value the long value to set for this LongOption\n*/",
        "org.apache.hadoop.util.Options$LongOption:getValue()": "/**\n* Retrieves the current value.\n* @return the current long value\n*/"
    },
    "org.apache.hadoop.io.DataInputByteBuffer$Buffer": {
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(byte[],int,int)": "/**\n* Reads bytes into the array from a buffer.\n* @param b byte array to store read data\n* @param off offset in the array to start storing\n* @param len maximum number of bytes to read\n* @return number of bytes read or -1 if no more data\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:reset(java.nio.ByteBuffer[])": "/**\n* Resets buffer indices and calculates total length from provided ByteBuffers.\n* @param buffers array of ByteBuffers to be processed\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getData()": "/**\n* Retrieves an array of ByteBuffer objects.\n* @return array of ByteBuffer instances\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getPosition()": "/**\n* Returns the current position value.\n* @return current position as an integer\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getLength()": "/**\n* Returns the length value.\n* @return the current length as an integer\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:read()": "/**\n* Reads a single byte and returns it as an int.\n* @return byte value as int or -1 if end of stream is reached\n*/"
    },
    "org.apache.hadoop.io.wrappedio.WrappedIO": {
        "org.apache.hadoop.io.wrappedio.WrappedIO:<init>()": "/**\n* Private constructor for WrappedIO class to prevent instantiation.\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if a path has a specific capability.\n* @param fs file system object\n* @param path path to check\n* @param capability capability to verify\n* @return true if capability exists, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)": "/**\n* Checks if the object has the specified stream capability.\n* @param object the object to check\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream)": "/**\n* Checks if InputStream is fully readable for ByteBuffer operations.\n* @param in InputStream to check\n* @return true if fully readable, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Deletes files in bulk and returns the page size.\n* @param fs the FileSystem to perform the operation\n* @param path the path for the bulk delete\n* @return the page size of the bulk delete operation\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)": "/**\n* Deletes multiple paths in bulk from the specified file system.\n* @param fs file system to perform the delete operation\n* @param base base path for the delete operation\n* @param paths collection of paths to delete\n* @return list of entries representing deleted paths and their statuses\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)": "/**\n* Opens a file in the filesystem with optional parameters.\n* @param fs the filesystem to use\n* @param path the file path to open\n* @param policy read policy for file access\n* @param status file status, can be null\n* @param length expected file length, can be null\n* @param options additional options for file opening\n* @return FSDataInputStream for the opened file\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)": "/**\n* Reads data fully into a ByteBuffer from a specified position in the InputStream.\n* @param in InputStream to read from, must be ByteBufferPositionedReadable\n* @param position the position to start reading from\n* @param buf ByteBuffer to fill with read data\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Retrieves the enclosing root Path from the given FileSystem and Path.\n* @param fs the FileSystem instance\n* @param path the Path to qualify\n* @return qualified root Path\n*/"
    },
    "org.apache.hadoop.util.dynamic.BindingUtils": {
        "org.apache.hadoop.util.dynamic.BindingUtils:loadClass(java.lang.String)": "/**\n* Loads a class by its name.\n* @param className fully qualified name of the class\n* @return Class object or null if not found\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:extractIOEs(java.util.function.Supplier)": "/**\n* Executes a supplier and extracts IOException from UncheckedIOException.\n* @param call a Supplier that returns a value of type T\n* @return result of type T from the supplier\n* @throws IOException if an IO error occurs\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:<init>()": "/**\n* Private constructor to prevent instantiation of the BindingUtils class.\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:loadClassSafely(java.lang.String)": "/**\n* Loads a class by name, throwing a runtime exception if not found.\n* @param className fully qualified class name to load\n* @return Class object corresponding to the specified class name\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:loadClass(java.lang.ClassLoader,java.lang.String)": "/**\n* Loads a class by name using the specified ClassLoader.\n* @param cl ClassLoader to load the class\n* @param className fully qualified name of the class\n* @return Class object or null if not found\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String)": "/**\n* Creates a NOOP UnboundMethod with the specified name.\n* @param name the name of the method to create\n* @return UnboundMethod instance\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[])": "/**\n* Checks if any method is a NOOP.\n* @param methods array of unbound methods to evaluate\n* @return true if all methods are implemented, false if any is NOOP\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)": "/**\n* Checks if the given method is available.\n* @param method the method to check for availability\n* @return true if method is available, otherwise false\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])": "/**\n* Loads a method by name from a source class; returns an UnboundMethod or NOOP if not found.\n* @param source the class to load the method from\n* @param returnType expected return type of the method\n* @param name the name of the method to load\n* @param parameterTypes the parameter types of the method\n* @return UnboundMethod instance or a NOOP method\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)": "/**\n* Validates method availability and throws exception if unavailable.\n* @param method the method to check for availability\n*/",
        "org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])": "/**\n* Loads a static method from a class.\n* @param source the class containing the method\n* @param returnType expected return type\n* @param name the name of the method\n* @param parameterTypes the method's parameter types\n* @return UnboundMethod instance or NOOP if not found\n*/"
    },
    "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO": {
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:instance()": "/**\n* Returns the singleton instance of DynamicWrappedIO.\n* @return DynamicWrappedIO instance\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:loaded()": "/**\n* Checks if the resource is loaded.\n* @return true if loaded, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable()": "/**\n* Checks if the resource is loaded.\n* @return true if loaded, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable()": "/**\n* Verifies all required methods are available; throws exception if any are unbound.\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available()": "/**\n* Checks if bulk delete method is available.\n* @return true if bulk delete method is available, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available()": "/**\n* Checks if the open file method is available in the file system.\n* @return true if available, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available()": "/**\n* Checks if the byte buffer positioned readable method is available.\n* @return true if available, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream)": "/**\n* Checks if InputStream can be fully read; returns true if available.\n* @param in InputStream to check for readability\n* @return true if fully readable, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if a path has a specific capability.\n* @param fs the file system object\n* @param path the path to check\n* @param capability the capability to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)": "/**\n* Checks if the object has the specified capability.\n* @param object the object to check\n* @param capability the capability to verify\n* @return true if capability exists, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Deletes files in bulk with a specified page size.\n* @param fileSystem the file system to operate on\n* @param path the path of the files to delete\n* @return number of files deleted\n* @throws IOException if an IO error occurs\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)": "/**\n* Deletes multiple paths from the file system.\n* @param fs file system to operate on\n* @param base base path for deletion\n* @param paths collection of paths to delete\n* @return list of deleted path entries\n* @throws IOException if an IO error occurs\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)": "/**\n* Opens a file in the filesystem.\n* @param fs the FileSystem instance\n* @param path the file path to open\n* @param policy access policy for the file\n* @param status optional file status\n* @param length optional length of the file\n* @param options optional configuration options\n* @return FSDataInputStream for the opened file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)": "/**\n* Reads fully from InputStream into ByteBuffer at specified position.\n* @param in InputStream to read from\n* @param position position in ByteBuffer to start reading\n* @param buf ByteBuffer to fill with data\n* @throws IOException if an IO error occurs\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String)": "/**\n* Initializes DynamicWrappedIO by loading a class and its static methods.\n* @param classname fully qualified name of the class to wrap\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>()": "/**\n* Initializes DynamicWrappedIO with a predefined class name.\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)": "/**\n* Opens a file with optimized read policies or defaults to standard open.\n* @param instance DynamicWrappedIO instance for file operations\n* @param fs FileSystem to access the file\n* @param status FileStatus for the file to open\n* @param readPolicies Policies governing read access\n* @return FSDataInputStream for the opened file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)": "/**\n* Opens a file using specified read policies.\n* @param fs FileSystem to access the file\n* @param status FileStatus for the file to open\n* @param readPolicies Policies governing read access\n* @return FSDataInputStream for the opened file\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.wrappedio.WrappedStatistics": {
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:<init>()": "/**\n* Private constructor for WrappedStatistics class; prevents instantiation.\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable)": "",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Applies a function to IOStatisticsSnapshot from a source.\n* @param source the source for IOStatisticsSnapshot\n* @param fun function to apply, handling IOExceptions\n* @return result of the function application\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled()": "/**\n* Checks if thread-level I/O statistics are enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Loads IOStatisticsSnapshot from the specified path in the given FileSystem.\n* @param fs the FileSystem to read from\n* @param path the path to the IOStatisticsSnapshot file\n* @return the loaded IOStatisticsSnapshot or throws unchecked IOException\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String)": "/**\n* Converts a JSON string to an IOStatisticsSnapshot.\n* @param json JSON representation of IOStatisticsSnapshot\n* @return Serializable IOStatisticsSnapshot object\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)": "/**\n* Aggregates IOStatistics from a snapshot.\n* @param snapshot the IOStatistics snapshot\n* @param statistics the statistics to aggregate, may be null\n* @return true if aggregation is successful, false if statistics is null\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)": "/**\n* Saves IOStatisticsSnapshot to a specified path.\n* @param snapshot the snapshot to save, can be null\n* @param fs the FileSystem to use for saving\n* @param path the destination path for the snapshot\n* @param overwrite flag to indicate if existing files should be overwritten\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable)": "/**\n* Retrieves IO statistics counters from a given source.\n* @param source the source for IOStatisticsSnapshot\n* @return Map of counter names to their values\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable)": "/**\n* Retrieves gauge metrics from IOStatisticsSnapshot.\n* @param source the source for IOStatisticsSnapshot\n* @return Map of gauge names and their corresponding values\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable)": "/**\n* Retrieves a map of minimum IO statistics from the given source.\n* @param source the source for IOStatisticsSnapshot\n* @return Map of minimums with String keys and Long values\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable)": "/**\n* Retrieves maximum IO statistics from the given source.\n* @param source the source for IOStatisticsSnapshot\n* @return Map of maximum values with their corresponding keys\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable)": "/**\n* Computes mean IO statistics from the given source.\n* @param source the source for IOStatisticsSnapshot\n* @return map of mean statistics as key-value pairs\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable)": "/**\n* Converts IOStatisticsSnapshot to its JSON string representation.\n* @param snapshot the IOStatisticsSnapshot to convert\n* @return JSON string of the snapshot\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object)": "/**\n* Converts IOStatistics to a formatted string or returns empty if null.\n* @param statistics IOStatistics object or null\n* @return formatted string representation of statistics\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent()": "/**\n* Retrieves the current IOStatisticsContext.\n* @return IOStatisticsContext for the current thread\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset()": "/**\n* Resets the current IOStatisticsContext.\n* @return void\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot()": "/**\n* Captures a snapshot of the current IOStatisticsContext.\n* @return Serializable snapshot of the IOStatisticsContext\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object)": "/**\n* Aggregates IOStatistics from the source if valid.\n* @param source object to retrieve IOStatistics from\n* @return true if aggregation succeeded, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object)": "/**\n* Sets the IOStatisticsContext for the current thread.\n* @param statisticsContext context to set or null to remove\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object)": "/**\n* Creates an IOStatisticsSnapshot from the given source object.\n* @param source optional IOStatistics for snapshotting\n* @return Serializable IOStatisticsSnapshot instance\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create()": "/**\n* Creates an IOStatisticsSnapshot with default parameters.\n* @return Serializable IOStatisticsSnapshot instance\n*/",
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object)": "/**\n* Retrieves IOStatisticsSnapshot from source or returns null if invalid.\n* @param source object to extract IOStatistics from\n* @return Serializable IOStatisticsSnapshot or null if stats are unavailable\n*/"
    },
    "org.apache.hadoop.util.JsonSerialization": {
        "org.apache.hadoop.util.JsonSerialization:toJson(java.lang.Object)": "/**\n* Converts an object to its JSON representation.\n* @param instance the object to convert\n* @return JSON string of the object\n* @throws JsonProcessingException if conversion fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:fromJson(java.lang.String)": "/**\n* Converts JSON string to an object of type T.\n* @param json JSON string to parse\n* @return Parsed object of type T\n* @throws IOException if parsing fails or json is empty\n*/",
        "org.apache.hadoop.util.JsonSerialization:mapReader()": "/**\n* Returns the ObjectReader instance for mapping.\n* @return ObjectReader instance for use in mapping operations\n*/",
        "org.apache.hadoop.util.JsonSerialization:writer()": "/**\n* Returns the ObjectWriter instance.\n* @return ObjectWriter instance for serialization\n*/",
        "org.apache.hadoop.util.JsonSerialization:getName()": "/**\n* Returns the simple name of the class type.\n* @return simple name of the class as a String\n*/",
        "org.apache.hadoop.util.JsonSerialization:fromJsonStream(java.io.InputStream)": "/**\n* Deserializes JSON from an InputStream into an object of type T.\n* @param stream InputStream containing JSON data\n* @return Object of type T created from JSON\n*/",
        "org.apache.hadoop.util.JsonSerialization:load(java.io.File)": "/**\n* Loads and parses a JSON file into an object of type T.\n* @param jsonFile the JSON file to load\n* @return an instance of T parsed from the file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.JsonSerialization:fromResource(java.lang.String)": "/**\n* Reads and parses a JSON resource into an object.\n* @param resource path to the JSON resource\n* @return parsed object of type T\n* @throws IOException if resource is not found or parsing fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:toBytes(java.lang.Object)": "/**\n* Converts an object to a byte array.\n* @param instance object to convert\n* @return byte array representation of the object\n* @throws IOException if conversion fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)": "",
        "org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object)": "/**\n* Converts an instance to its JSON string representation.\n* @param instance object to convert; must not be null\n* @return JSON string or error message if conversion fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object)": "/**\n* Converts an object to JSON and back to an instance.\n* @param instance the object to convert\n* @return Parsed object of type T\n* @throws IOException if conversion fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:fromBytes(byte[])": "/**\n* Converts byte array to an object of type T.\n* @param bytes byte array to convert\n* @return Parsed object of type T\n* @throws IOException if conversion fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)": "/**\n* Writes object as JSON bytes to output stream.\n* @param instance object to serialize\n* @param dataOutputStream output stream for byte data\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)": "/**\n* Saves the given instance to a file as JSON bytes.\n* @param file destination file to save data\n* @param instance object to serialize and save\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)": "/**\n* Saves an object as JSON to a specified path in the filesystem.\n* @param fs file system to write to\n* @param path destination path for the JSON file\n* @param instance object to serialize\n* @param overwrite flag to overwrite existing file\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)": "/**\n* Loads and deserializes JSON from a file in the filesystem.\n* @param fs the FileSystem to access the file\n* @param path the Path of the JSON file to load\n* @param status optional FileStatus for the file\n* @return deserialized object of type T\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Loads and deserializes JSON from a file in the filesystem.\n* @param fs the FileSystem to access the file\n* @param path the Path of the JSON file to load\n* @return deserialized object of type T\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.functional.FunctionRaisingIOE": {
        "org.apache.hadoop.util.functional.FunctionRaisingIOE:unchecked(java.lang.Object)": "/**\n* Applies a function to input, converting IOException to UncheckedIOException.\n* @param t input value\n* @return result of apply method\n*/"
    },
    "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$PureJavaComparer": {
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$PureJavaComparer:compareTo(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays for order based on specified offsets and lengths.\n* @return negative, zero, or positive integer based on comparison result\n*/"
    },
    "org.apache.hadoop.io.VIntWritable": {
        "org.apache.hadoop.io.VIntWritable:<init>()": "/**\n* Default constructor for VIntWritable, initializes a new instance.\n*/",
        "org.apache.hadoop.io.VIntWritable:set(int)": "/**\n* Sets the internal value to the specified integer.\n* @param value the integer to set\n*/",
        "org.apache.hadoop.io.VIntWritable:toString()": "/**\n* Returns the string representation of the integer value.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.io.VIntWritable:compareTo(org.apache.hadoop.io.VIntWritable)": "/**\n* Compares this VIntWritable with another for order.\n* @param o the VIntWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.VIntWritable:<init>(int)": "/**\n* Initializes VIntWritable with a specified integer value.\n* @param value the integer to set as the internal value\n*/",
        "org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput)": "/**\n* Writes an integer value to the output stream.\n* @param out output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and assigns to value.\n* @param in DataInput stream to read from\n*/"
    },
    "org.apache.hadoop.io.ElasticByteBufferPool$Key": {
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:<init>(int,long)": "/**\n* Constructs a Key with specified capacity and insertion time.\n* @param capacity maximum capacity of the Key\n* @param insertionTime timestamp of when the Key was created\n*/",
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:compareTo(org.apache.hadoop.io.ElasticByteBufferPool$Key)": "/**\n* Compares this Key object with another based on capacity and insertion time.\n* @param other the Key object to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:hashCode()": "/**\n* Computes the hash code based on capacity and insertion time.\n* @return generated hash code as an integer\n*/",
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object)": "/**\n* Checks equality of this Key object with another.\n* @param rhs object to compare\n* @return true if equal, false if not or if cast fails\n*/"
    },
    "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder": {
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer()": "/**\n* Selects the best comparer for byte arrays based on system architecture.\n* @return Comparer<byte[]> instance for comparison operations\n*/"
    },
    "org.apache.hadoop.util.Options$IntegerOption": {
        "org.apache.hadoop.util.Options$IntegerOption:<init>(int)": "/**\n* Constructs an IntegerOption with the specified value.\n* @param value the integer value to be assigned\n*/",
        "org.apache.hadoop.util.Options$IntegerOption:getValue()": "/**\n* Retrieves the current value.\n* @return the current integer value\n*/"
    },
    "org.apache.hadoop.io.UTF8": {
        "org.apache.hadoop.io.UTF8:<init>()": "/**\n* Constructs a UTF8 object, initializing it with an empty string.\n*/",
        "org.apache.hadoop.io.UTF8:set(org.apache.hadoop.io.UTF8)": "/**\n* Copies data from another UTF8 object.\n* @param other the UTF8 instance to copy from\n*/",
        "org.apache.hadoop.io.UTF8:utf8Length(java.lang.String)": "/**\n* Calculates the UTF-8 byte length of a given string.\n* @param string input string to evaluate\n* @return int UTF-8 byte length of the string\n*/",
        "org.apache.hadoop.io.UTF8:writeChars(java.io.DataOutput,java.lang.String,int,int)": "/**\n* Writes UTF-8 encoded characters from a substring to the output stream.\n* @param out DataOutput stream to write to\n* @param s source string containing characters\n* @param start starting index of substring\n* @param length number of characters to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.UTF8:readFields(java.io.DataInput)": "/**\n* Reads data fields from input stream and stores in byte array.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.UTF8:write(java.io.DataOutput)": "/**\n* Writes length and byte data to the output stream.\n* @param out DataOutput stream to write data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.UTF8:highSurrogate(int)": "/**\n* Calculates the high surrogate for a given Unicode code point.\n* @param codePoint the Unicode code point\n* @return the corresponding high surrogate character\n*/",
        "org.apache.hadoop.io.UTF8:lowSurrogate(int)": "/**\n* Computes the low surrogate character for a given Unicode code point.\n* @param codePoint Unicode code point value\n* @return low surrogate character\n*/",
        "org.apache.hadoop.io.UTF8:getBytes()": "/**\n* Retrieves the byte array.\n* @return byte array containing data\n*/",
        "org.apache.hadoop.io.UTF8:getLength()": "/**\n* Returns the length value.\n* @return the current length as an integer\n*/",
        "org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8)": "/**\n* Constructs a UTF8 object by copying data from another UTF8 instance.\n* @param utf8 the UTF8 instance to copy from\n*/",
        "org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)": "/**\n* Writes a UTF-8 encoded string to output, truncating if too long.\n* @param out DataOutput stream to write to\n* @param s input string to write\n* @return int length of the written string\n*/",
        "org.apache.hadoop.io.UTF8:skip(java.io.DataInput)": "/**\n* Skips a specified number of bytes from the DataInput stream.\n* @param in the DataInput stream to skip bytes from\n* @throws IOException if unable to read or skip bytes\n*/",
        "org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8)": "/**\n* Compares this UTF8 object with another for order.\n* @param o the UTF8 object to compare with\n* @return negative, zero, or positive integer based on comparison\n*/",
        "org.apache.hadoop.io.UTF8:equals(java.lang.Object)": "/**\n* Checks equality of UTF8 objects based on byte content and length.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.UTF8:hashCode()": "/**\n* Computes hash code for the current object.\n* @return computed hash value as an integer\n*/",
        "org.apache.hadoop.io.UTF8:set(java.lang.String)": "/**\n* Sets a string after truncation, computes its UTF-8 length, and manages byte buffer allocation.\n* @param string input string to set\n*/",
        "org.apache.hadoop.io.UTF8:getBytes(java.lang.String)": "/**\n* Converts a string to a UTF-8 byte array.\n* @param string input string to convert\n* @return byte array representation of the string\n*/",
        "org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)": "/**\n* Reads UTF-8 encoded characters from input stream into a StringBuilder.\n* @param in input data stream\n* @param buffer StringBuilder to append characters\n* @param nBytes number of bytes to read\n* @throws UTFDataFormatException if data is not valid UTF-8\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.UTF8:<init>(java.lang.String)": "/**\n* Constructs a UTF8 object from a given string.\n* @param string input string to set\n*/",
        "org.apache.hadoop.io.UTF8:toString()": "/**\n* Converts object to string representation using buffered input.\n* @return String representation of the object\n*/",
        "org.apache.hadoop.io.UTF8:toStringChecked()": "/**\n* Converts input buffer to string with error handling.\n* @return string representation of the buffer\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.UTF8:fromBytes(byte[])": "/**\n* Converts byte array to String.\n* @param bytes input byte array\n* @return decoded String from UTF-8 bytes\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.UTF8:readString(java.io.DataInput)": "/**\n* Reads a string from input stream with specified length.\n* @param in input data stream\n* @return the read string\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.WritableUtils": {
        "org.apache.hadoop.io.WritableUtils:skipFully(java.io.DataInput,int)": "/**\n* Skips a specified number of bytes in the input stream.\n* @param in the DataInput stream to skip bytes from\n* @param len number of bytes to skip\n* @throws IOException if unable to skip the specified bytes\n*/",
        "org.apache.hadoop.io.WritableUtils:decodeVIntSize(byte)": "/**\n* Determines the size of a variable-length integer.\n* @param value byte representation of the integer\n* @return size in bytes of the decoded integer\n*/",
        "org.apache.hadoop.io.WritableUtils:readString(java.io.DataInput)": "/**\n* Reads a UTF-8 encoded string from DataInput.\n* @param in DataInput source for reading the string\n* @return the read string or null if length is -1\n*/",
        "org.apache.hadoop.io.WritableUtils:writeString(java.io.DataOutput,java.lang.String)": "/**\n* Writes a string to DataOutput, or -1 if null.\n* @param out DataOutput stream to write to\n* @param s String to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:readCompressedByteArray(java.io.DataInput)": "/**\n* Reads and decompresses a byte array from input stream.\n* @param in data input stream\n* @return decompressed byte array or null if length is -1\n*/",
        "org.apache.hadoop.io.WritableUtils:displayByteArray(byte[])": "/**\n* Displays a byte array as hexadecimal values in rows of 16.\n* @param record the byte array to display\n*/",
        "org.apache.hadoop.io.WritableUtils:writeVLong(java.io.DataOutput,long)": "/**\n* Writes a variable-length long to a DataOutput stream.\n* @param stream output stream to write to\n* @param i long value to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:isNegativeVInt(byte)": "/**\n* Checks if a byte value is a negative variable-length integer.\n* @param value the byte to check\n* @return true if value is negative VInt, otherwise false\n*/",
        "org.apache.hadoop.io.WritableUtils:getVIntSize(long)": "/**\n* Calculates the size of a variable-length integer.\n* @param i the integer to evaluate\n* @return size in bytes required to encode the integer\n*/",
        "org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput)": "/**\n* Skips a compressed byte array from the input stream.\n* @param in the DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)": "/**\n* Reads an array of UTF-8 encoded strings from DataInput.\n* @param in DataInput source for reading the strings\n* @return String array or null if length is -1\n*/",
        "org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])": "/**\n* Writes an array of strings to DataOutput.\n* @param out DataOutput stream to write to\n* @param s Array of strings to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput)": "/**\n* Reads and decompresses a string from input stream.\n* @param in data input stream\n* @return decompressed string or null if input is invalid\n*/",
        "org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)": "/**\n* Writes a variable-length integer to a DataOutput stream.\n* @param stream output stream to write to\n* @param i integer value to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput)": "/**\n* Reads a variable-length long from the input stream.\n* @param stream DataInput stream to read from\n* @return decoded long value\n*/",
        "org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])": "/**\n* Compresses and writes a byte array to output.\n* @param out output stream for writing data\n* @param bytes byte array to compress and write\n* @return compression ratio or -1 if input is null\n*/",
        "org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)": "/**\n* Reads an array of compressed strings from input stream.\n* @param in data input stream\n* @return String array or null if length is -1\n*/",
        "org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput)": "/**\n* Reads a variable-length integer from stream.\n* @param stream DataInput stream to read from\n* @return decoded integer value\n*/",
        "org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)": "/**\n* Reads a bounded integer from stream, ensuring it's within specified range.\n* @param stream DataInput stream to read from\n* @param lower minimum acceptable value\n* @param upper maximum acceptable value\n* @return bounded integer value\n* @throws IOException if value is out of range\n*/",
        "org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[])": "/**\n* Converts Writable objects to a byte array.\n* @param writables variable number of Writable objects\n* @return byte array representation of the writables\n*/",
        "org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)": "/**\n* Compresses and writes a string to output.\n* @param out output stream for writing data\n* @param s string to compress and write\n* @return compression ratio or -1 if string is null\n*/",
        "org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)": "/**\n* Reads a UTF-8 string from input with length validation.\n* @param in DataInput stream to read from\n* @param maxLength maximum allowed string length\n* @return decoded UTF-8 string\n*/",
        "org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)": "/** Writes the name of the given enum to the output stream. \n* @param out output stream to write to \n* @param enumVal the enum value to encode \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])": "/**\n* Writes an array of compressed strings to output.\n* @param out output stream for writing data\n* @param s array of strings to compress and write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)": "/**\n* Clones data from source Writable to destination Writable.\n* @param dst destination Writable object\n* @param src source Writable object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)": "/**** Reads an enum value from input stream. \n* @param in input stream to read from \n* @param enumType class of the enum type \n* @return enum value corresponding to the input string \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)": "/**\n* Clones a Writable object with a given configuration.\n* @param orig original Writable object to clone\n* @param conf configuration for instantiation and copying\n* @return cloned Writable object\n*/"
    },
    "org.apache.hadoop.io.UTF8$1": {
        "org.apache.hadoop.io.UTF8$1:<init>()": "/**\n* Initializes a new UTF8 object with an empty string.\n*/"
    },
    "org.apache.hadoop.io.IntWritable$Comparator": {
        "org.apache.hadoop.io.IntWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays as integers.\n* @param b1 first byte array, s1 start index, l1 length\n* @param b2 second byte array, s2 start index, l2 length\n* @return negative if b1 < b2, zero if equal, positive if b1 > b2\n*/",
        "org.apache.hadoop.io.IntWritable$Comparator:<init>()": "/**\n* Constructs a Comparator for IntWritable keys.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter$SortPass": {
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:setProgressable(org.apache.hadoop.util.Progressable)": "/**\n* Sets the Progressable instance for tracking progress.\n* @param progressable the Progressable object to set\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(int[],int)": "/**\n* Expands an array to a new length, copying existing elements.\n* @param old array to be expanded\n* @param newLength desired length of the new array\n* @return new array with copied elements\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(org.apache.hadoop.io.SequenceFile$ValueBytes[],int)": "/**\n* Expands an array of ValueBytes to a new length, initializing new elements to null.\n* @param old the original ValueBytes array\n* @param newLength the desired length of the new array\n* @return a new ValueBytes array of specified length\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow()": "/**\n* Expands internal arrays to a new length for efficient storage.\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int)": "/**\n* Sorts a portion of the pointers array.\n* @param count number of elements to sort\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close()": "/**\n* Closes input and output streams, releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)": "/**\n* Flushes data to output, creating files and writing segments with optional compression.\n* @param count number of records to write\n* @param bytesProcessed total bytes processed\n* @param compressionType type of compression to use\n* @param codec compression codec for data\n* @param done indicates if flushing is complete\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean)": "/**\n* Processes input files, reading and flushing data segments.\n* @param deleteInput indicates whether to delete input files after processing\n* @return number of segments processed\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor": {
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:preserveInput(boolean)": "/**\n* Sets the preserveInput flag.\n* @param preserve true to preserve input, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:doSync()": "/**\n* Enables synchronization by setting ignoreSync to false.\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:getKey()": "/**\n* Retrieves the raw key data.\n* @return DataOutputBuffer containing the raw key\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:shouldPreserveInput()": "/**\n* Checks if input preservation is enabled.\n* @return true if input should be preserved, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:hashCode()": "/**\n* Computes the hash code based on segmentOffset.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object)": "/**\n* Compares SegmentDescriptor objects by length, offset, and path name.\n* @param o object to compare with\n* @return negative if less, positive if greater, zero if equal\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object)": "/**\n* Compares this SegmentDescriptor with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close()": "/**\n* Closes input stream and sets it to null.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup()": "/**\n* Cleans up resources; closes stream and deletes segment if not preserved.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes)": "/**\n* Retrieves the next raw value length from the input stream.\n* @param rawValue stores the retrieved data\n* @return Length of the retrieved value\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey()": "/**\n* Reads the next raw key, initializing the reader if necessary.\n* @return true if a key is read; false if end is reached\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer": {
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:getSegmentList()": "/**\n* Retrieves the list of segment descriptors.\n* @return List of SegmentDescriptor objects\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:cleanup()": "/**\n* Cleans up segments and deletes the file if all segments are cleaned.\n* @throws IOException if an I/O error occurs during file deletion\n*/"
    },
    "org.apache.hadoop.io.IntWritable": {
        "org.apache.hadoop.io.IntWritable:get()": "/**\n* Returns the current value.\n* @return the integer value\n*/",
        "org.apache.hadoop.io.IntWritable:<init>()": "/**\n* Default constructor for IntWritable, initializes to zero.\n*/",
        "org.apache.hadoop.io.IntWritable:set(int)": "/**\n* Sets the internal value to the specified integer.\n* @param value the integer to set\n*/",
        "org.apache.hadoop.io.IntWritable:readFields(java.io.DataInput)": "/**\n* Reads an integer value from the input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IntWritable:write(java.io.DataOutput)": "/**\n* Writes an integer value to the provided DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.IntWritable:toString()": "/**\n* Returns the string representation of the integer value.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.io.IntWritable:compareTo(org.apache.hadoop.io.IntWritable)": "/**\n* Compares this IntWritable object with another for order.\n* @param o IntWritable object to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.IntWritable:<init>(int)": "/***********************************************\n* Initializes IntWritable with the specified integer.\n* @param value the integer value to set\n***********************************************/"
    },
    "org.apache.hadoop.io.WritableName": {
        "org.apache.hadoop.io.WritableName:<init>()": "/**\n* Private constructor to prevent instantiation of WritableName class.\n*/",
        "org.apache.hadoop.io.WritableName:setName(java.lang.Class,java.lang.String)": "/**\n* Associates a class with a name in both mappings.\n* @param writableClass the class to associate\n* @param name the name to associate with the class\n*/",
        "org.apache.hadoop.io.WritableName:addName(java.lang.Class,java.lang.String)": "/**\n* Associates a name with a writable class in a synchronized manner.\n* @param writableClass the class to associate with the name\n* @param name the name to be linked to the writable class\n*/",
        "org.apache.hadoop.io.WritableName:getName(java.lang.Class)": "/**\n* Retrieves the name associated with a class or its canonical name if not found.\n* @param writableClass the class to lookup\n* @return the associated name or the class's canonical name\n*/",
        "org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves Class by name from cache or configuration.\n* @param name fully qualified class name\n* @param conf configuration for class retrieval\n* @return Class object\n* @throws IOException if class cannot be loaded\n*/"
    },
    "org.apache.hadoop.io.MapFile": {
        "org.apache.hadoop.io.MapFile:<init>()": "/**\n* Protected constructor for MapFile, prevents public instantiation.\n*/",
        "org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)": "/**\n* Renames a file or directory in the given file system.\n* @param fs the file system to operate on\n* @param oldName the current name of the file/directory\n* @param newName the new name to assign\n* @throws IOException if the rename operation fails\n*/",
        "org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)": "/**\n* Deletes specified files and directory from the file system.\n* @param fs the FileSystem instance to perform deletion\n* @param name the name of the directory to delete\n*/",
        "org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Fixes missing index for a SequenceFile in the specified directory.\n* @param fs the FileSystem instance\n* @param dir directory containing the data file\n* @param keyClass expected key class type\n* @param valueClass expected value class type\n* @param dryrun if true, performs a dry run without changes\n* @return count of processed records or -1 if index exists\n*/",
        "org.apache.hadoop.io.MapFile:main(java.lang.String[])": "/**\n* Copies entries from a MapFile to a new MapFile specified by command-line arguments.\n* @param args input and output file paths\n* @throws Exception if file operations fail\n*/"
    },
    "org.apache.hadoop.io.LongWritable": {
        "org.apache.hadoop.io.LongWritable:<init>()": "/**\n* Default constructor for LongWritable, initializes a new instance.\n*/",
        "org.apache.hadoop.io.LongWritable:set(long)": "/**\n* Sets the internal value to the specified long.\n* @param value the long value to set\n*/",
        "org.apache.hadoop.io.LongWritable:readFields(java.io.DataInput)": "/**\n* Reads a long value from the input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.LongWritable:write(java.io.DataOutput)": "/**\n* Writes a long value to the DataOutput stream.\n* @param out the output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.LongWritable:toString()": "/**\n* Returns the string representation of the long value.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.io.LongWritable:compareTo(org.apache.hadoop.io.LongWritable)": "/**\n* Compares this LongWritable with another for order.\n* @param o LongWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.LongWritable:get()": "/**\n* Retrieves the current value.\n* @return the current long value\n*/",
        "org.apache.hadoop.io.LongWritable:<init>(long)": "/**\n* Initializes LongWritable with a specified long value.\n* @param value the long value to set\n*/"
    },
    "org.apache.hadoop.util.Options$ClassOption": {
        "org.apache.hadoop.util.Options$ClassOption:<init>(java.lang.Class)": "/**\n* Initializes ClassOption with the specified class type.\n* @param value the class type to be set\n*/",
        "org.apache.hadoop.util.Options$ClassOption:getValue()": "/**\n* Retrieves the value of the class type.\n* @return Class<?> representing the value's type\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$CompressedBytes": {
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:<init>(org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Initializes CompressedBytes with a specified compression codec.\n* @param codec the compression codec to be used\n*/",
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:reset(java.io.DataInputStream,int)": "/**\n* Resets the data buffer and reads new data from the input stream.\n* @param in DataInputStream to read from\n* @param length number of bytes to read into the buffer\n*/",
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:writeCompressedBytes(java.io.DataOutputStream)": "/**\n* Writes compressed bytes to the output stream.\n* @param outStream the stream to write data to\n* @throws IllegalArgumentException if data is invalid\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:getSize()": "/**\n* Returns the size of the data.\n* @return the current size as an integer\n*/",
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream)": "/**\n* Writes uncompressed bytes to the output stream.\n* @param outStream output stream for writing data\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.ByteWritable": {
        "org.apache.hadoop.io.ByteWritable:<init>()": "/**\n* Default constructor for ByteWritable, initializes a new instance.\n*/",
        "org.apache.hadoop.io.ByteWritable:set(byte)": "/**\n* Sets the internal byte value.\n* @param value the byte value to set\n*/",
        "org.apache.hadoop.io.ByteWritable:readFields(java.io.DataInput)": "/**\n* Reads a byte value from the input stream.\n* @param in the DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ByteWritable:write(java.io.DataOutput)": "/**\n* Writes a byte value to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ByteWritable:toString()": "/**\n* Returns the string representation of the byte value.\n* @return string representation of the byte value\n*/",
        "org.apache.hadoop.io.ByteWritable:compareTo(org.apache.hadoop.io.ByteWritable)": "/**\n* Compares this ByteWritable with another for order.\n* @param o ByteWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.ByteWritable:<init>(byte)": "/**\n* Constructs a ByteWritable object with a specified byte value.\n* @param value the byte value to set\n*/"
    },
    "org.apache.hadoop.io.OutputBuffer": {
        "org.apache.hadoop.io.OutputBuffer:<init>(org.apache.hadoop.io.OutputBuffer$Buffer)": "/**\n* Initializes OutputBuffer with the specified Buffer.\n* @param buffer the Buffer to be wrapped\n*/",
        "org.apache.hadoop.io.OutputBuffer:<init>()": "/**\n* Constructs an OutputBuffer by initializing it with a new Buffer instance.\n*/",
        "org.apache.hadoop.io.OutputBuffer:getData()": "/**\n* Retrieves byte array data from the buffer.\n* @return byte array containing the buffer data\n*/",
        "org.apache.hadoop.io.OutputBuffer:getLength()": "/**\n* Retrieves the current length of the buffer.\n* @return the length of the buffer as an integer\n*/",
        "org.apache.hadoop.io.OutputBuffer:reset()": "/**\n* Resets the buffer and returns the current OutputBuffer instance.\n* @return OutputBuffer instance after reset\n*/",
        "org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)": "/**\n* Writes data from InputStream to buffer.\n* @param in InputStream to read from\n* @param length number of bytes to write\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.OutputBuffer$Buffer": {
        "org.apache.hadoop.io.OutputBuffer$Buffer:getData()": "/**\n* Retrieves the byte array data from the buffer.\n* @return byte array containing the buffer data\n*/",
        "org.apache.hadoop.io.OutputBuffer$Buffer:getLength()": "/**\n* Returns the current length count.\n* @return the length as an integer\n*/",
        "org.apache.hadoop.io.OutputBuffer$Buffer:reset()": "/**\n* Resets the count to zero.\n*/",
        "org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)": "/**** Writes data from InputStream to buffer, resizing if necessary. \n* @param in InputStream to read from\n* @param len number of bytes to read\n* @throws IOException if an I/O error occurs \n*/"
    },
    "org.apache.hadoop.io.FastByteComparisons": {
        "org.apache.hadoop.io.FastByteComparisons:compareTo(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays lexicographically.\n* @param b1 first byte array, s1 start index, l1 length\n* @param b2 second byte array, s2 start index, l2 length\n* @return negative, zero, or positive integer based on comparison\n*/",
        "org.apache.hadoop.io.FastByteComparisons:lexicographicalComparerJavaImpl()": "/**\n* Returns a lexicographical comparer for byte arrays.\n* @return Comparer<byte[]> instance for comparison\n*/"
    },
    "org.apache.hadoop.io.BytesWritable": {
        "org.apache.hadoop.io.BytesWritable:<init>()": "/**\n* Initializes a BytesWritable object with empty byte array and size zero.\n*/",
        "org.apache.hadoop.io.BytesWritable:<init>(byte[],int)": "/**\n* Constructs a BytesWritable with specified byte array and length.\n* @param bytes byte array to store\n* @param length number of bytes to consider from the array\n*/",
        "org.apache.hadoop.io.BytesWritable:copyBytes()": "/**\n* Creates a copy of the byte array up to the specified size.\n* @return a new byte array containing the copied bytes\n*/",
        "org.apache.hadoop.io.BytesWritable:getBytes()": "/**\n* Returns the byte array stored in the object.\n* @return byte array representing the object's data\n*/",
        "org.apache.hadoop.io.BytesWritable:getLength()": "/**\n* Returns the length of the collection.\n* @return the number of elements in the collection\n*/",
        "org.apache.hadoop.io.BytesWritable:getCapacity()": "/**\n* Returns the capacity of the byte array.\n* @return the length of the byte array\n*/",
        "org.apache.hadoop.io.BytesWritable:write(java.io.DataOutput)": "/**\n* Writes size and byte data to the output stream.\n* @param out DataOutput stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.BytesWritable:toString()": "/**\n* Converts byte array to a hex string representation.\n* @return formatted hex string of bytes\n*/",
        "org.apache.hadoop.io.BytesWritable:<init>(byte[])": "/**\n* Constructs a BytesWritable from a byte array.\n* @param bytes byte array to store\n*/",
        "org.apache.hadoop.io.BytesWritable:get()": "/**\n* Retrieves the byte array representation of the object.\n* @return byte array of the object's data\n*/",
        "org.apache.hadoop.io.BytesWritable:getSize()": "/**\n* Returns the size of the collection (deprecated).\n* @return number of elements in the collection\n*/",
        "org.apache.hadoop.io.BytesWritable:setCapacity(int)": "/**\n* Sets the capacity of the byte array.\n* @param capacity new capacity value for the byte array\n*/",
        "org.apache.hadoop.io.BytesWritable:setSize(int)": "/**\n* Sets the size of the array, adjusting capacity if needed.\n* @param size new size for the array\n*/",
        "org.apache.hadoop.io.BytesWritable:hashCode()": "/**\n* Computes the hash code for the object.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.io.BytesWritable:set(byte[],int,int)": "/**\n* Updates internal byte array with new data from specified offset.\n* @param newData source byte array\n* @param offset starting position in newData\n* @param length number of bytes to copy\n*/",
        "org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput)": "/**\n* Reads data from input stream and updates internal byte array size.\n* @param in DataInput stream to read from\n*/",
        "org.apache.hadoop.io.BytesWritable:equals(java.lang.Object)": "/**\n* Checks equality with another object, specifically BytesWritable.\n* @param right_obj object to compare for equality\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable)": "/**\n* Updates internal data using BytesWritable content.\n* @param newData source containing byte array and size\n*/"
    },
    "org.apache.hadoop.io.ElasticByteBufferPool": {
        "org.apache.hadoop.io.ElasticByteBufferPool:getBufferTree(boolean)": "/**\n* Returns the appropriate buffer TreeMap based on direct allocation preference.\n* @param direct true for direct buffers, false for regular buffers\n* @return TreeMap of buffers\n*/",
        "org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)": "/**\n* Retrieves a ByteBuffer, allocating a new one if none is available.\n* @param direct specifies buffer allocation type\n* @param length desired buffer length\n* @return ByteBuffer instance\n*/",
        "org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer)": "/**\n* Stores a ByteBuffer in a TreeMap using its capacity and timestamp as the key.\n* @param buffer the ByteBuffer to store\n*/",
        "org.apache.hadoop.io.ElasticByteBufferPool:size(boolean)": "/**\n* Returns the size of the buffer TreeMap.\n* @param direct true for direct buffers, false for regular buffers\n* @return number of buffers in the TreeMap\n*/"
    },
    "org.apache.hadoop.io.ArrayPrimitiveWritable": {
        "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>()": "/**\n* Default constructor for ArrayPrimitiveWritable.\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:getPrimitiveClass(java.lang.String)": "/**\n* Retrieves the primitive Class type for a given class name.\n* @param className name of the primitive type\n* @return Class<?> corresponding to the primitive type or null if not found\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeBooleanArray(java.io.DataOutput)": "/**\n* Writes a boolean array to the given DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeCharArray(java.io.DataOutput)": "/**\n* Writes a character array to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeByteArray(java.io.DataOutput)": "/**\n* Writes a byte array to the output stream.\n* @param out DataOutput stream to write the byte array\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeShortArray(java.io.DataOutput)": "/**\n* Writes an array of shorts to the specified DataOutput.\n* @param out DataOutput stream to write the shorts to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeIntArray(java.io.DataOutput)": "/**\n* Writes an integer array to the output stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeLongArray(java.io.DataOutput)": "/**\n* Writes a long array to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeFloatArray(java.io.DataOutput)": "/**\n* Writes a float array to the specified DataOutput stream.\n* @param out DataOutput stream to write the float array\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeDoubleArray(java.io.DataOutput)": "/**\n* Writes a double array to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readBooleanArray(java.io.DataInput)": "/**\n* Reads boolean values into an array from DataInput stream.\n* @param in DataInput stream to read boolean values from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readCharArray(java.io.DataInput)": "/**\n* Reads characters into an array from the provided DataInput stream.\n* @param in DataInput stream to read characters from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readByteArray(java.io.DataInput)": "/**\n* Reads a byte array from the given DataInput stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readShortArray(java.io.DataInput)": "/**\n* Reads an array of shorts from DataInput and stores it in the value array.\n* @param in DataInput source for reading short values\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readIntArray(java.io.DataInput)": "/**\n* Reads integers from input and stores them in an array.\n* @param in DataInput source for reading integers\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readLongArray(java.io.DataInput)": "/**\n* Reads a long array from the given DataInput stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readFloatArray(java.io.DataInput)": "/**\n* Reads float values into an array from DataInput stream.\n* @param in DataInput stream to read float values from\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readDoubleArray(java.io.DataInput)": "/**\n* Reads doubles into an array from DataInput stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:get()": "/**\n* Retrieves the stored value.\n* @return the current value object\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:getComponentType()": "/**\n* Retrieves the type of the component.\n* @return Class<?> representing the component's type\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:getDeclaredComponentType()": "/**\n* Returns the declared component type of the class.\n* @return Class<?> representing the declared component type\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:isDeclaredComponentType(java.lang.Class)": "/**\n* Checks if the given class matches the declared component type.\n* @param componentType class to compare with declared component type\n* @return true if matches, false otherwise\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class)": "/**\n* Validates the component type of an array.\n* @param componentType expected array component type\n* @throws HadoopIllegalArgumentException if null or not a primitive type\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class)": "/**\n* Validates the component type against the declared type.\n* @param componentType the type of the input array component\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object)": "/**\n* Validates that the input is a non-null array.\n* @param value object to check; must be a non-null array\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class)": "/**\n* Constructs ArrayPrimitiveWritable with specified component type.\n* @param componentType expected array component type\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object)": "/**\n* Sets the array value and validates its component type.\n* @param value non-null array to set\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput)": "/**\n* Writes component type and length to DataOutput, encoding based on type.\n* @param out DataOutput stream for writing\n* @throws IOException if an unsupported type is encountered\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object)": "/**\n* Constructs ArrayPrimitiveWritable and sets its value.\n* @param value non-null array to initialize the object\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput, setting array type and length, and populating values.\n* @param in DataInput stream to read fields from\n* @throws IOException if an I/O error occurs or data is invalid\n*/"
    },
    "org.apache.hadoop.util.Progress": {
        "org.apache.hadoop.util.Progress:<init>()": "/**\n* Constructs a new Progress instance.\n*/",
        "org.apache.hadoop.util.Progress:set(float)": "/**\n* Sets progress value, clamping it between 0 and 1.\n* @param progress the desired progress value\n*/",
        "org.apache.hadoop.util.Progress:setStatus(java.lang.String)": "/**\n* Sets the current status.\n* @param status the new status to be set\n*/",
        "org.apache.hadoop.util.Progress:setParent(org.apache.hadoop.util.Progress)": "/**\n* Sets the parent Progress object.\n* @param parent the Progress instance to set as the parent\n*/",
        "org.apache.hadoop.util.Progress:getProgressWeightage(int)": "/**\n* Retrieves progress weightage for a given phase.\n* @param phaseNum index of the phase\n* @return weightage as float, or equal weightage if fixed for all phases\n*/",
        "org.apache.hadoop.util.Progress:phase()": "/**\n* Retrieves the current phase from the phases list.\n* @return Progress object representing the current phase\n*/",
        "org.apache.hadoop.util.Progress:startNextPhase()": "/**\n* Increments the current phase in a synchronized manner.\n*/",
        "org.apache.hadoop.util.Progress:getParent()": "/**\n* Retrieves the parent Progress object.\n* @return parent Progress or null if none exists\n*/",
        "org.apache.hadoop.util.Progress:addNewPhase()": "/**\n* Creates and adds a new Progress phase.\n* @return the newly created Progress instance\n*/",
        "org.apache.hadoop.util.Progress:addPhase(float)": "/**\n* Adds a new phase with weightage and verifies total weightage limit.\n* @param weightage the weight assigned to the new phase\n* @return Progress instance for the added phase\n*/",
        "org.apache.hadoop.util.Progress:getInternal()": "/**\n* Calculates total progress based on current and completed phases.\n* @return total progress as a float\n*/",
        "org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder)": "/**\n* Appends status and current phase to StringBuilder buffer.\n* @param buffer StringBuilder to accumulate the output\n*/",
        "org.apache.hadoop.util.Progress:complete()": "/**\n* Completes current progress and notifies parent to start next phase if present.\n*/",
        "org.apache.hadoop.util.Progress:addPhase()": "/**\n* Adds a new Progress phase with equal weightage.\n* @return newly created Progress instance\n*/",
        "org.apache.hadoop.util.Progress:addPhases(int)": "/**\n* Adds specified number of phases and sets equal weightage for all.\n* @param n number of phases to add\n*/",
        "org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)": "/**\n* Adds a new phase with status and weightage.\n* @param status the status to set for the phase\n* @param weightage the weight assigned to the new phase\n* @return Progress instance for the added phase\n*/",
        "org.apache.hadoop.util.Progress:get()": "/**\n* Retrieves total progress from the root Progress object.\n* @return total progress as a float\n*/",
        "org.apache.hadoop.util.Progress:getProgress()": "/**\n* Retrieves the current progress as a float.\n* @return total progress based on internal calculations\n*/",
        "org.apache.hadoop.util.Progress:toString()": "/**\n* Converts the object to a string representation.\n* @return string representation of the object\n*/",
        "org.apache.hadoop.util.Progress:addPhase(java.lang.String)": "/**\n* Creates a new Progress phase with a specified status.\n* @param status the status to set for the new Progress phase\n* @return newly created Progress instance\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue": {
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getPassFactor(int,int)": "/**\n* Calculates pass factor based on pass number and segments.\n* @param passNo current pass number\n* @param numSegments total number of segments\n* @return calculated pass factor\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getSegmentDescriptors(int)": "/**\n* Retrieves an array of segment descriptors up to a specified count.\n* @param numDescriptors maximum number of descriptors to retrieve\n* @return array of SegmentDescriptor objects\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getKey()": "/**\n* Returns the raw key data.\n* @return DataOutputBuffer containing the key\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getProgress()": "/**\n* Retrieves the current progress.\n* @return Progress object representing the current state\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getValue()": "/**\n* Retrieves the raw value as ValueBytes.\n* @return ValueBytes object representing the raw value\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long)": "/**\n* Updates total processed bytes and adjusts progress if progPerByte is positive.\n* @param bytesProcessed number of bytes processed in the current update\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)": "/**\n* Compares two SegmentDescriptors for ordering.\n* @param a first SegmentDescriptor\n* @param b second SegmentDescriptor\n* @return true if a is less than b, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor)": "/**\n* Inserts a SegmentDescriptor into the heap with compression checks.\n* @param stream the SegmentDescriptor to add\n* @throws IOException if compression states differ\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close()": "/**\n* Closes all segments by cleaning up resources.\n* @throws IOException if an I/O error occurs during cleanup\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor)": "/**\n* Adjusts priority queue based on segment descriptor.\n* @param ms segment descriptor containing input stream data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next()": "/**\n* Advances to the next segment, updating the progress and retrieving key/value data.\n* @return true if next segment exists, false if no more segments are available\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge()": "/**\n* Merges sorted segments into a single output file.\n* @return RawKeyValueIterator for merged segments\n* @throws IOException if an I/O error occurs during merging\n*/"
    },
    "org.apache.hadoop.io.DataInputBuffer$Buffer": {
        "org.apache.hadoop.io.DataInputBuffer$Buffer:<init>()": "/**\n* Initializes an empty Buffer with a zero-length byte array.\n*/",
        "org.apache.hadoop.io.DataInputBuffer$Buffer:read(byte[],int,int)": "/**\n* Reads bytes into the array from a buffer.\n* @param b destination array, @param off start offset, @param len number of bytes to read\n* @return number of bytes read or -1 if end of buffer is reached\n*/",
        "org.apache.hadoop.io.DataInputBuffer$Buffer:reset(byte[],int,int)": "/**\n* Resets buffer state using input array and specified range.\n* @param input byte array to set as buffer\n* @param start starting index for the buffer\n* @param length number of bytes to include from the start\n*/",
        "org.apache.hadoop.io.DataInputBuffer$Buffer:getData()": "/**\n* Retrieves the byte array data from the buffer.\n* @return byte array containing the buffer data\n*/",
        "org.apache.hadoop.io.DataInputBuffer$Buffer:getPosition()": "/**\n* Retrieves the current position value.\n* @return the current position as an integer\n*/",
        "org.apache.hadoop.io.DataInputBuffer$Buffer:getLength()": "/**\n* Returns the current length count.\n* @return the length count as an integer\n*/"
    },
    "org.apache.hadoop.io.ReadaheadPool": {
        "org.apache.hadoop.io.ReadaheadPool:<init>()": "/**\n* Initializes a ReadaheadPool with a configured thread pool and policies.\n*/",
        "org.apache.hadoop.io.ReadaheadPool:resetInstance()": "/**\n* Resets the singleton instance of ReadaheadPool.\n*/",
        "org.apache.hadoop.io.ReadaheadPool:submitReadahead(java.lang.String,java.io.FileDescriptor,long,long)": "/**\n* Submits a readahead request for a file.\n* @param identifier unique request identifier\n* @param fd file descriptor for the target file\n* @param off starting offset for readahead\n* @param len length of data to prefetch\n* @return ReadaheadRequestImpl object representing the request\n*/",
        "org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)": "/**\n* Manages readahead requests for a file stream.\n* @param identifier unique request identifier\n* @param fd file descriptor for the target file\n* @param curPos current position in the stream\n* @param readaheadLength length of data to prefetch\n* @param maxOffsetToRead maximum allowable read offset\n* @param lastReadahead previous readahead request\n* @return ReadaheadRequest or null if no request is made\n*/",
        "org.apache.hadoop.io.ReadaheadPool:getInstance()": "/**\n* Retrieves the singleton instance of ReadaheadPool.\n* @return ReadaheadPool instance or null if not initialized\n*/"
    },
    "org.apache.hadoop.io.NullWritable": {
        "org.apache.hadoop.io.NullWritable:get()": "/**\n* Returns a singleton instance of NullWritable.\n* @return NullWritable instance\n*/",
        "org.apache.hadoop.io.NullWritable:<init>()": "/**\n* Private constructor to prevent instantiation of NullWritable class.\n*/",
        "org.apache.hadoop.io.NullWritable:compareTo(org.apache.hadoop.io.NullWritable)": "/**\n* Compares this NullWritable instance to another.\n* @param other another NullWritable instance\n* @return always returns 0 indicating equality\n*/",
        "org.apache.hadoop.io.NullWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from a DataInput stream.\n* @param in input stream to read data from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.NullWritable:write(java.io.DataOutput)": "/**\n* Writes data to the specified output stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl": {
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>(java.lang.String,java.io.FileDescriptor,long,long)": "/**\n* Constructs a ReadaheadRequestImpl with identifier, file descriptor, offset, and length.\n* @param identifier unique request identifier\n* @param fd file descriptor for the request\n* @param off offset in the file\n* @param len length of data to read\n*/",
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:toString()": "/**\n* Returns a string representation of the ReadaheadRequestImpl object.\n* @return formatted string with identifier, fd, off, and len values\n*/",
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:cancel()": "/**\n* Marks the current task as canceled; does not remove from work queue.\n*/",
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:getLength()": "/**\n* Returns the length of the object.\n* @return length as a long value\n*/",
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:getOffset()": "/**\n* Returns the current offset value.\n* @return current offset as a long\n*/",
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run()": "/**** Executes readahead advice for a file if not canceled. Handles potential IO exceptions. */"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getCacheManipulator()": "/**\n* Retrieves the CacheManipulator instance.\n* @return CacheManipulator object\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)": "/**\n* Advises the kernel on file access patterns if supported.\n* @param identifier a unique identifier for the operation\n* @param fd the file descriptor to advise on\n* @param offset the starting byte offset for advice\n* @param len the length in bytes for advice\n* @param flags advice flags for the operation\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:syncFileRangeIfPossible(java.io.FileDescriptor,long,long,int)": "/**\n* Synchronizes a file range to storage if possible.\n* @param fd file descriptor for the target file\n* @param offset starting byte position\n* @param nbytes number of bytes to sync\n* @param flags options for synchronization\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable()": "/**\n* Checks if native code is available.\n* @return true if native code is loaded and available, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int)": "/**\n* Sets PMDK support state based on the provided state code.\n* @param stateCode integer representing the desired support state\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage()": "/**\n* Retrieves PMDK support state message with library path if available.\n* @return descriptive message regarding PMDK support status\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable()": "/**** \n* Checks if PMDK support is available.\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)": "/**\n* Changes file permissions; throws IOException on failure.\n* @param path file path to modify permissions\n* @param mode new permission mode as an integer\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer)": "/**\n* Unmaps the given MappedByteBuffer if supported.\n* @param buffer the MappedByteBuffer to unmap\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)": "/**\n* Retrieves name by ID from cache or fetches it if not cached.\n* @param domain specifies user or group context\n* @param id unique identifier for the name\n* @return the cached or freshly fetched name\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded()": "/**\n* Asserts that native code is loaded; throws IOException if not.\n* @throws IOException if native code is not available\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor)": "/**\n* Retrieves file status for a given file descriptor.\n* @param fd file descriptor to retrieve status for\n* @return Stat object containing file status information\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String)": "/**\n* Retrieves file statistics for the given path.\n* @param path the file path to retrieve stats for\n* @return Stat object containing file information\n* @throws IOException if the path is null or an error occurs\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)": "/**\n* Locks a direct ByteBuffer in memory.\n* @param buffer the ByteBuffer to lock\n* @param len length of the memory to lock\n* @throws IOException if buffer is non-direct or native code is not loaded\n*/"
    },
    "org.apache.hadoop.io.ArrayFile$Reader": {
        "org.apache.hadoop.io.ArrayFile$Reader:next(org.apache.hadoop.io.Writable)": "/**\n* Retrieves the next Writable value; returns null if not found.\n* @param value Writable to store the result\n* @return Writable object or null if no next value exists\n*/",
        "org.apache.hadoop.io.ArrayFile$Reader:seek(long)": "/**\n* Seeks to a specific position in a stream.\n* @param n position to seek to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)": "/**\n* Retrieves a Writable object based on the provided long key.\n* @param n the long key to retrieve the Writable\n* @param value the Writable object to populate\n* @return the populated Writable object\n*/",
        "org.apache.hadoop.io.ArrayFile$Reader:key()": "/**\n* Retrieves the current key value.\n* @return current long key value\n*/",
        "org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Reader for a specified file in the given FileSystem.\n* @param fs the FileSystem to read from\n* @param file the path of the file to read\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.util.Options$FSDataInputStreamOption": {
        "org.apache.hadoop.util.Options$FSDataInputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream)": "/**\n* Initializes FSDataInputStreamOption with a given FSDataInputStream.\n* @param value the FSDataInputStream to be wrapped\n*/",
        "org.apache.hadoop.util.Options$FSDataInputStreamOption:getValue()": "/**\n* Retrieves the FSDataInputStream value.\n* @return FSDataInputStream associated with the method\n*/"
    },
    "org.apache.hadoop.io.MapFile$Writer$ComparatorOption": {
        "org.apache.hadoop.io.MapFile$Writer$ComparatorOption:<init>(org.apache.hadoop.io.WritableComparator)": "/**\n* Initializes ComparatorOption with a WritableComparator instance.\n* @param value the WritableComparator to be used\n*/",
        "org.apache.hadoop.io.MapFile$Writer$ComparatorOption:getValue()": "/**\n* Retrieves the WritableComparator instance.\n* @return WritableComparator associated with this object\n*/"
    },
    "org.apache.hadoop.io.DoubleWritable": {
        "org.apache.hadoop.io.DoubleWritable:<init>()": "/**\n* Default constructor for DoubleWritable, initializes a new instance.\n*/",
        "org.apache.hadoop.io.DoubleWritable:set(double)": "/**\n* Sets the value of the instance variable.\n* @param value the new value to set\n*/",
        "org.apache.hadoop.io.DoubleWritable:readFields(java.io.DataInput)": "/**\n* Reads a double value from the input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DoubleWritable:write(java.io.DataOutput)": "/**\n* Writes a double value to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DoubleWritable:hashCode()": "/**\n* Computes the hash code based on the double value.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.io.DoubleWritable:compareTo(org.apache.hadoop.io.DoubleWritable)": "/**\n* Compares this DoubleWritable with another for order.\n* @param o the DoubleWritable to compare with\n* @return negative, zero, or positive if less than, equal to, or greater than the specified object\n*/",
        "org.apache.hadoop.io.DoubleWritable:toString()": "/**\n* Returns a string representation of the value as a double.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.io.DoubleWritable:<init>(double)": "/**\n* Initializes DoubleWritable with a specified value.\n* @param value the value to set for this instance\n*/"
    },
    "org.apache.hadoop.io.VersionedWritable": {
        "org.apache.hadoop.io.VersionedWritable:write(java.io.DataOutput)": "/**\n* Writes the object's version to the specified DataOutput stream.\n* @param out output stream to write data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput)": "/**** Reads version byte from input and checks for version mismatch. Throws exception if mismatched. */"
    },
    "org.apache.hadoop.io.VersionMismatchException": {
        "org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)": "/**\n* Constructs a VersionMismatchException with expected and found version bytes.\n* @param expectedVersionIn expected version byte\n* @param foundVersionIn found version byte\n*/",
        "org.apache.hadoop.io.VersionMismatchException:toString()": "/**\n* Returns a string describing the version mismatch.\n* @return descriptive message of expected and found versions\n*/"
    },
    "org.apache.hadoop.io.MapFile$Reader$ComparatorOption": {
        "org.apache.hadoop.io.MapFile$Reader$ComparatorOption:<init>(org.apache.hadoop.io.WritableComparator)": "/**\n* Initializes ComparatorOption with a WritableComparator.\n* @param value the WritableComparator to be set\n*/",
        "org.apache.hadoop.io.MapFile$Reader$ComparatorOption:getValue()": "/**\n* Retrieves the WritableComparator instance.\n* @return WritableComparator object\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight": {
        "org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight:accessRight()": "/**\n* Retrieves the current access right value.\n* @return integer representing the access right level\n*/"
    },
    "org.apache.hadoop.util.NativeCodeLoader": {
        "org.apache.hadoop.util.NativeCodeLoader:isNativeCodeLoaded()": "/**\n* Checks if the native code is loaded.\n* @return true if native code is loaded, false otherwise\n*/",
        "org.apache.hadoop.util.NativeCodeLoader:<init>()": "/**\n* Private constructor to prevent instantiation of NativeCodeLoader class.\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:getStateCode()": "/**\n* Retrieves the current state code.\n* @return the current state code as an integer\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:getMessage()": "/**\n* Returns a message based on the state code of PMDK support.\n* @return descriptive message regarding PMDK support status\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIOException": {
        "org.apache.hadoop.io.nativeio.NativeIOException:getErrorCode()": "/**\n* Retrieves the current error code.\n* @return the error code as a long value\n*/",
        "org.apache.hadoop.io.nativeio.NativeIOException:<init>(java.lang.String,org.apache.hadoop.io.nativeio.Errno)": "/**\n* Constructs a NativeIOException with a message and error number.\n* @param msg error message\n* @param errno error number indicating the type of error\n*/",
        "org.apache.hadoop.io.nativeio.NativeIOException:getErrno()": "/**\n* Retrieves the current error number.\n* @return Errno object representing the error state\n*/",
        "org.apache.hadoop.io.nativeio.NativeIOException:<init>(java.lang.String,int)": "/**\n* Constructs a NativeIOException with a message and error code.\n* @param msg descriptive error message\n* @param errorCode specific error code associated with the exception\n*/",
        "org.apache.hadoop.io.nativeio.NativeIOException:toString()": "/**\n* Returns a string representation of the error code and message.\n* @return formatted error code and message string\n*/"
    },
    "org.apache.hadoop.util.CleanerUtil": {
        "org.apache.hadoop.util.CleanerUtil:getCleaner()": "/**\n* Retrieves the BufferCleaner instance.\n* @return BufferCleaner instance\n*/",
        "org.apache.hadoop.util.CleanerUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the CleanerUtil class.\n*/",
        "org.apache.hadoop.util.CleanerUtil:newBufferCleaner(java.lang.Class,java.lang.invoke.MethodHandle)": "/**\n* Creates a BufferCleaner for unmapping direct ByteBuffers.\n* @param unmappableBufferClass expected buffer class type\n* @param unmapper method handle for unmapping\n* @return BufferCleaner function to unmap the buffer\n*/",
        "org.apache.hadoop.util.CleanerUtil:unmapHackImpl()": "/**\n* Unmaps ByteBuffers using Unsafe or Cleaner methods.\n* @return BufferCleaner or error message if unmapping fails\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CachedName": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CachedName:<init>(java.lang.String,long)": "/**\n* Constructs a CachedName with specified name and timestamp.\n* @param name the name to be cached\n* @param timestamp the time the name was cached\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO": {
        "org.apache.hadoop.io.nativeio.NativeIO:getOperatingSystemPageSize()": "/**\n* Retrieves the operating system's page size.\n* @return page size in bytes, defaults to 4096 if retrieval fails\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:stripDomain(java.lang.String)": "/**\n* Strips domain from the given name.\n* @param name input string potentially containing a domain\n* @return name without the domain part\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:getShareDeleteFileDescriptor(java.io.File,long)": "/**\n* Retrieves a FileDescriptor for a file with delete permissions.\n* @param f the file to access\n* @param seekOffset the position to seek within the file\n* @return FileDescriptor for the specified file\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:renameTo(java.io.File,java.io.File)": "/**\n* Renames a file from src to dst.\n* @param src source file to rename\n* @param dst destination file name\n* @throws IOException if renaming fails\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)": "/**\n* Copies a file from source to destination without buffering.\n* @param src source file to copy from\n* @param dst destination file to copy to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)": "/**\n* Links source file to destination, using native or fallback method.\n* @param src source file to link from\n* @param dst destination file for the link\n* @throws IOException if linking fails\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:isAvailable()": "/**\n* Checks if native code is available.\n* @return true if native code is loaded and available, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit()": "/**\n* Returns memory lock limit or 0 if native code is unavailable.\n* @return long memory lock limit\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)": "/**\n* Creates a FileOutputStream for writing; throws exception if file exists.\n* @param f file to create or overwrite\n* @param permissions new file permissions as an integer\n* @return FileOutputStream for the specified file\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized()": "/**\n* Ensures the initialization of UID cache with a timeout setting.\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor)": "/**\n* Retrieves the owner of a file descriptor.\n* @param fd the file descriptor to query\n* @return owner's username as a string\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$CachedUid": {
        "org.apache.hadoop.io.nativeio.NativeIO$CachedUid:<init>(java.lang.String,long)": "/**\n* Constructs a CachedUid with a username and timestamp.\n* @param username the user's name\n* @param timestamp the time associated with the user\n*/"
    },
    "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory": {
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:<init>(java.lang.String,java.lang.String)": "/**\n* Initializes SharedFileDescriptorFactory with a prefix and path.\n* @param prefix file prefix for shared descriptors\n* @param path directory path for file storage\n*/",
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:createDescriptor(java.lang.String,int)": "/**\n* Creates a FileInputStream for a descriptor file.\n* @param info descriptor information string\n* @param length length of the descriptor\n* @return FileInputStream for the created descriptor\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason()": "/**\n* Retrieves the reason for loading failure.\n* @return error message or null if loading is successful\n*/",
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])": "/**\n* Creates SharedFileDescriptorFactory with specified prefix and paths.\n* @param prefix file prefix for shared descriptors\n* @param paths array of directory paths for file storage\n* @return SharedFileDescriptorFactory instance\n* @throws IOException if creation fails or paths are empty\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)": "/**\n* Locks a memory region identified by the given identifier.\n* @param identifier unique identifier for the memory region\n* @param buffer     ByteBuffer representing the memory to lock\n* @param len       length of the memory region to lock\n* @throws IOException if an I/O error occurs during locking\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:<init>(long,long,boolean)": "/**\n* Constructs a PmemMappedRegion with specified address, length, and PMEM status.\n* @param address memory address of the region\n* @param length size of the memory region\n* @param isPmem indicates if the region is persistent memory\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:isPmem()": "/**\n* Checks if the object is persistent memory.\n* @return true if persistent memory, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:getAddress()": "/**\n* Retrieves the address value.\n* @return long representing the address\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:getLength()": "/**\n* Returns the length of the object.\n* @return length as a long value\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:isPmem(long,long)": "/**\n* Checks if the given memory address is in persistent memory.\n* @param address memory address to check\n* @param length size of the memory region\n* @return true if address is in persistent memory, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:mapBlock(java.lang.String,long,boolean)": "/**\n* Maps a block of persistent memory from the specified file.\n* @param path file path to map\n* @param length size of the memory block\n* @param isFileExist flag for file existence check\n* @return PmemMappedRegion object for the mapped memory\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:unmapBlock(long,long)": "/**\n* Unmaps a memory block at the given address.\n* @param address starting address of the block\n* @param length size of the block to unmap\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memCopy(byte[],long,boolean,long)": "/**\n* Copies memory from source byte array to destination address.\n* @param src source byte array\n* @param dest destination address in memory\n* @param isPmem indicates if the destination is persistent memory\n* @param length number of bytes to copy\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:getPmdkLibPath()": "/**\n* Retrieves the path to the PMDK library.\n* @return String representing the PMDK library path\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion)": "/**\n* Synchronizes memory for the given PmemMappedRegion.\n* @param region the memory-mapped region to synchronize\n*/"
    },
    "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException": {
        "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException:<init>(java.lang.Throwable)": "/**\n* Constructs an AlreadyExistsException with the specified cause.\n* @param cause the throwable that caused this exception\n*/",
        "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException:<init>(java.lang.String)": "/**\n* Constructs an AlreadyExistsException with the specified message.\n* @param msg the detail message\n*/"
    },
    "org.apache.hadoop.io.ArrayWritable": {
        "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class)": "/**\n* Constructs an ArrayWritable with specified value class.\n* @param valueClass class of Writable elements in the array\n*/",
        "org.apache.hadoop.io.ArrayWritable:toStrings()": "/**\n* Converts values to string array.\n* @return array of string representations of values\n*/",
        "org.apache.hadoop.io.ArrayWritable:toArray()": "/**\n* Converts the internal values to an array.\n* @return a copy of the values array\n*/",
        "org.apache.hadoop.io.ArrayWritable:write(java.io.DataOutput)": "/**\n* Serializes an array of values to the specified DataOutput stream.\n* @param out the output stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.ArrayWritable:toString()": "/**\n* Returns a string representation of the ArrayWritable object.\n* @return formatted string with valueClass and values\n*/",
        "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])": "/**\n* Constructs an ArrayWritable with specified value class and values array.\n* @param valueClass class of Writable elements in the array\n* @param values array of Writable elements\n*/",
        "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[])": "/**\n* Constructs ArrayWritable from an array of strings.\n* @param strings array of input strings\n*/",
        "org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput)": "/**\n* Reads and stores Writable values from input stream.\n* @param in DataInput stream to read values from\n*/"
    },
    "org.apache.hadoop.io.BoundedByteArrayOutputStream": {
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:resetBuffer(byte[],int,int)": "/**\n* Resets the buffer with specified offset and limit.\n* @param buf byte array to set as buffer\n* @param offset starting position in the buffer\n* @param limit maximum size of the buffer\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:write(int)": "/**\n* Writes a byte to the buffer until the limit is reached.\n* @param b byte to write\n* @throws IOException if an I/O error occurs or limit is exceeded\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:write(byte[],int,int)": "/**\n* Writes a byte array to the buffer with offset and length checks.\n* @param b byte array to write from, @param off start offset, @param len number of bytes to write\n* @throws IOException if an I/O error occurs or limits are exceeded\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:reset(int)": "/**\n* Resets the current pointer and limit of the buffer.\n* @param newlim new limit for the buffer\n* @throws IndexOutOfBoundsException if newlim exceeds buffer size\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:reset()": "/**\n* Resets the buffer pointer and limit to initial values.\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:getBuffer()": "/**\n* Retrieves the current buffer as a byte array.\n* @return byte array representing the buffer\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:size()": "/**\n* Calculates the size based on current pointer and start offset.\n* @return size as an integer value\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)": "/**\n* Initializes BoundedByteArrayOutputStream with buffer settings.\n* @param buf byte array to set as buffer\n* @param offset starting position in the buffer\n* @param limit maximum size of the buffer\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)": "/**\n* Constructs BoundedByteArrayOutputStream with specified capacity and limit.\n* @param capacity initial buffer size\n* @param limit maximum size of the buffer\n*/",
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int)": "/**\n* Initializes BoundedByteArrayOutputStream with specified capacity and limit.\n* @param capacity initial buffer size\n*/"
    },
    "org.apache.hadoop.io.DataOutputBuffer$Buffer": {
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:<init>()": "/**\n* Initializes a new Buffer instance.\n*/",
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:<init>(int)": "/**\n* Initializes a Buffer with the specified size.\n* @param size the capacity of the buffer\n*/",
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:write(java.io.DataInput,int)": "/**\n* Writes data from input stream to buffer, expanding if necessary.\n* @param in input data stream\n* @param len number of bytes to read from the stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:getData()": "/**\n* Retrieves the byte array data from the buffer.\n* @return byte array containing the buffer data\n*/",
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:getLength()": "/**\n* Returns the current length count.\n* @return the length as an integer\n*/",
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int)": "/**\n* Updates count and returns the previous value.\n* @param newCount new count value (must be non-negative and within buffer length)\n* @return previous count value\n*/"
    },
    "org.apache.hadoop.io.TwoDArrayWritable": {
        "org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class)": "/**\n* Constructs a TwoDArrayWritable with the specified value class.\n* @param valueClass class type for array elements\n*/",
        "org.apache.hadoop.io.TwoDArrayWritable:toArray()": "/**\n* Converts a 2D array of values to an Object array.\n* @return 2D Object array representation of values\n*/",
        "org.apache.hadoop.io.TwoDArrayWritable:readFields(java.io.DataInput)": "/**\n* Reads matrix dimensions and values from input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.TwoDArrayWritable:write(java.io.DataOutput)": "/**\n* Serializes a 2D array of objects to the given DataOutput.\n* @param out the DataOutput to write data to\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])": "/**\n* Constructs a TwoDArrayWritable with specified value class and array values.\n* @param valueClass class type for array elements\n* @param values 2D array of Writable elements\n*/"
    },
    "org.apache.hadoop.io.NullWritable$Comparator": {
        "org.apache.hadoop.io.NullWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays but always returns 0 due to zero-length assertions.\n* @param b1 first byte array\n* @param s1 starting index for b1\n* @param l1 length of b1 (must be 0)\n* @param b2 second byte array\n* @param s2 starting index for b2\n* @param l2 length of b2 (must be 0)\n* @return 0 indicating both arrays are considered equal\n*/",
        "org.apache.hadoop.io.NullWritable$Comparator:<init>()": "/**\n* Initializes a Comparator for NullWritable keys.\n*/"
    },
    "org.apache.hadoop.io.EnumSetWritable": {
        "org.apache.hadoop.io.EnumSetWritable:<init>()": "/**\n* Constructs an empty EnumSetWritable instance.\n*/",
        "org.apache.hadoop.io.EnumSetWritable:iterator()": "/**\n* Returns an iterator over the elements in the collection.\n* @return Iterator for the collection's elements\n*/",
        "org.apache.hadoop.io.EnumSetWritable:size()": "/**\n* Returns the number of elements in the collection.\n* @return size of the collection as an integer\n*/",
        "org.apache.hadoop.io.EnumSetWritable:set(java.util.EnumSet,java.lang.Class)": "/**\n* Sets the EnumSet and element type, validating inputs.\n* @param value EnumSet of elements to set\n* @param elementType Class of the enum type\n*/",
        "org.apache.hadoop.io.EnumSetWritable:equals(java.lang.Object)": "/**\n* Compares this object to another for equality.\n* @param o object to compare with; must not be null\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.EnumSetWritable:hashCode()": "/**\n* Computes hash code for the object based on its value.\n* @return hash code as an integer, or 0 if value is null\n*/",
        "org.apache.hadoop.io.EnumSetWritable:toString()": "/**\n* Returns a string representation of the object or \"(null)\" if value is null.\n* @return string representation of value\n*/",
        "org.apache.hadoop.io.EnumSetWritable:add(java.lang.Object)": "/**\n* Adds an element to the EnumSet.\n* @param e element to be added\n* @return true if the element was added, false if it was already present\n*/",
        "org.apache.hadoop.io.EnumSetWritable:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.io.EnumSetWritable:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration to be set\n*/",
        "org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)": "/**\n* Constructs EnumSetWritable by initializing with EnumSet and element type.\n* @param value EnumSet of elements to set\n* @param elementType Class of the enum type\n*/",
        "org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet)": "/**\n* Constructs EnumSetWritable from given EnumSet.\n* @param value EnumSet of elements to set\n*/",
        "org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput to populate EnumSet.\n* @param in DataInput source for reading fields\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput)": "/**\n* Serializes an object array to DataOutput, handling null and empty cases.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.ObjectWritable": {
        "org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.conf.Configuration)": "/**\n* Reads an object from input stream with given configuration.\n* @param in DataInput stream to read from\n* @param conf Configuration settings for reading\n* @return The read object\n*/",
        "org.apache.hadoop.io.ObjectWritable:<init>()": "/**\n* Default constructor for ObjectWritable class.\n*/",
        "org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)": "/**\n* Sets the instance and its declared class.\n* @param instance object to set, determines declared class\n*/",
        "org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Class,java.lang.Object)": "/**\n* Initializes ObjectWritable with a class and its instance.\n* @param declaredClass the class type of the instance\n* @param instance the object instance of the declared class\n*/",
        "org.apache.hadoop.io.ObjectWritable:toString()": "/**\n* Returns a string representation of the object with class and value.\n* @return formatted string of class and instance values\n*/",
        "org.apache.hadoop.io.ObjectWritable:getStaticProtobufMethod(java.lang.Class,java.lang.String,java.lang.Class[])": "/**\n* Retrieves a static method from a protobuf class.\n* @param declaredClass the class containing the method\n* @param method the name of the method to retrieve\n* @param args the parameter types of the method\n* @return Method object representing the specified method\n*/",
        "org.apache.hadoop.io.ObjectWritable:getDeclaredClass()": "/**\n* Returns the declared class of the current instance.\n* @return Class object representing the declared class\n*/",
        "org.apache.hadoop.io.ObjectWritable:get()": "/**\n* Returns the current instance object.\n* @return instance of type Object\n*/",
        "org.apache.hadoop.io.ObjectWritable:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing current settings\n*/",
        "org.apache.hadoop.io.ObjectWritable:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration to be set\n*/",
        "org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object)": "/**\n* Initializes ObjectWritable with the given instance.\n* @param instance object to set, determines declared class\n*/",
        "org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)": "/**\n* Instantiates a protobuf Message from input stream or DataInput.\n* @param protoClass the protobuf class type\n* @param dataIn input data source\n* @return instantiated Message object\n* @throws IOException if reading fails or size is invalid\n*/",
        "org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Loads a class by name using configuration or default method.\n* @param conf configuration object, may be null\n* @param className fully qualified class name\n* @return Class object or RuntimeException if not found\n*/",
        "org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Writes an object to a DataOutput stream.\n* @param out DataOutput stream to write to\n* @param instance object to write\n* @param declaredClass class of the object\n* @param conf configuration settings\n* @param allowCompactArrays flag for compact array handling\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Writes an object to a DataOutput stream without compact arrays.\n* @param out DataOutput stream to write to\n* @param instance object to write\n* @param declaredClass class of the object\n* @param conf configuration settings\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)": "/**** Reads an object from input stream based on class name and stores it in ObjectWritable. \n* @param in DataInput stream to read from \n* @param objectWritable stores the declared class and instance \n* @param conf Configuration settings for loading classes \n* @return the read object \n* @throws IOException if reading fails \n*/",
        "org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput)": "/**\n* Writes an object to a DataOutput stream.\n* @param out DataOutput stream to write to\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream into the current object.\n* @param in DataInput stream to read from\n* @throws IOException if reading fails\n*/"
    },
    "org.apache.hadoop.io.EnumSetWritable$1": {
        "org.apache.hadoop.io.EnumSetWritable$1:<init>()": "/**\n* Constructs an empty EnumSetWritable instance.\n*/"
    },
    "org.apache.hadoop.io.SortedMapWritable": {
        "org.apache.hadoop.io.SortedMapWritable:firstKey()": "/**\n* Retrieves the first key from the instance.\n* @return the first key of type K\n*/",
        "org.apache.hadoop.io.SortedMapWritable:lastKey()": "/**\n* Retrieves the last key from the instance.\n* @return the last key of type K\n*/",
        "org.apache.hadoop.io.SortedMapWritable:clear()": "/**\n* Clears all elements from the instance.\n*/",
        "org.apache.hadoop.io.SortedMapWritable:containsKey(java.lang.Object)": "/**\n* Checks if the specified key exists in the instance.\n* @param key the key to check for existence\n* @return true if the key is present, false otherwise\n*/",
        "org.apache.hadoop.io.SortedMapWritable:containsValue(java.lang.Object)": "/**\n* Checks if the specified value is present in the collection.\n* @param value the value to search for\n* @return true if the value exists, false otherwise\n*/",
        "org.apache.hadoop.io.SortedMapWritable:entrySet()": "/**\n* Returns a set of entries from the instance map.\n* @return Set of map entries (key-value pairs)\n*/",
        "org.apache.hadoop.io.SortedMapWritable:get(java.lang.Object)": "/**\n* Retrieves a Writable object associated with the specified key.\n* @param key the key used to retrieve the Writable\n* @return Writable object or null if not found\n*/",
        "org.apache.hadoop.io.SortedMapWritable:isEmpty()": "/**\n* Checks if the instance is empty.\n* @return true if empty, false otherwise\n*/",
        "org.apache.hadoop.io.SortedMapWritable:keySet()": "/**\n* Returns a set of keys from the underlying instance.\n* @return Set of keys present in the instance\n*/",
        "org.apache.hadoop.io.SortedMapWritable:putAll(java.util.Map)": "/**\n* Inserts all entries from the specified map into this map.\n* @param t map containing entries to be added\n*/",
        "org.apache.hadoop.io.SortedMapWritable:remove(java.lang.Object)": "/**\n* Removes the specified key from the instance.\n* @param key the key to be removed\n* @return the value associated with the key, or null if not found\n*/",
        "org.apache.hadoop.io.SortedMapWritable:size()": "/**\n* Returns the number of elements in the instance.\n* @return the size of the instance as an integer\n*/",
        "org.apache.hadoop.io.SortedMapWritable:values()": "/**\n* Returns a collection of writable values from the instance.\n* @return Collection of Writable objects\n*/",
        "org.apache.hadoop.io.SortedMapWritable:hashCode()": "/**\n* Returns the hash code of the instance object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.io.SortedMapWritable:tailMap(java.lang.Object)": "/**\n* Returns a view of the portion of the map whose keys are greater than or equal to fromKey.\n* @param fromKey the key to start the view from\n* @return SortedMap containing entries from fromKey to the end\n*/",
        "org.apache.hadoop.io.SortedMapWritable:headMap(java.lang.Object)": "/**\n* Returns a view of the portion of the map whose keys are less than toKey.\n* @param toKey the upper key (exclusive) for the view\n* @return SortedMap containing keys less than toKey\n*/",
        "org.apache.hadoop.io.SortedMapWritable:subMap(java.lang.Object,java.lang.Object)": "/**\n* Returns a view of the portion of this map between fromKey and toKey.\n* @param fromKey starting key (inclusive)\n* @param toKey ending key (exclusive)\n* @return SortedMap containing the key-value pairs in the specified range\n*/",
        "org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object)": "/**\n* Compares this SortedMapWritable to another object for equality.\n* @param obj object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.SortedMapWritable:<init>()": "/**\n* Initializes a SortedMapWritable instance with a TreeMap.\n*/",
        "org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput)": "/**\n* Writes map size and key/value pairs to DataOutput.\n* @param out DataOutput stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable)": "/**\n* Copies data from another SortedMapWritable instance.\n* @param other the SortedMapWritable to copy from\n*/",
        "org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput)": "/**\n* Reads map entries from input stream and populates the instance map.\n* @param in input stream for reading map data\n* @throws IOException if reading fails\n*/"
    },
    "org.apache.hadoop.io.DataInputBuffer": {
        "org.apache.hadoop.io.DataInputBuffer:<init>(org.apache.hadoop.io.DataInputBuffer$Buffer)": "/**\n* Constructs DataInputBuffer with the specified buffer.\n* @param buffer the Buffer object to initialize the DataInputBuffer\n*/",
        "org.apache.hadoop.io.DataInputBuffer:<init>()": "/**\n* Initializes a new DataInputBuffer with a default Buffer instance.\n*/",
        "org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)": "/**\n* Resets the buffer with specified input data.\n* @param input byte array to set as buffer\n* @param length number of bytes to include from the start\n*/",
        "org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)": "/**\n* Resets the buffer state using the specified byte array segment.\n* @param input byte array to set as buffer\n* @param start starting index for the buffer\n* @param length number of bytes to include from the start\n*/",
        "org.apache.hadoop.io.DataInputBuffer:getData()": "/**\n* Retrieves byte array data from the buffer.\n* @return byte array containing the buffer data\n*/",
        "org.apache.hadoop.io.DataInputBuffer:getPosition()": "/**\n* Retrieves the current position from the buffer.\n* @return the current position as an integer\n*/",
        "org.apache.hadoop.io.DataInputBuffer:getLength()": "/**\n* Returns the length of the buffer.\n* @return current length count as an integer\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ErasureCodeConstants": {
        "org.apache.hadoop.io.erasurecode.ErasureCodeConstants:<init>()": "/**\n* Private constructor to prevent instantiation of the ErasureCodeConstants class.\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ECSchema": {
        "org.apache.hadoop.io.erasurecode.ECSchema:extractIntOption(java.lang.String,java.util.Map)": "/**\n* Extracts and validates an integer option from a map.\n* @param optionKey the key to search in options map\n* @param options   map containing option key-value pairs\n* @return the integer value or -1 if not found\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int,java.util.Map)": "/**\n* Initializes ECSchema with codec name, data, and parity unit counts.\n* @param codecName name of the codec\n* @param numDataUnits count of data units\n* @param numParityUnits count of parity units\n* @param extraOptions additional configuration options\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:toString()": "/**\n* Returns a string representation of the ECSchema with its properties.\n* @return formatted ECSchema details including codec and options\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:equals(java.lang.Object)": "/**\n* Compares this ECSchema object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:hashCode()": "/**\n* Computes the hash code for the object based on its fields.\n* @return integer hash code value\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:getNumDataUnits()": "/**\n* Retrieves the number of data units.\n* @return the count of data units as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:getNumParityUnits()": "/**\n* Retrieves the number of parity units.\n* @return int representing the count of parity units\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:getCodecName()": "/**\n* Retrieves the name of the codec.\n* @return codecName as a String\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map)": "/**\n* Constructs an ECSchema with options, validating required fields and extracting values.\n* @param allOptions map of schema options; must contain codec and unit settings\n*/",
        "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)": "/**\n* Constructs ECSchema with codec name and unit counts.\n* @param codecName name of the codec\n* @param numDataUnits count of data units\n* @param numParityUnits count of parity units\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ErasureCodecOptions": {
        "org.apache.hadoop.io.erasurecode.ErasureCodecOptions:<init>(org.apache.hadoop.io.erasurecode.ECSchema)": "/**\n* Constructs ErasureCodecOptions with the specified ECSchema.\n* @param schema the ECSchema to be used for configuration\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCodecOptions:getSchema()": "/**\n* Retrieves the current ECSchema instance.\n* @return ECSchema object representing the current schema\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ErasureCoderOptions": {
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int,boolean,boolean)": "/**\n* Initializes ErasureCoderOptions with data and parity unit counts.\n* @param numDataUnits number of data units\n* @param numParityUnits number of parity units\n* @param allowChangeInputs flag to allow input changes\n* @param allowVerboseDump flag to enable verbose output\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumDataUnits()": "/**\n* Retrieves the number of data units.\n* @return the count of data units as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumParityUnits()": "/**\n* Retrieves the number of parity units.\n* @return the count of parity units\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumAllUnits()": "/**\n* Retrieves the total number of units.\n* @return total count of all units\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:allowChangeInputs()": "/**\n* Checks if input changes are allowed.\n* @return true if changes are permitted, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:allowVerboseDump()": "/**\n* Checks if verbose dump is allowed.\n* @return true if verbose dump is enabled, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)": "/**\n* Constructs ErasureCoderOptions with specified data and parity units.\n* @param numDataUnits number of data units\n* @param numParityUnits number of parity units\n*/"
    },
    "org.apache.hadoop.io.erasurecode.codec.ErasureCodec": {
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getSchema()": "/**\n* Retrieves the current ECSchema instance.\n* @return the ECSchema object\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getCodecOptions()": "/**\n* Retrieves the current ErasureCodecOptions instance.\n* @return ErasureCodecOptions object representing codec settings\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getCoderOptions()": "/**\n* Retrieves the current ErasureCoderOptions.\n* @return ErasureCoderOptions object representing the coder options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCoderOptions(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Sets the ErasureCoderOptions for this instance.\n* @param options the ErasureCoderOptions to be set\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Sets codec options and updates the schema.\n* @param options ErasureCodecOptions to configure the codec\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Initializes ErasureCodec with configuration and options.\n* @param conf configuration settings\n* @param options erasure codec options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName()": "/**\n* Retrieves the codec name from the schema.\n* @return codec name as a String\n*/",
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper()": "/**\n* Creates a new BlockGrouper with the current ECSchema.\n* @return a BlockGrouper instance initialized with the schema\n*/"
    },
    "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper": {
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:setSchema(org.apache.hadoop.io.erasurecode.ECSchema)": "/**\n* Sets the ECSchema for the current object.\n* @param schema the ECSchema to be set\n*/",
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks()": "/**\n* Calculates required number of data blocks.\n* @return count of data units as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks()": "/**\n* Calculates the required number of parity blocks.\n* @return int representing the number of parity blocks needed\n*/",
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])": "/**\n* Creates an ECBlockGroup from data and parity blocks.\n* @param dataBlocks array of data blocks\n* @param parityBlocks array of parity blocks\n* @return ECBlockGroup instance\n*/",
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Checks if there are recoverable blocks in the group.\n* @param blockGroup group of EC blocks to evaluate\n* @return true if recoverable blocks exist, false otherwise\n*/"
    },
    "org.apache.hadoop.io.erasurecode.CodecRegistry": {
        "org.apache.hadoop.io.erasurecode.CodecRegistry:updateCoders(java.lang.Iterable)": "/**\n* Updates coder factories in the coder map and refreshes the coder name mappings.\n* @param coderFactories iterable collection of RawErasureCoderFactory instances\n*/",
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderNames(java.lang.String)": "/**\n* Retrieves coder names associated with the specified codec.\n* @param codecName the name of the codec\n* @return array of coder names or null if not found\n*/",
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoders(java.lang.String)": "/**\n* Retrieves a list of RawErasureCoderFactory by codec name.\n* @param codecName the name of the codec\n* @return list of RawErasureCoderFactory or null if not found\n*/",
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCodecNames()": "/**\n* Retrieves a set of codec names from the coder map.\n* @return Set of codec names as strings\n*/",
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getInstance()": "/**\n* Returns the singleton instance of CodecRegistry.\n* @return CodecRegistry instance\n*/",
        "org.apache.hadoop.io.erasurecode.CodecRegistry:<init>()": "/**\n* Initializes CodecRegistry and loads coder factories.\n*/",
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)": "/**\n* Retrieves a RawErasureCoderFactory by codec and coder name.\n* @param codecName the codec name\n* @param coderName the coder name\n* @return RawErasureCoderFactory or null if not found\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ECBlockGroup": {
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])": "/**\n* Initializes ECBlockGroup with data and parity blocks.\n* @param dataBlocks array of data blocks\n* @param parityBlocks array of parity blocks\n*/",
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:getParityBlocks()": "/**\n* Retrieves an array of parity blocks.\n* @return array of ECBlock objects representing parity blocks\n*/",
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:getDataBlocks()": "/**\n* Retrieves an array of ECBlock objects.\n* @return array of ECBlock instances\n*/",
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount()": "/**\n* Counts the number of erased data and parity blocks.\n* @return total count of erased blocks\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ECBlock": {
        "org.apache.hadoop.io.erasurecode.ECBlock:isErased()": "/**\n* Checks if the object is marked as erased.\n* @return true if erased, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.ECBlock:<init>(boolean,boolean)": "/**\n* Constructs an ECBlock with specified parity and erased status.\n* @param isParity indicates if the block is a parity block\n* @param isErased indicates if the block is marked as erased\n*/",
        "org.apache.hadoop.io.erasurecode.ECBlock:<init>()": "/**\n* Constructs an ECBlock with default non-parity and non-erased status.\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ErasureCodeNative": {
        "org.apache.hadoop.io.erasurecode.ErasureCodeNative:<init>()": "/**\n* Private constructor for ErasureCodeNative class, preventing instantiation.\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCodeNative:checkNativeCodeLoaded()": "/**\n* Checks if native code is loaded; throws exception if loading failed.\n*/",
        "org.apache.hadoop.io.erasurecode.ErasureCodeNative:getLoadingFailureReason()": "/**\n* Retrieves the reason for loading failure.\n* @return String containing the loading failure reason\n*/"
    },
    "org.apache.hadoop.io.erasurecode.CodecUtil": {
        "org.apache.hadoop.io.erasurecode.CodecUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the CodecUtil class.\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String)": "/**\n* Checks if a codec is available by its name.\n* @param codecName the name of the codec\n* @return true if codec exists, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)": "/**\n* Creates a RawErasureCoderFactory by codec and coder names.\n* @param coderName the coder name\n* @param codecName the codec name\n* @return RawErasureCoderFactory or null if not found\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Creates an ErasureCodec instance from the specified class name and options.\n* @param conf configuration settings\n* @param codecClassName fully qualified codec class name\n* @param options codec options\n* @return ErasureCodec instance\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves codec class name based on specified codec type.\n* @param conf configuration object\n* @param codec codec type name\n* @return codec class name as a String\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves raw coder names for a specified codec from configuration.\n* @param conf configuration object\n* @param codecName name of the codec\n* @return array of raw coder names or default values\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/****\n* Creates an ErasureEncoder instance.\n* @param conf configuration settings\n* @param options codec options\n* @return ErasureEncoder object\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Creates an ErasureDecoder instance.\n* @param conf configuration settings\n* @param options codec options\n* @return ErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder with fallback options.\n* @param conf configuration settings\n* @param codecName name of the codec\n* @param coderOptions options for the erasure coder\n* @return RawErasureEncoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder with fallback options.\n* @param conf configuration settings\n* @param codecName name of the codec\n* @param coderOptions options for the erasure coder\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder with specified configuration and codec.\n* @param conf configuration settings\n* @param codec codec name\n* @param coderOptions options for erasure coding\n* @return RawErasureEncoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder with specified configuration and codec options.\n* @param conf configuration settings\n* @param codec name of the codec\n* @param coderOptions options for the erasure coder\n* @return RawErasureDecoder instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.ECChunk": {
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(java.nio.ByteBuffer)": "/**\n* Initializes ECChunk with the provided ByteBuffer.\n* @param buffer ByteBuffer containing chunk data\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(java.nio.ByteBuffer,int,int)": "/**\n* Creates an ECChunk from a ByteBuffer with specified offset and length.\n* @param buffer source ByteBuffer\n* @param offset starting position in buffer\n* @param len length of the chunk\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(byte[])": "/**\n* Initializes ECChunk with the provided byte buffer.\n* @param buffer byte array to wrap in a ByteBuffer\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(byte[],int,int)": "/**\n* Initializes an ECChunk with a byte buffer starting at offset with specified length.\n* @param buffer byte array containing chunk data\n* @param offset starting position in the buffer\n* @param len length of the chunk to wrap\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:getBuffer()": "/**\n* Retrieves the current chunk buffer.\n* @return ByteBuffer containing the chunk data\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:toBytesArray()": "/**\n* Converts the remaining buffer data to a byte array.\n* @return byte array containing buffer data\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:isAllZero()": "/**\n* Checks if the state is all zeros.\n* @return true if all values are zero, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Converts ECChunk array to ByteBuffer array.\n* @param chunks array of ECChunk objects\n* @return array of ByteBuffer corresponding to chunks\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep": {
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder)": "/**\n* Initializes ErasureDecodingStep with input, output blocks, erased indexes, and decoder.\n* @param inputBlocks array of input ECBlocks\n* @param erasedIndexes indices of erased blocks\n* @param outputBlocks array for output ECBlocks\n* @param rawDecoder instance of RawErasureDecoder\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:finish()": "/**\n* Finalizes the decoder if necessary.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:getInputBlocks()": "/**\n* Retrieves an array of input blocks.\n* @return array of ECBlock objects\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:getOutputBlocks()": "/**\n* Returns an array of output blocks.\n* @return ECBlock[] array of output blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Performs coding by decoding input ECChunks into output ECChunks.\n* @param inputChunks array of ECChunks to decode\n* @param outputChunks array to store decoded ECChunks\n* @throws IOException if decoding fails\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep": {
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)": "/**\n* Constructs an ErasureEncodingStep with input/output blocks and a raw encoder.\n* @param inputBlocks array of input ECBlock objects\n* @param outputBlocks array of output ECBlock objects\n* @param rawEncoder instance of RawErasureEncoder\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:finish()": "/**\n* Finalizes the operation; no action is performed.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:getInputBlocks()": "/**\n* Returns an array of input blocks.\n* @return array of ECBlock objects\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:getOutputBlocks()": "/**\n* Retrieves the array of output blocks.\n* @return array of ECBlock objects\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Performs encoding on input ECChunk array and stores results in output array.\n* @param inputChunks array of ECChunk objects to encode\n* @param outputChunks array to store encoded ECChunk results\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:release()": "/**\n* Releases resources; no action required in this implementation.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes RawErasureEncoder with specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:preferDirectBuffer()": "/**\n* Indicates if direct buffer preference is enabled.\n* @return false, indicating direct buffers are not preferred\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits()": "/**\n* Retrieves the number of data units from coder options.\n* @return count of data units as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits()": "/**\n* Retrieves the number of parity units from coder options.\n* @return count of parity units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits()": "/**\n* Retrieves the total number of units from coder options.\n* @return total count of all units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs()": "/**\n* Checks if input changes are allowed.\n* @return true if changes are permitted, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump()": "/**\n* Checks if verbose dump is enabled.\n* @return true if verbose dump is allowed, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])": "/**\n* Encodes input byte arrays into output arrays.\n* @param inputs array of input byte arrays\n* @param outputs array of output byte arrays\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Encodes input ByteBuffers and updates their positions.\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Encodes ECChunk arrays to ByteBuffers and updates output array.\n* @param inputs array of ECChunk objects to encode\n* @param outputs array to store encoded ByteBuffers\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder": {
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Calculates the erasure coding step for a given block group.\n* @param blockGroup the ECBlockGroup to process\n* @return ErasureCodingStep for the specified block group\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getNumDataUnits()": "/**\n* Retrieves the number of data units.\n* @return the total count of data units\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getNumParityUnits()": "/**\n* Returns the number of parity units.\n* @return the count of parity units as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOptions()": "/**\n* Retrieves the current ErasureCoderOptions.\n* @return ErasureCoderOptions object containing configuration settings\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:preferDirectBuffer()": "/**\n* Indicates if direct buffer preference is enabled.\n* @return false, indicating direct buffers are not preferred\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:release()": "/**\n* Releases resources; default implementation does nothing.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Retrieves output blocks from the given block group.\n* @param blockGroup the ECBlockGroup to fetch parity blocks from\n* @return array of ECBlock objects representing output blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Retrieves ECBlock array from the specified block group.\n* @param blockGroup the ECBlockGroup containing data blocks\n* @return array of ECBlock instances\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs ErasureEncoder with specified options.\n* @param options configuration settings for the encoder\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder": {
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Calculates the erasure coding step for a given block group.\n* @param blockGroup the ECBlockGroup to process\n* @return ErasureCodingStep for the specified block group\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumDataUnits()": "/**\n* Retrieves the number of data units.\n* @return int representing the count of data units\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumParityUnits()": "/**\n* Returns the number of parity units.\n* @return int representing the count of parity units\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOptions()": "/**\n* Retrieves the current ErasureCoderOptions.\n* @return ErasureCoderOptions object containing configuration settings\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:preferDirectBuffer()": "/**\n* Indicates whether to prefer direct byte buffers.\n* @return false, indicating direct buffers are not preferred\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:release()": "/**\n* Releases resources; does nothing by default.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[])": "/**\n* Counts the number of erased blocks in the input array.\n* @param inputBlocks array of ECBlock objects\n* @return count of erased blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Combines data and parity blocks into a single array.\n* @param blockGroup the ECBlockGroup containing data and parity blocks\n* @return array of combined ECBlock instances\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Calculates total erased blocks in the given block group.\n* @param blockGroup ECBlockGroup containing blocks\n* @return total count of erased blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[])": "/**\n* Retrieves indexes of erased blocks from the input array.\n* @param inputBlocks array of ECBlock objects\n* @return array of indexes of erased blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Retrieves erased data and parity blocks from the given block group.\n* @param blockGroup ECBlockGroup containing blocks\n* @return array of erased ECBlock instances\n*/",
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes ErasureDecoder with specified options.\n* @param options configuration for data and parity units\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep": {
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])": "/**\n* Initializes HHErasureCodingStep with input and output EC blocks.\n* @param inputBlocks array of input EC blocks\n* @param outputBlocks array of output EC blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:finish()": "/**\n* Finalizes encoder/decoder resources if needed.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getInputBlocks()": "/**\n* Retrieves the array of input blocks.\n* @return array of ECBlock objects\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getOutputBlocks()": "/**\n* Retrieves an array of output blocks.\n* @return array of ECBlock objects representing output blocks\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getSubPacketSize()": "/**\n* Retrieves the size of the sub-packet.\n* @return int representing the sub-packet size\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:release()": "/**\n* Releases resources; does nothing in the current implementation.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes RawErasureDecoder with specified coding options.\n* @param coderOptions configuration for erasure coding\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:preferDirectBuffer()": "/**\n* Indicates preference for direct buffer usage.\n* @return false, indicating direct buffer is not preferred\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits()": "/**\n* Returns the number of data units from coder options.\n* @return count of data units as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits()": "/**\n* Returns the number of parity units from coder options.\n* @return count of parity units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits()": "/**\n* Returns the total number of units from coder options.\n* @return total count of all units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs()": "/**\n* Checks if input changes are permitted.\n* @return true if changes are allowed, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump()": "/**\n* Checks if verbose dump is enabled.\n* @return true if verbose dump is allowed, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])": "/**\n* Decodes input buffers and updates their positions based on erased data.\n* @param inputs array of input ByteBuffers\n* @param erasedIndexes indices of erased data\n* @param outputs array of output ByteBuffers\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])": "/**\n* Decodes input byte arrays using erased indexes to produce output arrays.\n* @param inputs input byte arrays\n* @param erasedIndexes indexes of erased inputs\n* @param outputs output byte arrays\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Decodes ECChunks using erased indexes and updates output chunks.\n* @param inputs array of ECChunk to decode\n* @param erasedIndexes indices of erased data\n* @param outputs array to store decoded ECChunks\n* @throws IOException if decoding fails\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.util.HHUtil": {
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:initPiggyBackIndexWithoutPBVec(int,int)": "/**\n* Initializes piggy back index based on data and parity units.\n* @param numDataUnits total data units\n* @param numParityUnits total parity units\n* @return array of piggy back indices\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:initPiggyBackFullIndexVec(int,int[])": "/**\n* Initializes an index vector for piggyback data units.\n* @param numDataUnits total number of data units\n* @param piggyBackIndex array of piggyback indices\n* @return array of filled piggyback full indices\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:allocateByteBuffer(boolean,int)": "/**\n* Allocates a ByteBuffer based on buffer type and size.\n* @param useDirectBuffer true for direct buffer, false for heap buffer\n* @param bufSize size of the buffer in bytes\n* @return allocated ByteBuffer\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the HHUtil class.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:cloneBufferData(java.nio.ByteBuffer)": "/**\n* Clones data from a source ByteBuffer to a new ByteBuffer.\n* @param srcBuffer source ByteBuffer to clone from\n* @return cloned ByteBuffer containing the same data\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[])": "/**\n* Returns the first non-null input from the array.\n* @param inputs array of inputs to check\n* @return first non-null input or throws exception if all are null\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)": "/**\n* Generates piggyback data for decoding based on input and output buffers.\n* @param inputs 2D array of input ByteBuffers\n* @param outputs 2D array of output ByteBuffers\n* @param pbParityIndex index for parity in piggyback\n* @param numDataUnits count of data units\n* @param numParityUnits count of parity units\n* @param pbIndex index for piggyback\n* @return ByteBuffer containing the generated piggyback data\n*/",
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)": "/**\n* Generates piggyback ByteBuffers from input buffers using an encoder.\n* @param inputs array of input ByteBuffers\n* @param piggyBackIndex indices for piggyback creation\n* @param numParityUnits number of parity units to generate\n* @param pgIndex index of the desired output buffer\n* @param encoder used for encoding input buffers\n* @return array of generated piggyback ByteBuffers\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep": {
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeByPiggyBack(java.nio.ByteBuffer[],java.nio.ByteBuffer,java.nio.ByteBuffer,int)": "/**\n* Decodes data using piggyback information to recover erased data.\n* @param inputs array of ByteBuffers containing input data\n* @param outputs ByteBuffer for storing decoded output\n* @param piggyBack ByteBuffer with recovery data\n* @param erasedLocationToFix index of the data to recover\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeByPiggyBack(byte[][],int[],byte[],int,byte[],int,int)": "/**\n* Decodes data using piggybacking technique.\n* @param inputs input byte arrays for decoding\n* @param inputOffsets offsets for each input array\n* @param outputs output byte array for results\n* @param outOffset offset in the output array\n* @param piggyBack byte array for intermediate results\n* @param erasedLocationToFix index of the erased data\n* @param bufSize size of the buffer to process\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)": "/**\n* Constructs HHXORErasureDecodingStep with input/output blocks and decoder/encoder.\n* @param inputBlocks array of input EC blocks\n* @param erasedIndexes indices of erased blocks\n* @param outputBlocks array of output EC blocks\n* @param rawDecoder decoder for raw erasure coding\n* @param rawEncoder encoder for raw erasure coding\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)": "/**\n* Decodes a single subpacket, handling erased data and piggybacking.\n* @param inputs input ByteBuffer arrays\n* @param outputs output ByteBuffer arrays\n* @param erasedLocationToFix index of erased data to recover\n* @param bufSize size of the buffer for processing\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)": "/**\n* Decodes data and parity units from input buffers.\n* @param inputs input ByteBuffers for decoding\n* @param outputs output ByteBuffers for results\n* @param erasedLocationToFix indices of erased data\n* @param bufSize size of the buffer for processing\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Performs coding on input buffers, decoding missing data or parity units.\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Processes ECChunks by converting to ByteBuffers and coding them.\n* @param inputChunks array of input ECChunk objects\n* @param outputChunks array for storing processed ECChunk objects\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep": {
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:encodeWithPiggyBacks(java.nio.ByteBuffer[],java.nio.ByteBuffer[][],int,boolean)": "/**\n* Encodes data with piggybacked buffers for parity units.\n* @param piggyBacks input ByteBuffers for encoding\n* @param outputs output ByteBuffers for storing results\n* @param numParityUnits number of parity units to process\n* @param bIsDirect indicates if encoding is direct or not\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)": "/**\n* Constructs HHXORErasureEncodingStep with encoders and initializes piggy back index.\n* @param inputBlocks array of input EC blocks\n* @param outputBlocks array of output EC blocks\n* @param rsRawEncoder raw erasure encoder for Reed-Solomon\n* @param xorRawEncoder raw erasure encoder for XOR\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])": "/**\n* Encodes input ByteBuffers into output buffers with parity units and piggyback data.\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Performs coding by encoding input ByteBuffers into output buffers.\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Encodes ECChunk arrays into ByteBuffer arrays for processing.\n* @param inputChunks array of input ECChunk objects\n* @param outputChunks array of output ECChunk objects\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:performDecodeImpl(java.nio.ByteBuffer[],int[],int,int[],java.nio.ByteBuffer[],int[])": "/**\n* Decodes input data from ByteBuffers into output ByteBuffers.\n* @param inputs array of input ByteBuffers\n* @param inputOffsets offsets for each input buffer\n* @param dataLen length of data to decode\n* @param erased array indicating erased symbols\n* @param outputs array for storing decoded ByteBuffers\n* @param outputOffsets offsets for each output buffer\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:release()": "/**\n* Releases resources by locking and destroying the implementation.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer()": "/**\n* Indicates if direct buffer preference is enabled.\n* @return true, as direct buffers are always preferred\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes NativeXORRawDecoder with coding options.\n* @param coderOptions configuration for erasure coding\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:preferDirectBuffer()": "/**\n* Indicates if direct buffer preference is enabled.\n* @return true if direct buffers are preferred, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs an AbstractNativeRawDecoder with specified coding options.\n* @param coderOptions configuration for erasure coding\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)": "/**\n* Decodes data from ByteArrayDecodingState to ByteBufferDecodingState.\n* @param decodingState input state for decoding\n* @throws IOException if decoding fails\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:performEncodeImpl(java.nio.ByteBuffer[],int[],int,java.nio.ByteBuffer[],int[])": "/**\n* Encodes input ByteBuffers and writes to output ByteBuffers.\n* @param inputs array of input ByteBuffers\n* @param inputOffsets array of offsets for each input\n* @param dataLen length of data to encode\n* @param outputs array of output ByteBuffers\n* @param outputOffsets array of offsets for each output\n* @throws IOException if encoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:release()": "/**\n* Releases resources by locking and calling destroyImpl.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer()": "/**\n* Indicates preference for using direct buffers.\n* @return always true, indicating direct buffers are preferred\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes NativeXORRawEncoder with coding options.\n* @param coderOptions settings for erasure coding\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:preferDirectBuffer()": "/**\n* Indicates preference for using direct buffers.\n* @return true if direct buffers are preferred, false otherwise\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)": "/**\n* Encodes input ByteBuffers and writes to output ByteBuffers.\n* @param encodingState contains input and output ByteBuffers\n* @throws IOException if the encoder is closed or an I/O error occurs\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs AbstractNativeRawEncoder with specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)": "/**\n* Encodes ByteArrayEncodingState to ByteBufferEncodingState.\n* @param encodingState input/output state for encoding\n* @throws IOException if an I/O error occurs during encoding\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.InvalidDecodingException": {
        "org.apache.hadoop.io.erasurecode.rawcoder.InvalidDecodingException:<init>(java.lang.String)": "/**\n* Constructs an InvalidDecodingException with a description message.\n* @param description error message for the exception\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField": {
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getFieldSize()": "/**\n* Retrieves the size of the field.\n* @return the size of the field as an integer\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:substitute(java.nio.ByteBuffer[],int,java.nio.ByteBuffer,int)": "/**\n* Substitutes values in ByteBuffer q using data from array p and a multiplication table.\n* @param p array of ByteBuffers for substitution input\n* @param len maximum length for ByteBuffer processing\n* @param q ByteBuffer to be modified\n* @param x index used for accessing the multiplication table\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],java.nio.ByteBuffer[],int)": "/**\n* Solves a Vandermonde system using input arrays.\n* @param x coefficients for the system\n* @param y ByteBuffers containing data to process\n* @param len length of the input arrays\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:substitute(byte[][],int[],int,byte[],int,int)": "/**\n* Substitutes values in array q based on offsets and input array p.\n* @param p 2D byte array for substitution\n* @param offsets starting indices in p for each row\n* @param len number of elements to substitute\n* @param q output byte array to modify\n* @param offset starting index in q for substitution\n* @param x index for multiplication table lookup\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],byte[][],int[],int,int)": "/**\n* Solves a Vandermonde system and updates the output in a byte array.\n* @param x input coefficients for the system\n* @param y output byte arrays to hold results\n* @param outputOffsets offsets for each output array\n* @param len number of equations in the system\n* @param dataLen length of data in each output array\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:remainder(java.nio.ByteBuffer[],int[])": "/**\n* Computes the remainder of byte buffers divided by a divisor array.\n* @param dividend array of ByteBuffer representing the dividend\n* @param divisor array of integers representing the divisor\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:remainder(byte[][],int[],int,int[])": "/**\n* Computes the remainder of byte arrays divided by a divisor.\n* @param dividend 2D byte array to be modified\n* @param offsets   array of offsets for dividend access\n* @param len      length of the segment to process\n* @param divisor   array of divisor values\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:<init>(int,int)": "/**\n* Initializes a Galois Field with specified size and primitive polynomial.\n* @param fieldSize size of the field, must be positive\n* @param primitivePolynomial irreducible polynomial, must be positive\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[],int)": "/**\n* Solves a Vandermonde system using given x and y arrays.\n* @param x input coefficients array\n* @param y output/results array\n* @param len length of the arrays\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)": "/**\n* Adds two non-negative integers after validating their range.\n* @param x first integer, must be within field size\n* @param y second integer, must be within field size\n* @return the bitwise XOR of x and y\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)": "/**\n* Multiplies two integers using a precomputed multiplication table.\n* @param x first multiplicand, must be within field size\n* @param y second multiplicand, must be within field size\n* @return product of x and y\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)": "/**\n* Divides two integers using a precomputed table.\n* @param x numerator index, must be non-negative and within field size\n* @param y denominator index, must be positive and within field size\n* @return the division result from divTable\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)": "/**\n* Calculates x raised to the power of n.\n* @param x base value, must be non-negative and within field size\n* @param n exponent value, non-negative integer\n* @return result of x^n\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)": "/**\n* Retrieves or creates a GaloisField instance by size and polynomial.\n* @param fieldSize size of the field, must be positive\n* @param primitivePolynomial irreducible polynomial, must be positive\n* @return GaloisField instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])": "/**\n* Solves a Vandermonde system using input coefficients.\n* @param x input coefficients array\n* @param y output/results array\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])": "/**\n* Adds two integer arrays element-wise.\n* @param p first array, may be of different length\n* @param q second array, may be of different length\n* @return resulting array after addition\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])": "/**\n* Multiplies two polynomials represented as integer arrays.\n* @param p first polynomial coefficients\n* @param q second polynomial coefficients\n* @return resulting polynomial coefficients after multiplication\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][])": "/**\n* Performs Gaussian elimination on the given matrix.\n* @param matrix 2D array representing the matrix to be transformed\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance()": "/**\n* Retrieves a GaloisField instance with default parameters.\n* @return GaloisField instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil": {
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpMatrix(byte[],int,int)": "/**\n* Prints the byte matrix to the console.\n* @param matrix byte array representing the matrix\n* @param numDataUnits number of data units (rows)\n* @param numAllUnits number of all units (columns)\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:bytesToHex(byte[],int)": "/**\n* Converts byte array to hex string with optional character limit.\n* @param bytes byte array to convert\n* @param limit maximum number of bytes to convert\n* @return hex string representation of the bytes\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:<init>()": "/**\n* Private constructor to prevent instantiation of DumpUtil class.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk)": "/**\n* Dumps hex representation of ECChunk or \"<EMPTY>\" if null.\n* @param chunk ECChunk object to convert and display\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Prints header and hex representation of each ECChunk in the array.\n* @param header text displayed before chunks\n* @param chunks array of ECChunk objects to display\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil": {
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getValidIndexes(java.lang.Object[])": "/**\n* Returns indexes of non-null elements from the input array.\n* @param inputs array of elements to check\n* @return array of valid indexes with non-null values\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getNullIndexes(java.lang.Object[])": "/**\n* Returns indexes of null elements in the provided array.\n* @param inputs array of elements to check for nulls\n* @return array of indexes where elements are null\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the CoderUtil class.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getEmptyChunk(int)": "/**\n* Retrieves an empty byte array of at least the specified length.\n* @param leastLength minimum required length of the byte array\n* @return byte array of at least leastLength\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:cloneAsDirectByteBuffer(byte[],int,int)": "/**\n* Clones a byte array into a direct ByteBuffer.\n* @param input byte array to clone from\n* @param offset starting position in the array\n* @param len number of bytes to clone\n* @return direct ByteBuffer or null if input is null\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[])": "/**\n* Returns the first non-null input from the array.\n* @param inputs array of inputs to search\n* @return first non-null input or throws an exception if all are null\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)": "/**\n* Resets buffer by filling it with an empty chunk of specified length.\n* @param buffer ByteBuffer to reset\n* @param len minimum length of the empty chunk\n* @return modified ByteBuffer\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)": "/**\n* Resets a portion of the buffer with an empty byte array.\n* @param buffer the byte array to reset\n* @param offset starting position in the buffer\n* @param len length of the portion to reset\n* @return the modified buffer\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)": "/**\n* Resets multiple output buffers with specified data length.\n* @param buffers array of ByteBuffers to reset\n* @param dataLen minimum length for the reset buffers\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Converts ECChunk array to ByteBuffer array, resetting buffers if all values are zero.\n* @param chunks array of ECChunk objects\n* @return array of ByteBuffer corresponding to the chunks\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)": "/**\n* Resets multiple output buffers using specified offsets and length.\n* @param buffers array of byte arrays to reset\n* @param offsets array of starting positions for each buffer\n* @param dataLen length of data to reset in each buffer\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256": {
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMul(byte,byte)": "/**\n* Multiplies two Galois field elements.\n* @param a first element, must be non-zero\n* @param b second element, must be non-zero\n* @return product of a and b in Galois field\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfVectMulInit(byte,byte[],int)": "/**\n* Initializes a lookup table for Galois Field vector multiplication.\n* @param c byte value for multiplication\n* @param tbl array to store computed values\n* @param offset starting index in tbl\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInv(byte)": "/**\n* Computes the multiplicative inverse in Galois Field.\n* @param a element in GF(2^8)\n* @return multiplicative inverse of a or 0 if a is 0\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMulTab()": "/**\n* Returns the Galois field multiplication table.\n* @return 2D byte array representing the multiplication table\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:<init>()": "/**\n* Private constructor for GF256 class; prevents instantiation.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)": "/**\n* Inverts a given matrix using Galois Field arithmetic.\n* @param inMatrix input matrix to be inverted\n* @param outMatrix output matrix for the inverse result\n* @param n dimension of the n x n matrix\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator": {
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder)": "/**\n* Constructs a DecodingValidator with a specified RawErasureDecoder.\n* @param decoder instance of RawErasureDecoder used for validation\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:markBuffers(java.nio.ByteBuffer[])": "/**\n* Marks each non-null ByteBuffer in the array for later reset.\n* @param buffers array of ByteBuffers to mark\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:allocateBuffer(boolean,int)": "/**\n* Allocates a ByteBuffer with specified capacity.\n* @param direct true for direct buffer, false for heap buffer\n* @param capacity the size of the buffer to allocate\n* @return the allocated ByteBuffer\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:toLimits(java.nio.ByteBuffer[])": "/**\n* Sets the position of each non-null ByteBuffer to its limit.\n* @param buffers array of ByteBuffers to update\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:resetBuffers(java.nio.ByteBuffer[])": "/**\n* Resets each ByteBuffer in the provided array.\n* @param buffers array of ByteBuffer objects to reset\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])": "/**\n* Validates input buffers and updates output buffers based on erased indexes.\n* @param inputs array of input ByteBuffers\n* @param erasedIndexes indices of erased data\n* @param outputs array of output ByteBuffers\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])": "/**\n* Validates ECChunks by converting to ByteBuffers and updating outputs.\n* @param inputs array of ECChunk objects\n* @param erasedIndexes indices of erased data\n* @param outputs array of ECChunk objects for results\n* @throws IOException if decoding fails\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:performEncodeImpl(java.nio.ByteBuffer[],int[],int,java.nio.ByteBuffer[],int[])": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:release()": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes NativeRSRawEncoder with coding options.\n* @param coderOptions configuration settings for the erasure coder\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:adjustOrder(java.lang.Object[],java.lang.Object[],int[],int[],java.lang.Object[],java.lang.Object[])": "/**\n* Adjusts the order of input and output arrays based on erased indexes.\n* @param inputs original data inputs\n* @param inputs2 reordered data inputs\n* @param erasedIndexes indexes of erased units in inputs\n* @param erasedIndexes2 reordered erased indexes\n* @param outputs original outputs\n* @param outputs2 reordered outputs\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:checkGetBytesArrayBuffer(byte[][],int,int)": "/**\n* Ensures the byte array at index is initialized to bufferLen if null or too short.\n* @param bytesArrayBuffers array of byte arrays\n* @param idx index of the byte array to check\n* @param bufferLen required length of the byte array\n* @return byte array at specified index\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:checkGetDirectBuffer(java.nio.ByteBuffer[],int,int)": "/**\n* Ensures a direct ByteBuffer is available at the specified index.\n* @param directBuffers array of ByteBuffers\n* @param idx index of the buffer to check\n* @param bufferLen required capacity of the buffer\n* @return a ByteBuffer with the specified capacity\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])": "/**\n* Decodes input data and substitutes values into output arrays.\n* @param inputs source byte arrays for decoding\n* @param inputOffsets starting indices for each input array\n* @param dataLen length of each input array\n* @param erasedIndexes indices of erased data\n* @param outputs destination byte arrays for results\n* @param outputOffsets starting indices for each output array\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])": "/**\n* Decodes data from inputs and fills outputs based on erased indexes.\n* @param inputs array of ByteBuffers to decode\n* @param erasedIndexes indexes of erased data\n* @param outputs array to store decoded results\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs RSLegacyRawDecoder with validation on data and parity units.\n* @param coderOptions configuration options for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)": "/**\n* Decodes data using state, adjusting output buffers based on erased indexes.\n* @param decodingState state containing input data and output buffers\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)": "/**\n* Decodes input data and prepares output buffers based on decoding state.\n* @param decodingState contains input data and output buffer info\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])": "/**\n* Decodes input buffers after adjusting their order based on erased indexes.\n* @param inputs array of input ByteBuffers\n* @param erasedIndexes indices of erased data\n* @param outputs array of output ByteBuffers\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])": "/**\n* Decodes input arrays after adjusting their order.\n* @param inputs original byte arrays\n* @param erasedIndexes indexes of erased inputs\n* @param outputs output byte arrays\n* @throws IOException if decoding fails\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState": {
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,int,byte[][],int[],byte[][],int[])": "/**\n* Initializes ByteArrayEncodingState with encoder and data parameters.\n* @param encoder RawErasureEncoder instance for encoding\n* @param encodeLength length of data to encode\n* @param inputs byte arrays to encode\n* @param inputOffsets offsets for input byte arrays\n* @param outputs byte arrays for encoded output\n* @param outputOffsets offsets for output byte arrays\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][])": "/**\n* Validates buffers for null and length constraints.\n* @param buffers array of byte arrays to check\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState()": "/**\n* Converts input buffers to ByteBufferEncodingState.\n* @return ByteBufferEncodingState initialized with new buffers\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])": "/**\n* Initializes ByteArrayEncodingState with encoder and buffer arrays.\n* @param encoder the RawErasureEncoder instance\n* @param inputs array of input byte arrays\n* @param outputs array of output byte arrays\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState": {
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,int,java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Initializes ByteBufferEncodingState with encoder, lengths, and buffer arrays.\n* @param encoder the RawErasureEncoder instance\n* @param encodeLength the length for encoding\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[])": "/**\n* Validates an array of ByteBuffers for null, length, and direct buffer status.\n* @param buffers array of ByteBuffers to check\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState()": "/**\n* Converts inputs and outputs to ByteArrayEncodingState.\n* @return ByteArrayEncodingState initialized with encoder and data\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Initializes ByteBufferEncodingState with encoder and buffers.\n* @param encoder the RawErasureEncoder instance\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState": {
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,int,int[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Initializes ByteBufferDecodingState with decoder and buffer parameters.\n* @param decoder the RawErasureDecoder instance\n* @param decodeLength length of data to decode\n* @param erasedIndexes indices of erased data\n* @param inputs array of input ByteBuffers\n* @param outputs array of output ByteBuffers\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[])": "/**\n* Validates output buffers for null, length, and direct buffer status.\n* @param buffers array of ByteBuffer objects to check\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState()": "/**\n* Converts input/output buffers to ByteArrayDecodingState.\n* @return ByteArrayDecodingState instance for decoding\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[])": "/**\n* Validates input buffers for decoding; throws exceptions on invalid conditions.\n* @param buffers array of ByteBuffer inputs to check\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])": "/**\n* Initializes decoding state with buffers and checks parameters for validity.\n* @param decoder the RawErasureDecoder instance\n* @param inputs array of input ByteBuffers\n* @param erasedIndexes indices of erased data\n* @param outputs array of output ByteBuffers\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState": {
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,int,int[],byte[][],int[],byte[][],int[])": "/**\n* Initializes ByteArrayDecodingState with decoder and data parameters.\n* @param decoder the RawErasureDecoder instance\n* @param decodeLength length of data to decode\n* @param erasedIndexes indices of erased data\n* @param inputs input byte arrays for decoding\n* @param inputOffsets offsets for input arrays\n* @param outputs output byte arrays for decoded data\n* @param outputOffsets offsets for output arrays\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][])": "/**\n* Validates output buffers; throws exception for null or incorrect length buffers.\n* @param buffers array of byte arrays to validate\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState()": "/**\n* Converts input/output buffers to ByteBufferDecodingState.\n* @return ByteBufferDecodingState initialized with buffers and decoder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][])": "/**\n* Validates input buffers against expected length and count.\n* @param buffers array of byte arrays to check\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])": "/**\n* Initializes decoding state with inputs, outputs, and erased indexes.\n* @param decoder decoding mechanism\n* @param inputs input byte arrays\n* @param erasedIndexes indexes of erased inputs\n* @param outputs output byte arrays\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil": {
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:<init>()": "/**\n* Private constructor to prevent instantiation of RSUtil class.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])": "/**\n* Initializes Galois Field tables using the coding matrix.\n* @param k number of columns, @param rows number of rows, \n* @param codingMatrix input matrix, @param matrixOffset start index, \n* @param gfTables output table for values\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)": "/**\n* Generates a Cauchy matrix and stores it in array a.\n* @param a output matrix, m rows and k columns\n* @param m total number of rows\n* @param k total number of columns\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])": "/**\n* Encodes input data using Galois field multiplication.\n* @param gfTables Galois field multiplication tables\n* @param dataLen length of data to encode\n* @param inputs input data arrays\n* @param inputOffsets offsets for each input\n* @param outputs output data arrays\n* @param outputOffsets offsets for each output\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])": "/**\n* Encodes input data using Galois field multiplication.\n* @param gfTables Galois field multiplication tables\n* @param inputs array of ByteBuffers containing input data\n* @param outputs array of ByteBuffers for encoded output data\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)": "/**\n* Computes powers of a primitive root for given data and parity units.\n* @param numDataUnits count of data units\n* @param numParityUnits count of parity units\n* @return array of computed powers\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:performDecodeImpl(java.nio.ByteBuffer[],int[],int,int[],java.nio.ByteBuffer[],int[])": "/**\n* Decodes input byte buffers into output byte buffers.\n* @param inputs source byte buffers for decoding\n* @param inputOffsets offsets for each input buffer\n* @param dataLen length of data to decode\n* @param erased indices of erased data\n* @param outputs destination byte buffers for results\n* @param outputOffsets offsets for each output buffer\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:release()": "/**\n* Releases resources by locking and destroying implementation.\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes NativeRSRawDecoder with coding options.\n* @param coderOptions configuration for erasure coding\n*/"
    },
    "org.apache.hadoop.io.LongWritable$Comparator": {
        "org.apache.hadoop.io.LongWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays as long values.\n* @param b1 first byte array, s1 start index, l1 length\n* @param b2 second byte array, s2 start index, l2 length\n* @return negative if b1 < b2, zero if equal, positive if b1 > b2\n*/",
        "org.apache.hadoop.io.LongWritable$Comparator:<init>()": "/**\n* Initializes a Comparator for LongWritable keys.\n*/"
    },
    "org.apache.hadoop.io.GenericWritable": {
        "org.apache.hadoop.io.GenericWritable:set(org.apache.hadoop.io.Writable)": "/**\n* Sets the Writable instance and updates its type based on registered classes.\n* @param obj Writable object to set\n* @throws RuntimeException if the instance type is not registered\n*/",
        "org.apache.hadoop.io.GenericWritable:toString()": "/**\n* Returns a string representation of the GW instance.\n* @return formatted string with class name and value or \"(null)\" if instance is null\n*/",
        "org.apache.hadoop.io.GenericWritable:write(java.io.DataOutput)": "/**\n* Writes the GenericWritable instance to the DataOutput.\n* @param out the output stream to write to\n* @throws IOException if type is not set or instance is null\n*/",
        "org.apache.hadoop.io.GenericWritable:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.io.GenericWritable:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration to set\n*/",
        "org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from input and initializes a Writable instance.\n* @param in DataInput source for reading fields\n*/"
    },
    "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream": {
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read(byte[])": "/**\n* Reads bytes into the provided array.\n* @param b byte array to fill with data\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read()": "/**\n* Reads a single byte of data from the input stream.\n* @return the byte read, or -1 if the end of the stream is reached\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read(byte[],int,int)": "/**\n* Reads bytes into an array from input stream.\n* @param b byte array to fill\n* @param off offset to start filling\n* @param len maximum number of bytes to read\n* @return number of bytes read\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:skip(long)": "/**\n* Skips a specified number of bytes in the input stream.\n* @param n number of bytes to skip\n* @return number of bytes actually skipped\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:available()": "/**\n* Returns the number of bytes that can be read without blocking.\n* @return number of available bytes\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream)": "/**\n* Initializes PassthroughDecompressorStream with an InputStream.\n* @param input InputStream for reading compressed data\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream": {
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:readStreamHeader()": "/**\n* Reads and processes the header of a bzip2 stream.\n* @return BufferedInputStream for further reading\n* @throws IOException if stream reading fails\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:resetState()": "/**\n* Resets the state of the object, marking it as needing a reset.\n* @throws IOException if an I/O error occurs during reset\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close()": "/**\n* Closes the input stream and marks it for reset.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean)": "/**\n* Updates the compressed stream position based on input and a flag.\n* @param shouldAddOn flag to determine if an additional byte is added\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)": "/**\n* Initializes BZip2CompressionInputStream with input parameters.\n* @param in input stream for decompression\n* @param start starting position in the stream\n* @param end ending position in the stream\n* @param readMode mode of reading (CONTINUOUS or BYBLOCK)\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset()": "/**\n* Resets the stream if needed, initializing input for decompression.\n* @throws IOException if stream operations fail\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream)": "/**\n* Constructs BZip2CompressionInputStream with default parameters.\n* @param in input stream for decompression\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)": "/**\n* Reads bytes into the provided array and manages stream state.\n* @param b byte array to store read bytes\n* @param off starting offset in b\n* @param len number of bytes to read\n* @return number of bytes read or special end values\n* @throws IOException if stream is closed or read fails\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read()": "/**\n* Reads a single byte and returns its unsigned value.\n* @return byte value or -1 if end of stream is reached\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream": {
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:close()": "/**\n* Closes the input stream and clears associated resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getProcessedByteCount()": "/**\n* Retrieves the count of processed bytes from the compressed stream.\n* @return long number of bytes processed\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateProcessedByteCount(int)": "/**\n* Updates the count of processed bytes from the compressed stream.\n* @param count number of bytes processed to add\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:reportCRCError()": "/**\n* Reports a CRC error by throwing an IOException.\n* @throws IOException indicating a CRC error occurred\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:makeMaps()": "/**\n* Creates mapping of used bytes to their sequential representation.\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:hbCreateDecodeTables(int[],int[],int[],char[],int,int,int)": "/**\n* Creates decoding tables for Huffman coding.\n* @param limit output array for limit values\n* @param base output array for base values\n* @param perm output array for permutation indices\n* @param length input array of symbol lengths\n* @param minLen minimum symbol length\n* @param maxLen maximum symbol length\n* @param alphaSize number of symbols in the alphabet\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int)": "/**\n* Updates reported byte count and processes the count.\n* @param count number of bytes to add to reported total\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream)": "/**\n* Reads a byte from InputStream and updates processed byte count.\n* @param inStream the input stream to read from\n* @return the byte read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock()": "/**\n* Finalizes block CRC and reports errors if CRC mismatch occurs.\n* @throws IOException if a CRC error is detected\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)": "/**\n* Creates Huffman decoding tables for specified groups.\n* @param alphaSize number of symbols in the alphabet\n* @param nGroups number of groups for decoding tables\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long)": "/**\n* Reads bits from InputStream and returns the specified number of bits.\n* @param n number of bits to read\n* @return long value representing the read bits\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit()": "/**\n* Retrieves a single bit from the bit stream.\n* @return true if the bit is 1, false if 0\n* @throws IOException if end of stream is reached unexpectedly\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)": "/**\n* Skips to the next marker in the stream.\n* @param marker the marker to find\n* @param markerBitLength number of bits for the marker\n* @return true if marker found, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte()": "/**\n* Retrieves an unsigned byte as a char.\n* @return char representation of the 8-bit value read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt()": "/**** Retrieves a 32-bit integer by reading 4 sets of 8 bits. \n* @return 32-bit integer value \n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int)": "/**\n* Decodes and retrieves a value based on group number from input stream.\n* @param groupNo the group index for decoding\n* @return decoded integer value\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables()": "/**\n* Receives and processes Huffman decoding tables from input.\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker()": "/**\n* Skips to the next block marker in the stream.\n* @return true if the marker is found, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete()": "/**\n* Completes the process and checks CRC validity.\n* @throws IOException if a CRC error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode()": "/**\n* Decodes data using Huffman coding and updates internal state.\n* @throws IOException if an I/O error occurs during decoding\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock()": "/**\n* Initializes a block for reading; checks integrity and sets up CRC.\n* @throws IOException if block header is invalid or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA()": "/**\n* Sets up random part A for data processing and manages state transitions.\n* @throws IOException if block header is invalid or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA()": "/**\n* Sets up state for non-random processing; updates CRC and manages block transitions.\n* @throws IOException if block header is invalid or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC()": "/**\n* Manages state and updates CRC in random part C processing.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock()": "/**\n* Prepares data block for processing; handles initialization and state management.\n* @throws IOException if data is corrupted or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC()": "/**\n* Prepares state for non-random processing; updates CRC and manages transitions.\n* @throws IOException if block header is invalid or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB()": "/**\n* Sets up random part B state and transitions to A or C based on conditions.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock()": "/**\n* Changes state to processing a block or sets to EOF based on skipResult flag.\n* @throws IOException if block initialization or setup fails\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init()": "/**\n* Initializes BZip2 stream; verifies magic byte and block size.\n* @throws IOException if format is invalid or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB()": "/**\n* Configures state for non-random processing based on channel changes.\n* @throws IOException if block header is invalid or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)": "/**\n* Initializes CBZip2InputStream with input stream and read mode.\n* @param in input stream for decompression\n* @param readMode mode of reading (CONTINUOUS or BYBLOCK)\n* @param skipDecompression flag to skip decompression\n* @throws IOException for stream errors\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0()": "/**\n* Reads the current character based on state; handles transitions and errors.\n* @return current character or special end values\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)": "/**\n* Constructs CBZip2InputStream for decompression.\n* @param in input stream for decompression\n* @param readMode mode of reading (CONTINUOUS or BYBLOCK)\n* @throws IOException for stream errors\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream)": "/**\n* Counts bytes until the next marker in a compressed input stream.\n* @param in input stream for decompression\n* @return number of bytes processed until the next marker\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)": "/**\n* Reads bytes into destination array; validates input and manages stream state.\n* @param dest byte array to store read bytes\n* @param offs starting offset in dest\n* @param len number of bytes to read\n* @return number of bytes read or special end values\n* @throws IOException if stream is closed or read fails\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream)": "/**\n* Initializes CBZip2InputStream for decompression.\n* @param in input stream for decompression\n* @throws IOException for stream errors\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read()": "/**\n* Reads a byte from the input stream.\n* @return byte value or negative if stream is closed\n* @throws IOException if stream is closed\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor": {
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:compress(byte[],int,int)": "/**\n* Compresses a byte array and returns the compressed length.\n* @param b byte array to compress, @param off offset, @param len length to compress\n* @return compressed length\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:end()": "/**\n* Throws UnsupportedOperationException when end operation is invoked.\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:finish()": "/**\n* Throws an exception to indicate the finish operation is unsupported.\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:finished()": "/**\n* Indicates if the operation is finished.\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:getBytesRead()": "/**\n* Returns the number of bytes read; always throws UnsupportedOperationException.\n* @return always throws an exception\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:getBytesWritten()": "/**\n* Indicates the number of bytes written, currently unsupported.\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:needsInput()": "/**\n* Indicates if input is required; always throws UnsupportedOperationException.\n* @return always throws exception\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary with the given byte array.\n* @param b byte array for the dictionary\n* @param off offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:setInput(byte[],int,int)": "/**\n* Sets input data; operation is unsupported.\n* @param b byte array of input data\n* @param off offset in the array\n* @param len length of data to set\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes the method with the given configuration.\n* @param conf configuration settings to apply\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:reset()": "/**\n* Resets the state of the object; no action is performed.\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor": {
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:initSymbols(java.lang.String)": "/**\n* Initializes symbols using the specified library name.\n* @param libname name of the library to initialize symbols from\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(int,int,int)": "/**\n* Initializes Bzip2Compressor with specified parameters.\n* @param blockSize size of the block for compression\n* @param workFactor determines compression effort\n* @param directBufferSize size of direct buffers for data\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInputFromSavedData()": "/**\n* Updates buffer with saved data from user input.\n* Adjusts user buffer length and offset accordingly.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary data, but operation is unsupported.\n* @param b byte array containing dictionary data\n* @param off offset to start reading from the array\n* @param len number of bytes to read\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finished()": "/**\n* Checks if the bzip2 process is finished and all data is consumed.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:compress(byte[],int,int)": "/**\n* Compresses data into a byte array.\n* @param b byte array to store compressed data\n* @param off offset in the array\n* @param len maximum number of bytes to compress\n* @return number of bytes written to the array\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:checkStream()": "/**\n* Validates the stream; throws NullPointerException if stream is zero.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:end()": "/**\n* Ends the active stream and resets its identifier.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finish()": "/**\n* Marks the process as finished.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>()": "/**\n* Constructs a Bzip2Compressor with default parameters.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)": "/**\n* Sets input buffer with validation and updates internal state.\n* @param b byte array input, @param off offset, @param len length of input\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput()": "/**\n* Determines if more input is needed based on buffer availability.\n* @return true if input is needed, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten()": "/**\n* Retrieves the total bytes written to the stream.\n* @return total bytes written as a long value\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead()": "/**\n* Returns the number of bytes read from the stream.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset()": "/**\n* Resets the stream and associated buffers to initial state.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs Bzip2Compressor using configuration settings.\n* @param conf configuration object for compression parameters\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes the compressor with new configuration settings.\n* @param conf configuration object or null for defaults\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor": {
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:initSymbols(java.lang.String)": "/**\n* Initializes symbols using the specified library name.\n* @param libname name of the library to initialize symbols from\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(boolean,int)": "/**\n* Initializes Bzip2Decompressor with memory settings and buffer sizes.\n* @param conserveMemory flag to minimize memory usage\n* @param directBufferSize size of the direct byte buffers\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInputFromSavedData()": "/**\n* Sets input data for bzip2 from saved user buffer.\n* Adjusts buffer sizes and updates offsets.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary data; operation is unsupported.\n* @param b byte array containing dictionary data\n* @param off offset from which to start\n* @param len length of data to set\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:finished()": "/**\n* Checks if the bzip2 compression is complete.\n* @return true if finished and all data is consumed, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:decompress(byte[],int,int)": "/**\n* Decompresses data into the provided byte array.\n* @param b byte array for decompressed data\n* @param off starting offset in the array\n* @param len maximum number of bytes to decompress\n* @return number of bytes decompressed\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:checkStream()": "/**\n* Validates the stream; throws NullPointerException if stream is zero.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:end()": "/**\n* Ends the active stream if it exists, resetting the stream identifier.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsDictionary()": "/**\n* Indicates if a dictionary is needed for processing.\n* @return false, as no dictionary is required\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>()": "/**\n* Constructs a Bzip2Decompressor with default settings.\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)": "/**\n* Sets input data for processing with validations.\n* @param b byte array input data, @param off offset, @param len length of data\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput()": "/**\n* Determines if more input is needed for processing.\n* @return true if input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten()": "/**\n* Retrieves the number of bytes written to the stream.\n* @return long representing bytes written\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead()": "/**\n* Retrieves the number of bytes read from the stream.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining()": "/**\n* Calculates remaining buffer length.\n* @return total remaining length in bytes\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset()": "/**\n* Resets the stream and buffer states for a fresh start.\n* @throws NullPointerException if stream is invalid\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data": {
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data:<init>(int)": "/**\n* Initializes Data with a byte array of specified block size.\n* @param blockSize100k size in hundreds of kilobytes for the byte array\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data:initTT(int)": "/**\n* Initializes or retrieves an array of ints of specified length.\n* @param length desired size of the int array\n* @return int array of at least the specified length\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.CRC": {
        "org.apache.hadoop.io.compress.bzip2.CRC:initialiseCRC()": "/**\n* Initializes the global CRC value to its maximum unsigned 32-bit integer.\n*/",
        "org.apache.hadoop.io.compress.bzip2.CRC:getFinalCRC()": "/**\n* Computes the final CRC value by inverting the global CRC.\n* @return inverted global CRC value\n*/",
        "org.apache.hadoop.io.compress.bzip2.CRC:updateCRC(int)": "/**\n* Updates the global CRC value using the provided input character.\n* @param inCh input character to update the CRC\n*/",
        "org.apache.hadoop.io.compress.bzip2.CRC:updateCRC(int,int)": "/**\n* Updates the CRC value based on input character and repeat count.\n* @param inCh character input for CRC calculation\n* @param repeat number of times to apply the update\n*/",
        "org.apache.hadoop.io.compress.bzip2.CRC:<init>()": "/**\n* Constructs a CRC instance and initializes its value.\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream": {
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:chooseBlockSize(long)": "/**\n* Determines block size based on input length.\n* @param inputLength length of input data\n* @return calculated block size (1-9) or MAX_BLOCKSIZE if inputLength is non-positive\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:flush()": "/**\n* Flushes the output stream to ensure all data is written out.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:getAllowableBlockSize(int)": "/**\n* Calculates allowable block size based on input.\n* @param blockSize100k block size in units of 100k\n* @return adjusted block size minus a paranoia constant\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsW(int,int)": "/**\n* Writes bits to output stream, updating buffer and live bit count.\n* @param n number of bits to write\n* @param v value of bits to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsFinishedWithStream()": "/**\n* Finalizes and writes remaining bits from buffer to output stream.\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues0(int,int)": "/**\n* Sends MTF values for specified groups and alpha size.\n* @param nGroups number of groups to process\n* @param alphaSize size of the alphabet\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues2(int,int)": "/**\n* Updates MTF values based on group and selector counts.\n* @param nGroups number of groups (max 8)\n* @param nSelectors number of selectors to process\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues6(int,int)": "/**\n* Sends MTF values to output stream, encoding with bit manipulation.\n* @param nGroups number of groups to process\n* @param alphaSize size of the alphabet for each group\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues7(int)": "/**\n* Sends MTF values to output stream based on selector indices.\n* @param nSelectors number of selectors to process\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:hbMakeCodeLengths(byte[],int[],org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int)": "/**\n* Computes code lengths for Huffman coding.\n* @param len array to store code lengths\n* @param freq frequency of each symbol\n* @param dat data structure containing heap and weights\n* @param alphaSize number of symbols\n* @param maxLen maximum allowed code length\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:hbAssignCodes(int[],byte[],int,int,int)": "/**\n* Assigns binary codes based on specified lengths for a given alphabet size.\n* @param code array to store assigned codes\n* @param length array of lengths for each symbol\n* @param minLen minimum code length\n* @param maxLen maximum code length\n* @param alphaSize size of the alphabet\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:generateMTFValues()": "/**\n* Generates MTF (Move-To-Front) values for data compression.\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:randomiseBlock()": "/**\n* Randomizes the data block using a predefined random number sequence.\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSimpleSort(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)": "/**\n* Performs a simple sorting operation on a subset of data.\n* @param dataShadow the data structure containing elements to sort\n* @param lo starting index for sorting\n* @param hi ending index for sorting\n* @param d offset used in sorting comparisons\n* @return true if sorting completes with work done exceeding limits\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:med3(byte,byte,byte)": "/**\n* Returns the median value of three byte numbers.\n* @param a first byte value\n* @param b second byte value\n* @param c third byte value\n* @return median byte value among a, b, and c\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:vswap(int[],int,int,int)": "/**\n* Swaps elements in fmap between indices p1 and p1+n.\n* @param fmap array of integers to modify\n* @param p1 starting index for the swap\n* @param p2 starting index to swap with\n* @param n number of elements to swap\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock()": "/**\n* Initializes block settings and resets in-use array.\n* @return void\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int)": "/**\n* Writes an unsigned byte to the output stream.\n* @param c value of the byte to write (0-255)\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int)": "/**\n* Writes an integer to the output stream in four bytes.\n* @param u the integer to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4()": "/**\n* Sends MTF values by writing their usage to the output stream.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)": "/**\n* Sends MTF values by writing group and selector counts to output stream.\n* @param nGroups number of groups to send\n* @param nSelectors number of selectors to send\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)": "/**\n* Sends MTF values and computes selector counts.\n* @param nGroups number of groups to process\n* @param alphaSize size of the alphabet\n* @return number of selectors generated\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)": "/**\n* Sends MTF values for groups, assigning codes based on symbol lengths.\n* @param nGroups number of groups to process\n* @param alphaSize size of the alphabet for code assignment\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)": "/**\n* Performs a 3-way quicksort on a Data object.\n* @param dataShadow data structure containing elements to sort\n* @param loSt starting index for sorting\n* @param hiSt ending index for sorting\n* @param dSt depth of recursion\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init()": "/**\n* Initializes data and writes magic bytes to the output stream.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression()": "/**\n* Finalizes compression by writing end marker and CRC to output stream.\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues()": "/**\n* Sends MTF values and coding tables to the output stream.\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort()": "/**\n* Executes the main sorting algorithm using frequency tables and quicksort.\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)": "/**\n* Constructs CBZip2OutputStream with specified block size and output stream.\n* @param out output stream for compressed data\n* @param blockSize size of blocks (1-9) for compression\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend()": "/**\n* Encodes and sends data using Move-To-Front strategy.\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort()": "/**\n* Sorts data blocks with a limit; randomizes on first attempt if needed.\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream)": "/**\n* Initializes CBZip2OutputStream with default block size.\n* @param out output stream for compressed data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock()": "/**\n* Finalizes the current block, computes CRC, and writes header and data to output.\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun()": "/**\n* Writes a run of characters to the data block, updating CRC and handling block limits.\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish()": "/**\n* Completes the compression process, writing remaining data and cleaning up resources.\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int)": "/**\n* Handles character writing with run-length encoding.\n* @param b character to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize()": "/**\n* Cleans up resources before object finalization.\n* @throws Throwable if an error occurs during finalization\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close()": "/**\n* Closes the output stream and releases resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int)": "/**\n* Writes a character if output is open; throws IOException if closed.\n* @param b character to write\n* @throws IOException if output is closed or an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)": "/****\n* Writes a byte array to output with bounds checking.\n* @param buf byte array to write from\n* @param offs starting offset in buf\n* @param len number of bytes to write\n* @throws IOException if an I/O error occurs or stream is closed\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data": {
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data:<init>(int)": "/**\n* Initializes Data with a specified block size.\n* @param blockSize100k size in 100k units for data block allocation\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor": {
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:decompress(byte[],int,int)": "/**\n* Decompresses a byte array from a specified offset and length.\n* @param b byte array to decompress\n* @param off starting offset in the byte array\n* @param len number of bytes to decompress\n* @return number of bytes decompressed\n* @throws IOException if decompression fails\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:end()": "/**\n* Throws UnsupportedOperationException to indicate method cannot be executed.\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:finished()": "/**\n* Indicates if the operation is finished.\n* @return false as this method is not supported\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:needsDictionary()": "/**\n* Indicates if a dictionary is required; always throws UnsupportedOperationException.\n* @return always throws an exception\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:needsInput()": "/**\n* Indicates if input is required; always throws UnsupportedOperationException.\n* @return always throws an exception\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:getRemaining()": "/**\n* Throws UnsupportedOperationException for remaining items retrieval.\n* @return always throws exception\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary with given byte array, offset, and length.\n* @param b byte array containing dictionary data\n* @param off starting offset in the byte array\n* @param len length of the dictionary data\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:setInput(byte[],int,int)": "/**\n* Sets input data but throws UnsupportedOperationException.\n* @param b byte array of input data\n* @param off offset to start from\n* @param len length of data to set\n*/",
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:reset()": "/**\n* Resets the state; currently, it performs no actions.\n*/"
    },
    "org.apache.hadoop.io.compress.CompressionOutputStream": {
        "org.apache.hadoop.io.compress.CompressionOutputStream:<init>(java.io.OutputStream)": "/**\n* Initializes CompressionOutputStream with the specified output stream.\n* @param out the output stream to write compressed data\n*/",
        "org.apache.hadoop.io.compress.CompressionOutputStream:setTrackedCompressor(org.apache.hadoop.io.compress.Compressor)": "/**\n* Sets the tracked compressor.\n* @param compressor the Compressor object to track\n*/",
        "org.apache.hadoop.io.compress.CompressionOutputStream:flush()": "/**\n* Flushes the output stream to ensure all data is written.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics for the current output object.\n* @return IOStatistics object or null if invalid\n*/",
        "org.apache.hadoop.io.compress.CompressionOutputStream:close()": "/**\n* Closes the output stream and returns the compressor to the pool if used.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.compress.BZip2Codec": {
        "org.apache.hadoop.io.compress.BZip2Codec:writeHeader(java.io.OutputStream)": "/**\n* Writes the BZ header to the output stream.\n* @param out the OutputStream to write the header to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:<init>()": "/**\n* Constructs a new BZip2Codec instance.\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing current settings\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration instance to be set\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:getDefaultExtension()": "/**\n* Returns the default file extension for BZIP2 codec.\n* @return default BZIP2 codec extension as a String\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream for the given OutputStream.\n* @param out the OutputStream to write compressed data\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream from the given InputStream.\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)": "/**\n* Creates a BZip2CompressionInputStream for decompression.\n* @param seekableIn input stream, must be Seekable\n* @param decompressor not used in this method\n* @param start starting position in the stream\n* @param end ending position in the stream\n* @param readMode mode of reading (CONTINUOUS or BYBLOCK)\n* @return BZip2CompressionInputStream instance\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a CompressionOutputStream based on native Bzip2 availability.\n* @param out OutputStream for compressed data\n* @param compressor Compressor instance for compression\n* @return CompressionOutputStream instance\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream based on native Bzip2 availability.\n* @param in input stream for decompression\n* @param decompressor decompressor to use with the stream\n* @return CompressionInputStream instance\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:getCompressorType()": "/**\n* Retrieves the Bzip2 compressor class type.\n* @return Class type of Bzip2 compressor based on configuration\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType()": "/**\n* Retrieves the Bzip2 decompressor class type.\n* @return Class of the Bzip2 decompressor\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createDecompressor()": "/**\n* Creates a Bzip2 decompressor using the current configuration.\n* @return Decompressor instance for Bzip2 compression\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec:createCompressor()": "/**\n* Creates a Bzip2 compressor using the provided configuration.\n* @return Compressor instance for Bzip2 compression\n*/"
    },
    "org.apache.hadoop.io.compress.CompressorStream": {
        "org.apache.hadoop.io.compress.CompressorStream:compress()": "/**\n* Compresses data from buffer and writes to output.\n* @throws IOException if an I/O error occurs during compression\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:resetState()": "/**\n* Resets the state of the compressor.\n* @throws IOException if an I/O error occurs during reset\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)": "/**\n* Initializes CompressorStream with output stream, compressor, and buffer size.\n* @param out the output stream for compressed data\n* @param compressor the compressor to use\n* @param bufferSize the size of the buffer\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream)": "/**\n* Initializes CompressorStream with the specified output stream.\n* @param out the output stream for compressed data\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)": "/**\n* Writes data to the output after validation.\n* @param b byte array to write, @param off start offset, @param len number of bytes to write\n* @throws IOException if write exceeds stream end or I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:finish()": "/**\n* Completes the compression process, ensuring all data is compressed.\n* @throws IOException if an I/O error occurs during compression\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Constructs a CompressorStream with default buffer size.\n* @param out the output stream for compressed data\n* @param compressor the compressor to use\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:write(int)": "/**\n* Writes a single byte to the output stream.\n* @param b the byte value to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.CompressorStream:close()": "/**\n* Closes the output stream and marks it as closed.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.compress.snappy.SnappyDecompressor": {
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(int)": "/**\n* Initializes SnappyDecompressor with specified direct buffer size.\n* @param directBufferSize size of the direct buffers for compression\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInputFromSavedData()": "/**\n* Sets input for Snappy from saved data buffer.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:finished()": "/**\n* Checks if the process is finished based on state and buffer status.\n* @return true if finished and buffer is empty, false otherwise\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirectBuf()": "/**\n* Decompresses data from a direct buffer and resets its state.\n* @return size of uncompressed data\n* @throws IOException if decompression fails\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:reset()": "/**\n* Resets the state of the object, clearing buffers and flags to initial values.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:end()": "/**\n* No operation method for ending processes; does nothing when called.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:getRemaining()": "/**\n* Returns the remaining bytes, always zero in this implementation.\n* @return int representing remaining bytes\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsDictionary()": "/**\n* Indicates if a dictionary is required for processing.\n* @return false, as no dictionary is needed\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary with the given byte array.\n* @param b byte array containing dictionary data\n* @param off offset in the byte array\n* @param len length of the dictionary data\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>()": "/**\n* Constructs a SnappyDecompressor with default buffer size.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)": "/**\n* Sets input buffer with specified offset and length, validating input parameters.\n* @param b byte array input, @param off offset in the array, @param len length of data\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput()": "/**\n* Determines if more input is needed based on buffer states.\n* @return true if input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)": "/**\n* Decompresses data into a byte array.\n* @param b byte array to store decompressed data\n* @param off offset in the array to start storing data\n* @param len maximum number of bytes to decompress\n* @return number of bytes decompressed\n* @throws IOException if decompression fails\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decompresses data from a source ByteBuffer to a destination ByteBuffer.\n* @param src source buffer containing compressed data\n* @param dst destination buffer for uncompressed data\n* @return size of uncompressed data\n* @throws IOException if decompression fails\n*/"
    },
    "org.apache.hadoop.io.compress.snappy.SnappyCompressor": {
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(int)": "/**\n* Initializes SnappyCompressor with specified direct buffer size.\n* @param directBufferSize size of the direct buffer in bytes\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setInput(byte[],int,int)": "/**\n* Sets input data from a byte array with specified offset and length.\n* @param b byte array containing input data\n* @param off starting offset in the array\n* @param len number of bytes to read from the array\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setInputFromSavedData()": "/**\n* Sets input data from saved buffer if available.\n* Updates userBufOff and userBufLen accordingly.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:needsInput()": "/**\n* Determines if input is needed based on buffer states.\n* @return true if input is needed, false otherwise\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:finished()": "/**\n* Checks if all data has been consumed and processing is complete.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:compressDirectBuf()": "/**\n* Compresses data from an uncompressed buffer to a compressed buffer.\n* @return size of compressed data, or 0 if no data to compress\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:reset()": "/**\n* Resets the state of the object, clearing buffers and counters.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:end()": "/**\n* Finalizes and cleans up resources used by the current process.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:finish()": "/**\n* Marks the process as finished.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:getBytesRead()": "/**\n* Returns the total number of bytes read.\n* @return long representing bytes read count\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:getBytesWritten()": "/**\n* Retrieves the total number of bytes written.\n* @return total bytes written as a long value\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary with the given byte array.\n* @param b byte array representing the dictionary\n* @param off offset to start from in the byte array\n* @param len length of the dictionary data\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>()": "/**\n* Constructs a SnappyCompressor with default buffer size.\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)": "/**\n* Compresses data into a byte array.\n* @param b byte array to store compressed data\n* @param off offset in the array\n* @param len maximum length of data to compress\n* @return number of bytes written\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes the object state using the provided configuration.\n* @param conf configuration settings for reinitialization\n*/"
    },
    "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor": {
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decompresses data from source ByteBuffer to destination ByteBuffer.\n* @param src ByteBuffer containing compressed data\n* @param dst ByteBuffer for decompressed output\n* @throws IOException if an I/O error occurs during decompression\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary for decompression; not supported for byte[] arrays.\n* @param b byte array for dictionary\n* @param off offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:decompress(byte[],int,int)": "/**\n* Throws exception for unsupported byte[] decompression.\n* @param b byte array to decompress\n* @param off offset in the array\n* @param len number of bytes to decompress\n* @throws UnsupportedOperationException if called\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished()": "/**\n* Determines if the process is finished based on input state.\n* @return true if input ends and super.finished() is true; false otherwise\n*/",
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset()": "/**\n* Resets the object's state and marks endOfInput as true.\n*/"
    },
    "org.apache.hadoop.io.compress.CompressionInputStream": {
        "org.apache.hadoop.io.compress.CompressionInputStream:<init>(java.io.InputStream)": "/**\n* Initializes CompressionInputStream with the provided InputStream.\n* @param in InputStream to read compressed data from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.CompressionInputStream:setTrackedDecompressor(org.apache.hadoop.io.compress.Decompressor)": "/**\n* Sets the tracked decompressor instance.\n* @param decompressor the Decompressor to be tracked\n*/",
        "org.apache.hadoop.io.compress.CompressionInputStream:getPos()": "/**\n* Retrieves the current position in the input stream.\n* @return current position as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.CompressionInputStream:seek(long)": "/**\n* Seeks to a specified position; always throws UnsupportedOperationException.\n* @param pos the position to seek to\n*/",
        "org.apache.hadoop.io.compress.CompressionInputStream:seekToNewSource(long)": "/**\n* Seeks to a new source position; always throws UnsupportedOperationException.\n* @param targetPos position to seek to\n* @return always throws exception\n*/",
        "org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics using the input source.\n* @return IOStatistics object or null if invalid input\n*/",
        "org.apache.hadoop.io.compress.CompressionInputStream:close()": "/**\n* Closes the input stream and returns the decompressor to the pool if tracked. \n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.compress.DecompressorStream": {
        "org.apache.hadoop.io.compress.DecompressorStream:checkStream()": "/**\n* Validates if the stream is closed and throws IOException if it is.\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:resetState()": "/**\n* Resets the decompressor's state.\n* @throws IOException if an I/O error occurs during reset\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:reset()": "/**\n* Resets the stream, throwing an exception as mark/reset is not supported.\n* @throws IOException if mark/reset is attempted\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:mark(int)": "/**\n* Marks the current position in the input stream with a read limit.\n* @param readlimit maximum number of bytes that can be read before the mark is invalidated\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:markSupported()": "/**\n* Indicates if marking is supported; always returns false.\n* @return false indicating marking is not supported\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)": "/**\n* Initializes DecompressorStream with input, decompressor, and buffer sizes.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @param bufferSize size of the buffer\n* @param skipBufferSize size for skipping bytes\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream)": "/**** Initializes DecompressorStream with the provided InputStream. \n* @param in InputStream for reading compressed data \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:getCompressedData()": "/**\n* Reads compressed data into a buffer.\n* @return number of bytes read from the input stream\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:available()": "/**\n* Returns the number of available bytes (1 or 0).\n* @return 1 if not EOF, otherwise 0\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)": "/**\n* Constructs DecompressorStream with input and decompressor, using default skip buffer size.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @param bufferSize size of the buffer\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)": "/**\n* Decompresses data from a byte array.\n* @param b byte array containing compressed data\n* @param off offset to start decompression\n* @param len length of data to decompress\n* @return number of bytes decompressed or -1 on EOF\n* @throws IOException if decompression fails\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Constructs DecompressorStream with input and decompressor.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)": "/**\n* Reads data into a byte array and decompresses it.\n* @param b byte array to store the read data\n* @param off offset to start reading from\n* @param len number of bytes to read\n* @return number of bytes read\n* @throws IOException if stream is closed or decompression fails\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:read()": "/**\n* Reads a byte from the stream.\n* @return byte value or -1 if end of stream is reached\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:skip(long)": "/**\n* Skips 'n' bytes in the stream and returns the number of bytes skipped.\n* @param n number of bytes to skip\n* @return number of bytes actually skipped\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.DecompressorStream:close()": "/**\n* Closes the stream, ensuring it's marked as closed afterward.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.compress.BlockCompressorStream": {
        "org.apache.hadoop.io.compress.BlockCompressorStream:rawWriteInt(int)": "/**\n* Writes an integer as four bytes to the output stream.\n* @param v the integer value to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BlockCompressorStream:compress()": "/**\n* Compresses data and writes the length and content to output.\n* @throws IOException if an I/O error occurs during compression or writing\n*/",
        "org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)": "/**\n* Constructs BlockCompressorStream with output, compressor, and sizes.\n* @param out output stream for compressed data\n* @param compressor compressor to use\n* @param bufferSize size of the buffer\n* @param compressionOverhead overhead size for compression\n*/",
        "org.apache.hadoop.io.compress.BlockCompressorStream:finish()": "/**\n* Finalizes compression, writing bytes read and compressing until complete.\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Initializes BlockCompressorStream with output and compressor.\n* @param out output stream for compressed data\n* @param compressor compressor to use\n*/",
        "org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)": "/**\n* Writes data to the compressor, handling various input conditions.\n* @param b byte array to write from\n* @param off offset in the array\n* @param len number of bytes to write\n* @throws IOException if an I/O error occurs or if conditions are violated\n*/"
    },
    "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor": {
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRecommendedBufferSize()": "/**\n* Retrieves the recommended buffer size for streaming.\n* @return int representing the recommended buffer size\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInputFromSavedData()": "/**\n* Updates input buffer from saved data, adjusting sizes and position.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setDictionary(byte[],int,int)": "/**\n* Throws an exception as dictionary support is not enabled.\n* @param b byte array for the dictionary\n* @param off offset in the byte array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finished()": "/**\n* Checks if decompression is complete.\n* @return true if finished and buffer is empty, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:checkStream()": "/**\n* Validates if the stream is initialized.\n* @throws NullPointerException if stream is not initialized\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:populateUncompressedBuffer(byte[],int,int,int)": "/**\n* Fills a buffer with uncompressed data.\n* @param b byte array to populate\n* @param off offset in the array\n* @param len maximum length to read\n* @param n requested number of bytes to read\n* @return actual number of bytes read\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:end()": "/**\n* Releases resources associated with the stream if it is active.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:inflateDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decompresses data from src ByteBuffer to dst ByteBuffer.\n* @param src source buffer containing compressed data\n* @param dst destination buffer for decompressed data\n* @return number of bytes written to dst\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:isNativeCodeLoaded()": "/**\n* Checks if the native code is loaded.\n* @return true if native code is loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsDictionary()": "/**\n* Indicates if a dictionary is needed for processing.\n* @return false, as no dictionary is required\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)": "/**\n* Sets input buffer with validation and updates from saved data.\n* @param b byte array input, @param off offset, @param len length of data\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput()": "/**\n* Determines if more input is needed based on buffer states.\n* @return true if input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining()": "/**\n* Calculates remaining bytes to consume from the buffer.\n* @return total remaining bytes as an integer\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset()": "/**\n* Resets the stream state and buffer parameters for reuse.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)": "/**\n* Decompresses data into the provided byte array.\n* @param b byte array to fill with decompressed data\n* @param off offset in the array to start writing\n* @param len maximum number of bytes to write\n* @return actual number of bytes decompressed\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int)": "/**\n* Initializes ZStandardDecompressor with buffer size and allocates buffers.\n* @param bufferSize size for direct buffers\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize()": "/**\n* Cleans up resources before garbage collection.\n* Calls reset() to reset stream state.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>()": "/**\n* Constructs a ZStandardDecompressor with default stream size.\n*/"
    },
    "org.apache.hadoop.io.compress.zstd.ZStandardCompressor": {
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getRecommendedBufferSize()": "/**\n* Retrieves the recommended buffer size for streaming.\n* @return int representing the recommended buffer size\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInputFromSavedData()": "/**\n* Updates buffer with data from saved input, adjusting offsets and lengths accordingly.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary; throws exception as support is not enabled.\n* @param b byte array for the dictionary\n* @param off starting offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finished()": "/**\n* Checks if compression is complete.\n* @return true if finished and no remaining compressed data, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:checkStream()": "/**\n* Validates if the stream is initialized; throws exception if not.\n* @throws NullPointerException if stream is zero\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:end()": "/**\n* Ends the stream if it is active, resetting its identifier to zero.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:isNativeCodeLoaded()": "/**\n* Checks if native code is loaded.\n* @return true if native code is loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finish()": "/**\n* Marks the process as finished.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)": "/**\n* Sets input buffer with validation and updates offsets.\n* @param b byte array input, @param off offset, @param len length of input\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput()": "/**\n* Determines if more input is needed based on buffer states.\n* @return true if user input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)": "/**\n* Compresses data into the provided byte array.\n* @param b byte array to store compressed data\n* @param off offset in the byte array\n* @param len maximum number of bytes to compress\n* @return number of bytes compressed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten()": "/**\n* Returns the number of bytes written after validating the stream.\n* @return long representing bytes written\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead()": "/**\n* Returns the number of bytes read from the stream.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset()": "/**\n* Resets the state of the object and associated buffers for new data processing.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)": "/**\n* Initializes ZStandardCompressor with specified parameters.\n* @param level compression level, inputBufferSize size of input buffer, outputBufferSize size of output buffer\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)": "/**\n* Initializes ZStandardCompressor with level and buffer sizes.\n* @param level compression level, @param bufferSize size of input/output buffer\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>()": "/**\n* Constructs a ZStandardCompressor with default level and buffer sizes.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes compressor with new configuration.\n* @param conf configuration object for compression settings\n*/"
    },
    "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor": {
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decompresses data from source ByteBuffer to destination ByteBuffer.\n* @param src input ByteBuffer containing compressed data\n* @param dst output ByteBuffer for decompressed data\n* @throws IOException if an I/O error occurs during decompression\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary using a byte array, unsupported for DirectDecompressor.\n* @param b byte array for dictionary\n* @param off offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:decompress(byte[],int,int)": "/**\n* Throws an exception as byte[] decompression is unsupported.\n* @param b byte array to decompress\n* @param off offset in the array\n* @param len length of data to decompress\n* @return UnsupportedOperationException\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished()": "/**\n* Checks if decompression is complete.\n* @return true if input is finished and buffer is empty, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset()": "/**\n* Resets the stream state and marks endOfInput as true.\n*/",
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int)": "/**\n* Constructs ZStandardDirectDecompressor with specified direct buffer size.\n* @param directBufferSize size for direct buffers\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibFactory": {
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionLevel(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the compression level from the configuration.\n* @param conf the configuration object\n* @return CompressionLevel enum value based on config or default\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionStrategy(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the compression strategy from configuration.\n* @param conf configuration object\n* @return CompressionStrategy enum value\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:isNativeZlibLoaded(org.apache.hadoop.conf.Configuration)": "/**\n* Checks if native Zlib compression is loaded.\n* @param conf configuration settings\n* @return true if native Zlib is loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getLibraryName()": "/**\n* Retrieves the name of the compression library.\n* @return String representing the library name\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionStrategy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy)": "/**\n* Sets the compression strategy in the given configuration.\n* @param conf configuration object to update\n* @param strategy compression strategy to apply\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionLevel(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel)": "/**\n* Sets the compression level in the given configuration.\n* @param conf configuration object to modify\n* @param level desired compression level\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration)": "/**\n* Returns the appropriate Zlib compressor class based on native support.\n* @param conf configuration settings\n* @return Compressor class type for Zlib compression\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration)": "/**\n* Returns the appropriate Zlib decompressor class based on native support.\n* @param conf configuration settings\n* @return Class of the decompressor to use\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib()": "/**** Loads and initializes the native Zlib library if native code is available. */",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration)": "/**\n* Returns a Zlib compressor based on native support.\n* @param conf configuration settings\n* @return Compressor instance for Zlib compression\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a ZlibDirectDecompressor if native Zlib is available.\n* @param conf configuration settings\n* @return ZlibDirectDecompressor or null if not available\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration)": "/**\n* Returns a Zlib decompressor based on native support.\n* @param conf configuration settings\n* @return Decompressor instance for Zlib decompression\n*/"
    },
    "org.apache.hadoop.io.compress.CodecConstants": {
        "org.apache.hadoop.io.compress.CodecConstants:<init>()": "/**\n* Private constructor to prevent instantiation of the CodecConstants class.\n*/"
    },
    "org.apache.hadoop.io.compress.AlreadyClosedException": {
        "org.apache.hadoop.io.compress.AlreadyClosedException:<init>(java.lang.String)": "/**\n* Constructs an AlreadyClosedException with a specified message.\n* @param message detail message for the exception\n*/"
    },
    "org.apache.hadoop.io.compress.PassthroughCodec": {
        "org.apache.hadoop.io.compress.PassthroughCodec:<init>()": "/**\n* Constructs a new instance of PassthroughCodec.\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:getDefaultExtension()": "/**\n* Returns the default file extension for the codec.\n* @return String representing the default extension\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a compression output stream.\n* @param out underlying output stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a compression output stream.\n* @param out the output stream to write compressed data\n* @param compressor the compressor to use for compression\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:getCompressorType()": "/**\n* Retrieves the compressor type; always throws UnsupportedOperationException.\n* @return Class of Compressor subtype\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:createCompressor()": "/**\n* Creates a compressor instance; always throws UnsupportedOperationException.\n* @return never returns a valid Compressor\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:createDecompressor()": "/**\n* Creates and returns a new instance of StubDecompressor.\n* @return a Decompressor instance\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:getConf()": "/**\n* Retrieves the current configuration object.\n* @return Configuration instance representing the current settings\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:getDecompressorType()": "/**\n* Returns the class type of the decompressor.\n* @return Class of the StubDecompressor\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream using the specified InputStream.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance (not used)\n* @return CompressionInputStream for reading decompressed data\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream from the given InputStream.\n* @param in InputStream for compressed data\n* @return CompressionInputStream for reading decompressed data\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and updates extension with a dot prefix if absent.\n* @param conf configuration object to set\n*/"
    },
    "org.apache.hadoop.io.compress.BlockDecompressorStream": {
        "org.apache.hadoop.io.compress.BlockDecompressorStream:rawReadInt()": "/**\n* Reads four bytes and returns them as an integer.\n* @return integer value constructed from four bytes\n* @throws IOException if an I/O error occurs or EOF is reached\n*/",
        "org.apache.hadoop.io.compress.BlockDecompressorStream:resetState()": "/**\n* Resets the decompressor's state and block size.\n* @throws IOException if an I/O error occurs during reset\n*/",
        "org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData()": "/**\n* Reads compressed data size and populates buffer.\n* @return size of the compressed data chunk\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream)": "/**** Initializes BlockDecompressorStream with an InputStream for decompression. \n* @param in InputStream for reading compressed data \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)": "/**\n* Decompresses data into the provided buffer.\n* @param b byte array for decompressed data\n* @param off offset in the array\n* @param len maximum length to decompress\n* @return number of bytes decompressed\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)": "/**\n* Initializes BlockDecompressorStream with input, decompressor, and buffer size.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @param bufferSize size of the buffer\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/******************************************************************************* \n* Initializes BlockDecompressorStream with input and decompressor. \n* @param in InputStream for compressed data \n* @param decompressor Decompressor instance \n* @throws IOException if an I/O error occurs \n*******************************************************************************/"
    },
    "org.apache.hadoop.io.compress.lz4.Lz4Decompressor": {
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(int)": "/**\n* Constructs an Lz4Decompressor with a specified buffer size.\n* @param directBufferSize size of the direct buffer for decompression\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInputFromSavedData()": "/**\n* Updates input buffer from saved data for compression.\n* Adjusts buffer lengths and rewinds direct buffer.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:finished()": "/**\n* Checks if the process is finished based on state and buffer.\n* @return true if finished and buffer is empty, false otherwise\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompressDirectBuf()": "/**\n* Decompresses data from a buffer and returns the uncompressed size.\n* @return size of the uncompressed data\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:reset()": "/**\n* Resets the state of the object for reuse.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:end()": "/**\n* Finalizes the current process; does not perform any action.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:getRemaining()": "/**\n* Returns the remaining bytes to be processed.\n* @return always returns 0 for this implementation\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsDictionary()": "/**\n* Indicates if a dictionary is needed.\n* @return false, as no dictionary is required\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary with the provided byte array.\n* @param b byte array containing dictionary data\n* @param off offset to start reading from the array\n* @param len number of bytes to read from the array\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>()": "/**\n* Initializes Lz4Decompressor with default buffer size.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)": "/****\n* Sets input buffer and validates parameters.\n* @param b byte array input, @param off offset, @param len length of input\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput()": "/**\n* Determines if more input is needed for compression.\n* @return true if input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)": "/**\n* Decompresses data into a byte array.\n* @param b byte array to store decompressed data\n* @param off offset in the array\n* @param len maximum number of bytes to decompress\n* @return number of bytes decompressed\n*/"
    },
    "org.apache.hadoop.io.compress.lz4.Lz4Compressor": {
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int,boolean)": "/**\n* Initializes Lz4Compressor with specified buffer size and compression type.\n* @param directBufferSize size of the direct buffer for compression\n* @param useLz4HC true for high compression, false for fast compression\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setInput(byte[],int,int)": "/** \n* Sets input data from a byte array with specified offset and length.\n* @param b byte array containing input data\n* @param off starting position in the array\n* @param len number of bytes to read from the array\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setInputFromSavedData()": "/**\n* Updates input buffer from saved data if available.\n* Adjusts user buffer length and offset accordingly.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:needsInput()": "/**\n* Determines if input is needed based on buffer states.\n* @return true if input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:finished()": "/**\n* Checks if all data has been consumed and processing is finished.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:compressDirectBuf()": "/**\n* Compresses the uncompressed direct buffer and resets its length.\n* @return size of the compressed buffer\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:reset()": "/**\n* Resets the state of the object, clearing buffers and counters.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:end()": "/**\n* Ends the current process, ensuring thread safety with synchronization.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:finish()": "/**\n* Marks the process as finished.\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:getBytesRead()": "/**\n* Returns the total number of bytes read.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:getBytesWritten()": "/**\n* Returns the total number of bytes written.\n* @return total bytes written as a long value\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary data, but currently does nothing.\n* @param b byte array containing dictionary data\n* @param off offset into the byte array\n* @param len length of the data to set\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int)": "/**\n* Constructs Lz4Compressor with specified buffer size and default compression type.\n* @param directBufferSize size of the direct buffer for compression\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)": "/**\n* Compresses data into a byte array.\n* @param b destination byte array\n* @param off offset in the array\n* @param len maximum number of bytes to write\n* @return number of bytes written\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes the object with the given configuration.\n* @param conf configuration settings for reinitialization\n*/",
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>()": "/**\n* Initializes Lz4Compressor with default buffer size.\n*/"
    },
    "org.apache.hadoop.io.compress.CompressionCodecFactory": {
        "org.apache.hadoop.io.compress.CompressionCodecFactory:toString()": "/**\n* Returns a string representation of the compression codecs map.\n* @return formatted string of codec entries\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByClassName(java.lang.String)": "/**\n* Retrieves a CompressionCodec by its class name.\n* @param classname the name of the codec class\n* @return CompressionCodec object or null if not found\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:removeSuffix(java.lang.String,java.lang.String)": "/**\n* Removes the specified suffix from the filename if present.\n* @param filename the original filename\n* @param suffix the suffix to remove\n* @return filename without the suffix or the original if not found\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Adds a CompressionCodec to the codec collections.\n* @param codec the CompressionCodec to add\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path)": "/**\n* Retrieves CompressionCodec based on the file's name.\n* @param file the path of the file\n* @return CompressionCodec or null if not found\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String)": "/**\n* Retrieves a CompressionCodec by its name or class name.\n* @param codecName name or class of the codec\n* @return CompressionCodec object or null if not found\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String)": "/**\n* Retrieves the class of a CompressionCodec by its name.\n* @param codecName name of the codec\n* @return Class of CompressionCodec or null if not found\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)": "/**\n* Sets codec classes in the configuration.\n* @param conf configuration object to update\n* @param classes list of codec class types\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a list of CompressionCodec classes from configuration and service providers.\n* @param conf configuration settings\n* @return list of CompressionCodec class types\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes CompressionCodecFactory with codecs from configuration.\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[])": "/**\n* Processes input/output files with compression based on command-line arguments.\n* @param args command-line arguments specifying input/output files\n* @throws Exception if an error occurs during processing\n*/"
    },
    "org.apache.hadoop.io.compress.CodecPool": {
        "org.apache.hadoop.io.compress.CodecPool:createCache(java.lang.Class)": "/**\n* Creates a loading cache mapping class types to AtomicInteger instances.\n* @param klass the class type for the cache key\n* @return a LoadingCache for the specified class type\n*/",
        "org.apache.hadoop.io.compress.CodecPool:borrow(java.util.Map,java.lang.Class)": "/**\n* Borrows an object from the specified pool based on its codec class.\n* @param pool a map of codec classes to their available objects\n* @param codecClass the class type of the object to borrow\n* @return the borrowed object or null if none available\n*/",
        "org.apache.hadoop.io.compress.CodecPool:getLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Class)": "/****\n* Returns the lease count for the specified codec class.\n* @param usageCounts cache of usage counts by codec class\n* @param codecClass the codec class to retrieve the count for\n* @return the current lease count as an integer\n*/",
        "org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)": "/**\n* Adds codec to the pool if not already present.\n* @param pool mapping of class types to codec sets\n* @param codec the codec to be added\n* @return true if codec was added, false otherwise\n*/",
        "org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)": "/**\n* Updates lease count for a codec class by delta.\n* @param usageCounts cache of usage counts by class\n* @param codec the codec instance to update\n* @param delta the amount to adjust the count by\n*/",
        "org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Returns the count of leased compressors for a given codec.\n* @param codec the CompressionCodec to check\n* @return the number of leased compressors as an integer\n*/",
        "org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Returns the count of leased decompressors for a given codec.\n* @param codec the CompressionCodec to check\n* @return number of leased decompressors, or 0 if codec is null\n*/",
        "org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves or creates a Compressor for the given CompressionCodec.\n* @param codec the codec to get the compressor from\n* @param conf configuration for compressor initialization\n* @return Compressor instance or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Retrieves a decompressor for the given codec, borrowing or creating as needed.\n* @param codec the compression codec to get the decompressor for\n* @return Decompressor instance or null if unavailable\n*/",
        "org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor)": "/**\n* Returns a compressor to the pool if reusable.\n* @param compressor the Compressor instance to return\n*/",
        "org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)": "/**\n* Returns a decompressor to the pool if reusable; resets or ends it otherwise.\n* @param decompressor the Decompressor instance to return\n*/",
        "org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Retrieves a Compressor using the specified CompressionCodec.\n* @param codec the codec to get the compressor from\n* @return Compressor instance or null if creation fails\n*/"
    },
    "org.apache.hadoop.util.ReflectionUtils": {
        "org.apache.hadoop.util.ReflectionUtils:getClass(java.lang.Object)": "/**\n* Returns the Class object of the given instance.\n* @param o the object to retrieve the class from\n* @return Class<T> representing the object's class\n*/",
        "org.apache.hadoop.util.ReflectionUtils:getDeclaredFieldsIncludingInherited(java.lang.Class)": "/**\n* Retrieves declared fields of a class, including inherited ones, sorted by name.\n* @param clazz the class to inspect\n* @return list of Field objects, including inherited fields\n*/",
        "org.apache.hadoop.util.ReflectionUtils:getDeclaredMethodsIncludingInherited(java.lang.Class)": "/**\n* Retrieves all declared methods of a class, including inherited ones.\n* @param clazz the class to inspect\n* @return List of Method objects from the class and its superclasses\n*/",
        "org.apache.hadoop.util.ReflectionUtils:setContentionTracing(boolean)": "/**\n* Enables or disables thread contention tracing.\n* @param val true to enable, false to disable tracing\n*/",
        "org.apache.hadoop.util.ReflectionUtils:getTaskName(long,java.lang.String)": "/**\n* Returns task name formatted with ID and optional name.\n* @param id unique task identifier\n* @param name optional task name\n* @return formatted task name string\n*/",
        "org.apache.hadoop.util.ReflectionUtils:clearCache()": "/**\n* Clears the constructor cache for efficient memory management.\n*/",
        "org.apache.hadoop.util.ReflectionUtils:getCacheSize()": "/**\n* Returns the current size of the constructor cache.\n* @return size of the constructor cache as an integer\n*/",
        "org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)": "/**\n* Configures the given object using JobConf if applicable.\n* @param theObject object to configure\n* @param conf configuration to use for setup\n*/",
        "org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)": "/**\n* Prints detailed information about active threads to the specified stream.\n* @param stream output stream for thread info\n* @param title title for the thread dump\n*/",
        "org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration for the given object if it's Configurable.\n* @param theObject object to configure\n* @param conf configuration to apply\n*/",
        "org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)": "/**\n* Logs thread info if log level is enabled and interval since last log is met.\n* @param log logger to output thread info\n* @param title title for the thread dump\n* @param minInterval minimum interval in seconds between logs\n*/",
        "org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)": "/**\n* Logs thread info if the minimum interval has passed.\n* @param log Logger instance for logging info\n* @param title Title for the thread dump\n* @param minInterval Minimum interval in seconds between logs\n*/",
        "org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)": "/**\n* Clones data from source Writable to destination Writable.\n* @param dst destination Writable object\n* @param src source Writable object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])": "/**\n* Creates a new instance of the specified class and configures it.\n* @param theClass class type to instantiate\n* @param conf configuration to apply to the instance\n* @param argTypes types of constructor parameters\n* @param values values for constructor parameters\n* @return new instance of the specified class\n*/",
        "org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**** \n* Creates a new instance of the specified class with a configuration.\n* @param theClass class type to instantiate\n* @param conf configuration to apply to the instance\n* @return new instance of the specified class\n*/",
        "org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves or initializes the SerializationFactory.\n* @param conf configuration for serializer settings\n* @return SerializationFactory instance\n*/",
        "org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)": "/**\n* Copies data from source to destination using serialization and deserialization.\n* @param conf configuration for serializer settings\n* @param src source object to copy from\n* @param dst destination object to copy to\n* @return the destination object after copying\n*/"
    },
    "org.apache.hadoop.io.compress.ZStandardCodec": {
        "org.apache.hadoop.io.compress.ZStandardCodec:getLibraryName()": "/**\n* Retrieves the name of the compression library.\n* @return String representing the library name\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getConf()": "/**\n* Retrieves the current configuration settings.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration object to be set\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getDefaultExtension()": "/**\n* Returns the default file extension for Zstandard codec.\n* @return default extension as a String\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded()": "/**\n* Validates native zStandard library availability; throws RuntimeException if missing.\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded()": "/**\n* Checks if both native compressor and decompressor code are loaded.\n* @return true if both are loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType()": "/**\n* Returns the compressor type for ZStandard compression.\n* @return Class of the ZStandardCompressor\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType()": "/**\n* Retrieves the class type of the decompressor.\n* @return Class of ZStandardDecompressor\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream for writing compressed data.\n* @param out the OutputStream to write to\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream from the provided InputStream.\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the Zstandard compression level from configuration.\n* @param conf configuration object\n* @return compression level as an integer\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves buffer size from configuration.\n* @param conf configuration object\n* @return buffer size as an integer\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Determines compression buffer size based on configuration.\n* @param conf configuration object\n* @return buffer size as an integer, or recommended size if config size is zero\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Calculates decompression buffer size based on configuration.\n* @param conf configuration object\n* @return buffer size as an integer\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a CompressionOutputStream for data compression.\n* @param out the output stream for compressed data\n* @param compressor the compressor to use\n* @return CompressionOutputStream instance\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createCompressor()": "/**\n* Creates a ZStandard compressor after verifying native library availability.\n* @return Compressor instance configured with level and buffer size\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream with specified decompressor.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @return CompressionInputStream for reading decompressed data\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor()": "/**\n* Creates a ZStandard decompressor after validating native code availability.\n* @return Decompressor instance\n*/",
        "org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor()": "/**\n* Creates a DirectDecompressor using a calculated buffer size.\n* @return DirectDecompressor instance\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibCompressor": {
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:isNativeZlibLoaded()": "/**\n* Checks if the native Zlib library is loaded.\n* @return true if native Zlib is loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInputFromSavedData()": "/**\n* Updates buffer with saved data, adjusting offsets and lengths accordingly.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary from a byte array.\n* @param b byte array containing dictionary data\n* @param off starting offset in the byte array\n* @param len number of bytes to use from the array\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:finished()": "/**\n* Checks if the compression process is complete.\n* @return true if finished and no remaining data, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:compress(byte[],int,int)": "/**\n* Compresses data into the provided byte array.\n* @param b byte array to store compressed data\n* @param off offset in the array to start storing\n* @param len maximum number of bytes to compress\n* @return number of bytes written to the array\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:checkStream()": "/**\n* Validates the stream; throws NullPointerException if stream is zero.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:end()": "/**\n* Ends the stream if it is active and resets the stream identifier.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)": "/**\n* Constructs a compression configuration.\n* @param level compression level setting\n* @param strategy compression strategy to use\n* @param header compression header information\n* @param directBufferSize size of the direct buffer\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:finish()": "/**\n* Marks the process as finished.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)": "/**\n* Initializes ZlibCompressor with compression settings and buffer sizes.\n* @param level compression level settings\n* @param strategy compression strategy settings\n* @param header compression header information\n* @param directBufferSize size for direct buffers\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)": "/**\n* Sets input buffer and validates parameters.\n* @param b byte array input, @param off offset, @param len length of input\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput()": "/**\n* Determines if more input is needed for processing.\n* @return true if input is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten()": "/**\n* Returns the number of bytes written to the stream.\n* @return long representing bytes written\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead()": "/**\n* Returns the number of bytes read from the stream.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset()": "/**\n* Resets the stream and buffer states for reuse.\n* @throws NullPointerException if stream is invalid\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>()": "/**\n* Constructs a ZlibCompressor with default settings.\n* Initializes with default compression level, strategy, header, and buffer size.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ZlibCompressor with configuration settings.\n* @param conf configuration object for compression settings\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes compressor with new configuration settings.\n* @param conf the configuration object for compression\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibDecompressor": {
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:isNativeZlibLoaded()": "/**\n* Checks if the native Zlib library is loaded.\n* @return true if loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finished()": "/**\n* Checks if compression is complete and all data has been consumed.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInputFromSavedData()": "/**\n* Sets input data for zlib from saved user buffer.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary for the stream using a byte array.\n* @param b byte array containing the dictionary data\n* @param off starting offset in the array\n* @param len number of bytes to use from the array\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:decompress(byte[],int,int)": "/**\n* Decompresses data into the provided byte array.\n* @param b byte array to store decompressed data\n* @param off offset in the array to start storing data\n* @param len maximum number of bytes to decompress\n* @return number of bytes decompressed\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:checkStream()": "/**\n* Validates the stream; throws NullPointerException if stream is zero.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:end()": "/**\n* Ends the current stream if it's active, resetting the stream to zero.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:inflateDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decompresses data from src ByteBuffer to dst ByteBuffer.\n* @param src compressed data buffer\n* @param dst destination buffer for uncompressed data\n* @return number of bytes decompressed\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsDictionary()": "/**\n* Indicates if a dictionary is required.\n* @return true if a dictionary is needed, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)": "/**\n* Initializes ZlibDecompressor with header and buffer size.\n* @param header CompressionHeader for decompression settings\n* @param directBufferSize size for direct buffers\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)": "/**\n* Sets input data for processing with validation.\n* @param b byte array input, @param off offset, @param len length of data\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput()": "/**\n* Determines if additional input is needed for processing.\n* @return true if no input is left, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten()": "/**\n* Returns the number of bytes written to the stream.\n* @return long representing bytes written\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead()": "/**\n* Returns the number of bytes read from the stream.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining()": "/**\n* Calculates remaining buffer length after stream validation.\n* @return total remaining length from user buffer and stream\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset()": "/**\n* Resets the stream and internal state variables.\n* @throws NullPointerException if stream is invalid\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize()": "/**\n* Cleans up resources by ending the current stream on finalization.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>()": "/**\n* Constructs a ZlibDecompressor with default settings.\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel": {
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:compressionLevel()": "/**\n* Retrieves the current compression level.\n* @return integer representing the compression level\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater": {
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>(int)": "/**\n* Constructs a BuiltInZlibDeflater with specified compression level.\n* @param level compression level (0-9) for the deflater\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>(int,boolean)": "/**\n* Constructs a BuiltInZlibDeflater with specified compression level and nowrap option.\n* @param level compression level (0-9)\n* @param nowrap if true, disables zlib header/footer\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>()": "/**\n* Constructs a BuiltInZlibDeflater instance.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:compress(byte[],int,int)": "/**\n* Compresses a byte array using deflation.\n* @param b byte array to compress\n* @param off offset to start compression\n* @param len length of bytes to compress\n* @return number of bytes written to the array\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes compressor with new configuration settings.\n* @param conf configuration object for compression settings\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater": {
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:<init>()": "/**\n* Constructs a BuiltInZlibInflater instance.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:<init>(boolean)": "/**\n* Constructs a BuiltInZlibInflater with optional header processing.\n* @param nowrap if true, skips zlib header and checksum\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:decompress(byte[],int,int)": "/**\n* Decompresses data into the provided byte array.\n* @param b byte array to store decompressed data\n* @param off offset to start writing in the array\n* @param len number of bytes to decompress\n* @return number of bytes decompressed\n* @throws IOException if decompression fails\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy": {
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:compressionStrategy()": "/**\n* Returns the current compression strategy value.\n* @return an integer representing the compression strategy\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader": {
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader:windowBits()": "/**\n* Returns the value of windowBits.\n* @return integer representing the windowBits value\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor": {
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:finished()": "/**\n* Checks if the decompression process is complete.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:needsInput()": "/**\n* Checks if input is needed for deflation process.\n* @return true if input is needed, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:writeHeader(byte[],int,int)": "/**\n* Writes part of the GZIP header to the output buffer.\n* @param b byte array to write the header to\n* @param off offset in the byte array\n* @param len maximum length of header to write\n* @return number of bytes written\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:fillTrailer()": "/**\n* Fills the GZIP trailer with CRC and buffer length if in TRAILER_CRC state.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:writeTrailer(byte[],int,int)": "/**\n* Writes GZIP trailer data to the specified byte array.\n* @param b destination byte array\n* @param off offset in the array\n* @param len number of bytes to write\n* @return number of bytes written\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:getBytesRead()": "/**\n* Returns the total number of input bytes read by the deflater.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:getBytesWritten()": "/**\n* Returns the total number of bytes written.\n* @return total bytes including extra and deflated output\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:end()": "/**\n* Ends the deflation process and updates the state to ENDED.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:finish()": "/**\n* Completes the compression process for the deflater.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reset()": "/**\n* Resets the decompressor state and associated buffers for reuse.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary for compression.\n* @param b byte array containing the dictionary\n* @param off offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:setInput(byte[],int,int)": "/**\n* Sets input data for compression and updates CRC-32 checksum.\n* @param b byte array input data, @param off offset, @param len length of data\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes deflater with compression level and strategy from configuration.\n* @param conf configuration object\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)": "/**\n* Compresses data into GZIP format.\n* @param b byte array for compressed output\n* @param off offset in the output array\n* @param len number of bytes to compress\n* @return number of bytes written to the output array\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a BuiltInGzipCompressor using the provided configuration.\n* @param conf configuration object for initialization\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration)": "/**\n* Reinitializes the object with the given configuration.\n* @param conf configuration object for initialization\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor": {
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decompresses data from source ByteBuffer to destination ByteBuffer.\n* @param src source buffer containing compressed data\n* @param dst destination buffer for decompressed data\n* @throws IOException if an I/O error occurs during decompression\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary using a byte array, not supported in DirectDecompressor.\n* @param b byte array containing the dictionary\n* @param off offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:decompress(byte[],int,int)": "/**\n* Throws exception for unsupported byte array decompression.\n* @param b byte array to decompress (not supported)\n* @param off offset in the array\n* @param len length of the data to decompress\n* @return UnsupportedOperationException\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished()": "/**\n* Checks if compression is complete and all data has been consumed.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>()": "/**\n* Constructs a ZlibDirectDecompressor with default settings.\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)": "/**\n* Constructs ZlibDirectDecompressor with specified header and buffer size.\n* @param header CompressionHeader for decompression settings\n* @param directBufferSize size for direct buffers\n*/",
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset()": "/**\n* Resets the stream and marks end of input as true.\n* @throws NullPointerException if stream is invalid\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader": {
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:windowBits()": "/**\n* Retrieves the current windowBits value.\n* @return the windowBits integer value\n*/"
    },
    "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor": {
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsInput()": "/**\n* Determines if input is needed for decompression.\n* @return true if input is required, false if finished\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:setInput(byte[],int,int)": "/**\n* Sets input buffer with specified offset and length.\n* @param b byte array input, @param off offset, @param len length of input\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndCopyBytesToLocal(int)": "/**\n* Copies bytes from user buffer to local buffer and updates offsets and CRC.\n* @param len number of bytes to copy\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUShortLE(byte[],int)": "/**\n* Reads an unsigned short from a byte array in little-endian order.\n* @param b byte array to read from\n* @param off offset in the array\n* @return unsigned short value as an int\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndSkipBytes(int)": "/**\n* Updates CRC and adjusts buffer offsets after skipping specified bytes.\n* @param len number of bytes to skip in the buffer\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndSkipBytesUntilNull()": "/**\n* Checks and skips bytes in the buffer until a null byte is found.\n* @return true if a null byte is encountered, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:copyBytesToLocal(int)": "/**\n* Copies bytes from user buffer to local buffer and updates offsets and byte counts.\n* @param len number of bytes to copy\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUIntLE(byte[],int)": "/**\n* Reads a 4-byte unsigned integer from a byte array in little-endian order.\n* @param b byte array containing the integer\n* @param off offset to start reading from\n* @return the unsigned integer value as a long\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:getBytesRead()": "/**\n* Computes total bytes read from the header, inflater, and trailer.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsDictionary()": "/**\n* Checks if a dictionary is needed for inflation.\n* @return true if a dictionary is required, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary for the inflater.\n* @param b byte array containing the dictionary\n* @param off offset in the array\n* @param len length of the dictionary\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:reset()": "/**\n* Resets the Gzip decompressor state and buffers to initial conditions.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:end()": "/**\n* Ends the GZIP compression process and updates the state to ENDED.\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUByte(byte[],int)": "/**\n* Reads an unsigned byte from the specified array at the given offset.\n* @param b byte array containing data\n* @param off offset to read from\n* @return unsigned byte value as an int\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:finished()": "/**\n* Checks if the Gzip process is finished.\n* @return true if finished, false otherwise\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:getRemaining()": "/**\n* Returns the remaining buffer length.\n* @return remaining buffer length as an integer\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>()": "",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState()": "/**\n* Processes gzip trailer state, verifying CRC and decompressed size.\n* @throws IOException if CRC or size verification fails\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader()": "/**\n* Validates and processes the GZIP header from the local buffer.\n* @throws IOException if the header is invalid or not in GZIP format\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState()": "/**\n* Processes GZIP header states and manages transitions based on user buffer content.\n* @throws IOException if a CRC failure occurs during header validation\n*/",
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)": "/**\n* Decompresses data from a byte array into the provided buffer.\n* @param b destination buffer for decompressed data\n* @param off offset in the buffer to start writing\n* @param len maximum number of bytes to write\n* @return number of bytes decompressed\n*/"
    },
    "org.apache.hadoop.io.compress.SnappyCodec": {
        "org.apache.hadoop.io.compress.SnappyCodec:createDirectDecompressor()": "/**\n* Creates a new instance of SnappyDirectDecompressor.\n* @return DirectDecompressor instance for decompression\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration to be set\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:getCompressorType()": "/**\n* Returns the class type of the compressor used.\n* @return Class of the compressor, specifically SnappyCompressor\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:getDecompressorType()": "/**\n* Returns the class type of the decompressor.\n* @return Class of the SnappyDecompressor\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:getDefaultExtension()": "/**\n* Returns the default codec extension for Snappy.\n* @return default codec extension as a String\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream for writing compressed data.\n* @param out the OutputStream to write to\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream from the provided InputStream.\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a CompressionOutputStream with specified output and compressor.\n* @param out output stream for compressed data\n* @param compressor compressor to use\n* @return CompressionOutputStream for writing compressed data\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:createCompressor()": "/**\n* Creates a SnappyCompressor with a specified buffer size from configuration.\n* @return SnappyCompressor instance\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream for decompression.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @return CompressionInputStream for reading decompressed data\n*/",
        "org.apache.hadoop.io.compress.SnappyCodec:createDecompressor()": "/**\n* Creates a SnappyDecompressor with buffer size from configuration.\n* @return SnappyDecompressor instance\n*/"
    },
    "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool": {
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBufferTree(boolean)": "/**\n* Retrieves a TreeMap of ByteBuffers based on allocation type.\n* @param isDirect true for direct buffers, false for heap buffers\n* @return TreeMap of WeakReferences to ByteBuffers\n*/",
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:release()": "/**\n* Clears heap and direct buffers to release resources.\n*/",
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getCurrentBuffersCount(boolean)": "/**\n* Returns the count of current buffers based on type.\n* @param isDirect true for direct buffers, false for heap buffers\n* @return count of buffers as an integer\n*/",
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)": "/**\n* Retrieves a ByteBuffer from the pool or allocates a new one if unavailable.\n* @param direct true for direct buffer, false for heap buffer\n* @param length required buffer size\n* @return ByteBuffer from pool or newly allocated buffer\n*/",
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer)": "/**\n* Adds a ByteBuffer to a synchronized buffer tree by capacity and timestamp.\n* @param buffer the ByteBuffer to be added\n*/"
    },
    "org.apache.hadoop.io.Text$1": {
        "org.apache.hadoop.io.Text$1:<init>()": "/**\n* Constructs a new Text object.\n*/"
    },
    "org.apache.hadoop.io.Text$2": {
        "org.apache.hadoop.io.Text$2:<init>()": "/**\n* Constructs a new instance of the Text class.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$UncompressedBytes": {
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:<init>()": "/**\n* Initializes an instance of UncompressedBytes with null data and size zero.\n*/",
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:reset(java.io.DataInputStream,int)": "/**\n* Resets data buffer and reads new data from input stream.\n* @param in input stream to read data from\n* @param length number of bytes to read into the buffer\n*/",
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:writeUncompressedBytes(java.io.DataOutputStream)": "/**\n* Writes uncompressed bytes to the output stream.\n* @param outStream the output stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:writeCompressedBytes(java.io.DataOutputStream)": "/**\n* Throws an exception as uncompressed bytes cannot be compressed.\n* @param outStream output stream for writing data\n* @throws IllegalArgumentException if compression is attempted\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:getSize()": "/**\n* Returns the current size of the data.\n* @return the size as an integer\n*/"
    },
    "org.apache.hadoop.io.DoubleWritable$Comparator": {
        "org.apache.hadoop.io.DoubleWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays as doubles.\n* @param b1 first byte array, @param s1 start index, @param l1 length\n* @param b2 second byte array, @param s2 start index, @param l2 length\n* @return negative, zero, or positive if b1 <, =, or > b2\n*/",
        "org.apache.hadoop.io.DoubleWritable$Comparator:<init>()": "/**\n* Initializes a Comparator for DoubleWritable keys.\n*/"
    },
    "org.apache.hadoop.io.CompressedWritable": {
        "org.apache.hadoop.io.CompressedWritable:<init>()": "/**\n* Default constructor for CompressedWritable class.\n*/",
        "org.apache.hadoop.io.CompressedWritable:readFields(java.io.DataInput)": "/**\n* Reads compressed data from input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.CompressedWritable:ensureInflated()": "/**\n* Decompresses and reads fields if compressed data is present.\n*/",
        "org.apache.hadoop.io.CompressedWritable:write(java.io.DataOutput)": "/**\n* Writes compressed data to output.\n* @param out DataOutput stream to write compressed data\n* @throws IOException if an I/O error occurs during writing\n*/"
    },
    "org.apache.hadoop.io.MD5Hash$Comparator": {
        "org.apache.hadoop.io.MD5Hash$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays based on specified offsets and lengths.\n* @param b1 first byte array\n* @param s1 start index for first array\n* @param l1 length for first array\n* @param b2 second byte array\n* @param s2 start index for second array\n* @param l2 length for second array\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.MD5Hash$Comparator:<init>()": "/**\n* Initializes a new Comparator for MD5Hash keys.\n*/"
    },
    "org.apache.hadoop.io.AbstractMapWritable": {
        "org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class,byte)": "/**\n* Adds a class-ID mapping, ensuring no conflicts exist.\n* @param clazz class to map\n* @param id unique identifier for the class\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:getClass(byte)": "/**\n* Retrieves the Class associated with the given byte ID.\n* @param id byte identifier for the Class\n* @return Class<?> corresponding to the ID or null if not found\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:getId(java.lang.Class)": "/**\n* Retrieves the ID for the given class.\n* @param clazz the class to find the ID for\n* @return the ID as a byte, or -1 if not found\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing the settings\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration to be set\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:getNewClasses()": "/**\n* Retrieves the number of new classes.\n* @return byte representing the count of new classes\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class)": "/**\n* Adds a class to the map with a unique ID if not already present.\n* @param clazz class to map\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:<init>()": "/**\n* Initializes AbstractMapWritable and maps class types to unique IDs.\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput)": "/**\n* Reads class data from input stream and maps classes to IDs.\n* @param in input stream for reading class information\n* @throws IOException if reading fails or class not found\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput)": "/**\n* Writes the class table size and names of unknown classes to output.\n* @param out DataOutput stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable)": "/**\n* Copies data from another Writable instance.\n* @param other source Writable to copy data from\n* @throws IllegalArgumentException if other is null or an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.ShortWritable$Comparator": {
        "org.apache.hadoop.io.ShortWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays as unsigned shorts.\n* @param b1 first byte array, s1 start index, l1 length\n* @param b2 second byte array, s2 start index, l2 length\n* @return negative if b1 < b2, zero if equal, positive if b1 > b2\n*/",
        "org.apache.hadoop.io.ShortWritable$Comparator:<init>()": "/**\n* Initializes a Comparator for ShortWritable type keys.\n*/"
    },
    "org.apache.hadoop.io.FloatWritable$Comparator": {
        "org.apache.hadoop.io.FloatWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays as floats.\n* @param b1 first byte array, @param s1 start index, @param l1 length\n* @param b2 second byte array, @param s2 start index, @param l2 length\n* @return comparison result of the two floats\n*/",
        "org.apache.hadoop.io.FloatWritable$Comparator:<init>()": "/**\n* Constructs a Comparator for FloatWritable keys.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Metadata": {
        "org.apache.hadoop.io.SequenceFile$Metadata:<init>(java.util.TreeMap)": "/**\n* Initializes Metadata with provided TreeMap or an empty map if null.\n* @param arg TreeMap of Text key-value pairs\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:get(org.apache.hadoop.io.Text)": "/**\n* Retrieves the Text object associated with the given name.\n* @param name the key to look up the Text object\n* @return the corresponding Text object or null if not found\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:set(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)": "/**\n* Sets a metadata entry with the specified name and value.\n* @param name  the key for the metadata entry\n* @param value the value associated with the key\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:getMetadata()": "/**\n* Returns a copy of the metadata as a TreeMap.\n* @return TreeMap containing metadata key-value pairs\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:hashCode()": "/**\n* Returns a constant hash code for the object.\n* @return arbitrary integer hash code\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:<init>()": "/**** Initializes Metadata with an empty TreeMap of Text key-value pairs. */",
        "org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput)": "/**\n* Writes metadata size and entries to output stream.\n* @param out output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:toString()": "/**\n* Returns a string representation of the object's metadata.\n* @return formatted string with size and key-value pairs\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput)": "/**\n* Reads metadata from input stream and populates theMetadata map.\n* @param in DataInput stream to read from\n* @throws IOException if size is negative or an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Compares this Metadata object with another for equality.\n* @param other Metadata object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object)": "/**\n* Checks equality with another object, ensuring type compatibility.\n* @param other object to compare for equality\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.io.MultipleIOException$Builder": {
        "org.apache.hadoop.io.MultipleIOException$Builder:add(java.lang.Throwable)": "/**\n* Adds a Throwable to the exceptions list as an IOException.\n* @param t the Throwable to add\n*/",
        "org.apache.hadoop.io.MultipleIOException$Builder:isEmpty()": "/**\n* Checks if the exceptions list is empty or null.\n* @return true if exceptions is null or empty, false otherwise\n*/",
        "org.apache.hadoop.io.MultipleIOException$Builder:build()": "/****\n* Builds an IOException from a list of exceptions.\n* @return IOException or null if exceptions list is empty\n*/"
    },
    "org.apache.hadoop.io.VLongWritable": {
        "org.apache.hadoop.io.VLongWritable:<init>()": "/**\n* Default constructor for VLongWritable class.\n*/",
        "org.apache.hadoop.io.VLongWritable:set(long)": "/**\n* Sets the internal value to the specified long.\n* @param value the long value to set\n*/",
        "org.apache.hadoop.io.VLongWritable:toString()": "/**\n* Returns the string representation of the long value.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.io.VLongWritable:compareTo(org.apache.hadoop.io.VLongWritable)": "/**\n* Compares this VLongWritable with another for order.\n* @param o VLongWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput)": "/**\n* Writes a variable-length long value to the output stream.\n* @param out output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.VLongWritable:<init>(long)": "/**\n* Initializes VLongWritable with a specified long value.\n* @param value the long value to set\n*/",
        "org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and decodes a variable-length long.\n* @param in DataInput stream to read from\n*/"
    },
    "org.apache.hadoop.io.serializer.SerializationFactory": {
        "org.apache.hadoop.io.serializer.SerializationFactory:getSerialization(java.lang.Class)": "/**\n* Retrieves Serialization for the specified class type.\n* @param c class type to find a matching Serialization\n* @return Serialization instance or null if not found\n*/",
        "org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class)": "/**\n* Retrieves a Serializer for the specified class type.\n* @param c class type to find a matching Serializer\n* @return Serializer instance or null if not found\n*/",
        "org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class)": "/**\n* Retrieves a Deserializer for the specified class type.\n* @param c class type to find a matching Deserializer\n* @return Deserializer instance or null if not found\n*/",
        "org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**** Adds a new Serialization instance from configuration by its name. \n* @param conf configuration containing class information \n* @param serializationName name of the serialization class \n*/",
        "org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes SerializationFactory with serializers from configuration.\n* @param conf configuration object containing serializer names\n*/"
    },
    "org.apache.hadoop.io.serializer.JavaSerializationComparator": {
        "org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.io.Serializable,java.io.Serializable)": "/**\n* Compares two objects for order.\n* @param o1 first object to compare\n* @param o2 second object to compare\n* @return negative if o1 < o2, zero if equal, positive if o1 > o2\n*/",
        "org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)": "/**\n* Compares two objects of type T for order.\n* @param o1 first object to compare\n* @param o2 second object to compare\n* @return negative if o1 < o2, zero if equal, positive if o1 > o2\n*/",
        "org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>()": "/**\n* Constructs a JavaSerializationComparator using a JavaDeserializer.\n* @throws IOException if an error occurs during initialization\n*/"
    },
    "org.apache.hadoop.io.serializer.avro.AvroSerialization": {
        "org.apache.hadoop.io.serializer.avro.AvroSerialization:getDeserializer(java.lang.Class)": "/**\n* Returns a deserializer for the specified class type.\n* @param c class type for deserialization\n* @return Deserializer instance for the given class\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSerialization:getSerializer(java.lang.Class)": "/**\n* Returns a serializer for the specified class type.\n* @param c the class type to serialize\n* @return Serializer instance for the given class\n*/"
    },
    "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer": {
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:close()": "/**\n* Closes the input stream to release resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:deserialize(java.lang.Object)": "/**\n* Deserializes an object of type T using a reader and decoder.\n* @param t object to deserialize\n* @return deserialized object of type T\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:open(java.io.InputStream)": "/**\n* Initializes input stream and decoder for binary data processing.\n* @param in InputStream to read binary data from\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization": {
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getReader(java.lang.Class)": "/**\n* Creates a DatumReader for the specified class type.\n* @param clazz the class type for the DatumReader\n* @return a DatumReader instance for the given class\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getSchema(java.lang.Object)": "/**\n* Retrieves the schema for the given object.\n* @param t the object for which to retrieve the schema\n* @return Schema associated with the object's class\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getWriter(java.lang.Class)": "/**\n* Returns a DatumWriter instance for the specified class type.\n* @param clazz the class type for which the writer is created\n* @return a new ReflectDatumWriter instance\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages()": "/**\n* Collects and stores unique package names from configuration.\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class)": "/**\n* Checks if the class is serializable or belongs to a specified package.\n* @param c the class to check\n* @return true if serializable or in the package, false otherwise\n*/"
    },
    "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization": {
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:accept(java.lang.Class)": "/**\n* Checks if the given class is a subtype of SpecificRecord.\n* @param c class to check\n* @return true if c is a SpecificRecord subtype, false otherwise\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getReader(java.lang.Class)": "/**\n* Creates a DatumReader for the specified SpecificRecord class.\n* @param clazz the SpecificRecord class to read\n* @return a DatumReader instance for the given class\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getSchema(org.apache.avro.specific.SpecificRecord)": "/**\n* Retrieves the schema from a SpecificRecord instance.\n* @param t SpecificRecord object to extract the schema from\n* @return Schema associated with the SpecificRecord\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getWriter(java.lang.Class)": "/**\n* Returns a DatumWriter for the specified SpecificRecord class.\n* @param clazz the SpecificRecord class type\n* @return a SpecificDatumWriter instance\n*/"
    },
    "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer": {
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:close()": "/**\n* Closes the output stream and flushes the encoder.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:open(java.io.OutputStream)": "/**\n* Initializes the output stream and encoder for binary data.\n* @param out OutputStream to write binary data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:serialize(java.lang.Object)": "/**\n* Serializes the given object to the output using a specific schema.\n* @param t object to serialize\n* @throws IOException if an I/O error occurs during serialization\n*/"
    },
    "org.apache.hadoop.io.serializer.WritableSerialization": {
        "org.apache.hadoop.io.serializer.WritableSerialization:accept(java.lang.Class)": "/**\n* Checks if a class is assignable from Writable.\n* @param c the class to check\n* @return true if c is a Writable subclass, false otherwise\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization:getSerializer(java.lang.Class)": "/**\n* Returns a serializer for the specified Writable class.\n* @param c class type of Writable\n* @return WritableSerializer instance\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class)": "/**\n* Returns a Writable deserializer for the specified class type.\n* @param c class type to create a deserializer for\n* @return Deserializer instance for the given class type\n*/"
    },
    "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer": {
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:open(java.io.OutputStream)": "/**\n* Initializes ObjectOutputStream without a header.\n* @param out OutputStream to write serialized objects\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:serialize(java.io.Serializable)": "/**\n* Serializes the given object, resetting back-references.\n* @param object the Serializable object to serialize\n* @throws IOException if an I/O error occurs during serialization\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:close()": "/**\n* Closes the output stream, releasing any system resources.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer": {
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:open(java.io.OutputStream)": "/**\n* Initializes DataOutputStream from the provided OutputStream.\n* @param out the output stream to be wrapped\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:serialize(org.apache.hadoop.io.Writable)": "/**\n* Serializes data to the given Writable object.\n* @param w Writable target for serialization\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:close()": "/**\n* Closes the data output stream.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer": {
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:open(java.io.InputStream)": "/**\n* Initializes ObjectInputStream without a header from the given InputStream.\n* @param in InputStream to read objects from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.io.Serializable)": "/**\n* Deserializes an object from an input stream.\n* @param object ignored; returns the deserialized object\n* @return deserialized object of type T\n* @throws IOException if deserialization fails\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:close()": "/**\n* Closes the ObjectInputStream resource.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object)": "/**\n* Deserializes an object from an input stream.\n* @param object ignored; returns the deserialized object\n* @return deserialized object of type T\n* @throws IOException if deserialization fails\n*/"
    },
    "org.apache.hadoop.io.serializer.JavaSerialization": {
        "org.apache.hadoop.io.serializer.JavaSerialization:accept(java.lang.Class)": "/**\n* Checks if the given class implements Serializable.\n* @param c class to check for Serializable implementation\n* @return true if c is Serializable, false otherwise\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization:getDeserializer(java.lang.Class)": "/**\n* Returns a deserializer for the specified Serializable class.\n* @param c class of the Serializable type\n* @return a JavaSerializationDeserializer instance\n*/",
        "org.apache.hadoop.io.serializer.JavaSerialization:getSerializer(java.lang.Class)": "/**\n* Returns a serializer for the specified Serializable class.\n* @param c the class of the Serializable type\n* @return a JavaSerializationSerializer instance\n*/"
    },
    "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer": {
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:open(java.io.InputStream)": "/**\n* Initializes DataInputStream from the provided InputStream.\n* @param in InputStream to read data from\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:close()": "/**\n* Closes the input stream to release resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)": "/**\n* Constructs a WritableDeserializer with specified configuration and class type.\n* @param conf configuration settings\n* @param c class type for deserialization\n*/",
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable)": "/**\n* Deserializes a Writable object or creates a new instance if null.\n* @param w Writable object to deserialize or null to create a new one\n* @return deserialized Writable object\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption:<init>(org.apache.hadoop.fs.FileSystem)": "/**\n* Constructs a FileSystemOption with the specified FileSystem.\n* @param value the FileSystem to be associated with this option\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption:getValue()": "/**\n* Retrieves the current FileSystem value.\n* @return the FileSystem instance\n*/"
    },
    "org.apache.hadoop.io.DataInputByteBuffer": {
        "org.apache.hadoop.io.DataInputByteBuffer:<init>(org.apache.hadoop.io.DataInputByteBuffer$Buffer)": "/**\n* Initializes DataInputByteBuffer with the given buffer.\n* @param buffers the buffer to read data from\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[])": "/**\n* Resets the buffer indices using provided ByteBuffers.\n* @param input array of ByteBuffers to be processed\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer:<init>()": "/**\n* Constructs a DataInputByteBuffer with a new Buffer instance.\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer:getData()": "/**\n* Retrieves an array of ByteBuffer objects.\n* @return array of ByteBuffer instances from buffers\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer:getPosition()": "/**\n* Retrieves the current position from buffers.\n* @return current position as an integer\n*/",
        "org.apache.hadoop.io.DataInputByteBuffer:getLength()": "/**\n* Returns the length of the buffers.\n* @return the current length as an integer\n*/"
    },
    "org.apache.hadoop.util.bloom.Key": {
        "org.apache.hadoop.util.bloom.Key:<init>()": "/**\n* Default constructor for the Key class.\n*/",
        "org.apache.hadoop.util.bloom.Key:set(byte[],double)": "/**\n* Sets byte array and weight; throws if value is null.\n* @param value byte array to set\n* @param weight double representing the weight\n*/",
        "org.apache.hadoop.util.bloom.Key:getBytes()": "/**\n* Retrieves the byte array.\n* @return byte array containing data\n*/",
        "org.apache.hadoop.util.bloom.Key:hashCode()": "/**\n* Computes hash code based on byte array and weight.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.util.bloom.Key:write(java.io.DataOutput)": "/**\n* Writes byte array length, bytes, and weight to DataOutput.\n* @param out the DataOutput stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.bloom.Key:readFields(java.io.DataInput)": "/**\n* Reads byte array and weight from DataInput stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.util.bloom.Key:getWeight()": "/**\n* Retrieves the weight of the object.\n* @return current weight as a double value\n*/",
        "org.apache.hadoop.util.bloom.Key:<init>(byte[],double)": "/**\n* Constructs a Key object with specified byte array and weight.\n* @param value byte array to set\n* @param weight double representing the weight\n*/",
        "org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key)": "/**\n* Compares this Key object with another for order.\n* @param other the Key object to compare against\n* @return negative, zero, or positive integer as this is less than, equal to, or greater than other\n*/",
        "org.apache.hadoop.util.bloom.Key:<init>(byte[])": "/**\n* Constructs a Key object with specified byte array.\n* @param value byte array to set\n*/",
        "org.apache.hadoop.util.bloom.Key:equals(java.lang.Object)": "/**\n* Checks equality of this Key with another object.\n* @param o object to compare\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$RecordCompressWriter": {
        "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)": "/**\n* Appends a key-value pair to output stream.\n* @param keyData byte array of key data\n* @param keyOffset starting index in keyData\n* @param keyLength length of the key\n* @param val ValueBytes object containing value data\n* @throws IOException if keyLength is negative or write fails\n*/",
        "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)": "/**** Appends a key-value pair to the output stream after validation and compression. \n* @param key the key to append, must match keyClass type \n* @param val the value to append, must match valClass type \n*/",
        "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])": "/**\n* Initializes RecordCompressWriter with configuration and options.\n* @param conf configuration settings\n* @param options variable options for file handling\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Initializes CompressionOption with specified type and codec.\n* @param value compression type; defaults to NONE if codec is null\n* @param codec compression codec; uses DefaultCodec if value is not NONE and codec is null\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:getValue()": "/**\n* Retrieves the current compression type.\n* @return CompressionType instance representing the value\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:getCodec()": "/**\n* Retrieves the compression codec used.\n* @return CompressionCodec instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Constructs CompressionOption with specified type and default codec.\n* @param value compression type; defaults to NONE if codec is null\n*/"
    },
    "org.apache.hadoop.io.DefaultStringifier": {
        "org.apache.hadoop.io.DefaultStringifier:close()": "/**\n* Closes all buffered streams and serializers.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String)": "/**\n* Converts a Base64 string to an object.\n* @param str Base64 encoded string\n* @return Deserialized object of type T\n*/",
        "org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object)": "/**\n* Serializes an object to a Base64 encoded string.\n* @param obj the object to serialize\n* @return Base64 encoded string representation of the object\n*/",
        "org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)": "/**\n* Initializes DefaultStringifier with serializer and deserializer for class type.\n* @param conf configuration settings\n* @param c class type for serialization\n*/",
        "org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)": "/**** Stores an item as a Base64 string in the configuration. \n* @param conf configuration settings \n* @param item the item to store \n* @param keyName the key under which to store the item \n* @throws IOException if an I/O error occurs during storage \n*/",
        "org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)": "/**** Loads an object of type K from configuration by key name. \n* @param conf configuration settings \n* @param keyName the key name to fetch the value \n* @param itemClass class type for the object \n* @return object of type K or null if not found \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)": "/**\n* Stores an array of items in configuration under a specified key.\n* @param conf configuration settings\n* @param items array of items to store\n* @param keyName key under which items are stored\n* @throws IOException if an I/O error occurs during storage\n*/",
        "org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)": "/**\n* Loads an array of type K from configuration by key name.\n* @param conf configuration settings\n* @param keyName key to retrieve value\n* @param itemClass class type of array elements\n* @return array of type K populated from configuration\n*/"
    },
    "org.apache.hadoop.util.GenericsUtil": {
        "org.apache.hadoop.util.GenericsUtil:getClass(java.lang.Object)": "/**\n* Returns the Class object of the provided instance.\n* @param t instance of type T\n* @return Class<T> representing the type of the instance\n*/",
        "org.apache.hadoop.util.GenericsUtil:toArray(java.lang.Class,java.util.List)": "/**\n* Converts a List to an array of specified type.\n* @param c class type of the array elements\n* @param list input List to convert\n* @return array of type T containing list elements\n*/",
        "org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.String)": "/**\n* Checks if the given logger is a Log4j logger.\n* @param logger name of the logger to check\n* @return true if it is a Log4j logger, false otherwise\n*/",
        "org.apache.hadoop.util.GenericsUtil:toArray(java.util.List)": "/**\n* Converts a List to an array of the same type.\n* @param list input List to convert\n* @return array of type T containing list elements\n*/",
        "org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class)": "/**\n* Checks if the given class is a Log4j logger.\n* @param clazz the class to check\n* @return true if it's a Log4j logger, false if clazz is null or not a Log4j logger\n*/"
    },
    "org.apache.hadoop.util.Options": {
        "org.apache.hadoop.util.Options:prependOptions(java.lang.Object[],java.lang.Object[])": "/**\n* Prepends new options to an existing array.\n* @param oldOpts the original array of options\n* @param newOpts options to prepend\n* @return a new array with newOpts followed by oldOpts\n*/"
    },
    "org.apache.hadoop.io.InputBuffer$Buffer": {
        "org.apache.hadoop.io.InputBuffer$Buffer:<init>()": "/**\n* Initializes a new empty Buffer instance.\n*/",
        "org.apache.hadoop.io.InputBuffer$Buffer:reset(byte[],int,int)": "/**\n* Resets buffer state with new input data.\n* @param input byte array to set as buffer\n* @param start starting index for the buffer\n* @param length number of bytes to read from input\n*/",
        "org.apache.hadoop.io.InputBuffer$Buffer:getPosition()": "/**\n* Returns the current position value.\n* @return the current position as an integer\n*/",
        "org.apache.hadoop.io.InputBuffer$Buffer:getLength()": "/**\n* Returns the current length value.\n* @return the length as an integer\n*/"
    },
    "org.apache.hadoop.io.InputBuffer": {
        "org.apache.hadoop.io.InputBuffer:<init>(org.apache.hadoop.io.InputBuffer$Buffer)": "/**\n* Constructs an InputBuffer with the specified Buffer.\n* @param buffer the Buffer to initialize the InputBuffer\n*/",
        "org.apache.hadoop.io.InputBuffer:<init>()": "/**\n* Initializes a new InputBuffer with a default Buffer instance.\n*/",
        "org.apache.hadoop.io.InputBuffer:reset(byte[],int)": "/**\n* Resets buffer with new input data.\n* @param input byte array to set as buffer\n* @param length number of bytes to read from input\n*/",
        "org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)": "/**\n* Resets the buffer state with new input data.\n* @param input byte array to set as buffer\n* @param start starting index for the buffer\n* @param length number of bytes to read from input\n*/",
        "org.apache.hadoop.io.InputBuffer:getPosition()": "/**\n* Retrieves the current position from the buffer.\n* @return current position as an integer\n*/",
        "org.apache.hadoop.io.InputBuffer:getLength()": "/**\n* Retrieves the current length of the buffer.\n* @return the length of the buffer as an integer\n*/"
    },
    "org.apache.hadoop.io.MD5Hash$1": {
        "org.apache.hadoop.io.MD5Hash$1:<init>()": "/**\n* Initializes an MD5Hash instance with a byte array for the MD5 digest.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption:<init>(org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Initializes MetadataOption with the given Metadata value.\n* @param value Metadata object to be assigned\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption:getValue()": "/**\n* Retrieves the current metadata value.\n* @return Metadata object representing the value\n*/"
    },
    "org.apache.hadoop.util.Options$FSDataOutputStreamOption": {
        "org.apache.hadoop.util.Options$FSDataOutputStreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream)": "/**\n* Constructs FSDataOutputStreamOption with a specified FSDataOutputStream.\n* @param value the FSDataOutputStream to be encapsulated\n*/",
        "org.apache.hadoop.util.Options$FSDataOutputStreamOption:getValue()": "/**\n* Retrieves the FSDataOutputStream instance.\n* @return FSDataOutputStream object\n*/"
    },
    "org.apache.hadoop.io.BytesWritable$Comparator": {
        "org.apache.hadoop.io.BytesWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays, excluding a fixed-length prefix.\n* @param b1 first byte array\n* @param s1 start index in b1\n* @param l1 length of b1\n* @param b2 second byte array\n* @param s2 start index in b2\n* @param l2 length of b2\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.BytesWritable$Comparator:<init>()": "/**\n* Initializes a Comparator for BytesWritable class.\n*/"
    },
    "org.apache.hadoop.io.LongWritable$DecreasingComparator": {
        "org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable)": "/**\n* Compares two WritableComparable objects in reverse order.\n* @param a first object to compare\n* @param b second object to compare\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays in reverse order.\n* @param b1 first byte array, b2 second byte array\n* @return comparison result as int\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer": {
        "org.apache.hadoop.io.SequenceFile$Writer:isCompressed()": "/**\n* Checks if the current compression type is not NONE.\n* @return true if compressed, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:isBlockCompressed()": "/**\n* Checks if the current compression type is block compression.\n* @return true if block compressed, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:flush()": "/**\n* Flushes the output stream if it is not null.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:ownStream()": "/**\n* Sets the output stream as owned and returns the current Writer instance.\n* @return this Writer instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:getCompressionCodec()": "/**\n* Retrieves the current compression codec.\n* @return CompressionCodec instance used for compression\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:getConf()": "/**\n* Retrieves the current configuration object.\n* @return Configuration instance or null if not set\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:getKeyClass()": "/**\n* Retrieves the class type of the key.\n* @return Class representing the key type\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:getValueClass()": "/**\n* Returns the class type of the value.\n* @return Class representing the value type\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:syncFs()": "/****\n* Flushes the output stream to the file system.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:hflush()": "/**\n* Flushes the wrapped stream if it's not null.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:hsync()": "/**\n* Synchronizes the output stream if it's not null.\n* @throws IOException if an I/O error occurs during synchronization\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem)": "/**\n* Creates a FileSystemOption for the given FileSystem.\n* @param fs the FileSystem to associate with the option\n* @return FileSystemOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Creates a CompressionOption with specified type and codec.\n* @param value compression type\n* @param codec compression codec\n* @return CompressionOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata)": "/****\n* Creates a MetadataOption from the given Metadata.\n* @param value Metadata object to be used\n* @return MetadataOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:sync()": "/**\n* Synchronizes output if conditions are met, updating last sync position.\n* @throws IOException on output errors\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:getLength()": "/**\n* Returns the current length by fetching the cached position.\n* @return current position as a long\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable)": "/**\n* Creates a ProgressableOption from a given Progressable.\n* @param value the Progressable object to wrap\n* @return ProgressableOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:blockSize(long)": "/**\n* Creates a BlockSizeOption with the specified block size.\n* @param value the block size as a long\n* @return a new BlockSizeOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int)": "/**\n* Creates a SyncIntervalOption with the specified sync interval value.\n* @param value sync interval value to set\n* @return new SyncIntervalOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:replication(short)": "/**\n* Creates a ReplicationOption from a short value.\n* @param value the short value for replication option\n* @return a new ReplicationOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int)": "/**\n* Creates a BufferSizeOption with the specified buffer size.\n* @param value the integer value for buffer size\n* @return BufferSizeOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class)": "/**\n* Creates an Option using the specified class type.\n* @param value the class type to be set\n* @return ValueClassOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class)": "/**\n* Creates a KeyClassOption with the specified class type.\n* @param value the class type to be set\n* @return KeyClassOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Creates a CompressionOption based on the specified CompressionType.\n* @param value compression type to be used\n* @return CompressionOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream)": "/**\n* Creates a StreamOption from the provided FSDataOutputStream.\n* @param value the FSDataOutputStream to be encapsulated\n* @return StreamOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean)": "/**\n* Creates an Option to append if it exists based on the boolean value.\n* @param value determines if appending is enabled\n* @return new AppendIfExistsOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path)": "/**\n* Creates a FileOption from the provided Path.\n* @param value the Path to be associated with the FileOption\n* @return a new FileOption instance\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String)": "/**\n* Checks if the stream has a specific capability.\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync()": "/**\n* Checks conditions to emit sync and updates output if necessary.\n* @throws IOException on output errors\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:close()": "/**\n* Closes serializers and returns compressor to the pool.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)": "/**\n* Appends a key-value pair to the output buffer.\n* @param key the key object to append\n* @param val the value object to append\n* @throws IOException on class mismatch or output errors\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)": "/**\n* Appends raw key-value data to output stream.\n* @param keyData byte array of key data\n* @param keyOffset starting offset in keyData\n* @param keyLength length of the key\n* @param val ValueBytes object to write\n* @throws IOException for negative lengths or output errors\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader()": "/**** Writes the file header including version and metadata. \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)": "/**\n* Appends a key-value pair to the output buffer.\n* @param key the key object to append\n* @param val the value object to append\n* @throws IOException on class mismatch or output errors\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)": "/**\n* Initializes serialization and output stream settings.\n* @param config configuration for serialization settings\n* @param outStream output stream for data\n* @param ownStream indicates if stream is owned\n* @param key class type for keys\n* @param val class type for values\n* @param compCodec compression codec for output\n* @param meta metadata for serialization\n* @param syncIntervalVal interval for synchronization\n* @throws IOException if serialization or stream errors occur\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Constructs a Writer for file output with specified parameters.\n* @param fs file system to write to\n* @param conf configuration settings\n* @param name file path\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param bufferSize size of the buffer\n* @param replication number of file replicas\n* @param blockSize size of each block\n* @param progress progress callback\n* @param metadata serialization metadata\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Constructs a Writer for file output with specified configuration and metadata.\n* @param fs file system to use\n* @param conf configuration settings\n* @param name path for the file\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param progress progress callback\n* @param metadata serialization metadata\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)": "/**\n* Constructs a Writer for the specified file system and path.\n* @param fs file system to write to\n* @param conf configuration settings\n* @param name path to the output file\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])": "/**\n* Constructs a Writer with specified options and initializes output stream settings.\n* @param conf configuration settings\n* @param opts variable options for file handling and stream management\n* @throws IOException if an I/O error occurs during initialization\n*/"
    },
    "org.apache.hadoop.util.ProtoUtil": {
        "org.apache.hadoop.util.ProtoUtil:readRawVarint32(java.io.DataInput)": "/**\n* Reads a 32-bit variable-length integer from input.\n* @param in DataInput stream to read from\n* @return decoded integer value\n* @throws IOException if reading fails\n*/",
        "org.apache.hadoop.util.ProtoUtil:convert(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto)": "/**\n* Converts RpcKindProto to RPC.RpcKind.\n* @param kind RpcKindProto to convert\n* @return corresponding RPC.RpcKind or null if invalid\n*/",
        "org.apache.hadoop.util.ProtoUtil:convert(org.apache.hadoop.ipc.RPC$RpcKind)": "/**\n* Converts RPC kind to its corresponding RpcKindProto.\n* @param kind the RPC kind to convert\n* @return corresponding RpcKindProto or null if not found\n*/",
        "org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates an RpcRequestHeaderProto with tracing and caller context.\n* @param rpcKind type of RPC request\n* @param operation operation type of the request\n* @param callId unique identifier for the call\n* @param retryCount number of retries attempted\n* @param uuid client identifier\n* @param alignmentContext context for request alignment\n* @return constructed RpcRequestHeaderProto object\n*/",
        "org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Creates an IPC connection context with user info and protocol.\n* @param protocol connection protocol\n* @param ugi user group information\n* @param authMethod authentication method\n* @return built IpcConnectionContextProto object\n*/",
        "org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])": "/**\n* Creates an RpcRequestHeaderProto for an RPC request.\n* @param rpcKind type of RPC request\n* @param operation operation type of the request\n* @param callId unique identifier for the call\n* @param retryCount number of retries attempted\n* @param uuid client identifier\n* @return constructed RpcRequestHeaderProto object\n*/",
        "org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto)": "/**\n* Retrieves UserGroupInformation based on user info.\n* @param userInfo user information containing effective and real users\n* @return UserGroupInformation or null if not applicable\n*/",
        "org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto)": "/**\n* Retrieves UserGroupInformation from context if user info is present.\n* @param context IPC connection context with user info\n* @return UserGroupInformation or null if no user info\n*/"
    },
    "org.apache.hadoop.io.BooleanWritable$Comparator": {
        "org.apache.hadoop.io.BooleanWritable$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays from specified offsets and lengths.\n* @param b1 first byte array\n* @param s1 start index in first array\n* @param l1 length of first array segment\n* @param b2 second byte array\n* @param s2 start index in second array\n* @param l2 length of second array segment\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.BooleanWritable$Comparator:<init>()": "/**\n* Initializes a Comparator for BooleanWritable key class.\n*/"
    },
    "org.apache.hadoop.io.MapWritable": {
        "org.apache.hadoop.io.MapWritable:clear()": "/**\n* Clears the contents of the instance.\n*/",
        "org.apache.hadoop.io.MapWritable:containsKey(java.lang.Object)": "/**\n* Checks if the specified key is present in the instance.\n* @param key the key to check for presence\n* @return true if the key exists, false otherwise\n*/",
        "org.apache.hadoop.io.MapWritable:containsValue(java.lang.Object)": "/**\n* Checks if the specified value is present in the instance.\n* @param value the value to search for\n* @return true if the value exists, false otherwise\n*/",
        "org.apache.hadoop.io.MapWritable:entrySet()": "/**\n* Returns a set of entries from the underlying instance.\n* @return Set of Map.Entry with Writable keys and values\n*/",
        "org.apache.hadoop.io.MapWritable:size()": "/**\n* Returns the number of elements in the instance.\n* @return the size of the instance as an integer\n*/",
        "org.apache.hadoop.io.MapWritable:get(java.lang.Object)": "/**\n* Retrieves a Writable object by its key.\n* @param key the identifier used to fetch the Writable\n* @return Writable associated with the key or null if not found\n*/",
        "org.apache.hadoop.io.MapWritable:hashCode()": "/**\n* Computes the hash code based on the instance's hash code.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.io.MapWritable:isEmpty()": "/**\n* Checks if the instance is empty.\n* @return true if empty, false otherwise\n*/",
        "org.apache.hadoop.io.MapWritable:keySet()": "/**\n* Returns the set of keys in the instance.\n* @return Set of Writable keys from the instance\n*/",
        "org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)": "/**\n* Stores a key-value pair in the map and tracks their classes.\n* @param key the key to store\n* @param value the value to store\n* @return previous value associated with key or null if none\n*/",
        "org.apache.hadoop.io.MapWritable:remove(java.lang.Object)": "/**\n* Removes the specified key from the collection.\n* @param key the key to be removed\n* @return the value associated with the key, or null if not found\n*/",
        "org.apache.hadoop.io.MapWritable:values()": "/**\n* Returns a collection of writable values from the instance.\n* @return Collection of Writable objects\n*/",
        "org.apache.hadoop.io.MapWritable:toString()": "/**\n* Returns the string representation of the instance.\n* @return string representation of the instance\n*/",
        "org.apache.hadoop.io.MapWritable:equals(java.lang.Object)": "/**\n* Compares this MapWritable instance to another object for equality.\n* @param obj object to compare with this instance\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.MapWritable:putAll(java.util.Map)": "/**\n* Inserts all key-value pairs from the provided map.\n* @param t map of key-value pairs to insert\n*/",
        "org.apache.hadoop.io.MapWritable:<init>()": "/**\n* Constructs a MapWritable instance with an empty HashMap.\n*/",
        "org.apache.hadoop.io.MapWritable:write(java.io.DataOutput)": "/**** Writes the map entries and their sizes to the DataOutput stream. \n* @param out DataOutput stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable)": "/**\n* Constructs a MapWritable instance by copying from another MapWritable.\n* @param other the MapWritable instance to copy from\n*/",
        "org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)": "/**\n* Reads and populates instance map from DataInput stream.\n* @param in input stream for reading key/value pairs\n* @throws IOException if reading fails\n*/"
    },
    "org.apache.hadoop.util.Options$BooleanOption": {
        "org.apache.hadoop.util.Options$BooleanOption:<init>(boolean)": "/**\n* Constructs a BooleanOption with the specified value.\n* @param value the boolean value to be stored\n*/",
        "org.apache.hadoop.util.Options$BooleanOption:getValue()": "/**\n* Retrieves the current boolean value.\n* @return the current value as a boolean\n*/"
    },
    "org.apache.hadoop.util.Options$PathOption": {
        "org.apache.hadoop.util.Options$PathOption:<init>(org.apache.hadoop.fs.Path)": "/**\n* Constructs a PathOption with the specified Path value.\n* @param value the Path to be associated with this PathOption\n*/",
        "org.apache.hadoop.util.Options$PathOption:getValue()": "/**\n* Retrieves the current value as a Path object.\n* @return Path representing the current value\n*/"
    },
    "org.apache.hadoop.io.BooleanWritable": {
        "org.apache.hadoop.io.BooleanWritable:<init>()": "/**\n* Default constructor for BooleanWritable.\n*/",
        "org.apache.hadoop.io.BooleanWritable:set(boolean)": "/**\n* Sets the value of the boolean field.\n* @param value the boolean value to set\n*/",
        "org.apache.hadoop.io.BooleanWritable:readFields(java.io.DataInput)": "/**\n* Reads a boolean value from the input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.BooleanWritable:write(java.io.DataOutput)": "/**\n* Writes a boolean value to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.BooleanWritable:get()": "/**\n* Returns the current boolean value.\n* @return current boolean value of the object\n*/",
        "org.apache.hadoop.io.BooleanWritable:compareTo(org.apache.hadoop.io.BooleanWritable)": "/**\n* Compares this BooleanWritable with another for order.\n* @param o the BooleanWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.BooleanWritable:<init>(boolean)": "/**\n* Constructs a BooleanWritable object with the specified boolean value.\n* @param value the boolean value to set\n*/",
        "org.apache.hadoop.io.BooleanWritable:toString()": "/**\n* Returns the string representation of the current boolean value.\n* @return string representation of the boolean value\n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor": {
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:isRunning(org.apache.hadoop.util.Daemon)": "/**\n* Checks if the given daemon is currently running.\n* @param d the Daemon instance to check\n* @return true if the daemon is running, false otherwise\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart()": "/**\n* Initiates an asynchronous processing thread if not already running.\n* @throws IllegalStateException if the state check fails\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon)": "/**\n* Kills the specified daemon.\n* @param d the Daemon to be killed; throws IllegalStateException if not running\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon)": "/**\n* Attempts to stop a daemon if the queue is empty for a specified grace period.\n* @param d the Daemon to be killed if conditions are met\n*/"
    },
    "org.apache.hadoop.util.concurrent.AsyncGet$Util": {
        "org.apache.hadoop.util.concurrent.AsyncGet$Util:wait(java.lang.Object,long,java.util.concurrent.TimeUnit)": "/**\n* Waits on the given object for a specified timeout.\n* @param obj the object to wait on\n* @param timeout the maximum wait time\n* @param unit the time unit of the timeout\n* @throws InterruptedException if the current thread is interrupted\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicy$RetryAction": {
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long,java.lang.String)": "/**\n* Initializes a RetryAction with specified decision, delay, and reason.\n* @param action the decision to take on retry\n* @param delayTime the delay time in milliseconds before retrying\n* @param reason the reason for the retry action\n*/",
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:toString()": "/**\n* Returns a string representation of the object with action, delay, and reason details.\n* @return formatted string of object's properties\n*/",
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision)": "/**\n* Constructs a RetryAction with specified decision, default delay, and no reason.\n* @param action the decision to take on retry\n*/",
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)": "/**\n* Constructs a RetryAction with specified decision and delay.\n* @param action the decision to take on retry\n* @param delayTime the delay before retrying in milliseconds\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail": {
        "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param obj the object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:hashCode()": "/**\n* Returns the hash code of the class of the current object.\n* @return int hash code representing the class\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines retry action based on exception and retry parameters.\n* @param e exception encountered during operation\n* @param retries number of retries attempted\n* @param failovers number of failovers attempted\n* @param isIdempotentOrAtMostOnce indicates retry semantics\n* @return RetryAction indicating failure on first attempt\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited": {
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:<init>(int,long,java.util.concurrent.TimeUnit)": "/**\n* Initializes a retry mechanism with limits.\n* @param maxRetries maximum number of retry attempts\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:constructReasonString(int)": "/**\n* Constructs a reason string for retry failures.\n* @param retries number of retries attempted\n* @return formatted failure reason string\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:toString()": "/**\n* Returns a string representation of the object with maxRetries and sleepTime.\n* @return formatted string with class name and parameters\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason()": "/**\n* Retrieves the reason for retry failures.\n* @return formatted failure reason string based on maxRetries\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode()": "/**\n* Generates a hash code based on the string representation of the object.\n* @return integer hash code derived from toString() output\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param that object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)": "/****\n* Determines if an action should be retried based on exceptions and attempts.\n* @param e the exception encountered\n* @param retries current retry count\n* @param failovers number of failovers attempted\n* @param isIdempotentOrAtMostOnce indicates retry strategy\n* @return RetryAction indicating whether to retry or fail\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry": {
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)": "/**\n* Initializes FailoverOnNetworkExceptionRetry with specified policies and limits.\n* @param fallbackPolicy the retry policy to use on failure\n* @param maxFailovers maximum number of failover attempts\n* @param maxRetries maximum number of retries per failover\n* @param delayMillis initial delay between retries in milliseconds\n* @param maxDelayBase maximum base delay for exponential backoff\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:getFailoverOrRetrySleepTime(int)": "/**\n* Calculates sleep time for failover or retry based on attempt count.\n* @param times number of retry attempts\n* @return sleep time in milliseconds\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)": "/**\n* Constructs FailoverOnNetworkExceptionRetry with fallback policy and max failover attempts.\n* @param fallbackPolicy the retry policy to use on failure\n* @param maxFailovers maximum number of failover attempts\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)": "/**\n* Constructs FailoverOnNetworkExceptionRetry with specified parameters.\n* @param fallbackPolicy retry policy on failure\n* @param maxFailovers maximum failover attempts\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines retry action based on exception and retry limits.\n* @param e exception encountered during operation\n* @param retries number of retry attempts made\n* @param failovers number of failover attempts made\n* @param isIdempotentOrAtMostOnce indicates if operation is idempotent\n* @return RetryAction decision for the next action to take\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry": {
        "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Constructs a RemoteExceptionDependentRetry with a default policy and exception mappings.\n* @param defaultPolicy the default retry policy for exceptions\n* @param exceptionToPolicyMap mapping of specific exceptions to their retry policies\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines if an action should be retried based on an exception and retry policies.\n* @param e Exception encountered\n* @param retries Number of retry attempts made\n* @param failovers Number of failover attempts made\n* @param isIdempotentOrAtMostOnce Indicates retry behavior type\n* @return RetryAction indicating retry decision\n*/"
    },
    "org.apache.hadoop.ipc.RemoteException": {
        "org.apache.hadoop.ipc.RemoteException:getClassName()": "/**\n* Retrieves the name of the class.\n* @return String representing the class name\n*/",
        "org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto)": "/**\n* Constructs a RemoteException with a class name, message, and error code.\n* @param className the name of the class where the exception occurred\n* @param msg the detail message for the exception\n* @param erCode the error code associated with the exception, defaults to unspecified if null\n*/",
        "org.apache.hadoop.ipc.RemoteException:getErrorCode()": "/**\n* Retrieves the error code as an RpcErrorCodeProto.\n* @return RpcErrorCodeProto representing the error code\n*/",
        "org.apache.hadoop.ipc.RemoteException:instantiateException(java.lang.Class)": "/**\n* Instantiates an IOException using a constructor with a String parameter.\n* @param cls class type of the IOException to instantiate\n* @return newly created IOException instance\n*/",
        "org.apache.hadoop.ipc.RemoteException:toString()": "/**\n* Returns a string representation of the object with class name and message.\n* @return formatted string containing class name and message\n*/",
        "org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a RemoteException with class name and message.\n* @param className the name of the class where the exception occurred\n* @param msg the detail message for the exception\n*/",
        "org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[])": "/**\n* Unwraps a remote exception based on provided class types.\n* @param lookupTypes potential exception classes to match\n* @return IOException or the current instance if no match found\n*/",
        "org.apache.hadoop.ipc.RemoteException:unwrapRemoteException()": "/**\n* Unwraps and instantiates an IOException from a remote exception class.\n* @return IOException instance or the current exception if instantiation fails\n*/",
        "org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes)": "/**\n* Creates a RemoteException from given attributes.\n* @param attrs contains class and message details\n* @return a new RemoteException instance\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry": {
        "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:calculateSleepTime(int)": "/**\n* Calculates sleep time based on retry count.\n* @param retries number of retry attempts\n* @return calculated sleep time in milliseconds\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)": "/**** Initializes exponential backoff retry mechanism with specified limits. \n* @param maxRetries maximum number of retry attempts \n* @param sleepTime time to wait between retries \n* @param timeUnit unit of time for sleep duration \n*/"
    },
    "org.apache.hadoop.util.Daemon": {
        "org.apache.hadoop.util.Daemon:<init>()": "/**\n* Constructs a new Daemon instance.\n*/",
        "org.apache.hadoop.util.Daemon:<init>(java.lang.Runnable)": "/**\n* Constructs a Daemon thread with a specified Runnable.\n* @param runnable the Runnable to execute in the thread\n*/",
        "org.apache.hadoop.util.Daemon:<init>(java.lang.ThreadGroup,java.lang.Runnable)": "/**\n* Constructs a Daemon thread with specified group and runnable.\n* @param group the thread group for this daemon\n* @param runnable the task to be executed by this daemon\n*/",
        "org.apache.hadoop.util.Daemon:getRunnable()": "/**\n* Returns the Runnable instance associated with this object.\n* @return Runnable instance, or null if not set\n*/"
    },
    "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy": {
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:<init>(org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry,java.lang.String)": "/**\n* Initializes WrapperRetryPolicy with retry settings and exception type.\n* @param multipleLinearRandomRetry configuration for retry logic\n* @param remoteExceptionToRetry specific exception to trigger retries\n*/",
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:toString()": "/**\n* Returns a string representation of the RetryPolicy instance.\n* @return formatted string with retry policy details\n*/",
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode()": "/**\n* Computes hash code based on multipleLinearRandomRetry object.\n* @return integer hash code value\n*/",
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object)": "/**\n* Compares this WrapperRetryPolicy with another object for equality.\n* @param obj object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines retry policy based on exception type and conditions.\n* @param e the encountered exception\n* @param retries current retry count\n* @param failovers number of failovers attempted\n* @param isMethodIdempotent indicates retry behavior\n* @return RetryAction indicating retry decision\n*/"
    },
    "org.apache.hadoop.io.retry.RetryInvocationHandler$Call": {
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)": "/**\n* Initializes a call with method details, arguments, and retry policy.\n* @param method the method to be called\n* @param args arguments for the method\n* @param isRpc indicates if the call is an RPC\n* @param callId unique identifier for the call\n* @param retryInvocationHandler handles retry logic\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getWaitTime(long)": "/**\n* Calculates remaining wait time based on current time.\n* @param now current time in milliseconds\n* @return remaining wait time in milliseconds or null if retryInfo is absent\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:toString()": "/**\n* Returns a string representation of the object with method details.\n* @return formatted string with class and method information\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getCallId()": "/**\n* Retrieves the current call identifier.\n* @return the callId integer value\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getCounters()": "/**\n* Retrieves the current counters.\n* @return Counters object containing the current count values\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod()": "/**\n* Invokes a method with optional call ID setting for testing.\n* @throws Throwable if invocation fails\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo()": "/**\n* Processes retry information and handles failover if needed.\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke()": "/**\n* Invokes a method and returns the result wrapped in CallReturn.\n* @return CallReturn object containing the method result\n* @throws Throwable if the method invocation fails\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo()": "/**\n* Processes wait time and retry info, handling interruptions during sleep.\n* @return CallReturn indicating retry status\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce()": "/**\n* Invokes a method with retry logic on failure.\n* @return CallReturn with method result or exception state\n*/"
    },
    "org.apache.hadoop.io.retry.CallReturn": {
        "org.apache.hadoop.io.retry.CallReturn:getState()": "/**\n* Retrieves the current state.\n* @return the current State object\n*/",
        "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)": "/****\n* Initializes CallReturn with result, exception, and state.\n* @param r the result object or null\n* @param t the Throwable object or null\n* @param s the State object\n*/",
        "org.apache.hadoop.io.retry.CallReturn:getReturnValue()": "/**\n* Retrieves return value or throws exception based on current state.\n* @return the return value if state is RETURNED\n* @throws Throwable if state is EXCEPTION or invalid\n*/",
        "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object)": "/**\n* Constructs CallReturn with result and default values for exception and state.\n* @param r the result object or null\n*/",
        "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable)": "/**\n* Constructs CallReturn with exception state and validates Throwable.\n* @param t the Throwable object, must not be null\n*/",
        "org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State)": "/**\n* Constructs CallReturn with a given state.\n* @param s the State object for this CallReturn\n*/"
    },
    "org.apache.hadoop.ipc.Client": {
        "org.apache.hadoop.ipc.Client:isAsynchronousMode()": "/**\n* Checks if the system is in asynchronous mode.\n* @return true if asynchronous mode is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client:setAsynchronousMode(boolean)": "/**\n* Sets the asynchronous mode flag.\n* @param async true to enable async mode, false to disable it\n*/",
        "org.apache.hadoop.ipc.Client:nextCallId()": "/**\n* Generates the next call ID, ensuring a positive integer.\n* @return next unique call ID as a positive integer\n*/",
        "org.apache.hadoop.ipc.Client:getAsyncRpcResponse()": "/**\n* Retrieves an asynchronous RPC response.\n* @return AsyncGet object containing the response or null\n*/",
        "org.apache.hadoop.ipc.Client:setCallIdAndRetryCountUnprotected(java.lang.Integer,int,java.lang.Object)": "/**\n* Sets call ID, retry count, and external handler for the current context.\n* @param cid call identifier\n* @param rc retry count\n* @param externalHandler handler for external calls\n*/",
        "org.apache.hadoop.ipc.Client:getRetryCount()": "/**\n* Retrieves the current retry count.\n* @return current retry count or 0 if not set\n*/",
        "org.apache.hadoop.ipc.Client:getExternalHandler()": "/**\n* Retrieves the external call handler instance.\n* @return Object representing the external handler\n*/",
        "org.apache.hadoop.ipc.Client:incCount()": "/**\n* Increments the reference count by one.\n*/",
        "org.apache.hadoop.ipc.Client:decAndGetCount()": "/**\n* Decrements the reference count and returns the updated value.\n* @return updated reference count after decrement\n*/",
        "org.apache.hadoop.ipc.Client:createCall(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)": "/**\n* Creates a new Call instance with specified RPC kind and request.\n* @param rpcKind type of RPC call\n* @param rpcRequest the request data for the call\n* @return a new Call object\n*/",
        "org.apache.hadoop.ipc.Client:stop()": "/**\n* Stops the client and closes all connections.\n* @return void\n*/",
        "org.apache.hadoop.ipc.Client:getConnection(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.ipc.Client$Call,int,java.util.concurrent.atomic.AtomicBoolean)": "/**\n* Retrieves or creates a connection for a given remote ID and call.\n* @param remoteId identifier for the remote connection\n* @param call the call associated with the connection\n* @param serviceClass classification of the service\n* @param fallbackToSimpleAuth flag for simple authentication fallback\n* @return established Connection object\n*/",
        "org.apache.hadoop.ipc.Client:releaseAsyncCall()": "/**\n* Decrements the asynchronous call counter.\n*/",
        "org.apache.hadoop.ipc.Client:getAsyncCallCount()": "/**\n* Retrieves the current count of asynchronous calls.\n* @return number of async calls made\n*/",
        "org.apache.hadoop.ipc.Client:getConnectionIds()": "/**\n* Retrieves a set of connection IDs from the connections map.\n* @return Set of unique ConnectionId objects\n*/",
        "org.apache.hadoop.ipc.Client:getSocketFactory()": "/**\n* Retrieves the current SocketFactory instance.\n* @return SocketFactory object used for creating sockets\n*/",
        "org.apache.hadoop.ipc.Client:getCallId()": "/**\n* Retrieves the current call ID or generates a new one if absent.\n* @return current call ID or next unique call ID as a positive integer\n*/",
        "org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)": "/**\n* Sets call ID and retry count; validates inputs before proceeding.\n* @param cid call identifier\n* @param rc retry count\n* @param externalHandler handler for external calls\n*/",
        "org.apache.hadoop.ipc.Client:close()": "/**\n* Closes the client by stopping it and closing all connections.\n* @throws Exception if an error occurs during closing\n*/",
        "org.apache.hadoop.ipc.Client:checkAsyncCall()": "/**\n* Checks and limits asynchronous calls; throws exception if exceeded.\n* @throws IOException if an error occurs during the check\n*/",
        "org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)": "/**\n* Retrieves RPC response for a call, handling timeouts and exceptions.\n* @param call the RPC call object\n* @param connection the connection for remote address\n* @param timeout maximum wait time\n* @param unit time unit of the timeout\n* @return Writable RPC response or null if timeout occurs\n* @throws IOException if an error occurs during the call\n*/",
        "org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto)": "/**\n* Validates RpcResponseHeaderProto and checks client ID consistency.\n* @param header response header to validate\n* @throws IOException if client IDs do not match or header is null\n*/",
        "org.apache.hadoop.ipc.Client:toString()": "/**\n* Returns a string representation of the object's class and client ID in hex format.\n* @return formatted string with class name and client ID\n*/",
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Sends an RPC request and handles response asynchronously or synchronously.\n* @param rpcKind type of RPC call\n* @param rpcRequest the request data for the call\n* @param remoteId identifier for the remote connection\n* @param serviceClass classification of the service\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @param alignmentContext context for alignment\n* @return Writable RPC response or null if async\n* @throws IOException if an error occurs during the call\n*/",
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)": "/**\n* Executes an RPC call with specified parameters.\n* @param rpcKind type of RPC call\n* @param rpcRequest request data for the call\n* @param remoteId identifier for the remote connection\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @return Writable RPC response\n* @throws IOException if an error occurs during the call\n*/",
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Executes an RPC call with specified parameters.\n* @param rpcKind type of RPC call\n* @param rpcRequest the request data\n* @param remoteId identifier for the remote connection\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @param alignmentContext context for alignment\n* @return Writable RPC response\n* @throws IOException if an error occurs during the call\n*/",
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)": "/**\n* Executes an RPC call with specified parameters.\n* @param rpcKind type of RPC call\n* @param rpcRequest request data for the call\n* @param remoteId identifier for the remote connection\n* @param serviceClass classification of the service\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @return Writable RPC response or null if async\n* @throws IOException if an error occurs during the call\n*/",
        "org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)": "/**\n* Sets the ping interval in the given configuration.\n* @param conf the Configuration object to update\n* @param pingInterval the interval value to set\n*/",
        "org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)": "/**\n* Sets the connection timeout in the given configuration.\n* @param conf configuration object to update\n* @param timeout connection timeout value in milliseconds\n*/",
        "org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the ping interval from configuration.\n* @param conf configuration object\n* @return ping interval as an integer\n*/",
        "org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves RPC timeout from configuration, ensuring non-negative value.\n* @param conf configuration object\n* @return RPC timeout in milliseconds\n*/",
        "org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Initializes a Client with configuration and socket factory.\n* @param valueClass class type for writable values\n* @param conf configuration settings\n* @param factory socket factory for connections\n*/",
        "org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves RPC timeout or ping interval based on configuration settings.\n* @param conf configuration object\n* @return timeout in milliseconds or -1 if conditions are not met\n*/",
        "org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Client with specified value class and configuration.\n* @param valueClass class type for writable values\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.io.retry.RetryInvocationHandler$Counters": {
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Counters:isZeros()": "/**\n* Checks if both retries and failovers are zero.\n* @return true if both are zero, false otherwise\n*/"
    },
    "org.apache.hadoop.io.retry.MultiException": {
        "org.apache.hadoop.io.retry.MultiException:getExceptions()": "/**\n* Retrieves a map of exceptions.\n* @return Map containing exception details with string keys\n*/",
        "org.apache.hadoop.io.retry.MultiException:<init>(java.util.Map)": "/**\n* Constructs a MultiException with a map of exceptions.\n* @param exes map of exception names to Exception objects\n*/",
        "org.apache.hadoop.io.retry.MultiException:toString()": "/**\n* Returns a string representation of MultiException with contained exceptions.\n* @return formatted string of exceptions\n*/"
    },
    "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo": {
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:toString()": "/**\n* Returns a string representation of the RetryInfo object.\n* @return formatted string with retry details\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFailover()": "/**\n* Checks if the current action requires failover and retry.\n* @return true if failover is needed, otherwise false\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFail()": "/**\n* Checks if the action indicates a failure.\n* @return true if action is not null and indicates failure, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:getFailException()": "/**\n* Retrieves the failure exception.\n* @return the failException instance\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)": "/**\n* Initializes RetryInfo with delay, action, expected failovers, and failure exception.\n* @param delay time to wait before retrying\n* @param action action to perform on retry\n* @param expectedFailoverCount anticipated number of failures\n* @param failException exception encountered during the last attempt\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)": "/**\n* Creates RetryInfo based on policy and exception details.\n* @param policy retry policy to evaluate actions\n* @param e exception encountered during operation\n* @param counters counters for retries and failovers\n* @param idempotentOrAtMostOnce specifies retry behavior\n* @param expectedFailoverCount anticipated number of failures\n* @return RetryInfo object with delay and action details\n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue": {
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:iterator()": "/**\n* Returns an iterator for the elements in the queue.\n* @return Iterator for the queue elements\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object)": "/**\n* Adds an element to the queue; throws exception if addition fails.\n* @param c element to add\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long)": "/**\n* Checks if the queue is empty based on time since emptyStartTime.\n* @param time duration in milliseconds to compare with elapsed time\n* @return true if queue is empty and time threshold is exceeded, false otherwise\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty()": "/**\n* Sets the empty start time if the queue is empty.\n* @return void\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry": {
        "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Initializes retry policies for exceptions.\n* @param defaultPolicy default retry policy\n* @param exceptionToPolicyMap mapping of exceptions to their retry policies\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines if an action should be retried based on the exception and retry parameters.\n* @param e exception encountered\n* @param retries current retry count\n* @param failovers number of failover attempts\n* @param isIdempotentOrAtMostOnce indicates if the action is idempotent\n* @return RetryAction indicating retry decision\n*/"
    },
    "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider": {
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:<init>(java.lang.Class,java.lang.Object)": "/**\n* Initializes a failover proxy provider with the specified interface and proxy instance.\n* @param iface the interface class type\n* @param proxy the proxy instance to be used\n*/",
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getInterface()": "",
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:performFailover(java.lang.Object)": "",
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy()": "/**\n* Creates a ProxyInfo instance with the current proxy.\n* @return ProxyInfo object containing the proxy details\n*/",
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close()": "/**\n* Closes the RPC proxy connection.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo": {
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:<init>(java.lang.Object,java.lang.String)": "/**\n* Constructs ProxyInfo with specified proxy and its information.\n* @param proxy the proxy object\n* @param proxyInfo details about the proxy\n*/",
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:proxyName()": "/**\n* Returns the simple name of the proxy class or \"UnknownProxy\" if null.\n* @return proxy class name as String\n*/",
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String)": "/**\n* Constructs a string combining proxy name, method name, and proxy info.\n* @param methodName name of the method being referenced\n* @return formatted string with proxy details\n*/",
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString()": "/**\n* Returns string representation of the object with proxy details.\n* @return formatted string of proxy name and info\n*/"
    },
    "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor": {
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider)": "/**\n* Initializes ProxyDescriptor with a FailoverProxyProvider.\n* @param fpp the FailoverProxyProvider to obtain proxy information\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:idempotentOrAtMostOnce(java.lang.reflect.Method)": "/**\n* Checks if the method is idempotent or at most once.\n* @param method the method to check\n* @return true if annotated, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:close()": "/**\n* Closes the file pointer, releasing system resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getFailoverCount()": "/**\n* Retrieves the current failover count.\n* @return the number of failovers that have occurred\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getProxy()": "/**\n* Retrieves the proxy instance in a thread-safe manner.\n* @return the proxy object of type T\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getProxyInfo()": "/**\n* Retrieves the current proxy information.\n* @return ProxyInfo object containing proxy details\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)": "/**\n* Handles failover logic based on expected count and current state.\n* @param expectedFailoverCount expected count for failover validation\n* @param method method triggering the failover\n* @param callId identifier for the current call\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair": {
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair:<init>(int,int)": "/**\n* Initializes a Pair with retry parameters.\n* @param numRetries number of retry attempts\n* @param sleepMillis delay between retries in milliseconds\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair:toString()": "/**\n* Returns a string representation of retry configuration.\n* @return formatted string \"numRetries x sleepMillis ms\"\n*/"
    },
    "org.apache.hadoop.io.retry.RetryInvocationHandler": {
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getRetryPolicy(java.lang.reflect.Method)": "/**\n* Retrieves the retry policy for a given method.\n* @param method the method to get the retry policy for\n* @return the associated RetryPolicy or defaultPolicy if not found\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:isRpcInvocation(java.lang.Object)": "/**\n* Checks if the given proxy is an RPC invocation.\n* @param proxy the object to check\n* @return true if it's an RPC invocation, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getProxyProvider()": "/**\n* Retrieves the FailoverProxyProvider instance.\n* @return FailoverProxyProvider of type T\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Constructs a RetryInvocationHandler with proxy and retry policies.\n* @param proxyProvider the FailoverProxyProvider for proxy info\n* @param defaultPolicy the default retry policy\n* @param methodNameToPolicyMap mapping of method names to retry policies\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:close()": "/**\n* Closes the file pointer and releases system resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount()": "/**\n* Returns the current failover count from the proxy descriptor.\n* @return number of failovers that have occurred\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes a method on the proxy object with given arguments.\n* @param method the method to invoke\n* @param args the arguments for the method call\n* @return result of the method invocation\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId()": "/**\n* Retrieves ConnectionId associated with the proxy object.\n* @return ConnectionId for the current proxy\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Constructs a RetryInvocationHandler with proxy and retry policies.\n* @param proxyProvider the FailoverProxyProvider for proxy info\n* @param retryPolicy the default retry policy\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)": "/**\n* Logs method invocation details and failover attempts.\n* @param method invoked method, isFailover indicates failover status,\n* @param failovers count of failover attempts, retries current retry count,\n* @param delay time to wait before retrying, ex exception encountered\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)": "/**\n* Creates a new Call or AsyncCall based on async mode.\n* @param method the method to invoke\n* @param args arguments for the method\n* @param isRpc indicates if the call is an RPC\n* @param callId unique identifier for the call\n* @return Call instance for the method invocation\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)": "/**\n* Handles exceptions by creating RetryInfo and logging details.\n* @param method the method that encountered an exception\n* @param callId unique identifier for the method call\n* @param policy retry policy for handling the exception\n* @param counters counters for tracking retries and failovers\n* @param expectFailoverCount anticipated number of failovers\n* @param e the exception that occurred\n* @return RetryInfo object with retry details\n*/",
        "org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes a method with RPC handling and retry logic.\n* @param proxy the proxy object for invocation\n* @param method the method to invoke\n* @param args arguments for the method\n* @return result of the method invocation or null for async calls\n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler": {
        "org.apache.hadoop.io.retry.AsyncCallHandler:hasSuccessfulCall()": "/**\n* Checks if the last call was successful.\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler:setLowerLayerAsyncReturn(org.apache.hadoop.util.concurrent.AsyncGet)": "/**\n* Sets the asynchronous return for the lower layer.\n* @param asyncReturn the asynchronous result or exception\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn()": "/**\n* Retrieves and clears the lower layer async return object.\n* @return AsyncGet object, must be non-null\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)": "/**\n* Creates a new asynchronous call instance.\n* @param method the method to be called\n* @param args arguments for the method\n* @param isRpc indicates if the call is an RPC\n* @param callId unique identifier for the call\n* @param retryInvocationHandler handles retry logic\n* @return AsyncCall instance\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn()": "/**\n* Retrieves and clears the async return object.\n* @return AsyncGet object or lower layer async return if null\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)": "/**\n* Initializes an async call and sets up its return handling.\n* @param asyncCall the AsyncCall to be processed\n* @param asyncCallReturn handles the async call's return value\n*/"
    },
    "org.apache.hadoop.ipc.RPC": {
        "org.apache.hadoop.ipc.RPC:getConnectionIdForProxy(java.lang.Object)": "/**\n* Retrieves ConnectionId from a proxy object.\n* @param proxy the proxy object to extract ConnectionId from\n* @return ConnectionId associated with the proxy\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolVersion(java.lang.Class)": "/**\n* Retrieves the protocol version from the given class.\n* @param protocol the class containing protocol information\n* @return the protocol version as a long\n*/",
        "org.apache.hadoop.ipc.RPC:getSuperInterfaces(java.lang.Class[])": "/**\n* Retrieves all superinterfaces of given child interfaces extending VersionedProtocol.\n* @param childInterfaces array of child interface classes\n* @return array of superinterface classes\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolName(java.lang.Class)": "/**\n* Retrieves the protocol name from a class or its annotation.\n* @param protocol the class representing the protocol\n* @return protocol name or class name if annotation is absent\n*/",
        "org.apache.hadoop.ipc.RPC:<init>()": "/**\n* Private constructor to prevent instantiation of the RPC class.\n*/",
        "org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object)": "/**\n* Closes a proxy or its invocation handler if it's closeable.\n* @param proxy the proxy object to close\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class)": "/**\n* Retrieves superinterfaces of the given protocol class.\n* @param protocol the class representing a protocol\n* @return array of superinterface classes\n*/",
        "org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object)": "/**\n* Retrieves server address from the given proxy object.\n* @param proxy the proxy object to extract ConnectionId from\n* @return InetSocketAddress of the server\n*/",
        "org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)": "/**\n* Sets the protocol engine in the configuration if not already set.\n* @param conf configuration object to update\n* @param protocol protocol class type\n* @param engine engine class type to set\n*/",
        "org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves RPC timeout value from configuration.\n* @param conf configuration object\n* @return RPC timeout as an integer\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves or creates an RpcEngine for the specified protocol.\n* @param protocol the protocol class\n* @param conf configuration for the engine\n* @return RpcEngine instance for the protocol\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Retrieves a proxy for the specified protocol.\n* @param protocol the protocol class\n* @param clientVersion the client's version\n* @param connId connection identifier\n* @param conf configuration settings\n* @param factory socket factory\n* @param alignmentContext context for alignment\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)": "/**\n* Creates a proxy for the specified protocol with security checks.\n* @param protocol the protocol class\n* @param clientVersion the version of the client\n* @param addr the server address\n* @param ticket user credentials\n* @param conf configuration settings\n* @param factory socket factory\n* @param rpcTimeout timeout for RPC calls\n* @param connectionRetryPolicy retry policy for connections\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @return ProtocolProxy instance\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates a protocol proxy for the specified protocol and connection settings.\n* @param protocol the protocol class\n* @param clientVersion the client's version\n* @param addr the server address\n* @param ticket user authentication information\n* @param conf configuration settings\n* @param factory socket factory for connections\n* @param rpcTimeout timeout for RPC calls\n* @param connectionRetryPolicy retry policy for connections\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @param alignmentContext context for alignment\n* @return ProtocolProxy instance for the specified protocol\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Retrieves a proxy for the specified protocol.\n* @param protocol the protocol class\n* @param clientVersion the client's version\n* @param connId connection identifier\n* @param conf configuration settings\n* @param factory socket factory\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Creates a protocol proxy with security checks.\n* @param protocol the protocol class\n* @param clientVersion the version of the client\n* @param addr the server address\n* @param ticket user credentials\n* @param conf configuration settings\n* @param factory socket factory\n* @param rpcTimeout timeout for RPC calls\n* @param connectionRetryPolicy retry policy for connections\n* @return ProtocolProxy instance\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Creates a protocol proxy with security checks.\n* @param protocol protocol class, clientVersion version, addr server address, ticket user credentials,\n* @param conf configuration settings, factory socket factory\n* @return ProtocolProxy instance\n*/",
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)": "/**\n* Retrieves a protocol proxy instance with the given parameters.\n* @param protocol the protocol class\n* @param clientVersion the version of the client\n* @param addr the server address\n* @param ticket user credentials\n* @param conf configuration settings\n* @param factory socket factory\n* @param rpcTimeout timeout for RPC calls\n* @return proxy of type T\n*/",
        "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)": "/**\n* Waits for a protocol proxy, retrying on connection issues.\n* @param protocol the protocol class\n* @param clientVersion the client version\n* @param addr server address\n* @param conf configuration settings\n* @param rpcTimeout timeout for RPC calls\n* @param connectionRetryPolicy retry policy for connections\n* @param timeout maximum wait time\n* @return ProtocolProxy instance\n* @throws IOException if unable to connect within timeout\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Creates a protocol proxy using user credentials and configuration.\n* @param protocol protocol class, clientVersion version, addr server address, conf settings, factory socket factory\n* @return ProtocolProxy instance\n* @throws IOException if user info retrieval fails\n*/",
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Retrieves a protocol proxy instance for the specified parameters.\n* @param protocol the protocol class type\n* @param clientVersion the version of the client\n* @param addr the server address\n* @param ticket user credentials\n* @param conf configuration settings\n* @param factory socket factory\n* @return the proxy of type T\n*/",
        "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)": "/**\n* Waits for a protocol proxy using specified parameters.\n* @param protocol the protocol class\n* @param clientVersion the client version\n* @param addr server address\n* @param conf configuration settings\n* @param connTimeout maximum wait time\n* @return ProtocolProxy instance\n* @throws IOException if unable to connect\n*/",
        "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)": "/**\n* Waits for a protocol proxy and retrieves it.\n* @param protocol the protocol class\n* @param clientVersion the client version\n* @param addr server address\n* @param conf configuration settings\n* @param rpcTimeout timeout for RPC calls\n* @param timeout maximum wait time\n* @return proxy of type T\n* @throws IOException if unable to connect\n*/",
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Retrieves a protocol proxy instance for the specified protocol.\n* @param protocol the protocol class\n* @param clientVersion the version of the client\n* @param addr the server address\n* @param conf configuration settings\n* @param factory socket factory\n* @return proxy of type T\n* @throws IOException if proxy retrieval fails\n*/",
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)": "/**** Retrieves a ProtocolProxy for the specified protocol and client version. \n* @param protocol the protocol class, clientVersion the version, addr server address, conf settings \n* @return ProtocolProxy instance \n* @throws IOException if user info retrieval fails \n*/",
        "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)": "/**\n* Waits for a protocol proxy with specified parameters.\n* @param protocol the protocol class\n* @param clientVersion the client version\n* @param addr server address\n* @param conf configuration settings\n* @return ProtocolProxy instance\n* @throws IOException if unable to connect\n*/",
        "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)": "/**\n* Waits for a protocol proxy and retrieves it.\n* @param protocol the protocol class\n* @param clientVersion the client version\n* @param addr server address\n* @param conf configuration settings\n* @param connTimeout maximum wait time\n* @return Proxy instance of type T\n* @throws IOException if unable to connect\n*/",
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a proxy for the specified protocol and client version.\n* @param protocol the protocol class\n* @param clientVersion the version of the client\n* @param addr server address\n* @param conf configuration settings\n* @return proxy instance of type T\n* @throws IOException if proxy retrieval fails\n*/",
        "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)": "/**\n* Waits for a protocol proxy and returns the proxy instance.\n* @param protocol the protocol class\n* @param clientVersion the client version\n* @param addr server address\n* @param conf configuration settings\n* @return Proxy instance of type T\n* @throws IOException if unable to connect\n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue": {
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:isDone()": "/**\n* Checks if the operation is completed.\n* @return true if completed, false otherwise\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)": "/**\n* Waits for a value asynchronously with a timeout.\n* @param timeout maximum wait time\n* @param unit time unit of the timeout\n* @return the value if available; throws TimeoutException if timed out\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object)": "/**\n* Sets a new value if current value is null.\n* @param v the new value to set\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry": {
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:<init>(java.util.List)": "/**\n* Initializes MultipleLinearRandomRetry with a list of pairs.\n* @param pairs list of Pair objects, must not be null or empty\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:searchPair(int)": "/**\n* Searches for a Pair based on current retry count.\n* @param curRetry remaining retries to consider\n* @return Pair object or null if no suitable Pair found\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:toString()": "/**\n* Returns the string representation of the object.\n* @return formatted string with class name and pairs\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parsePositiveInt(java.lang.String[],int,java.lang.String)": "/**\n* Parses a positive integer from a string array element.\n* @param elements array of strings to parse\n* @param i index of the element to parse\n* @param originalString the original string for logging context\n* @return parsed positive integer or -1 if invalid\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode()": "/**\n* Computes hash code based on string representation of the object.\n* @return integer hash code value\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param that object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String)": "/**\n* Parses a comma-separated string into MultipleLinearRandomRetry.\n* @param s input string to parse\n* @return MultipleLinearRandomRetry object or null if input is invalid\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines if an action should be retried based on exceptions and retry conditions.\n* @param e exception encountered\n* @param curRetry current retry count\n* @param failovers number of failovers attempted\n* @param isIdempotentOrAtMostOnce indicates retry behavior\n* @return RetryAction indicating retry decision and sleep time\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry": {
        "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Initializes ExceptionDependentRetry with a default policy and exception-specific policies.\n* @param defaultPolicy the fallback retry policy\n* @param exceptionToPolicyMap mapping of exceptions to their respective retry policies\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)": "/**\n* Determines if an action should be retried based on exception and retry policies.\n* @param e exception encountered\n* @param retries number of retries attempted\n* @param failovers number of failover attempts\n* @param isIdempotentOrAtMostOnce indicates if action is idempotent\n* @return RetryAction indicating retry decision\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies": {
        "org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int,long)": "/**\n* Calculates exponential backoff time with a cap.\n* @param time initial time in milliseconds\n* @param retries number of retry attempts\n* @param cap maximum allowed time in milliseconds\n* @return randomized backoff time based on retries\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:isSaslFailure(java.lang.Exception)": "/**\n* Checks if the given exception is a SaslException or caused by one.\n* @param e the exception to check\n* @return true if a SaslException is found, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:hasWrappedAccessControlException(java.lang.Exception)": "/**\n* Checks if the exception or its cause is an AccessControlException.\n* @param e the exception to check\n* @return true if AccessControlException is found, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)": "/**\n* Creates a RetryPolicy for network exception failover.\n* @param fallbackPolicy the retry policy on failure\n* @param maxFailovers maximum number of failovers\n* @param maxRetries maximum retries per failover\n* @param delayMillis initial delay between retries\n* @param maxDelayBase maximum base delay for backoff\n* @return configured RetryPolicy instance\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Creates a RemoteExceptionDependentRetry with specified policies.\n* @param defaultPolicy the default retry policy for exceptions\n* @param exceptionToPolicyMap mapping of specific exceptions to their retry policies\n* @return a new RetryPolicy instance\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Creates a retry policy excluding remote and SASL exceptions.\n* @param defaultPolicy the default retry policy\n* @param exceptionToPolicyMap mapping of exceptions to their retry policies\n* @return RetryPolicy instance for specific exceptions\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)": "/**\n* Creates a retry policy based on exceptions.\n* @param defaultPolicy fallback policy for retries\n* @param exceptionToPolicyMap mapping of exceptions to retry policies\n* @return ExceptionDependentRetry instance\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)": "/**\n* Calculates exponential backoff time based on initial time and retry attempts.\n* @param time initial time in milliseconds\n* @param retries number of retry attempts\n* @return randomized backoff time capped at Long.MAX_VALUE\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)": "/**\n* Creates a retry policy that retries indefinitely with fixed sleep duration.\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n* @return RetryPolicy instance for unlimited retries\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)": "/**\n* Creates a RetryPolicy for fixed sleep between a maximum number of retries.\n* @param maxRetries maximum number of retry attempts\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n* @return RetryPolicy instance\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)": "/**\n* Creates an exponential backoff retry policy.\n* @param maxRetries maximum retry attempts\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n* @return RetryPolicy instance for exponential backoff\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)": "/**\n* Creates a retry policy with a maximum count and proportional sleep.\n* @param maxRetries maximum retry attempts\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n* @return RetryPolicy instance\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)": "/**\n* Creates a FailoverOnNetworkExceptionRetry with specified parameters.\n* @param fallbackPolicy retry policy on failure\n* @param maxFailovers maximum failover attempts\n* @param delayMillis initial delay in milliseconds\n* @param maxDelayBase maximum delay base in milliseconds\n* @return configured RetryPolicy instance\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception)": "/**\n* Determines if failover is needed based on the exception type.\n* @param e the exception to evaluate\n* @return true if failover should occur, false otherwise\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception)": "/**\n* Wraps a RemoteException into a RetriableException if applicable.\n* @param e the Exception to be checked and wrapped\n* @return RetriableException or null if not a RemoteException\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)": "/**\n* Creates a retry policy with max duration and fixed sleep intervals.\n* @param maxTime maximum total retry duration\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n* @return RetryPolicy instance for configured retries\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)": "/**** Creates a retry policy for network exceptions. \n* @param fallbackPolicy base retry policy \n* @param maxFailovers maximum retry attempts \n* @return configured RetryPolicy instance \n*/",
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int)": "/**\n* Configures retry policy for network exceptions.\n* @param maxFailovers maximum retry attempts\n* @return configured RetryPolicy instance\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep": {
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:constructReasonString(long,java.util.concurrent.TimeUnit)": "/**\n* Constructs a reason string for exceeded maximum allowed time.\n* @param maxTime maximum time limit in the specified unit\n* @param timeUnit unit of time for the maxTime\n* @return formatted reason string\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason()": "/**\n* Retrieves a reason string based on max time and time unit.\n* @return formatted reason string for exceeding max time\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)": "/**\n* Initializes retry mechanism with max time and fixed sleep duration.\n* @param maxTime maximum total retry duration\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n*/"
    },
    "org.apache.hadoop.ipc.RetriableException": {
        "org.apache.hadoop.ipc.RetriableException:<init>(java.lang.String)": "/**\n* Constructs a RetriableException with the specified detail message.\n* @param msg the detail message\n*/",
        "org.apache.hadoop.ipc.RetriableException:<init>(java.lang.Exception)": "/**\n* Constructs a RetriableException with the specified cause.\n* @param e the underlying exception that caused this exception\n*/"
    },
    "org.apache.hadoop.io.MultipleIOException": {
        "org.apache.hadoop.io.MultipleIOException:<init>(java.util.List)": "/**\n* Constructs MultipleIOException with a list of IOExceptions.\n* @param exceptions list of IOException instances\n*/",
        "org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List)": "/**\n* Creates an IOException from a list of exceptions.\n* @param exceptions list of IOException instances\n* @return single IOException or MultipleIOException, or null if input is empty\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Compression": {
        "org.apache.hadoop.io.file.tfile.Compression:<init>()": "/**\n* Private constructor for Compression class; prevents instantiation.\n*/",
        "org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String)": "/**\n* Retrieves the compression algorithm by its name.\n* @param compressName the name of the compression algorithm\n* @return Algorithm corresponding to the name\n*/",
        "org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms()": "/**\n* Returns an array of supported algorithm names.\n* @return String array of supported algorithm names\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Compression$Algorithm": {
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getName()": "/**\n* Retrieves the compressed name.\n* @return String representing the compressed name\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor()": "/**\n* Retrieves a Decompressor for the current codec or null if not available.\n* @return Decompressor instance or null\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor)": "/**\n* Returns a reusable compressor to the pool if not null.\n* @param compressor the Compressor instance to return\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)": "/**\n* Returns a decompressor to the pool if not null; logs its hash code if debug enabled.\n* @param decompressor the Decompressor instance to return\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor()": "/**\n* Retrieves a Compressor instance from the CodecPool.\n* @return Compressor or null if no codec available\n*/"
    },
    "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator": {
        "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(java.lang.Object,java.lang.Object)": "/**\n* Throws exception as object comparison is not supported.\n* @param o1 first object to compare\n* @param o2 second object to compare\n* @return RuntimeException indicating unsupported operation\n*/",
        "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays from specified offsets and lengths.\n* @param b1 first byte array, b2 second byte array\n* @return comparison result: negative, zero, or positive integer\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile": {
        "org.apache.hadoop.io.file.tfile.BCFile:<init>()": "/**\n* Private constructor for BCFile; prevents instantiation of this class.\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Utils$Version": {
        "org.apache.hadoop.io.file.tfile.Utils$Version:<init>(short,short)": "/**\n* Constructs a Version object with major and minor version numbers.\n* @param major major version number\n* @param minor minor version number\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:toString()": "/**\n* Returns a string representation of the version in 'major.minor' format.\n* @return version string\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:size()": "/**\n* Calculates the size in bytes of two short values.\n* @return size in bytes\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:write(java.io.DataOutput)": "/**\n* Writes major and minor version numbers to the output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:<init>(java.io.DataInput)": "/**\n* Constructs a Version object from input data.\n* @param in DataInput stream containing version information\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:compatibleWith(org.apache.hadoop.io.file.tfile.Utils$Version)": "/**\n* Checks if the current version is compatible with another.\n* @param other the Version to compare with\n* @return true if major versions match, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:compareTo(org.apache.hadoop.io.file.tfile.Utils$Version)": "/**\n* Compares this Version object with another for order.\n* @param that the Version object to compare with\n* @return negative if this is less, positive if greater, zero if equal\n*/",
        "org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object)": "/**\n* Checks if this Version is equal to another object.\n* @param other the object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFileDumper": {
        "org.apache.hadoop.io.file.tfile.TFileDumper:<init>()": "/**\n* Prevents instantiation of the TFileDumper class.\n*/",
        "org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)": "/**\n* Dumps file info including version, size, and block details to the output stream.\n* @param file path to the file, @param out output stream for printing info, @param conf configuration settings\n* @throws IOException if file operations fail\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$DataIndex": {
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:getBlockRegionList()": "/**\n* Retrieves the list of block regions.\n* @return ArrayList of BlockRegion objects\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:getDefaultCompressionAlgorithm()": "/**\n* Retrieves the default compression algorithm.\n* @return Algorithm object representing the default compression method\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:addBlockRegion(org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)": "/**\n* Adds a BlockRegion to the list of regions.\n* @param region the BlockRegion to be added\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String)": "/**\n* Initializes DataIndex with a default compression algorithm.\n* @param defaultCompressionAlgorithmName name of the compression algorithm\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput)": "/**\n* Initializes DataIndex from input stream, reading compression and block regions.\n* @param in DataInput stream for reading data\n* @throws IOException if reading fails\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput)": "/**\n* Writes compression algorithm name and region list to output.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion": {
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getCompressedSize()": "/**\n* Retrieves the compressed size value.\n* @return the size in bytes of the compressed data\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getRawSize()": "/**\n* Retrieves the raw size value.\n* @return the raw size as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getOffset()": "/**\n* Retrieves the current offset value.\n* @return the current offset as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(long,long,long)": "/**\n* Constructs a BlockRegion with specified offset and sizes.\n* @param offset position in the data stream\n* @param compressedSize size of compressed data\n* @param rawSize size of uncompressed data\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:magnitude()": "",
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput)": "/**\n* Initializes BlockRegion by reading offset, compressedSize, and rawSize from input stream.\n* @param in DataInput stream to read block region data from\n* @throws IOException if reading fails\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput)": "/**\n* Writes offset and sizes to the output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry": {
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getRegion()": "/**\n* Retrieves the BlockRegion associated with this object.\n* @return BlockRegion instance representing the region\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getCompressionAlgorithm()": "/**\n* Retrieves the current compression algorithm.\n* @return the CompressionAlgorithm instance in use\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getMetaName()": "/**\n* Retrieves the meta name.\n* @return the metaName string value\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)": "/**\n* Constructs a MetaIndexEntry with specified parameters.\n* @param metaName name of the metadata\n* @param compressionAlgorithm algorithm for compression\n* @param region block region associated with the entry\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput)": "/**\n* Constructs MetaIndexEntry from input stream data.\n* @param in DataInput stream to read meta data\n* @throws IOException if data is corrupted or reading fails\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput)": "/**\n* Writes metadata and region information to the output stream.\n* @param out DataOutput stream for writing data\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Magic": {
        "org.apache.hadoop.io.file.tfile.BCFile$Magic:size()": "/**\n* Returns the size of the AB_MAGIC_BCFILE array.\n* @return length of the AB_MAGIC_BCFILE array\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Magic:write(java.io.DataOutput)": "/**\n* Writes magic bytes to the output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput)": "/**\n* Reads and verifies data against AB_MAGIC_BCFILE.\n* @param in input stream to read data from\n* @throws IOException if data does not match expected format\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFileDumper$Align": {
        "org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(java.lang.String,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)": "/**\n* Formats a string to a specified width with alignment.\n* @param s string to format, @param width target width, @param align alignment type\n* @return formatted string\n*/",
        "org.apache.hadoop.io.file.tfile.TFileDumper$Align:calculateWidth(java.lang.String,long)": "/**\n* Calculates the maximum width based on caption length and max value.\n* @param caption the text to measure\n* @param max the maximum value to consider\n* @return the greater of caption length or max value length\n*/",
        "org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)": "/**\n* Formats a long number to a specified width with alignment.\n* @param l number to format, @param width target width, @param align alignment type\n* @return formatted string representation of the number\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$TFileIndex": {
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getEntry(int)": "/**\n* Retrieves a TFileIndexEntry by its bid.\n* @param bid the index identifier\n* @return TFileIndexEntry associated with the bid or null if not found\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getFirstKey()": "/**\n* Retrieves the first key.\n* @return RawComparable representing the first key\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)": "/**\n* Initializes TFileIndex with a comparator.\n* @param comparator used for comparing index entries\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry)": "/**\n* Adds a key entry to the index and updates the cumulative entry count.\n* @param keyEntry the entry to be added\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Finds the index of the first element not less than the key.\n* @param key value to compare against\n* @return index or -1 if not found or unsorted\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable)": "/**** Finds the index to insert key in a sorted index. \n* @param key element to compare\n* @return index or -1 if not found or unsorted \n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Calculates record number based on location.\n* @param location specifies the record's location\n* @return total record number as a long value\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)": "/**\n* Sets the first key from a byte array starting at offset with specified length.\n* @param key byte array source, @param offset starting position, @param length number of bytes\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey()": "/**\n* Retrieves the last key from the index.\n* @return RawComparable of the last key or null if index is empty\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long)": "/**\n* Retrieves Location based on record number.\n* @param recNum record number to find location for\n* @return Location object representing the position\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)": "/**\n* Constructs TFileIndex with entries from input stream.\n* @param entryCount number of entries to read\n* @param in DataInput stream for reading entries\n* @param comparator used for comparing entries\n* @throws IOException if reading from stream fails\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput)": "/**\n* Writes data to output, handling firstKey and index entries.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex": {
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>()": "/**\n* Initializes a new MetaIndex with an empty TreeMap for storing entries.\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:getMetaByName(java.lang.String)": "/**\n* Retrieves a MetaIndexEntry by its name.\n* @param name the name of the MetaIndexEntry\n* @return MetaIndexEntry associated with the name or null if not found\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry)": "/****\n* Adds a MetaIndexEntry to the index using its meta name as the key.\n* @param indexEntry the entry to be added to the index\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput)": "/**\n* Constructs MetaIndex from input stream data.\n* @param in DataInput stream to read meta index entries\n* @throws IOException if data is corrupted or reading fails\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput)": "/**\n* Writes metadata and index entries to output stream.\n* @param out DataOutput stream for writing data\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists": {
        "org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists:<init>(java.lang.String)": "/**\n* Exception thrown when a meta block already exists.\n* @param s message detailing the error\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry": {
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyBuffer()": "/**\n* Retrieves the key buffer as a byte array.\n* @return byte array containing the key buffer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyLength()": "/**\n* Retrieves the length of the key.\n* @return the length of the key as an integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValueStream()": "/**\n* Retrieves the value input stream if not previously accessed.\n* @return DataInputStream for value data\n* @throws IOException if an I/O error occurs\n* @throws IllegalStateException if accessed multiple times\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeKey(java.io.OutputStream)": "/**\n* Writes key data to the output stream.\n* @param out the output stream to write to\n* @return number of bytes written\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[],int)": "/**\n* Copies a key into the provided buffer at the specified offset.\n* @param buf byte array to store the key\n* @param offset position in the buffer to start copying\n* @return length of the key copied\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValueLength()": "/**\n* Returns the length of a value.\n* @return length of the value, or throws RuntimeException if unknown\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:isValueLengthKnown()": "/**\n* Checks if the known value length is non-negative.\n* @return true if length is known, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode()": "/**\n* Computes hash code for the object using the key buffer.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[])": "/**\n* Retrieves key and stores it in the provided buffer.\n* @param buf byte array to store the key\n* @return length of the key copied\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)": "/**\n* Reads value into buffer; returns bytes read or throws IndexOutOfBoundsException.\n* @param buf byte array to hold the value\n* @param offset starting position in the buffer\n* @return number of bytes read\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream()": "/**\n* Resets and returns the key data input stream.\n* @return DataInputStream for key data\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[])": "/**\n* Retrieves value into buffer starting at offset 0.\n* @param buf byte array to hold the value\n* @return number of bytes read\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable)": "/**\n* Retrieves the key length and populates the provided BytesWritable.\n* @param key BytesWritable to store the key\n* @return length of the key\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Compares this object with another RawComparable.\n* @param key the RawComparable to compare with\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)": "/**\n* Compares this object with a byte array from specified offset and length.\n* @param buf byte array to compare\n* @param offset starting position in byte array\n* @param length number of bytes to include in comparison\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable)": "/**\n* Reads bytes into value from input stream.\n* @param value destination for read bytes\n* @return total length of bytes read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream)": "/**\n* Writes buffered value data to an output stream.\n* @param out OutputStream to write data to\n* @return Total bytes written to the output stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[])": "/**\n* Compares this object with a byte array.\n* @param buf byte array to compare\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object)": "/**\n* Checks equality of this Entry with another object.\n* @param other object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)": "/**\n* Retrieves key and value from input streams.\n* @param key BytesWritable to store the key\n* @param value BytesWritable to store the value\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator": {
        "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays from specified offsets and lengths.\n* @param a first byte array\n* @param off1 offset in the first array\n* @param len1 length of the first array segment\n* @param b second byte array\n* @param off2 offset in the second array\n* @param len2 length of the second array segment\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:<init>(org.apache.hadoop.io.RawComparator)": "/**\n* Initializes BytesComparator with a custom raw comparator.\n* @param cmp the raw comparator for byte comparison\n*/",
        "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Compares two RawComparable objects based on their byte buffers.\n* @param o1 first RawComparable object\n* @param o2 second RawComparable object\n* @return comparison result as integer\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Utils": {
        "org.apache.hadoop.io.file.tfile.Utils:readVLong(java.io.DataInput)": "/**\n* Reads a variable-length long from input stream.\n* @param in DataInput stream to read from\n* @return decoded long value\n* @throws IOException if reading fails\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:writeVLong(java.io.DataOutput,long)": "/**\n* Writes a variable-length long to the output stream.\n* @param out DataOutput stream to write to\n* @param n the long value to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:lowerBound(java.util.List,java.lang.Object,java.util.Comparator)": "/**\n* Finds the index of the first element not less than the key.\n* @param list sorted list of elements\n* @param key value to compare against\n* @param cmp comparator for comparing elements\n* @return index of the lower bound in the list\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:<init>()": "/**\n* Private constructor to prevent instantiation of the Utils class.\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:upperBound(java.util.List,java.lang.Object,java.util.Comparator)": "/**\n* Finds the index to insert key in a sorted list.\n* @param list sorted list of elements\n* @param key element to compare\n* @param cmp comparator for ordering\n* @return index where key can be inserted\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:lowerBound(java.util.List,java.lang.Object)": "/**\n* Finds the index of the first element greater than or equal to the key.\n* @param list sorted list of comparable elements\n* @param key element to compare against\n* @return index of the lower bound in the list\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:upperBound(java.util.List,java.lang.Object)": "/**\n* Finds the upper bound index of a key in a sorted list.\n* @param list sorted list of comparable elements\n* @param key element to compare against\n* @return index of the first element greater than key\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput)": "/**\n* Reads a variable-length integer from input stream.\n* @param in DataInput stream to read from\n* @return decoded integer value\n* @throws IOException if reading fails or value is out of int range\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)": "/**\n* Writes a variable-length integer to the output stream.\n* @param out DataOutput stream to write to\n* @param n the integer value to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput)": "/**\n* Reads a UTF-8 encoded string from input stream.\n* @param in DataInput stream to read from\n* @return decoded String or null if length is -1\n*/",
        "org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)": "/**\n* Writes a string to output, encoding its length and bytes.\n* @param out DataOutput stream to write to\n* @param s input string to encode, null for -1 length\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$TFileMeta": {
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:isSorted()": "/**\n* Checks if the string comparator is not empty.\n* @return true if the comparator is not empty, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getRecordCount()": "/**\n* Returns the total number of records.\n* @return long representing the record count\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getComparator()": "/**\n* Retrieves the current BytesComparator instance.\n* @return BytesComparator object used for comparison\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getComparatorString()": "/**\n* Retrieves the string representation of the comparator.\n* @return String representation of the comparator\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:incRecordCount()": "/**\n* Increments the record count by one.\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String)": "/**** Creates a BytesComparator based on the given comparator string. \n* @param comparator specifies the type of comparator\n* @return BytesComparator or null for unsorted keys\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String)": "/**\n* Initializes TFileMeta with API version and comparator.\n* @param comparator string defining the comparator type\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput)": "/**\n* Initializes TFileMeta from DataInput stream, checking version compatibility.\n* @param in DataInput stream for reading TFile metadata\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput)": "/**\n* Writes API version, record count, and string comparator to output.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.compress.DefaultCodec": {
        "org.apache.hadoop.io.compress.DefaultCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf Configuration object to be set\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:getDefaultExtension()": "/**\n* Returns the default codec file extension.\n* @return default file extension as a String\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:getCompressorType()": "/**\n* Retrieves the Zlib compressor class type.\n* @return Compressor class type for Zlib compression\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType()": "/**\n* Retrieves the Zlib decompressor class type.\n* @return Class of the Zlib decompressor based on configuration\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream for the given OutputStream.\n* @param out the OutputStream to write compressed data\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream for the given InputStream.\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createCompressor()": "/**\n* Creates a Zlib compressor using the provided configuration.\n* @return Compressor instance for Zlib compression\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor()": "/**\n* Creates a DirectDecompressor using ZlibFactory.\n* @return DirectDecompressor or null if native Zlib is unavailable\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createDecompressor()": "/**\n* Creates a Zlib decompressor instance.\n* @return Decompressor for Zlib decompression\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a CompressionOutputStream with specified output and compressor.\n* @param out output stream for compressed data\n* @param compressor the compressor to use\n* @return CompressionOutputStream instance\n*/",
        "org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream using the provided InputStream and Decompressor.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @return CompressionInputStream for reading decompressed data\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream": {
        "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:<init>(org.apache.hadoop.io.compress.CompressionOutputStream)": "/**\n* Initializes FinishOnFlushCompressionStream with a CompressionOutputStream.\n* @param cout the CompressionOutputStream to wrap\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:write(byte[],int,int)": "/**\n* Writes a specified number of bytes from an array to the output stream.\n* @param b byte array to write from\n* @param off offset in the array\n* @param len number of bytes to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush()": "/**\n* Flushes the compression output stream and resets its state.\n* @throws IOException if an I/O error occurs during flush\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder": {
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:<init>(java.io.DataOutputStream,byte[])": "/**\n* Initializes ChunkEncoder with output stream and buffer.\n* @param out DataOutputStream for writing data\n* @param buf byte array for chunk data storage\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)": "/**\n* Writes a data chunk to output, handling last chunk differently.\n* @param chunk byte array to write\n* @param offset starting position in chunk\n* @param len length of data to write\n* @param last indicates if this is the last chunk\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)": "/**\n* Writes buffered data to output stream if count + len is positive.\n* @param data byte array to write\n* @param offset starting position in data\n* @param len number of bytes to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer()": "/**\n* Flushes the buffer by writing its contents if not empty.\n* @throws IOException if an I/O error occurs during write\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close()": "/**\n* Closes the resource, writing any remaining data if necessary.\n* @throws IOException if an I/O error occurs during write\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)": "/**\n* Writes bytes to buffer or flushes if overflow occurs.\n* @param b byte array to write, @param off offset, @param len number of bytes\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int)": "/**\n* Writes a byte to the buffer and flushes if full.\n* @param b byte to write\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush()": "/**\n* Flushes the output stream and buffer.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[])": "/**\n* Writes an entire byte array to the buffer.\n* @param b byte array to write\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Reader": {
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:close()": "/**\n* Closes the resource; currently no action is performed.\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount()": "/**\n* Returns the count of block regions.\n* @return number of BlockRegion objects in the list\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName()": "/**\n* Retrieves the name of the default compression algorithm.\n* @return String representing the default compression name\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long)": "/**\n* Retrieves the index of the block region near the specified offset.\n* @param offset the position to find the nearest block region\n* @return index of the block region or -1 if not found\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)": "/**\n* Creates a BlockReader for the specified BlockRegion using the compression algorithm.\n* @param compressAlgo algorithm for data compression\n* @param region the block region to read\n* @return a BlockReader instance\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String)": "/****\n* Retrieves a BlockReader for the specified MetaIndexEntry by name.\n* @param name the name of the MetaIndexEntry\n* @return BlockReader for the associated BlockRegion\n* @throws IOException if an I/O error occurs\n* @throws MetaBlockDoesNotExist if no entry is found\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int)": "/**\n* Retrieves a BlockReader for the specified data block index.\n* @param blockIndex index of the data block\n* @return BlockReader for the block\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Reader with input stream and configuration; reads metadata and verifies version.\n* @param fin input stream for reading data\n* @param fileLength length of the file to seek\n* @param conf configuration settings\n* @throws IOException if reading or seeking fails\n*/"
    },
    "org.apache.hadoop.io.file.tfile.ByteArray": {
        "org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[],int,int)": "/**\n* Constructs a ByteArray with specified buffer, offset, and length.\n* @param buffer byte array source\n* @param offset starting position in buffer\n* @param len number of bytes to include\n*/",
        "org.apache.hadoop.io.file.tfile.ByteArray:buffer()": "/**\n* Returns the byte array buffer.\n* @return byte array representing the buffer\n*/",
        "org.apache.hadoop.io.file.tfile.ByteArray:size()": "/**\n* Returns the number of elements in the collection.\n* @return the size as an integer\n*/",
        "org.apache.hadoop.io.file.tfile.ByteArray:offset()": "",
        "org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable)": "/***************\n* Constructs a ByteArray from BytesWritable.\n* @param other BytesWritable source object\n***************/",
        "org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[])": "/**\n* Constructs a ByteArray from the given byte buffer.\n* @param buffer byte array source\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry": {
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:entries()": "/**\n* Returns the number of key-value entries.\n* @return long representing the total entries count\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(byte[],int,int,long)": "/**\n* Initializes a TFileIndexEntry with a byte key and entry count.\n* @param newkey byte array containing the key data\n* @param offset starting position in newkey\n* @param len length of the key to copy\n* @param entries number of key-value entries\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:buffer()": "/**\n* Returns the byte array representing the key.\n* @return byte array of the key\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:offset()": "",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:size()": "",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput)": "/**\n* Constructs TFileIndexEntry from input stream.\n* @param in DataInput stream to read from\n* @throws IOException if reading fails\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput)": "/**\n* Writes key length, key data, and kvEntries to the output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong": {
        "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong:<init>(long)": "/**\n* Initializes ScalarLong with a specified magnitude.\n* @param m the magnitude value for the ScalarLong instance\n*/",
        "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong:magnitude()": ""
    },
    "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream": {
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,long,long)": "/**\n* Initializes a BoundedRangeFileInputStream with a specified offset and length.\n* @param in FSDataInputStream to read from\n* @param offset starting position in the stream\n* @param length number of bytes to read from the offset\n*/",
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:available()": "/**\n* Returns the number of bytes available to read.\n* @return number of available bytes, limited by the end position\n*/",
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:skip(long)": "/**\n* Skips a specified number of bytes in the file.\n* @param n number of bytes to skip\n* @return actual bytes skipped, limited by file end\n*/",
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:reset()": "/**\n* Resets position to the last marked point.\n* @throws IOException if mark is invalid (negative)\n*/",
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)": "/**\n* Reads bytes into an array from the input stream.\n* @param b byte array to store read bytes, off offset, len number of bytes to read\n* @return number of bytes read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[])": "/**\n* Reads bytes into an array from the input stream.\n* @param b byte array to store read bytes\n* @return number of bytes read or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read()": "/**\n* Reads a single byte from the input stream.\n* @return byte value (0-255) or -1 if end of stream is reached\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState": {
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getOutputStream()": "/**\n* Retrieves the output stream.\n* @return OutputStream associated with this object\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getStartPos()": "/**\n* Retrieves the starting position value.\n* @return the starting position as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos()": "/**\n* Computes the current position in the output stream.\n* @return total position as a long, combining fsOut and buffered size\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize()": "/**\n* Calculates the compressed size from the current position.\n* @return compressed size as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish()": "/**\n* Finalizes output stream and returns compressor to the pool.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes WBlockState with compression algorithm and output stream.\n* @param compressionAlgo algorithm for compression\n* @param fsOut file system output stream\n* @param fsOutputBuffer buffer for output data\n* @param conf configuration for buffer size\n* @throws IOException if stream creation fails\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender": {
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getRawSize()": "/**\n* Returns the raw size of a block as a long.\n* @return size in bytes, capped at 4GB\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize()": "/**\n* Retrieves the compressed size of the current block state.\n* @return compressed size as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close()": "/**\n* Closes the output stream, finalizing and registering the block state.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist": {
        "org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist:<init>(java.lang.String)": "/**\n* Constructs a MetaBlockDoesNotExist exception with a message.\n* @param s error message for the exception\n*/"
    },
    "org.apache.hadoop.io.file.tfile.CompareUtils": {
        "org.apache.hadoop.io.file.tfile.CompareUtils:<init>()": "/**\n* Private constructor to prevent instantiation of CompareUtils class.\n*/"
    },
    "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream": {
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:<init>(java.io.OutputStream,byte[])": "/**\n* Initializes a SimpleBufferedOutputStream with a specified buffer.\n* @param out the underlying OutputStream\n* @param buf the buffer for output data\n*/",
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flushBuffer()": "/**\n* Writes buffered data to output if available and resets the buffer count.\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:size()": "/**\n* Returns the current size count.\n* @return the number of elements counted\n*/",
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int)": "/**\n* Writes a byte to the buffer and flushes if full.\n* @param b byte to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)": "/**\n* Writes bytes to buffer or output, flushing if necessary.\n* @param b byte array to write from, @param off offset, @param len number of bytes to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush()": "/**\n* Flushes the output stream and buffers.\n* @throws IOException if an I/O error occurs during flushing\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder": {
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:<init>()": "/**\n* Initializes ChunkDecoder with lastChunk and closed set to true.\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:reset(java.io.DataInputStream)": "/**\n* Resets the input stream and internal state variables.\n* @param downStream the DataInputStream to reset to\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isClosed()": "/**\n* Checks if the object is closed.\n* @return true if closed, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:<init>(java.io.DataInputStream)": "/**\n* Initializes ChunkDecoder with input stream.\n* @param in DataInputStream for reading chunk data\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength()": "/**** Reads length from input stream and determines chunk status. */",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF()": "/**\n* Checks for end-of-file condition.\n* @return true if end-of-file is reached, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk()": "/**\n* Determines if the current data chunk is the last one.\n* @return true if it's the last chunk, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain()": "/**\n* Returns the remaining count of items.\n* @return remaining count as an integer\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read()": "/**\n* Reads a byte from the stream, checking for EOF and handling errors.\n* @return byte read or -1 if EOF is reached\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)": "/****\n* Reads bytes into an array, checking for EOF and bounds.\n* @param b byte array to fill, @param off offset, @param len number of bytes to read\n* @return number of bytes read or -1 if EOF is reached\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long)": "/**\n* Skips specified bytes in the input stream.\n* @param n number of bytes to skip\n* @return actual bytes skipped or 0 if EOF is reached\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[])": "/**\n* Reads bytes into the provided array.\n* @param b byte array to fill\n* @return number of bytes read or -1 if EOF is reached\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close()": "/**\n* Closes the stream, skipping to the end if not already closed.\n* @throws IOException if an I/O error occurs during operation\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Reader$Location": {
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:getBlockIndex()": "/**\n* Retrieves the current block index.\n* @return the current block index as an integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:getRecordIndex()": "/**\n* Retrieves the current record index.\n* @return the current record index as a long value\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(int,long)": "/**\n* Compares block and record indices for ordering.\n* @param bid block index to compare\n* @param rid record index to compare\n* @return positive, negative, or zero based on comparison\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(int,long)": "/**\n* Sets block and record indices after validating non-negative values.\n* @param blockIndex index of the block\n* @param recordIndex index of the record\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:incRecordIndex()": "/**\n* Increments the record index by one.\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:equals(java.lang.Object)": "/**\n* Compares this Location object with another for equality.\n* @param obj the object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Compares this Location with another for ordering.\n* @param other Location to compare with\n* @return comparison result as int\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)": "/**\n* Constructs a Location by setting block and record indices.\n* @param blockIndex index of the block\n* @param recordIndex index of the record\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Sets block and record indices from another Location object.\n* @param other Location object with block and record indices\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone()": "/**\n* Clones the current Location object.\n* @return a new Location instance with same block and record indices\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Copies block and record indices from another Location object.\n* @param other Location object to copy from\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Reader": {
        "org.apache.hadoop.io.file.tfile.TFile$Reader:begin()": "/**\n* Returns the starting location.\n* @return Location object representing the start\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:end()": "/**\n* Returns the end location.\n* @return Location object representing the end point\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted()": "/**\n* Checks if the associated tfileMeta is sorted.\n* @return true if sorted, false otherwise.\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount()": "/**\n* Retrieves the total number of entries.\n* @return long representing the entry count\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:close()": "/**\n* Closes the resource by invoking the close method on readerBCF.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName()": "/**\n* Retrieves the name of the comparator.\n* @return String representation of the comparator\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int)": "/**\n* Retrieves the count of entries for a specific block ID.\n* @param curBid block identifier\n* @return long count of entries associated with the block\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator()": "/**\n* Returns a comparator for Entry objects if sorted; throws if unsorted.\n* @return Comparator for Scanner.Entry\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte array segments if sorted.\n* @param a first byte array, @param o1 offset, @param l1 length; \n* @param b second byte array, @param o2 offset, @param l2 length.\n* @return comparison result as integer.\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Compares two RawComparable keys if sorted; throws exception if unsorted.\n* @param a first key to compare\n* @param b second key to compare\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long)": "/**\n* Retrieves a Location object near the specified offset.\n* @param offset the position to find the nearest Location\n* @return Location object or end if not found\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex()": "/**\n* Initializes tfileIndex if it's null using BlockReader and block count.\n* @throws IOException if an I/O error occurs during initialization\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String)": "/**\n* Retrieves a BlockReader for the specified MetaIndexEntry by name.\n* @param name the name of the MetaIndexEntry\n* @return DataInputStream for the associated BlockRegion\n* @throws IOException if an I/O error occurs\n* @throws MetaBlockDoesNotExist if no entry is found\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int)": "/**\n* Retrieves a BlockReader for the specified data block index.\n* @param blockIndex index of the data block\n* @return BlockReader for the block\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey()": "/**\n* Retrieves the first key from the TFile index.\n* @return RawComparable representing the first key\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey()": "/**\n* Retrieves the last key from the TFile index.\n* @return RawComparable of the last key or null if index is empty\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)": "/**\n* Retrieves the block containing the specified key.\n* @param key the key to search for\n* @param greater if true, finds the upper bound; otherwise, finds the lower bound\n* @return Location of the block or end if not found\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long)": "/**\n* Retrieves Location by record number after validating data index.\n* @param recNum record number to find location for\n* @return Location object representing the position\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Retrieves record number based on the specified location.\n* @param location specifies the record's location\n* @return total record number as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long)": "/**\n* Retrieves the key near the specified offset.\n* @param offset position to find the nearest key\n* @return RawComparable key or null if not found\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Reader with file stream and metadata.\n* @param fsdis input stream for file data\n* @param fileLength length of the file\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long)": "/**\n* Retrieves record number near the specified offset.\n* @param offset position to find the nearest record\n* @return total record number as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner()": "/**\n* Creates a Scanner for reading data within specified bounds.\n* @return Scanner instance for data reading\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)": "/**\n* Creates a Scanner for specified record number range.\n* @param beginRecNum starting record number\n* @param endRecNum ending record number\n* @return Scanner object for the specified range\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)": "/**\n* Creates a Scanner for a byte range.\n* @param offset start position, @param length number of bytes to scan\n* @return Scanner instance for the specified range\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Creates a Scanner with specified key bounds.\n* @param beginKey start key, can be null\n* @param endKey end key, can be null\n* @return Scanner instance based on key bounds\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])": "/**\n* Creates a Scanner based on the provided key bounds.\n* @param beginKey start key byte array, can be null\n* @param endKey end key byte array, can be null\n* @return Scanner instance for the specified key range\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Creates a Scanner with specified key bounds.\n* @param beginKey start key, can be null\n* @param endKey end key, can be null\n* @return Scanner instance based on key bounds\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])": "/**\n* Creates a Scanner with deprecated method signature.\n* @param beginKey start key byte array, can be null\n* @param endKey end key byte array, can be null\n* @return Scanner instance for the specified key range\n*/"
    },
    "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator": {
        "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator:compare(org.apache.hadoop.io.file.tfile.CompareUtils$Scalar,org.apache.hadoop.io.file.tfile.CompareUtils$Scalar)": "/**\n* Compares two Scalar objects based on their magnitude.\n* @param o1 first Scalar object\n* @param o2 second Scalar object\n* @return negative if o1 < o2, positive if o1 > o2, or 0 if equal\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Chunk": {
        "org.apache.hadoop.io.file.tfile.Chunk:<init>()": "/**\n* Private constructor for Chunk class; prevents instantiation.\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3": {
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)": "/**\n* Creates a decompression stream from the given input stream.\n* @param downStream input stream to decompress\n* @param decompressor decompressor instance to use\n* @param downStreamBufferSize buffer size for the input stream\n* @return InputStream for decompressed data\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)": "/**\n* Creates a compression stream for the provided output stream.\n* @param downStream the output stream to compress data into\n* @param compressor the compressor to use for compression\n* @param downStreamBufferSize the buffer size for the output stream\n* @return OutputStream for writing compressed data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:getCodec()": "/**\n* Retrieves the CompressionCodec instance.\n* @return CompressionCodec object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:isSupported()": "/**\n* Checks if the current implementation is supported.\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState": {
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getInputStream()": "/**\n* Returns the InputStream associated with this object.\n* @return InputStream instance or null if not initialized\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getBlockRegion()": "/**\n* Retrieves the current block region.\n* @return BlockRegion object representing the region\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName()": "/**\n* Retrieves the name of the compression algorithm.\n* @return String representing the compressed name\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish()": "/**\n* Closes input stream and returns decompressor to the pool.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes RBlockState with compression algorithm and input stream.\n* @param compressionAlgo algorithm for compression\n* @param fsin input stream for reading compressed data\n* @param region block region details\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile": {
        "org.apache.hadoop.io.file.tfile.TFile:<init>()": "/**\n* Private constructor for TFile class; prevents instantiation.\n*/",
        "org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms()": "/**\n* Retrieves an array of supported compression algorithms.\n* @return String array of supported algorithm names\n*/",
        "org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String)": "/**\n* Creates a comparator for RawComparable based on the specified name.\n* @param name the name of the comparator type\n* @return Comparator<RawComparable> or null for unsorted keys\n*/",
        "org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves chunk buffer size from configuration.\n* @param conf configuration object\n* @return buffer size or default value if invalid\n*/",
        "org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the file system input buffer size from configuration.\n* @param conf configuration object\n* @return buffer size in bytes, defaults to 256KB if not set\n*/",
        "org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the file system output buffer size from configuration.\n* @param conf configuration object\n* @return buffer size in bytes, defaulting to 256KB if not set\n*/",
        "org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[])": "/**\n* Main method to execute TFile dumping with specified file paths.\n* @param args file paths for TFile processing\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder": {
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(int)": "/**\n* Writes a byte to the output if remaining bytes allow.\n* @param b byte to write\n* @throws IOException if exceeding advertised size\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[],int,int)": "/**\n* Writes a byte array to output if within remaining size limit.\n* @param b byte array to write, @param off offset, @param len number of bytes to write\n* @throws IOException if len exceeds remaining size\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:flush()": "/**\n* Flushes the output stream to ensure all data is written.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:close()": "/**\n* Closes the resource, ensuring all bytes are written.\n* @throws IOException if not all bytes are written\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[])": "/**\n* Writes a byte array to output.\n* @param b byte array to write\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)": "/**\n* Initializes SingleChunkEncoder with output stream and size.\n* @param out DataOutputStream to write encoded data\n* @param size size of the chunk to encode\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.DataOutputOutputStream": {
        "org.apache.hadoop.io.DataOutputOutputStream:<init>(java.io.DataOutput)": "/**\n* Initializes DataOutputOutputStream with the specified DataOutput.\n* @param out the DataOutput to be used for output operations\n*/",
        "org.apache.hadoop.io.DataOutputOutputStream:write(int)": "/**\n* Writes a byte to the output stream.\n* @param b byte to be written\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DataOutputOutputStream:write(byte[],int,int)": "/**\n* Writes a portion of a byte array to the output stream.\n* @param b byte array to write from\n* @param off starting offset in the byte array\n* @param len number of bytes to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DataOutputOutputStream:write(byte[])": "/**\n* Writes the given byte array to the output stream.\n* @param b byte array to be written\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput)": "/**\n* Constructs an OutputStream from a DataOutput.\n* @param out the DataOutput to convert\n* @return OutputStream instance based on the input\n*/"
    },
    "org.apache.hadoop.io.FloatWritable": {
        "org.apache.hadoop.io.FloatWritable:<init>()": "/**\n* Default constructor for FloatWritable, initializing to zero.\n*/",
        "org.apache.hadoop.io.FloatWritable:set(float)": "/**\n* Sets the internal value to the specified float.\n* @param value the float value to set\n*/",
        "org.apache.hadoop.io.FloatWritable:readFields(java.io.DataInput)": "/**\n* Reads a float value from the input stream.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.FloatWritable:write(java.io.DataOutput)": "/**\n* Writes a float value to the specified DataOutput stream.\n* @param out the DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.FloatWritable:hashCode()": "/**\n* Computes the hash code for the float value.\n* @return integer hash code derived from the float value\n*/",
        "org.apache.hadoop.io.FloatWritable:compareTo(org.apache.hadoop.io.FloatWritable)": "/**\n* Compares this FloatWritable to another based on their float values.\n* @param o the FloatWritable to compare with\n* @return negative if less, zero if equal, positive if greater\n*/",
        "org.apache.hadoop.io.FloatWritable:toString()": "/**\n* Returns a string representation of the float value.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.io.FloatWritable:<init>(float)": "/**\n* Constructs a FloatWritable object and sets its value.\n* @param value the float value to initialize the object\n*/"
    },
    "org.apache.hadoop.io.MapFile$Merger": {
        "org.apache.hadoop.io.MapFile$Merger:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the Merger with the specified configuration.\n* @param conf configuration settings for the Merger\n*/",
        "org.apache.hadoop.io.MapFile$Merger:close()": "/**\n* Closes input readers and output writer, releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.MapFile$Merger:mergePass()": "/**\n* Merges key-value pairs from multiple input readers into a single output stream.\n* @throws IOException if an I/O error occurs during merging\n*/",
        "org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)": "/**\n* Opens input map files for reading and prepares output file for writing.\n* @param inMapFiles array of input map file paths\n* @param outMapFile output map file path\n* @throws IOException if file operations fail or classes mismatch\n*/",
        "org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)": "/**\n* Merges input map files and optionally deletes them.\n* @param inMapFiles array of input map file paths\n* @param deleteInputs flag to delete input files after merging\n* @param outMapFile output map file path\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer": {
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:lessThanUnsigned(long,long)": "/**\n* Compares two long values for unsigned less-than.\n* @param x1 first long value\n* @param x2 second long value\n* @return true if x1 is less than x2, false otherwise\n*/",
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays with specified offsets and lengths.\n* @return negative if buffer1 < buffer2, positive if buffer1 > buffer2, else 0\n*/"
    },
    "org.apache.hadoop.io.UTF8$Comparator": {
        "org.apache.hadoop.io.UTF8$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays based on specified offsets and lengths.\n* @param b1 first byte array\n* @param s1 start index in b1\n* @param l1 length of data in b1\n* @param b2 second byte array\n* @param s2 start index in b2\n* @param l2 length of data in b2\n* @return comparison result as an integer\n*/",
        "org.apache.hadoop.io.UTF8$Comparator:<init>()": "/**\n* Constructs a Comparator for UTF8 class keys.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile": {
        "org.apache.hadoop.io.SequenceFile:<init>()": "/**\n* Private constructor to prevent instantiation of SequenceFile class.\n*/",
        "org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)": "/****\n* Sets the default compression type for a job configuration.\n* @param job the job configuration to update\n* @param val the compression type to set\n*/",
        "org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the default compression type from the job configuration.\n* @param job the job configuration\n* @return CompressionType based on configuration or RECORD if not set\n*/",
        "org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the buffer size from configuration.\n* @param conf configuration object\n* @return buffer size as an integer\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])": "/**** Creates a Writer with specified compression options. \n* @param conf configuration settings \n* @param opts optional compression settings \n* @return Writer instance based on compression type \n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)": "/**\n* Creates a Writer for file output with specified configuration and classes.\n* @param fs file system to write to\n* @param conf configuration settings\n* @param name output file path\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @return Writer instance for writing data\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Creates a Writer for SequenceFiles with specified configurations.\n* @param fs file system for writing\n* @param conf configuration settings\n* @param name output file path\n* @param keyClass type of keys\n* @param valClass type of values\n* @param compressionType type of compression\n* @return Writer instance for SequenceFile\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)": "/**\n* Creates a Writer for SequenceFile with specified parameters.\n* @param fs file system, conf config, name file path, keyClass & valClass types, compressionType, progress\n* @return Writer instance\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Creates a Writer for SequenceFile with specified configuration and compression settings.\n* @param fs the FileSystem to write to\n* @param conf configuration settings\n* @param name the Path for the SequenceFile\n* @param keyClass the class type for keys\n* @param valClass the class type for values\n* @param compressionType the type of compression\n* @param codec the compression codec\n* @return Writer instance for writing SequenceFile\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Creates a Writer for SequenceFile with specified parameters.\n* @param fs file system, conf configuration, name output path, keyClass/keyClass type, \n* valClass/value type, compressionType/compression codec, progress tracking, metadata info\n* @return Writer instance\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Creates a SequenceFile Writer with specified configurations.\n* @param fs the FileSystem to write to\n* @param conf configuration settings\n* @param name the Path for the SequenceFile\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param bufferSize size of the buffer\n* @param replication replication factor\n* @param blockSize block size for the file\n* @param compressionType type of compression\n* @param codec compression codec\n* @param progress Progressable for tracking progress\n* @param metadata Metadata for the file\n* @return Writer instance for writing SequenceFile\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)": "/**\n* Creates a Writer for SequenceFile with specified parameters.\n* @param fs file system, conf configuration, name output path, keyClass key type,\n* valClass value type, compressionType type, codec for compression, progress for updates\n* @return Writer instance\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Creates a Writer for SequenceFile with specified configurations.\n* @param conf configuration settings\n* @param out output stream for writing data\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param compressionType type of compression\n* @param codec compression codec\n* @param metadata additional metadata\n* @return Writer instance\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Creates a Writer for SequenceFile with specified configurations and output stream.\n* @param conf configuration settings\n* @param out output stream for writing data\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param compressionType type of compression\n* @param codec compression codec\n* @return Writer instance\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])": "/**\n* Creates a Writer for a SequenceFile with specified configurations.\n* @param fc file context, @param conf configuration settings, @param name output path\n* @param keyClass class type for keys, @param valClass class type for values\n* @param compressionType type of compression, @param codec compression codec\n* @param metadata additional metadata, @param createFlag create options\n* @param opts additional settings\n* @return Writer instance for writing data\n*/",
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Creates a Writer for a SequenceFile with specified configurations.\n* @param fs filesystem, @param conf configuration, @param name output path, \n* @param keyClass key type, @param valClass value type, @param bufferSize size, \n* @param replication replication factor, @param blockSize size, \n* @param createParent create parent dir, @param compressionType compression type, \n* @param codec compression codec, @param metadata additional metadata\n* @return Writer instance\n*/"
    },
    "org.apache.hadoop.net.DNSDomainNameResolver": {
        "org.apache.hadoop.net.DNSDomainNameResolver:getAllByDomainName(java.lang.String)": "/**\n* Retrieves all IP addresses associated with a domain name.\n* @param domainName the domain to resolve\n* @return array of InetAddress objects for the domain\n* @throws UnknownHostException if the domain name cannot be resolved\n*/",
        "org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress)": "/**\n* Retrieves hostname from IP address, performing reverse lookup if necessary.\n* @param address IP address to resolve\n* @return hostname or null if not found\n*/",
        "org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)": "/**\n* Resolves hostnames for a given domain name.\n* @param domainName the domain to resolve\n* @param useFQDN flag for fully qualified domain name\n* @return array of resolved hostnames or IP addresses\n*/"
    },
    "org.apache.hadoop.net.DNS": {
        "org.apache.hadoop.net.DNS:reverseDns(java.net.InetAddress,java.lang.String)": "/**\n* Performs reverse DNS lookup for a given IP address.\n* @param hostIp IP address to reverse lookup\n* @param ns optional nameserver, defaults to system nameserver\n* @return hostname associated with the IP address\n*/",
        "org.apache.hadoop.net.DNS:getSubinterface(java.lang.String)": "/**\n* Retrieves a subinterface by name.\n* @param strInterface name of the subinterface\n* @return NetworkInterface object or null if not found\n*/",
        "org.apache.hadoop.net.DNS:getSubinterfaceInetAddrs(java.net.NetworkInterface)": "/**\n* Retrieves all InetAddress from a network interface's subinterfaces.\n* @param nif the parent NetworkInterface\n* @return a set of InetAddress from subinterfaces\n*/",
        "org.apache.hadoop.net.DNS:resolveLocalHostname()": "/**\n* Resolves the local hostname or returns 'localhost' if not found.\n* @return Canonical hostname as a String\n*/",
        "org.apache.hadoop.net.DNS:resolveLocalHostIPAddress()": "/**\n* Resolves the local host's IP address.\n* @return local IP address as a String or null if resolution fails\n*/",
        "org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)": "/**\n* Retrieves IP addresses for a given interface or its subinterfaces.\n* @param strInterface name of the network interface\n* @param returnSubinterfaces flag to include subinterface IPs\n* @return array of IP address strings\n*/",
        "org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)": "/**\n* Retrieves a list of InetAddress for a given network interface.\n* @param strInterface name of the network interface\n* @param returnSubinterfaces whether to include subinterface addresses\n* @return List of InetAddress objects\n*/",
        "org.apache.hadoop.net.DNS:getIPs(java.lang.String)": "/**\n* Retrieves IP addresses for a given network interface.\n* @param strInterface name of the network interface\n* @return array of IP address strings\n*/",
        "org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)": "/**\n* Retrieves hostnames for a given network interface.\n* @param strInterface network interface name\n* @param nameserver optional nameserver for DNS lookup\n* @param tryfallbackResolution flag to attempt fallback resolution\n* @return array of hostnames or cached hostname if resolution fails\n*/",
        "org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String)": "/**\n* Retrieves the default IP address for a specified network interface.\n* @param strInterface name of the network interface\n* @return first IP address as a string\n*/",
        "org.apache.hadoop.net.DNS:getHosts(java.lang.String)": "/**\n* Retrieves hostnames for a specified network interface.\n* @param strInterface network interface name\n* @return array of hostnames\n*/",
        "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)": "/**\n* Retrieves the default hostname for a given interface and nameserver.\n* @param strInterface network interface name\n* @param nameserver optional nameserver for DNS lookup\n* @param tryfallbackResolution flag for fallback resolution\n* @return the default hostname or cached hostname\n*/",
        "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String)": "/**\n* Retrieves default hostname for a given interface.\n* @param strInterface network interface name\n* @return default hostname as a String\n*/",
        "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)": "/**\n* Retrieves the default hostname for a given interface and nameserver.\n* @param strInterface network interface name\n* @param nameserver optional nameserver for DNS lookup\n* @return default hostname as a String\n*/"
    },
    "org.apache.hadoop.net.NetworkTopology": {
        "org.apache.hadoop.net.NetworkTopology:init(org.apache.hadoop.net.InnerNode$Factory)": "/**\n* Initializes network topology with a specified factory.\n* @param factory the InnerNode.Factory to create inner nodes\n* @return the updated NetworkTopology instance\n*/",
        "org.apache.hadoop.net.NetworkTopology:incrementRacks()": "/**\n* Increments the number of racks and updates multi-rack status if applicable.\n*/",
        "org.apache.hadoop.net.NetworkTopology:contains(org.apache.hadoop.net.Node)": "/**\n* Checks if the node is part of the cluster map.\n* @param node the Node to check for containment\n* @return true if the node is contained, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopology:getNumOfLeaves()": "/**\n* Retrieves the number of leaves from the cluster map.\n* @return number of leaves as an integer\n*/",
        "org.apache.hadoop.net.NetworkTopology:isSameParents(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Checks if two nodes have the same parent.\n* @param node1 first node to compare\n* @param node2 second node to compare\n* @return true if both nodes share the same parent, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopology:setRandomSeed(long)": "/**\n* Sets the random seed for generating random numbers.\n* @param seed the seed value for the random number generator\n*/",
        "org.apache.hadoop.net.NetworkTopology:getRandom()": "/**\n* Retrieves a Random instance, using ThreadLocalRandom if none is available.\n* @return Random instance from thread-local or ThreadLocalRandom\n*/",
        "org.apache.hadoop.net.NetworkTopology:isChildScope(java.lang.String,java.lang.String)": "/**\n* Checks if the childScope is a sub-scope of the parentScope.\n* @param parentScope the scope to check against\n* @param childScope the scope to verify as a child\n* @return true if childScope is a sub-scope, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopology:getFirstHalf(java.lang.String)": "/**\n* Retrieves the first half of a network location string.\n* @param networkLocation the full network location string\n* @return substring before the last path separator\n*/",
        "org.apache.hadoop.net.NetworkTopology:getLastHalf(java.lang.String)": "/**\n* Retrieves the last half of the network location string.\n* @param networkLocation the full network location\n* @return substring from the last path separator to the end\n*/",
        "org.apache.hadoop.net.NetworkTopology:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Calculates the weight between two nodes based on their hierarchy levels.\n* @param reader the reference node for comparison\n* @param node the target node to evaluate\n* @return the calculated weight as an integer\n*/",
        "org.apache.hadoop.net.NetworkTopology:normalizeNetworkLocationPath(java.lang.String)": "/**\n* Normalizes a network location path by ensuring it starts with a separator and has no trailing one.\n* @param path the network location path to normalize\n* @return normalized path or ROOT if input is null or empty\n*/",
        "org.apache.hadoop.net.NetworkTopology:countEmptyRacks()": "/**\n* Counts and logs the number of empty racks in the rackMap.\n*/",
        "org.apache.hadoop.net.NetworkTopology:getNumOfNonEmptyRacks()": "/**\n* Calculates the number of non-empty racks.\n* @return count of non-empty racks\n*/",
        "org.apache.hadoop.net.NetworkTopology:getNumOfRacks()": "/**\n* Returns the number of racks.\n* @return integer representing the count of racks\n*/",
        "org.apache.hadoop.net.NetworkTopology:hasClusterEverBeenMultiRack()": "/**\n* Checks if the cluster has ever been multi-rack.\n* @return true if multi-rack, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Calculates the distance between two nodes in a tree structure.\n* @param node1 first node, may be null\n* @param node2 second node, may be null\n* @return distance as an integer, or Integer.MAX_VALUE if a node is missing\n*/",
        "org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)": "/**\n* Checks if a node's location is within the specified scope.\n* @param node the Node to check\n* @param scope the scope as a String\n* @return true if node is in scope, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String)": "/**\n* Retrieves data nodes in a specified rack location.\n* @param loc the network location of the rack\n* @return list of child nodes in the rack or an empty list if not found\n*/",
        "org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String)": "/**\n* Retrieves a node by its location.\n* @param loc the location string to retrieve the node\n* @return Node object corresponding to the location\n*/",
        "org.apache.hadoop.net.NetworkTopology:toString()": "/**\n* Returns a string representation of racks and leaves in the cluster.\n* @return formatted string with rack and leaf information\n*/",
        "org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Checks if two nodes are in the same rack.\n* @param node1 first node to compare\n* @param node2 second node to compare\n* @return true if nodes share the same parent, false if either is null\n*/",
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)": "/**\n* Selects a random valid node excluding specified nodes.\n* @param parentNode the parent node to choose from\n* @param excludedScopeNode node to exclude from selection\n* @param excludedNodes collection of nodes to exclude\n* @param totalInScopeNodes total nodes available for selection\n* @param availableNodes count of nodes that can be returned\n* @return a randomly chosen valid node or null if not found\n*/",
        "org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Calculates weight based on network location similarity.\n* @param reader Node representing the reader\n* @param node Node representing the target node\n* @return calculated weight, or Integer.MAX_VALUE if inputs are null\n*/",
        "org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node)": "/**\n* Adds a node to the rackMap and counts empty racks.\n* @param node the Node to be added, ignored if null\n*/",
        "org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Calculates distance between two Nodes based on their path components.\n* @param node1 first Node for distance calculation\n* @param node2 second Node for distance calculation\n* @return distance as an integer, or Integer.MAX_VALUE if one node is null\n*/",
        "org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node)": "/**\n* Retrieves a Node by its network location.\n* @param node the Node from which to extract the network location\n* @return Node object corresponding to the network location\n*/",
        "org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String)": "/**\n* Retrieves leaf nodes from a given scope.\n* @param scope location string to find the node\n* @return List of leaf Node objects\n*/",
        "org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)": "/**\n* Counts available nodes in a scope, excluding specified nodes.\n* @param scope the scope to evaluate\n* @param excludedNodes collection of nodes to exclude\n* @return number of available nodes in scope\n*/",
        "org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node)": "/**\n* Removes a node from its rack if the rack is empty.\n* @param node the Node to be removed\n*/",
        "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)": "/**\n* Sorts nodes by distance using weights; applies secondary sort if provided.\n* @param reader reference node for weight calculation\n* @param nodes array of nodes to sort\n* @param activeLen number of active nodes in the array\n* @param secondarySort optional secondary sorting method\n* @param nonDataNodeReader flag for weight calculation method\n*/",
        "org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node)": "/**\n* Recommissions a node, adding it to the rackMap if valid.\n* @param node the Node to recommission, ignored if null or InnerNode\n*/",
        "org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node)": "/**\n* Adds a node to the topology, validating its depth and location.\n* @param node the Node to be added; ignored if null\n*/",
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)": "/**\n* Selects a random node from a scope, excluding specified nodes and scope.\n* @param scope the scope to choose from\n* @param excludedScope the scope to exclude from selection\n* @param excludedNodes collection of nodes to exclude\n* @return a randomly chosen Node or null if no valid node is found\n*/",
        "org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node)": "/**\n* Removes a node from the cluster, ensuring it's not an InnerNode.\n* @param node the Node to be removed\n*/",
        "org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node)": "/**\n* Decommissions a node, ensuring it's not an inner node.\n* @param node the Node to be decommissioned\n*/",
        "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)": "/**\n* Sorts nodes by distance using the specified reader and optional secondary sort.\n* @param reader reference node for weight calculation\n* @param nodes array of nodes to sort\n* @param activeLen number of active nodes in the array\n* @param secondarySort optional secondary sorting method\n*/",
        "org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)": "/**\n* Sorts nodes by distance using a reference node.\n* @param reader reference node for weight calculation\n* @param nodes array of nodes to sort\n* @param activeLen number of active nodes in the array\n* @param secondarySort optional secondary sorting method\n*/",
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)": "/**\n* Chooses a random Node based on scope, excluding specified nodes.\n* @param scope the scope for selection\n* @param excludedNodes nodes to exclude from selection\n* @return a randomly chosen Node or null if none is valid\n*/",
        "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)": "/**\n* Sorts nodes by distance using a reference node.\n* @param reader reference node for weight calculation\n* @param nodes array of nodes to sort\n* @param activeLen number of active nodes in the array\n*/",
        "org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)": "/**\n* Sorts nodes by distance using a reference node.\n* @param reader reference node for weight calculation\n* @param nodes array of nodes to sort\n* @param activeLen number of active nodes in the array\n*/",
        "org.apache.hadoop.net.NetworkTopology:<init>()": "/**\n* Initializes NetworkTopology with a factory and root InnerNode.\n* @return a new NetworkTopology instance\n*/",
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String)": "/**\n* Selects a random Node based on the provided scope.\n* @param scope the scope for selection\n* @return a randomly chosen Node or null if none is valid\n*/",
        "org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)": "/**\n* Retrieves a NetworkTopology instance initialized with a factory.\n* @param conf configuration settings\n* @param factory factory for creating inner nodes\n* @return initialized NetworkTopology instance\n*/",
        "org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a NetworkTopology instance with default inner node factory.\n* @param conf configuration settings\n* @return initialized NetworkTopology instance\n*/"
    },
    "org.apache.hadoop.net.NodeBase": {
        "org.apache.hadoop.net.NodeBase:getPath(org.apache.hadoop.net.Node)": "/**\n* Constructs a file path from the node's network location and name.\n* @param node the Node object to retrieve path information from\n* @return the constructed file path as a String\n*/",
        "org.apache.hadoop.net.NodeBase:normalize(java.lang.String)": "/**\n* Normalizes a network path by removing duplicates and ensuring proper format.\n* @param path the network location to normalize\n* @return the normalized path as a String\n*/",
        "org.apache.hadoop.net.NodeBase:<init>()": "/**\n* Constructs a new instance of NodeBase.\n*/",
        "org.apache.hadoop.net.NodeBase:set(java.lang.String,java.lang.String)": "/**\n* Sets the name and location, validating the name for invalid characters.\n* @param name network location name, must not contain '/'\n* @param location network location\n*/",
        "org.apache.hadoop.net.NodeBase:getLevel()": "/**\n* Retrieves the current level value.\n* @return the current level as an integer\n*/",
        "org.apache.hadoop.net.NodeBase:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.net.NodeBase:getNetworkLocation()": "/**\n* Retrieves the network location.\n* @return the network location as a String\n*/",
        "org.apache.hadoop.net.NodeBase:getParent()": "/**\n* Retrieves the parent node.\n* @return parent Node or null if no parent exists\n*/",
        "org.apache.hadoop.net.NodeBase:setLevel(int)": "/**\n* Sets the level of the object.\n* @param level the new level to be set\n*/",
        "org.apache.hadoop.net.NodeBase:setNetworkLocation(java.lang.String)": "/**\n* Sets the network location.\n* @param location the new network location as a String\n*/",
        "org.apache.hadoop.net.NodeBase:setParent(org.apache.hadoop.net.Node)": "/**\n* Sets the parent node for this node.\n* @param parent the parent Node to be assigned\n*/",
        "org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node)": "/**\n* Splits the file path from a Node into its components.\n* @param node the Node object for path retrieval\n* @return array of path components as Strings\n*/",
        "org.apache.hadoop.net.NodeBase:equals(java.lang.Object)": "/**\n* Compares this NodeBase object with another for equality.\n* @param to object to compare with this NodeBase\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.net.NodeBase:hashCode()": "/**\n* Computes the hash code based on the object's file path.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.net.NodeBase:toString()": "/**\n* Returns the string representation by constructing a file path from the current node.\n* @return constructed file path as a String\n*/",
        "org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String)": "/**\n* Calculates the depth of a normalized network path.\n* @param location the network path to analyze\n* @return the depth as an integer count of separators\n*/",
        "org.apache.hadoop.net.NodeBase:<init>(java.lang.String)": "/**\n* Initializes NodeBase with a normalized path.\n* @param path the network location to normalize and set\n*/",
        "org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a NodeBase with validated name and normalized location.\n* @param name network location name, must not contain '/'\n* @param location network location to normalize\n*/",
        "org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)": "/**\n* Constructs a NodeBase with validated name and normalized location.\n* @param name network location name, must not contain '/'\n* @param location network location to normalize\n* @param parent parent Node\n* @param level depth level of the node\n*/"
    },
    "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException": {
        "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException:<init>(java.lang.String)": "/**\n* Constructs an InvalidTopologyException with the specified message.\n* @param msg detailed error message\n*/"
    },
    "org.apache.hadoop.net.SocketIOWithTimeout": {
        "org.apache.hadoop.net.SocketIOWithTimeout:checkChannelValidity(java.lang.Object)": "/**\n* Validates the provided channel object for null and type.\n* @param channel the channel object to validate\n* @throws IOException if channel is null or not a SelectableChannel\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:isOpen()": "/**\n* Checks if the channel is open and not closed.\n* @return true if the channel is open, false otherwise\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:timeoutExceptionString(java.nio.channels.SelectableChannel,long,int)": "/**\n* Generates a timeout exception message for a channel operation.\n* @param channel the SelectableChannel being monitored\n* @param timeout duration of the timeout in milliseconds\n* @param ops the operation type (read, write, connect)\n* @return formatted timeout exception message string\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:close()": "/**\n* Marks the resource as closed.\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:getChannel()": "/**\n* Retrieves the associated selectable channel.\n* @return SelectableChannel instance\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:setTimeout(long)": "/**\n* Sets the timeout duration in milliseconds.\n* @param timeoutMs duration to set as timeout\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)": "/**\n* Initializes SocketIOWithTimeout with a channel and timeout.\n* @param channel the SelectableChannel for I/O operations\n* @param timeout the timeout duration in milliseconds\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)": "/**\n* Performs I/O operations on a ByteBuffer with specified ops.\n* @param buf the ByteBuffer containing data to read/write\n* @param ops operations to perform on the channel\n* @return number of bytes processed or -1 if closed\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)": "/**** Connects to a socket channel with a specified timeout. \n* @param channel the SocketChannel to connect \n* @param endpoint the target SocketAddress \n* @param timeout maximum wait time in milliseconds \n* @throws IOException if connection fails or times out \n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int)": "/**\n* Waits for IO operations on a channel with a timeout.\n* @param ops operations to perform (read, write, etc.)\n* @throws IOException if a timeout occurs\n*/"
    },
    "org.apache.hadoop.net.SocketOutputStream": {
        "org.apache.hadoop.net.SocketOutputStream:write(java.nio.ByteBuffer)": "/**\n* Writes data from ByteBuffer to a destination.\n* @param src buffer containing data to write\n* @return number of bytes written\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.SocketOutputStream:close()": "/**\n* Closes the output channel and associated writer.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.net.SocketOutputStream:isOpen()": "/**\n* Checks if the writer is currently open.\n* @return true if the writer is open, false otherwise\n*/",
        "org.apache.hadoop.net.SocketOutputStream:waitForWritable()": "/**\n* Waits until the writer is ready for writing.\n* @throws IOException if an I/O error occurs during waiting\n*/",
        "org.apache.hadoop.net.SocketOutputStream:getChannel()": "/**\n* Retrieves the writable byte channel from the writer.\n* @return WritableByteChannel associated with the writer\n*/",
        "org.apache.hadoop.net.SocketOutputStream:setTimeout(int)": "/**\n* Sets the timeout duration for the writer.\n* @param timeoutMs timeout in milliseconds\n*/",
        "org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)": "/**\n* Writes a byte array to a destination with error handling.\n* @param b byte array to write, @param off start offset, @param len number of bytes to write\n* @throws IOException if an I/O error occurs or stream is closed\n*/",
        "org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)": "/**\n* Transfers bytes from a FileChannel, tracking wait and transfer times.\n* @param fileCh the FileChannel to transfer from\n* @param position starting position in the channel\n* @param count number of bytes to transfer\n* @param waitForWritableTime records total wait time\n* @param transferToTime records total transfer time\n* @throws IOException if an I/O error occurs during transfer\n*/",
        "org.apache.hadoop.net.SocketOutputStream:write(int)": "/**\n* Writes a single byte to the output stream.\n* @param b the byte to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)": "/**\n* Transfers bytes from a FileChannel starting at a specified position.\n* @param fileCh the FileChannel to transfer from\n* @param position starting position in the channel\n* @param count number of bytes to transfer\n* @throws IOException if an I/O error occurs during transfer\n*/",
        "org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)": "/**\n* Initializes SocketOutputStream with a channel and timeout.\n* @param channel the WritableByteChannel for writing\n* @param timeout the timeout duration in milliseconds\n*/",
        "org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)": "/**\n* Creates a SocketOutputStream from a Socket with a specified timeout.\n* @param socket the Socket to initialize the stream from\n* @param timeout the timeout duration in milliseconds\n*/"
    },
    "org.apache.hadoop.net.ConnectTimeoutException": {
        "org.apache.hadoop.net.ConnectTimeoutException:<init>(java.lang.String)": "/**\n* Constructs a ConnectTimeoutException with the specified message.\n* @param msg the detail message for the exception\n*/"
    },
    "org.apache.hadoop.net.SocketInputStream$Reader": {
        "org.apache.hadoop.net.SocketInputStream$Reader:performIO(java.nio.ByteBuffer)": "/**\n* Reads data from the channel into the provided ByteBuffer.\n* @param buf buffer to store read data\n* @return number of bytes read\n*/",
        "org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)": "/**\n* Constructs a Reader with a channel and timeout.\n* @param channel the ReadableByteChannel for I/O operations\n* @param timeout the timeout duration in milliseconds\n*/"
    },
    "org.apache.hadoop.net.TableMapping": {
        "org.apache.hadoop.net.TableMapping:getRawMapping()": "/**\n* Retrieves the RawTableMapping instance.\n* @return RawTableMapping object from rawMapping\n*/",
        "org.apache.hadoop.net.TableMapping:getConf()": "/**\n* Retrieves the Configuration from the RawTableMapping.\n* @return Configuration object from the raw mapping\n*/",
        "org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration for the current instance and its raw mapping.\n* @param conf the Configuration object to be set\n*/",
        "org.apache.hadoop.net.TableMapping:<init>()": "/**\n* Constructs a TableMapping instance using a new RawTableMapping.\n*/",
        "org.apache.hadoop.net.TableMapping:reloadCachedMappings()": "/**\n* Reloads cached mappings and updates raw mappings.\n*/"
    },
    "org.apache.hadoop.net.CachedDNSToSwitchMapping": {
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:reloadCachedMappings()": "/**\n* Clears the cached mappings to refresh data.\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:getUncachedHosts(java.util.List)": "/**\n* Retrieves hosts not present in the cache.\n* @param names list of host names to check\n* @return list of uncached host names\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:cacheResolvedHosts(java.util.List,java.util.List)": "/**\n* Caches resolved hosts for the given uncached hosts.\n* @param uncachedHosts list of hosts to be cached\n* @param resolvedHosts list of corresponding resolved hosts\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:getCachedHosts(java.util.List)": "/**\n* Retrieves cached network locations for given host names.\n* @param names list of host names to lookup\n* @return list of network locations or null if any name is not found\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:getSwitchMap()": "/**\n* Returns a copy of the switch map from the cache.\n* @return a Map containing switch key-value pairs\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:toString()": "/**\n* Returns a string representation of the cached switch mapping.\n* @return String describing the raw mapping\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:isSingleSwitch()": "/**\n* Checks if the mapping is a single switch.\n* @return true if single switch, otherwise false\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:reloadCachedMappings(java.util.List)": "/**\n* Removes specified names from the cache.\n* @param names list of names to be removed from the cache\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping)": "/**\n* Initializes CachedDNSToSwitchMapping with a raw mapping.\n* @param rawMapping the DNSToSwitchMapping to cache\n*/",
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List)": "/**\n* Resolves host names to IP addresses, caching results for future lookups.\n* @param names list of host names to resolve\n* @return list of resolved IP addresses\n*/"
    },
    "org.apache.hadoop.net.AbstractDNSToSwitchMapping": {
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf Configuration object to be set\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:<init>()": "/**\n* Constructor for AbstractDNSToSwitchMapping class.\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes AbstractDNSToSwitchMapping with the given configuration.\n* @param conf configuration settings for DNS to switch mapping\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:getSwitchMap()": "/**\n* Retrieves a map of switches as key-value pairs.\n* @return Map of switch names and their corresponding values\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitch()": "/**\n* Checks if the current state is a single switch.\n* @return false indicating not a single switch\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:getConf()": "/**\n* Returns the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology()": "/**\n* Dumps the network topology as a formatted string.\n* @return String representation of the topology or a message if none exists.\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)": "/**\n* Checks if the mapping is a single switch.\n* @param mapping the DNSToSwitchMapping to evaluate\n* @return true if it's a single switch, false otherwise\n*/",
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy()": "/**\n* Checks if a single switch is configured by script policy.\n* @return true if no script file is set; false otherwise\n*/"
    },
    "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping": {
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:toString()": "/**\n* Returns a string representation of the script name or a default message.\n* @return formatted script name or NO_SCRIPT if scriptName is null\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:reloadCachedMappings()": "/**\n* Reloads cached mappings; no action needed as there is no cache to reload.\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:reloadCachedMappings(java.util.List)": "/**\n* Reloads cached mappings; no operation for RawScriptBasedMapping as it has no cache.\n* @param names list of mapping names (ignored)\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:isSingleSwitch()": "/**\n* Checks if the scriptName is null to determine single switch status.\n* @return true if scriptName is null, false otherwise\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>()": "/**\n* Initializes a new instance of RawScriptBasedMapping.\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)": "/**\n* Executes a command with arguments and returns combined output.\n* @param args list of command arguments, @param commandScriptName name of the command script\n* @return concatenated output string or null if execution fails\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List)": "/**\n* Resolves names to their corresponding switch info.\n* @param names list of names to resolve\n* @return list of switch info or null if an error occurs\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and initializes script parameters.\n* @param conf Configuration object to be set\n*/"
    },
    "org.apache.hadoop.net.ScriptBasedMapping": {
        "org.apache.hadoop.net.ScriptBasedMapping:getRawMapping()": "/**\n* Retrieves the raw script-based mapping.\n* @return RawScriptBasedMapping instance\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping:getConf()": "/**\n* Retrieves the configuration from the raw mapping.\n* @return Configuration object from the raw script-based mapping\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping:toString()": "/**\n* Returns a string representation of the script-based mapping.\n* @return formatted mapping description including raw mapping details\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping)": "/**\n* Constructs ScriptBasedMapping using a DNSToSwitchMapping.\n* @param rawMap the DNSToSwitchMapping to initialize the mapping\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping:<init>()": "/**\n* Initializes ScriptBasedMapping with a new RawScriptBasedMapping instance.\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration for the current context and raw mapping.\n* @param conf Configuration object to be set\n*/",
        "org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ScriptBasedMapping with given configuration.\n* @param conf Configuration object for mapping setup\n*/"
    },
    "org.apache.hadoop.util.CloseableReferenceCount": {
        "org.apache.hadoop.util.CloseableReferenceCount:reference()": "/**\n* Increments status and checks if channel is closed; throws exception if closed.\n* @throws ClosedChannelException if the channel is closed\n*/",
        "org.apache.hadoop.util.CloseableReferenceCount:<init>()": "/**\n* Constructs a new CloseableReferenceCount instance.\n*/",
        "org.apache.hadoop.util.CloseableReferenceCount:unreferenceCheckClosed()": "/**\n* Checks and decrements status; throws exception if channel is closed.\n* @throws ClosedChannelException if status indicates closed channel\n*/",
        "org.apache.hadoop.util.CloseableReferenceCount:isOpen()": "/**\n* Checks if the status indicates the object is open.\n* @return true if open, false if closed\n*/",
        "org.apache.hadoop.util.CloseableReferenceCount:setClosed()": "/**\n* Marks the channel as closed and returns previous status bits.\n* @return previous status bits without the closed mask\n* @throws ClosedChannelException if the channel is already closed\n*/",
        "org.apache.hadoop.util.CloseableReferenceCount:getReferenceCount()": "/**\n* Returns the reference count after masking closed status.\n* @return int reference count excluding closed status\n*/",
        "org.apache.hadoop.util.CloseableReferenceCount:unreference()": "/**\n* Decrements reference count and checks for invalid state.\n* @return true if count equals STATUS_CLOSED_MASK, false otherwise\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet": {
        "org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet:<init>()": "/**\n* Initializes FdSet and allocates memory for data.\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry": {
        "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:getDomainSocket()": "/**\n* Retrieves the current domain socket.\n* @return DomainSocket instance associated with this object\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:getHandler()": "/**\n* Retrieves the current handler instance.\n* @return the Handler object\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:<init>(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)": "/**\n* Constructs an Entry with a DomainSocket and a Handler.\n* @param socket the DomainSocket for communication\n* @param handler the Handler for processing events\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocket": {
        "org.apache.hadoop.net.unix.DomainSocket:getInputStream()": "/**\n* Retrieves the input stream for domain data.\n* @return DomainInputStream instance\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:getEffectivePath(java.lang.String,int)": "/**\n* Replaces \"_PORT\" in the path with the specified port number.\n* @param path the original path string\n* @param port the port number to insert\n* @return modified path with port number included\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:toString()": "/**\n* Returns a string representation of the DomainSocket with file descriptor and path.\n* @return formatted string of DomainSocket details\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:getOutputStream()": "/**\n* Retrieves the output stream for domain operations.\n* @return DomainOutputStream instance\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)": "/**\n* Initializes DomainSocket with file descriptor and path.\n* @param path socket file path\n* @param fd file descriptor for the socket\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:isOpen()": "/**\n* Checks if the object is open based on reference count status.\n* @return true if open, false if closed\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:close()": "/**\n* Closes the DomainSocket, waiting for all references to be released.\n* @throws IOException if an I/O error occurs during closure\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:unreference(boolean)": "/**\n* Decrements reference count; checks channel status if specified.\n* @param checkClosed true to check for closed channel before decrementing\n* @throws ClosedChannelException if channel is closed\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String)": "/**\n* Binds to a socket path and starts listening.\n* @param path socket file path\n* @return DomainSocket object with file descriptor\n* @throws IOException if binding fails\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:socketpair()": "/**\n* Creates a pair of connected DomainSocket instances.\n* @return an array of two DomainSocket objects\n* @throws IOException if socket creation fails\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String)": "/**\n* Connects to a DomainSocket using the specified path.\n* @param path socket file path\n* @return DomainSocket instance\n* @throws IOException if connection fails\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:accept()": "/**\n* Accepts a connection and returns a DomainSocket.\n* @return DomainSocket instance for the accepted connection\n* @throws IOException if an I/O error occurs during the operation\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)": "/**\n* Sets an attribute with specified type and size.\n* @param type attribute type\n* @param size attribute size\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:getAttribute(int)": "/**\n* Retrieves an attribute by type, managing reference count.\n* @param type the attribute type to fetch\n* @return the attribute value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:shutdown()": "/**\n* Shuts down the resource, managing reference count and handling exceptions.\n* @throws IOException if an I/O error occurs during shutdown\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)": "/**\n* Sends file descriptors with a buffer; manages reference count.\n* @param descriptors array of file descriptors to send\n* @param jbuf byte buffer for data\n* @param offset starting position in buffer\n* @param length number of bytes to send\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)": "/****\n* Receives file descriptors into input streams.\n* @param streams array to populate with FileInputStreams\n* @param buf buffer for data transfer\n* @param offset starting position in buffer\n* @param length number of bytes to read\n* @return number of bytes received\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocketWatcher": {
        "org.apache.hadoop.net.unix.DomainSocketWatcher:isClosed()": "/**\n* Checks if the resource is closed.\n* @return true if closed, false otherwise\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:toString()": "/**\n* Returns a string representation of the DomainSocketWatcher instance.\n* @return string with instance identity hash code\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)": "/**\n* Sends callback for a file descriptor; returns true if closed, false otherwise.\n* @param caller identifier of the caller method\n* @param entries mapping of file descriptors to entries\n* @param fdSet set of file descriptors\n* @param fd file descriptor to process\n* @return true if the handler closes the fd, false otherwise\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)": "/**\n* Adds a notification socket to entries and updates the FdSet.\n* @param entries map of socket file descriptors to Entry objects\n* @param fdSet set for managing file descriptors\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:kick()": "/**\n* Sends a kick signal if not already kicked, logging errors on failure.\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)": "/**\n* Sends callback and removes entry if callback succeeds.\n* @param caller identifier of the caller method\n* @param entries mapping of file descriptors to entries\n* @param fdSet set of file descriptors\n* @param fd file descriptor to process\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:close()": "/**\n* Closes the DomainSocket and waits for the watcher thread to terminate.\n* @throws IOException if an I/O error occurs during closure\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)": "/**\n* Adds a DomainSocket and Handler; handles closed socket scenarios.\n* @param sock the DomainSocket to add\n* @param handler the Handler for processing the socket\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket)": "/**\n* Removes a DomainSocket and waits for its processing.\n* @param sock DomainSocket to be removed\n*/",
        "org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)": "/**** Initializes DomainSocketWatcher with interrupt check period and source identifier. \n* @param interruptCheckPeriodMs interval in milliseconds for checks, must be > 0\n* @param src source identifier for the watcher thread name\n* @throws IOException if socket creation fails\n* @throws UnsupportedOperationException if loading fails\n*/"
    },
    "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo": {
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo:close()": "/**\n* Closes the selector if it is not null, logging any IOException encountered.\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo:<init>(java.nio.channels.spi.SelectorProvider,java.nio.channels.Selector)": "/**\n* Constructs a SelectorInfo with a provider and selector.\n* @param provider the SelectorProvider instance\n* @param selector the Selector instance\n*/"
    },
    "org.apache.hadoop.net.InnerNodeImpl": {
        "org.apache.hadoop.net.InnerNodeImpl:getNumOfChildren()": "/**\n* Returns the number of children.\n* @return count of children in the collection\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:isRack()": "/**\n* Checks if the node is a rack based on its children.\n* @return true if no children or all are leaf nodes, false if any child is an InnerNode\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:isAncestor(org.apache.hadoop.net.Node)": "/**\n* Checks if the current node is an ancestor of the given node.\n* @param n the node to check against\n* @return true if this node is an ancestor, false otherwise\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:isParent(org.apache.hadoop.net.Node)": "/**\n* Checks if the given node is a parent based on its network location.\n* @param n the Node to check\n* @return true if n is a parent, false otherwise\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:getLoc(java.lang.String)": "/**\n* Retrieves a child node by location string.\n* @param loc location path, returns this if null or empty\n* @return Node corresponding to the location or null if not found\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:getNumOfLeaves()": "/**\n* Returns the number of leaves in the structure.\n* @return the count of leaves as an integer\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:getChildren()": "/**\n* Retrieves the list of child nodes.\n* @return List of Node objects representing children\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:isLeafParent()": "/**\n* Determines if the node is a leaf parent by checking if it's a rack.\n* @return true if the node is a rack, false otherwise\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node)": "/**\n* Retrieves the name of the next ancestor node.\n* @param n the node to check\n* @return name of the next ancestor\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object)": "/**\n* Compares this NodeBase object with another for equality.\n* @param to object to compare with this NodeBase\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:hashCode()": "/**\n* Returns the hash code for the object, based on its file path.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String)": "/**\n* Constructs InnerNodeImpl with a normalized path.\n* @param path the network location to normalize and set\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)": "/**\n* Constructs an InnerNodeImpl with validated name and location.\n* @param name network location name, must not contain '/'\n* @param location network location to normalize\n* @param parent parent InnerNode\n* @param level depth level of the node\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)": "/**\n* Retrieves a leaf node by index, excluding a specified node.\n* @param leafIndex index of the desired leaf\n* @param excludedNode node to exclude from the search\n* @return the leaf node or null if out of bounds or not found\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node)": "/**\n* Removes a node from the tree if it's a descendant.\n* @param n the node to remove\n* @return true if removed, false otherwise\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String)": "/**\n* Creates a new InnerNodeImpl as the parent node.\n* @param parentName name for the new parent node\n* @return InnerNodeImpl instance with specified name and properties\n*/",
        "org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node)": "/**\n* Adds a node if it's a descendant; creates a parent if necessary.\n* @param n the node to add\n* @return true if added, false if already exists\n*/"
    },
    "org.apache.hadoop.net.InnerNodeImpl$Factory": {
        "org.apache.hadoop.net.InnerNodeImpl$Factory:<init>()": "/**\n* Protected constructor for the Factory class, prevents direct instantiation.\n*/",
        "org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String)": "/**\n* Creates a new InnerNodeImpl with a normalized network path.\n* @param path the network location to normalize\n* @return a new InnerNodeImpl instance\n*/"
    },
    "org.apache.hadoop.net.SocksSocketFactory": {
        "org.apache.hadoop.net.SocksSocketFactory:<init>()": "/**\n* Constructs a SocksSocketFactory with no proxy settings.\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:<init>(java.net.Proxy)": "/**\n* Constructs a SocksSocketFactory with the specified proxy.\n* @param proxy the Proxy to be used for socket connections\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:createSocket()": "/**\n* Creates a new socket connected to the specified proxy.\n* @return a Socket instance\n* @throws IOException if an I/O error occurs during socket creation\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:hashCode()": "/**\n* Returns the hash code of the proxy object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:equals(java.lang.Object)": "/**\n* Compares this SocksSocketFactory to another object for equality.\n* @param obj object to compare with this instance\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:setProxy(java.lang.String)": "/**\n* Configures a SOCKS proxy using the provided host and port string.\n* @param proxyStr formatted as 'host:port'\n* @throws RuntimeException if the format is invalid\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:getConf()": "",
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)": "/**\n* Creates and connects a socket to the specified address and port.\n* @param addr the InetAddress to connect to\n* @param port the port number to connect to\n* @return a connected Socket instance\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)": "/**\n* Creates and connects a socket to a specified address and port.\n* @param addr destination address, @param port destination port,\n* @param localHostAddr local address, @param localPort local port\n* @return a connected Socket instance\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)": "/**\n* Creates and connects a socket to the specified host and port.\n* @param host the host name or IP address\n* @param port the port number\n* @return a connected Socket instance\n* @throws IOException if an I/O error occurs\n* @throws UnknownHostException if the host is unknown\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)": "/**\n* Creates and connects a socket to the specified host and port.\n* @param host the target host to connect to\n* @param port the target port on the host\n* @param localHostAddr local address for socket binding\n* @param localPort local port for socket binding\n* @return a connected Socket instance\n* @throws IOException if an I/O error occurs\n* @throws UnknownHostException if the IP address of the host cannot be determined\n*/",
        "org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration and configures SOCKS proxy if specified.\n* @param conf configuration object containing settings\n*/"
    },
    "org.apache.hadoop.net.StandardSocketFactory": {
        "org.apache.hadoop.net.StandardSocketFactory:<init>()": "/**\n* Constructs a new StandardSocketFactory instance.\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:createSocket()": "/**\n* Creates a non-blocking NIO socket.\n* @return Socket instance with associated SocketChannel\n* @throws IOException if socket creation fails\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:equals(java.lang.Object)": "/**\n* Checks if the current object is equal to the specified object.\n* @param obj object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:hashCode()": "/**\n* Returns the hash code based on the class type.\n* @return integer hash code of the class\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)": "/**\n* Creates and connects a socket to the specified address and port.\n* @param addr target InetAddress for connection\n* @param port target port number\n* @return connected Socket instance\n* @throws IOException if socket creation or connection fails\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)": "/**\n* Creates and connects a socket to specified addresses and ports.\n* @param addr destination address, @param port destination port,\n* @param localHostAddr local address, @param localPort local port\n* @return connected Socket instance\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)": "/**\n* Creates and connects a socket to the specified host and port.\n* @param host the server hostname\n* @param port the server port number\n* @return connected Socket instance\n* @throws IOException if socket creation or connection fails\n*/",
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)": "/**\n* Creates and connects a socket to the specified host and port.\n* @param host target host address\n* @param port target port number\n* @param localHostAddr local address to bind\n* @param localPort local port number\n* @return connected Socket instance\n* @throws IOException if socket operations fail\n* @throws UnknownHostException if the host is unknown\n*/"
    },
    "org.apache.hadoop.net.ScriptBasedMappingWithDependency": {
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:getRawMapping()": "/**\n* Retrieves the raw mapping as RawScriptBasedMappingWithDependency.\n* @return RawScriptBasedMappingWithDependency instance\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString()": "/**\n* Returns a string representation of the script-based mapping.\n* @return formatted string with raw mapping details\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String)": "/**\n* Retrieves cached dependencies for a normalized host name.\n* @param name the host name to normalize and retrieve dependencies for\n* @return List of dependency names or empty list if not found\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>()": "/**\n* Constructs ScriptBasedMappingWithDependency using RawScriptBasedMappingWithDependency.\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration for the current instance and its raw mapping.\n* @param conf Configuration object to be set\n*/"
    },
    "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency": {
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:toString()": "/**\n* Returns a string representation of the dependency script.\n* @return formatted string with dependency script name\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:getDependency(java.lang.String)": "/**\n* Retrieves a list of dependencies for the given name.\n* @param name the name for which dependencies are requested\n* @return List of dependency names or null if an error occurs\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>()": "/**\n* Constructs a new RawScriptBasedMappingWithDependency instance.\n*/",
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and initializes the dependency script name.\n* @param conf Configuration object to be set\n*/"
    },
    "org.apache.hadoop.net.SocketInputStream": {
        "org.apache.hadoop.net.SocketInputStream:setTimeout(long)": "/**\n* Sets the timeout duration for the reader.\n* @param timeoutMs timeout in milliseconds\n*/",
        "org.apache.hadoop.net.SocketInputStream:read(java.nio.ByteBuffer)": "/**\n* Reads data into the provided ByteBuffer.\n* @param dst ByteBuffer to store read data\n* @return number of bytes read\n*/",
        "org.apache.hadoop.net.SocketInputStream:close()": "/**\n* Closes the input channel and reader, releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.net.SocketInputStream:isOpen()": "/**\n* Checks if the reader is currently open.\n* @return true if the reader is open, false otherwise\n*/",
        "org.apache.hadoop.net.SocketInputStream:waitForReadable()": "/**\n* Waits for the reader to be ready for reading.\n* @throws IOException if an I/O error occurs during waiting\n*/",
        "org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)": "/**\n* Reads bytes into the specified array from the input stream.\n* @param b byte array to store read data\n* @param off offset in the array to start storing data\n* @param len number of bytes to read\n* @return number of bytes read\n*/",
        "org.apache.hadoop.net.SocketInputStream:read()": "/**** Reads a single byte from the input stream. \n* @return byte value or -1 if end of stream is reached \n* @throws IOException if read fails \n*/",
        "org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)": "/**\n* Initializes SocketInputStream with a channel and timeout.\n* @param channel the ReadableByteChannel for I/O operations\n* @param timeout the timeout duration in milliseconds\n*/",
        "org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)": "/**\n* Constructs SocketInputStream from a Socket with a specified timeout.\n* @param socket the Socket for I/O operations\n* @param timeout the timeout duration in milliseconds\n*/",
        "org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket)": "/**\n* Initializes SocketInputStream from a Socket.\n* @param socket the Socket for I/O operations\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.net.DomainNameResolverFactory": {
        "org.apache.hadoop.net.DomainNameResolverFactory:<init>()": "/**\n* Prevents instantiation of the utility class DomainNameResolverFactory.\n*/",
        "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Creates a DomainNameResolver instance based on configuration.\n* @param conf configuration settings\n* @param configKey key to retrieve resolver class\n* @return DomainNameResolver instance\n*/",
        "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Creates a DomainNameResolver using configuration and host-specific key.\n* @param conf configuration settings\n* @param host hostname for resolver\n* @param configKey base key for resolver configuration\n* @return DomainNameResolver instance\n*/",
        "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)": "/**\n* Creates a DomainNameResolver for a given URI.\n* @param conf configuration settings\n* @param uri the URI to extract the hostname\n* @param configKey base key for resolver configuration\n* @return DomainNameResolver instance\n*/"
    },
    "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup": {
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isRack()": "/**\n* Determines if the current node is a rack.\n* @return true if it is a rack, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isNodeGroup()": "/**\n* Determines if the current node is a group node based on its children.\n* @return true if no children or all are leaf nodes; false if any child is an InnerNode\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String)": "/**\n* Constructs InnerNodeWithNodeGroup with a normalized path.\n* @param path the network location to normalize and set\n*/"
    },
    "org.apache.hadoop.net.NetworkTopologyWithNodeGroup": {
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Checks if two nodes are on the same rack based on their parents.\n* @param node1 the first node\n* @param node2 the second node\n* @return true if both nodes share the same parent, false otherwise\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameNodeGroup(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Checks if two nodes belong to the same group.\n* @param node1 first node to compare\n* @param node2 second node to compare\n* @return true if in same group, false if not or if either is null\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node)": "/**\n* Removes a node from the cluster and updates topology.\n* @param node the Node to be removed; must not be an InnerNode\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String)": "/**\n* Retrieves rack location from the given network location.\n* @param loc network location string\n* @return rack location or null if not found or not a rack\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String)": "/**\n* Retrieves the node group for a given location.\n* @param loc the network location to check\n* @return the node group location or null if it's a rack or not found\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)": "/**\n* Calculates weight based on node proximity: local, group, rack, or off rack.\n* @param reader the reference node\n* @param node the node to compare\n* @return weight as int indicating proximity level\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node)": "/**\n* Retrieves a node for a given network location, adding default info if necessary.\n* @param node the input Node to process\n* @return the corresponding Node from the network location\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node)": "/**\n* Adds a node to the network topology, validating its location and type.\n* @param node the Node object to add; must not be null or an InnerNode\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)": "/**\n* Sorts nodes by distance, replacing reader if not in NetworkTopology.\n* @param reader reference node for weight calculation\n* @param nodes array of nodes to sort\n* @param activeLen number of active nodes in the array\n*/",
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>()": "/**** Constructs a NetworkTopologyWithNodeGroup initializing clusterMap with root node. */"
    },
    "org.apache.hadoop.net.SocketOutputStream$Writer": {
        "org.apache.hadoop.net.SocketOutputStream$Writer:performIO(java.nio.ByteBuffer)": "/**\n* Writes data from the buffer to the channel.\n* @param buf data to be written\n* @return number of bytes written\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)": "/**** Initializes Writer with a channel and timeout. \n* @param channel the WritableByteChannel for writing\n* @param timeout the timeout duration in milliseconds\n*/"
    },
    "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered": {
        "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered:extraCheck(java.lang.Object)": "/**\n* Checks if the given value is equal to the string \"NaN\".\n* @param value the object to check\n* @return true if value is \"NaN\", false otherwise\n*/",
        "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered:extraWrite(java.lang.Object,java.lang.String,com.fasterxml.jackson.core.JsonGenerator)": "/**\n* Replaces NaN values with 0.0 during JSON writing.\n* @param value the original value to check\n* @param attName the attribute name being processed\n* @param jg the JsonGenerator to write the value\n*/"
    },
    "org.apache.hadoop.jmx.JMXJsonServlet": {
        "org.apache.hadoop.jmx.JMXJsonServlet:init()": "/**\n* Initializes the servlet, setting up MBean server and JSON factory.\n* @throws ServletException if initialization fails\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:doTrace(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Sends a 405 Method Not Allowed error in response to a trace request.\n* @param req the HttpServletRequest object\n* @param resp the HttpServletResponse object\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,java.lang.String,java.lang.Object)": "/**\n* Writes an attribute to a JSON generator.\n* @param jg JsonGenerator instance to write to\n* @param attName name of the attribute\n* @param value value of the attribute to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:extraCheck(java.lang.Object)": "/**\n* Performs an additional check on the given value.\n* @param value the object to be checked\n* @return false indicating no conditions met\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:extraWrite(java.lang.Object,java.lang.String,com.fasterxml.jackson.core.JsonGenerator)": "/**\n* Placeholder for writing extra attributes to JSON.\n* @param value the value to be written\n* @param attName the attribute name\n* @param jg the JsonGenerator for output\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)": "/**\n* Writes a readable MBean attribute to a JSON generator.\n* @param jg JsonGenerator to write to\n* @param oname ObjectName of the MBean\n* @param attr MBeanAttributeInfo of the attribute\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)": "/**\n* Writes an object to a JSON generator, handling various types and structures.\n* @param jg the JsonGenerator for output\n* @param value the object to write\n* @param attName the attribute name\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)": "/**\n* Lists MBeans and their attributes in JSON format.\n* @param jg JSON generator for output\n* @param qry query to filter MBeans\n* @param attribute specific MBean attribute to retrieve\n* @param response HTTP response to set status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Checks if instrumentation access is permitted for the given HTTP request.\n* @param request the HTTP request to check access\n* @param response the HTTP response for handling errors\n* @return true if access is allowed, false otherwise\n*/",
        "org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests for JMX data, responding with JSON formatted MBeans.\n* @param request HTTP request containing parameters for querying MBeans\n* @param response HTTP response to send JSON data or errors\n*/"
    },
    "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction": {
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:<init>(int)": "/**\n* Initializes an array of SummaryStatistics with specified size.\n* @param valueCount number of SummaryStatistics to create\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getStats(int)": "/**\n* Retrieves SummaryStatistics at a specified index.\n* @param idx index of the statistics to retrieve\n* @return SummaryStatistics object at the given index\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:recordValues(double[])": "/**\n* Records statistical values, ensuring the count matches expected length.\n* @param values array of double values to record\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:hasLogged()": "/**\n* Checks if the user has logged in.\n* @return true if logged in, false otherwise\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:setShouldLog()": "/**\n* Sets the logging flag to true.\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:setHasLogged()": "/**\n* Sets the hasLogged flag to true, indicating the user has logged in.\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getValueCount()": "/**\n* Returns the count of elements in the stats array.\n* @return number of elements in stats\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:shouldLog()": "/**\n* Checks if logging is enabled.\n* @return true if logging is enabled; false otherwise\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getCount()": ""
    },
    "org.apache.hadoop.log.LogLevel": {
        "org.apache.hadoop.log.LogLevel:isValidProtocol(java.lang.String)": "/**\n* Checks if the given protocol is valid (HTTP or HTTPS).\n* @param protocol the protocol string to validate\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.log.LogLevel:printUsage()": "/**\n* Prints usage information for the application.\n*/",
        "org.apache.hadoop.log.LogLevel:main(java.lang.String[])": "/**\n* Main entry point for the application, initializes CLI and runs it with args.\n* @param args command-line arguments for the tool\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLFactory": {
        "org.apache.hadoop.security.ssl.SSLFactory:createSSLSocketFactory()": "/**\n* Creates an SSLSocketFactory in CLIENT mode.\n* @throws GeneralSecurityException if a security error occurs\n* @throws IOException if an I/O error occurs\n* @return SSLSocketFactory instance\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(java.lang.String)": "/**\n* Retrieves a HostnameVerifier based on the specified type.\n* @param verifier type of hostname verifier\n* @return corresponding HostnameVerifier instance\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:destroy()": "/**\n* Cleans up resources by destroying the keystores factory.\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:disableExcludedCiphers(javax.net.ssl.SSLEngine)": "/**\n* Disables specified excluded cipher suites in the given SSLEngine.\n* @param sslEngine the SSLEngine to modify\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:createSSLServerSocketFactory()": "/**\n* Creates an SSLServerSocketFactory if in SERVER mode.\n* @return SSLServerSocketFactory for secure connections\n* @throws GeneralSecurityException if security issues occur\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier()": "/**\n* Retrieves the HostnameVerifier if in CLIENT mode.\n* @return HostnameVerifier instance\n* @throws IllegalStateException if not in CLIENT mode\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine()": "/**\n* Creates and configures an SSLEngine for client or server mode.\n* @return configured SSLEngine instance\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection)": "/**\n* Configures HttpURLConnection for SSL; returns modified connection.\n* @param conn HttpURLConnection to configure\n* @return configured HttpURLConnection instance\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a HostnameVerifier based on the configuration.\n* @param conf configuration object containing settings\n* @return HostnameVerifier instance for the specified type\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:init()": "/**\n* Initializes SSL context and socket factory based on configuration and mode.\n* @throws GeneralSecurityException if a security error occurs\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)": "/**\n* Reads SSL configuration based on mode and input configuration.\n* @param conf input Configuration object\n* @param mode mode indicating client or server\n* @return updated Configuration object for SSL settings\n*/",
        "org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs SSLFactory with mode and configuration.\n* @param mode mode indicating client or server\n* @param conf input Configuration object\n*/"
    },
    "org.apache.hadoop.util.ServletUtil": {
        "org.apache.hadoop.util.ServletUtil:initHTML(javax.servlet.ServletResponse,java.lang.String)": "/**\n* Initializes HTML response with title and stylesheet.\n* @param response Servlet response to write HTML to\n* @param title   Title of the HTML document\n* @return PrintWriter for writing HTML content\n*/",
        "org.apache.hadoop.util.ServletUtil:getParameter(javax.servlet.ServletRequest,java.lang.String)": "/**\n* Retrieves and trims a request parameter by name.\n* @param request the ServletRequest object\n* @param name the parameter name to retrieve\n* @return trimmed parameter value or null if not found or empty\n*/",
        "org.apache.hadoop.util.ServletUtil:parseLongParam(javax.servlet.ServletRequest,java.lang.String)": "/**\n* Parses a long parameter from the request.\n* @param request the servlet request containing the parameter\n* @param param the name of the parameter to parse\n* @return the parsed long value\n* @throws IOException if the parameter is missing or invalid\n*/",
        "org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)": "/**\n* Extracts raw path from request URI after the specified servlet name.\n* @param request HTTP request object\n* @param servletName name of the servlet to trim from the URI\n* @return raw path as a String\n*/"
    },
    "org.apache.hadoop.log.LogLevel$Servlet": {
        "org.apache.hadoop.log.LogLevel$Servlet:process(org.apache.log4j.Logger,java.lang.String,java.io.PrintWriter)": "/**\n* Processes and sets the logging level, reporting changes to output.\n* @param log Logger instance to modify level\n* @param level Desired logging level as a string\n* @param out PrintWriter for outputting messages\n*/",
        "org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests to set log levels for loggers.\n* @param request HTTP request containing parameters\n* @param response HTTP response to write output\n*/"
    },
    "org.apache.hadoop.log.LogThrottlingHelper": {
        "org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String,org.apache.hadoop.util.Timer)": "/**\n* Constructs LogThrottlingHelper with specified parameters.\n* @param minLogPeriodMs minimum log period in milliseconds\n* @param primaryRecorderName name of the primary recorder\n* @param timer Timer instance for managing log timing\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:getLogSupressionMessage(org.apache.hadoop.log.LogThrottlingHelper$LogAction)": "/**\n* Generates a suppression message for logging based on action count.\n* @param action LogAction object containing the count\n* @return String message indicating suppression or empty if none\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:reset()": "/**\n* Resets the state of the logger by clearing logs and timestamps.\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)": "/**\n* Retrieves current statistics for a given recorder name and index.\n* @param recorderName name of the recorder\n* @param idx index of the statistics to retrieve\n* @return SummaryStatistics object or null if recorder not found\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)": "/**\n* Initializes LogThrottlingHelper with log period and recorder name.\n* @param minLogPeriodMs minimum log period in milliseconds\n* @param primaryRecorderName name of the primary recorder\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])": "/**\n* Records values for a given recorder and manages logging actions.\n* @param recorderName name of the recorder\n* @param currentTimeMs current timestamp in milliseconds\n* @param values variable number of double values to log\n* @return LoggingAction if logged; otherwise, DO_NOT_LOG\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:<init>(long)": "/**\n* Initializes LogThrottlingHelper with a minimum log period.\n* @param minLogPeriodMs minimum log period in milliseconds\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper:record(double[])": "/**\n* Records values using default recorder and current time.\n* @param values variable number of double values to log\n* @return LogAction indicating logging status\n*/"
    },
    "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction": {
        "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:getCount()": "/**\n* Throws an exception indicating logging is not yet initialized.\n* @return always throws IllegalStateException\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:getStats(int)": "/**\n* Retrieves summary statistics by index.\n* @param idx index of the statistics to retrieve\n* @throws IllegalStateException if logging is not enabled\n*/",
        "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:shouldLog()": ""
    },
    "org.apache.hadoop.http.HttpServer2Metrics": {
        "org.apache.hadoop.http.HttpServer2Metrics:asyncDispatches()": "/**\n* Retrieves the count of asynchronously dispatched requests.\n* @return number of async dispatches handled\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:asyncRequests()": "/**\n* Retrieves the total number of asynchronous requests.\n* @return count of async requests processed\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:asyncRequestsWaiting()": "/**\n* Retrieves the count of currently waiting asynchronous requests.\n* @return number of waiting async requests\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:asyncRequestsWaitingMax()": "/**\n* Retrieves the maximum number of waiting asynchronous requests.\n* @return maximum number of waiting requests as an integer\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatched()": "/**\n* Retrieves the count of dispatched items.\n* @return number of dispatched items as an integer\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedActive()": "/**\n* Returns the count of currently active dispatches.\n* @return number of active dispatches\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedActiveMax()": "/**\n* Retrieves the maximum number of active dispatches currently handled.\n* @return maximum active dispatches count\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeMax()": "/**\n* Retrieves the maximum dispatched time in milliseconds.\n* @return maximum dispatched time from the handler\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeMean()": "/**\n* Retrieves the mean dispatched handling time in milliseconds.\n* @return average dispatched time as a double\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeStdDev()": "/**\n* Retrieves the standard deviation of dispatched handling times in milliseconds.\n* @return standard deviation as a double\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeTotal()": "/**\n* Retrieves total dispatched handling time in milliseconds.\n* @return total dispatched time as a long value\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:expires()": "/**\n* Retrieves the count of expired asynchronous requests.\n* @return number of expired requests\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requests()": "/**\n* Retrieves the total number of requests handled.\n* @return total number of requests as an integer\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requestsActive()": "/**\n* Retrieves the current number of active requests.\n* @return number of active requests as an integer\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requestsActiveMax()": "/**\n* Retrieves the maximum number of active requests.\n* @return maximum active requests count\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeMax()": "/**\n* Retrieves the maximum request handling time in milliseconds.\n* @return maximum handling time as a long value\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeMean()": "/**\n* Retrieves the mean time spent handling requests in milliseconds.\n* @return mean time as a double\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeStdDev()": "/**\n* Retrieves the standard deviation of request handling time in milliseconds.\n* @return standard deviation as a double\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeTotal()": "/**\n* Retrieves total time spent on all request handling in milliseconds.\n* @return total request handling time in ms\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:responses1xx()": "/**\n* Returns the count of requests with 1xx response status.\n* @return number of 1xx responses\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:responses2xx()": "/**\n* Retrieves the count of responses with 2xx status codes.\n* @return number of 2xx responses\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:responses3xx()": "/**\n* Retrieves the count of responses with 3xx status codes.\n* @return number of 3xx responses\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:responses4xx()": "/**\n* Retrieves the count of requests that resulted in a 4xx response status.\n* @return number of 4xx responses\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:responses5xx()": "/**\n* Retrieves the count of responses with 5xx status codes.\n* @return number of 5xx responses\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:responsesBytesTotal()": "/**\n* Retrieves total bytes of all responses.\n* @return total number of bytes as a long\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:statsOnMs()": "/**\n* Retrieves statistics in milliseconds.\n* @return long representing collected stats in milliseconds\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:<init>(org.eclipse.jetty.server.handler.StatisticsHandler,int)": "/**\n* Initializes HttpServer2Metrics with a StatisticsHandler and port number.\n* @param handler StatisticsHandler for metrics collection\n* @param port Port number for the server\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:remove()": "/**\n* Removes the source associated with the given port.\n* @param port the port number of the source to remove\n*/",
        "org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)": "/**\n* Creates and registers HttpServer2Metrics with a given handler and port.\n* @param handler metrics handler for collection\n* @param port port number for the metrics instance\n* @return registered HttpServer2Metrics instance\n*/"
    },
    "org.apache.hadoop.http.ProfileServlet$Event": {
        "org.apache.hadoop.http.ProfileServlet$Event:getInternalName()": "/**\n* Retrieves the internal name.\n* @return the internal name as a String\n*/",
        "org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String)": "/**\n* Retrieves Event by internal name.\n* @param name the internal name to search for\n* @return corresponding Event or null if not found\n*/"
    },
    "org.apache.hadoop.http.ProfileServlet": {
        "org.apache.hadoop.http.ProfileServlet:setResponseHeader(javax.servlet.http.HttpServletResponse)": "/**\n* Sets CORS response headers for HTTP response.\n* @param response the HttpServletResponse to modify\n*/",
        "org.apache.hadoop.http.ProfileServlet:getAsyncProfilerHome()": "/**\n* Retrieves the Async Profiler home directory from environment or system properties.\n* @return Async Profiler home path as a String, or null if not set\n*/",
        "org.apache.hadoop.http.ProfileServlet:getInteger(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.Integer)": "/**\n* Retrieves an Integer from request parameters or returns default if invalid.\n* @param req HttpServletRequest containing parameters\n* @param param name of the parameter to retrieve\n* @param defaultValue value to return if parameter is absent or invalid\n* @return Integer value or defaultValue if not found or invalid\n*/",
        "org.apache.hadoop.http.ProfileServlet:getOutput(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves Output type from request parameter; defaults to HTML if invalid or absent.\n* @param req HTTP request containing parameters\n* @return Output enum value based on \"output\" parameter\n*/",
        "org.apache.hadoop.http.ProfileServlet:getLong(javax.servlet.http.HttpServletRequest,java.lang.String)": "/**\n* Retrieves a Long value from request parameters.\n* @param req HTTP request object\n* @param param parameter name to fetch\n* @return Long value or null if not found/invalid\n*/",
        "org.apache.hadoop.http.ProfileServlet:getMinWidth(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves minimum width from request parameters.\n* @param req HTTP request containing parameters\n* @return Minimum width as Double or null if not present/invalid\n*/",
        "org.apache.hadoop.http.ProfileServlet:<init>()": "/**\n* Initializes ProfileServlet with async profiler home and current process ID.\n*/",
        "org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves an Event based on request parameter or defaults to CPU.\n* @param req HTTP request containing event parameter\n* @return Event object or default Event.CPU if not found\n*/",
        "org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/****\n* Handles GET requests for profiling, validating access and parameters.\n* @param req HTTP request containing profiling parameters\n* @param resp HTTP response for status and output messages\n*/"
    },
    "org.apache.hadoop.http.ProfileOutputServlet": {
        "org.apache.hadoop.http.ProfileOutputServlet:sanitize(java.lang.String)": "/**\n* Sanitizes input to prevent XSS by ensuring it is alphanumeric.\n* @param input string to sanitize\n* @return sanitized string if valid, otherwise throws RuntimeException\n*/",
        "org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests, checks access, and serves profiling output or refreshes page.\n* @param req the HTTP request\n* @param resp the HTTP response\n*/"
    },
    "org.apache.hadoop.http.NoCacheFilter": {
        "org.apache.hadoop.http.NoCacheFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)": "/**\n* Filters HTTP response to prevent caching.\n* @param req the servlet request\n* @param res the servlet response\n* @param chain the filter chain for further processing\n*/"
    },
    "org.apache.hadoop.util.ProcessUtils": {
        "org.apache.hadoop.util.ProcessUtils:getPid()": "/**\n* Retrieves the current Java process ID (PID).\n* @return Integer representing the PID or null if unavailable\n*/",
        "org.apache.hadoop.util.ProcessUtils:runCmdAsync(java.util.List)": "/**\n* Executes a command asynchronously.\n* @param cmd list of command and arguments to run\n* @return Process object representing the running command\n*/",
        "org.apache.hadoop.util.ProcessUtils:<init>()": "/**\n* Private constructor to prevent instantiation of the utility class.\n*/"
    },
    "org.apache.hadoop.http.HtmlQuoting": {
        "org.apache.hadoop.http.HtmlQuoting:needsQuoting(byte[],int,int)": "/**\n* Checks if data requires quoting for special characters.\n* @param data byte array to check\n* @param off starting index in the array\n* @param len number of bytes to check\n* @return true if quoting is needed, false otherwise\n*/",
        "org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.io.OutputStream,byte[],int,int)": "/**\n* Encodes HTML special characters in the given byte buffer to an OutputStream.\n* @param output the OutputStream to write encoded characters to\n* @param buffer the byte array containing characters to encode\n* @param off the starting offset in the buffer\n* @param len the number of bytes to process from the buffer\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.http.HtmlQuoting:quoteOutputStream(java.io.OutputStream)": "/**\n* Wraps an OutputStream to escape HTML characters during write operations.\n* @param out the underlying OutputStream to write to\n* @return a new OutputStream that quotes HTML characters\n*/",
        "org.apache.hadoop.http.HtmlQuoting:unquoteHtmlChars(java.lang.String)": "/**\n* Converts HTML character entities in a string to their corresponding characters.\n* @param item input string with HTML entities\n* @return string with HTML entities replaced or null if input is null\n*/",
        "org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String)": "/**\n* Determines if a string requires quoting for special characters.\n* @param str input string to evaluate\n* @return true if quoting is needed, false if null or not required\n*/",
        "org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String)": "/**\n* Encodes HTML special characters in the given string.\n* @param item input string to encode, or null\n* @return encoded string or null if input is null\n*/",
        "org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[])": "/**\n* Processes command-line arguments, quoting and unquoting HTML characters.\n* @param args command-line arguments\n*/"
    },
    "org.apache.hadoop.http.HttpServer2": {
        "org.apache.hadoop.http.HttpServer2:getWebAppsPath(java.lang.String)": "/**\n* Retrieves the web application's path by name.\n* @param appName name of the web application\n* @return path to the web application directory\n* @throws FileNotFoundException if the app is not found\n*/",
        "org.apache.hadoop.http.HttpServer2:setContextAttributes(org.eclipse.jetty.servlet.ServletContextHandler,org.apache.hadoop.conf.Configuration)": "/**\n* Sets context attributes in the ServletContext.\n* @param context ServletContextHandler to set attributes in\n* @param conf Configuration object to be stored in context\n*/",
        "org.apache.hadoop.http.HttpServer2:getWebAppContext()": "/**\n* Retrieves the current WebAppContext instance.\n* @return WebAppContext object associated with this instance\n*/",
        "org.apache.hadoop.http.HttpServer2:addListener(org.eclipse.jetty.server.ServerConnector)": "/**\n* Adds a ServerConnector to the listeners list.\n* @param connector the ServerConnector to be added\n*/",
        "org.apache.hadoop.http.HttpServer2:setAttribute(java.lang.String,java.lang.Object)": "/**\n* Sets an attribute in the web application context.\n* @param name  the name of the attribute\n* @param value the value to be associated with the attribute\n*/",
        "org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String,java.util.Map)": "/**\n* Registers a Jersey resource package with specified path and parameters.\n* @param packageName the resource package to add\n* @param pathSpec the URL path to map the package\n* @param params additional initialization parameters for the resource\n*/",
        "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,boolean)": "/**\n* Adds a servlet with a specified name and path, replacing existing mappings if necessary.\n* @param name servlet name; can be null\n* @param pathSpec URL pattern for the servlet\n* @param clazz servlet class type\n* @param requireAuth indicates if authentication is required\n*/",
        "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,java.util.Map)": "/**\n* Adds or replaces a servlet mapping for a specified path.\n* @param name    the name of the servlet\n* @param pathSpec the URL pattern for the servlet\n* @param clazz   the servlet class type\n* @param params  initialization parameters for the servlet\n*/",
        "org.apache.hadoop.http.HttpServer2:addHandlerAtFront(org.eclipse.jetty.server.Handler)": "/**\n* Adds a handler to the front of the handler list.\n* @param handler the Handler to be added\n*/",
        "org.apache.hadoop.http.HttpServer2:addHandlerAtEnd(org.eclipse.jetty.server.Handler)": "/**\n* Adds a handler to the end of the handler list.\n* @param handler the Handler object to be added\n*/",
        "org.apache.hadoop.http.HttpServer2:getFilterHolder(java.lang.String,java.lang.String,java.util.Map)": "/**\n* Creates a FilterHolder with specified name, class, and optional parameters.\n* @param name the filter's name\n* @param classname the filter's class name\n* @param parameters optional initialization parameters\n* @return a FilterHolder instance\n*/",
        "org.apache.hadoop.http.HttpServer2:getFilterMapping(java.lang.String,java.lang.String[])": "/**\n* Creates a FilterMapping with specified name and URL patterns.\n* @param name filter name\n* @param urls array of URL patterns for the filter\n* @return configured FilterMapping instance\n*/",
        "org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,org.eclipse.jetty.servlet.FilterHolder,org.eclipse.jetty.servlet.FilterMapping)": "/**\n* Defines and adds a filter to the servlet context.\n* @param ctx the servlet context handler\n* @param holder the filter holder\n* @param fmap the filter mapping configuration\n*/",
        "org.apache.hadoop.http.HttpServer2:addFilterPathMapping(java.lang.String,org.eclipse.jetty.servlet.ServletContextHandler)": "/**\n* Adds filter path mappings for specified filter names.\n* @param pathSpec the path specification for the filter\n* @param webAppCtx the ServletContextHandler to add mappings to\n*/",
        "org.apache.hadoop.http.HttpServer2:getAttribute(java.lang.String)": "/**\n* Retrieves an attribute from the web application context.\n* @param name the name of the attribute to retrieve\n* @return the attribute value or null if not found\n*/",
        "org.apache.hadoop.http.HttpServer2:getPort()": "/**\n* Retrieves the local port of the server connector.\n* @return local port number as an integer\n*/",
        "org.apache.hadoop.http.HttpServer2:setThreads(int,int)": "/**\n* Configures the minimum and maximum thread limits for the server's thread pool.\n* @param min minimum number of threads\n* @param max maximum number of threads\n*/",
        "org.apache.hadoop.http.HttpServer2:loadListeners()": "/**\n* Loads and adds all connectors to the web server.\n*/",
        "org.apache.hadoop.http.HttpServer2:bindListener(org.eclipse.jetty.server.ServerConnector)": "/**\n* Binds a server listener by closing and reopening it.\n* @param listener the ServerConnector to bind\n* @throws Exception if an error occurs during binding\n*/",
        "org.apache.hadoop.http.HttpServer2:constructBindException(org.eclipse.jetty.server.ServerConnector,java.io.IOException)": "/**\n* Constructs a BindException for a server connector.\n* @param listener the ServerConnector instance\n* @param ex optional IOException to set as the cause\n* @return a BindException with port usage details\n*/",
        "org.apache.hadoop.http.HttpServer2:addMultiException(org.eclipse.jetty.util.MultiException,java.lang.Exception)": "/**\n* Adds an exception to a MultiException instance.\n* @param exception the MultiException to add to, or null to create a new one\n* @param e the Exception to be added\n* @return the updated MultiException instance\n*/",
        "org.apache.hadoop.http.HttpServer2:join()": "/**\n* Waits for the web server to finish execution.\n* @throws InterruptedException if the current thread is interrupted\n*/",
        "org.apache.hadoop.http.HttpServer2:isAlive()": "/**\n* Checks if the web server is running.\n* @return true if the server is started, false otherwise\n*/",
        "org.apache.hadoop.http.HttpServer2:getDefaultHeaders()": "/**\n* Generates default HTTP headers for security.\n* @return Map of header names and their corresponding values\n*/",
        "org.apache.hadoop.http.HttpServer2:getConnectorAddress(int)": "/**\n* Retrieves connector address by index.\n* @param index position of the connector\n* @return InetSocketAddress or null if not valid\n*/",
        "org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)": "/**\n* Adds a Jersey resource package with a specified path.\n* @param packageName the resource package to add\n* @param pathSpec the URL path to map the package\n*/",
        "org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)": "/**\n* Adds a servlet with a specified name and path.\n* @param name servlet name; can be null\n* @param pathSpec URL pattern for the servlet\n* @param clazz servlet class type\n*/",
        "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)": "/**\n* Adds a servlet with a specified name and path.\n* @param name servlet name; can be null\n* @param pathSpec URL pattern for the servlet\n* @param clazz servlet class type\n*/",
        "org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)": "/**\n* Adds a filter to the web application context and default contexts.\n* @param name filter name\n* @param classname filter class name\n* @param parameters optional initialization parameters\n*/",
        "org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)": "/**\n* Adds a global filter to the web application context.\n* @param name filter name, @param classname filter's class name, @param parameters init parameters\n*/",
        "org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])": "/**\n* Defines and adds a filter to the servlet context.\n* @param ctx the servlet context handler\n* @param name the filter's name\n* @param classname the filter's class name\n* @param parameters optional initialization parameters\n* @param urls array of URL patterns for the filter\n*/",
        "org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)": "/**\n* Binds a ServerConnector to a specified port, retrying if an IOException occurs.\n* @param listener the ServerConnector to bind\n* @param port initial port number to attempt binding\n* @throws Exception if binding fails with no available ports\n*/",
        "org.apache.hadoop.http.HttpServer2:toString()": "/**\n* Returns a string representation of the HttpServer's state and listeners.\n* @return formatted server status and listening addresses\n*/",
        "org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)": "/**\n* Adds Async Profiler servlet if home path is set; otherwise, disables it.\n* @param contexts collection of context handlers\n* @param conf configuration to set in context\n*/",
        "org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler)": "/**\n* Adds a no-cache filter to the servlet context.\n* @param ctxt the servlet context handler\n*/",
        "org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)": "/**** Binds a server listener to a port range, handling binding exceptions.\n* @param listener the ServerConnector to bind\n* @param startPort the initial port to start binding from\n* @throws Exception if all ports fail to bind\n*/",
        "org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)": "/**\n* Adds a servlet context handler with optional no-cache filter.\n* @param ctxt the servlet context handler\n* @param isFiltered flag indicating if filtering is applied\n*/",
        "org.apache.hadoop.http.HttpServer2:stop()": "/**\n* Stops the web application and handles exceptions during shutdown.\n* @throws Exception if errors occur while stopping components\n*/",
        "org.apache.hadoop.http.HttpServer2:openListeners()": "/**** Opens server listeners, binding them to ports if not already bound. \n* @throws Exception if binding fails. */",
        "org.apache.hadoop.http.HttpServer2:start()": "/**\n* Starts the HTTP server, initializing listeners and metrics.\n* @throws IOException if server fails to start or bind\n*/",
        "org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration)": "/**\n* Sets HTTP headers based on configuration and options.\n* @param conf configuration object for header settings\n* @return map of headers and their values\n*/",
        "org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)": "/**\n* Creates and configures a WebAppContext for a given application.\n* @param b builder containing app details\n* @param adminsAcl access control list for admins\n* @param appDir directory of the application\n* @return configured WebAppContext instance\n*/",
        "org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration)": "/**\n* Configures Prometheus servlet based on the provided configuration.\n* @param conf application configuration settings\n*/",
        "org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Configures default application contexts for logs and static resources.\n* @param parent parent context handler\n* @param appDir directory for static resources\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration)": "/**\n* Configures default servlets based on provided configuration settings.\n* @param configuration settings for servlet initialization\n*/",
        "org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves an array of FilterInitializer instances from configuration.\n* @param conf configuration object\n* @return array of FilterInitializer or null if conf is null or classes not found\n*/",
        "org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)": "/**\n* Initializes SPNEGO authentication with configuration parameters.\n* @param conf configuration settings\n* @param hostName hostname for server principal\n* @param authFilterConfigurationPrefixes filter configuration prefixes\n* @param usernameConfKey key for username config\n* @param keytabConfKey key for keytab config\n*/",
        "org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])": "/**\n* Initializes the web server with configurations and handlers.\n* @param name web application name, @param hostName server hostname,\n* @param conf configuration settings, @param pathSpecs array of path specifications\n* @throws IOException if an I/O error occurs during initialization\n*/",
        "org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)": "/**\n* Retrieves properties filtered by specified prefixes.\n* @param conf Configuration object for fetching properties\n* @param prefixes List of prefixes to filter properties\n* @return Properties object containing filtered key-value pairs\n*/",
        "org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)": "/**\n* Constructs a SignerSecretProvider using builder and servlet context.\n* @param b Builder containing configuration details\n* @param ctx Servlet context for provider construction\n* @return SignerSecretProvider instance\n*/",
        "org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder)": "/**\n* Constructs an HttpServer2 instance with specified configurations.\n* @param b Builder containing server settings and configurations\n* @throws IOException if an error occurs during server initialization\n*/",
        "org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)": "/**\n* Checks if a user has administrator access.\n* @param servletContext the servlet context for access control\n* @param remoteUser the username of the remote user\n* @return true if the user is an admin, false otherwise\n*/",
        "org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Checks if the user has administrator access.\n* @param servletContext the servlet context for access control\n* @param request the HTTP request containing user info\n* @param response the HTTP response to send errors\n* @return true if the user is an admin, false otherwise\n*/",
        "org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Determines if instrumentation access is allowed based on admin rights.\n* @param servletContext the servlet context for configuration access\n* @param request the HTTP request to check user info\n* @param response the HTTP response for error handling\n* @return true if access is allowed, false otherwise\n*/"
    },
    "org.apache.hadoop.http.HttpRequestLog": {
        "org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String)": "/**\n* Retrieves a RequestLog for the specified component name.\n* @param name the component name to look up\n* @return a RequestLog instance configured for the component\n*/",
        "org.apache.hadoop.http.HttpRequestLog:<init>()": "/**\n* Private constructor for HttpRequestLog class.\n*/"
    },
    "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink": {
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:<init>()": "/**\n* Constructs a new instance of PrometheusMetricsSink.\n*/",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:prometheusName(java.lang.String,java.lang.String)": "/**\n* Converts record and metric names to a Prometheus-compatible format.\n* @param recordName the name of the record\n* @param metricName the name of the metric\n* @return formatted string suitable for Prometheus\n*/",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:flush()": "/**\n* Resets metrics by flushing current to next, preparing for new data collection.\n*/",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:parseTopMetricsTags(java.lang.String)": "/**\n* Parses tags from a metric name.\n* @param metricName the name of the metric to parse\n* @return list of extracted tags as strings\n*/",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:init(org.apache.commons.configuration2.SubsetConfiguration)": "",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Stores metrics in a Prometheus-compatible format.\n* @param metricsRecord contains metrics to be stored\n*/",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)": "/**\n* Constructs a metric key from a pattern and adds tags to extendTags.\n* @param promMetricKey the original metric key\n* @param metric the metric object for tag extraction\n* @param extendTags list to add extracted tags\n* @return formatted metric key or original if pattern not matched\n*/",
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer)": "/**\n* Writes formatted metrics to a Writer.\n* @param writer the output destination for metric strings\n* @throws IOException if an I/O error occurs during writing\n*/"
    },
    "org.apache.hadoop.http.HttpServer2$XFrameOption": {
        "org.apache.hadoop.http.HttpServer2$XFrameOption:toString()": "/**\n* Returns the string representation of the object.\n* @return name of the object as a String\n*/",
        "org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String)": "/**\n* Retrieves XFrameOption by string value.\n* @param value string representation of XFrameOption\n* @return corresponding XFrameOption or throws IllegalArgumentException\n*/"
    },
    "org.apache.hadoop.http.IsActiveServlet": {
        "org.apache.hadoop.http.IsActiveServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests, responding with status based on server activity.\n* @param req the HttpServletRequest object\n* @param resp the HttpServletResponse object\n*/"
    },
    "org.apache.hadoop.http.HttpServer2$QuotingInputFilter": {
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:initHttpHeaderMap()": "/**\n* Initializes HTTP header map from configuration parameters matching a regex pattern.\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:inferMimeType(javax.servlet.ServletRequest)": "/**\n* Infers MIME type from the request URI.\n* @param request the servlet request\n* @return MIME type as a String or null if not found\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes filter configuration and HTTP header map.\n* @param config filter configuration parameters\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)": "/**\n* Filters HTTP requests and sets appropriate response MIME types.\n* @param request the servlet request to filter\n* @param response the servlet response to modify\n* @param chain the filter chain to continue processing\n*/"
    },
    "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter": {
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:<init>(javax.servlet.http.HttpServletRequest)": "/**\n* Initializes RequestQuoter with the given HTTP request.\n* @param rawRequest the HTTP request to process\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterNames()": "/**\n* Returns an enumeration of parameter names with HTML-quoted characters.\n* @return Enumeration of parameter names\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String)": "/**\n* Retrieves and encodes a request parameter by name.\n* @param name the parameter name to retrieve\n* @return encoded parameter value or null if not found\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String)": "/**\n* Retrieves parameter values by name, decoding and encoding HTML characters.\n* @param name parameter name with possible HTML entities\n* @return array of encoded parameter values or null if not found\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap()": "/**\n* Returns a map of parameter names and HTML-encoded values.\n* @return Map of parameter names to encoded string arrays\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL()": "/**\n* Retrieves and encodes the request URL as a StringBuffer.\n* @return StringBuffer containing the encoded URL\n*/",
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName()": "/**\n* Retrieves the server name, encoding HTML special characters.\n* @return encoded server name or null if not available\n*/"
    },
    "org.apache.hadoop.http.HttpConfig$Policy": {
        "org.apache.hadoop.http.HttpConfig$Policy:fromString(java.lang.String)": "/**\n* Converts a string to a Policy enum instance.\n* @param value string representation of a Policy\n* @return corresponding Policy or null if not found\n*/"
    },
    "org.apache.hadoop.http.PrometheusServlet": {
        "org.apache.hadoop.http.PrometheusServlet:getPrometheusSink()": "/**\n* Retrieves the PrometheusMetricsSink from the servlet context.\n* @return PrometheusMetricsSink or null if not set\n*/",
        "org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests by publishing metrics and writing to the response.\n* @param req HTTP request object\n* @param resp HTTP response object\n* @throws ServletException if a servlet error occurs\n* @throws IOException if an I/O error occurs during writing\n*/"
    },
    "org.apache.hadoop.http.WebServlet": {
        "org.apache.hadoop.http.WebServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests, redirecting to index.html with query parameters if present.\n* @param request  the HttpServletRequest object\n* @param response the HttpServletResponse object\n* @throws ServletException if a servlet-specific error occurs\n* @throws IOException if an input or output error occurs\n*/"
    },
    "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter": {
        "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)": "/**\n* Filters requests to manage user authentication.\n* @param request the servlet request\n* @param response the servlet response\n* @param chain the filter chain to continue processing\n*/",
        "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes filter with user configuration.\n* @param conf Filter configuration containing initialization parameters\n*/"
    },
    "org.apache.hadoop.http.lib.StaticUserWebFilter$User": {
        "org.apache.hadoop.http.lib.StaticUserWebFilter$User:<init>(java.lang.String)": "/**\n* Constructs a User with the specified name.\n* @param name the name of the user\n*/",
        "org.apache.hadoop.http.lib.StaticUserWebFilter$User:hashCode()": "/**\n* Computes the hash code based on the object's name.\n* @return hash code integer of the name\n*/",
        "org.apache.hadoop.http.lib.StaticUserWebFilter$User:equals(java.lang.Object)": "/**\n* Compares this User object with another for equality.\n* @param other the object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.http.HttpServer2$Builder": {
        "org.apache.hadoop.http.HttpServer2$Builder:addEndpoint(java.net.URI)": "/**\n* Adds an endpoint URI to the builder.\n* @param endpoint URI to be added\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:setAuthFilterConfigurationPrefix(java.lang.String)": "/**\n* Sets the authentication filter configuration prefix.\n* @param value the configuration prefix to set\n* @return the Builder instance for chaining\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:setAuthFilterConfigurationPrefixes(java.lang.String[])": "/**\n* Sets authentication filter configuration prefixes.\n* @param prefixes array of prefix strings to set\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:setXFrameOption(java.lang.String)": "/**\n* Sets the X-Frame-Options header.\n* @param option the X-Frame-Options value to set\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)": "/**** Creates a Timer to monitor configuration changes in keystore and truststore. \n* @param reloadInterval interval for reloading certificates \n* @param sslContextFactory SSL context factory for reloading \n* @return Timer instance for scheduled tasks \n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory)": "/**\n* Configures enabled and excluded SSL protocols for the given SslContextFactory.\n* @param sslContextFactory the SSL context factory to configure\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)": "/**\n* Creates an HTTP channel connector for the server.\n* @param server the server instance\n* @param httpConfig configuration for HTTP connection\n* @return ServerConnector instance\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)": "/****\n* Creates an HTTPS channel connector with SSL configuration.\n* @param server the server instance\n* @param httpConfig configuration for HTTP connection\n* @return ServerConnector for secure connections\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Converts password from char[] to String.\n* @param conf configuration object for fetching password\n* @param name credential name to fetch\n* @return password as String or null if not found\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration()": "/**\n* Loads SSL configuration from sslConf, throwing IOException for missing properties.\n*/",
        "org.apache.hadoop.http.HttpServer2$Builder:build()": "/**\n* Builds and configures an HttpServer2 instance.\n* @return configured HttpServer2 object\n* @throws IOException if SSL configuration is missing\n*/"
    },
    "org.apache.hadoop.security.ssl.FileMonitoringTimerTask": {
        "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:run()": "/**\n* Monitors file changes and triggers a callback on modification.\n* @param filePaths list of file paths to monitor\n*/",
        "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)": "/**\n* Initializes FileMonitoringTimerTask with file paths and change callbacks.\n* @param filePaths paths to monitor for changes\n* @param onFileChange action to perform on file change\n* @param onChangeFailure action for handling errors\n*/",
        "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)": "/**\n* Initializes FileMonitoringTimerTask for a single file path.\n* @param filePath path to monitor for changes\n* @param onFileChange action to perform on file change\n* @param onChangeFailure action for handling errors\n*/"
    },
    "org.apache.hadoop.http.JettyUtils": {
        "org.apache.hadoop.http.JettyUtils:<init>()": "/**\n* Private constructor to prevent instantiation of the JettyUtils class.\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsFilter": {
        "org.apache.hadoop.metrics2.MetricsFilter:accepts(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Checks if the MetricsRecord is accepted based on its name and tags.\n* @param record the MetricsRecord to evaluate\n* @return true if accepted, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsException": {
        "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.String)": "/**\n* Constructs a MetricsException with a specified error message.\n* @param message detailed error description\n*/",
        "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a MetricsException with a message and a cause.\n* @param message error message\n* @param cause underlying throwable cause\n*/",
        "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.Throwable)": "/**\n* Constructs a MetricsException with the specified cause.\n* @param cause the underlying reason for the exception\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsTag": {
        "org.apache.hadoop.metrics2.MetricsTag:name()": "/**\n* Retrieves the name from the info object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:description()": "/**\n* Retrieves the description from the info object.\n* @return a String containing the description\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:info()": "/**\n* Retrieves the MetricsInfo object.\n* @return MetricsInfo instance containing metrics data\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:value()": "/**\n* Retrieves the current value as a String.\n* @return the current value\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:hashCode()": "/**\n* Computes the hash code based on info and value fields.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/**\n* Constructs a MetricsTag with non-null info and a value.\n* @param info MetricsInfo object, must not be null\n* @param value String representing the tag value\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object)": "/**\n* Compares this MetricsTag with another object for equality.\n* @param obj the object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.metrics2.MetricsTag:toString()": "/**\n* Returns a string representation of the object with info and value.\n* @return formatted string including info and current value\n*/"
    },
    "org.apache.hadoop.metrics2.MetricStringBuilder": {
        "org.apache.hadoop.metrics2.MetricStringBuilder:<init>(org.apache.hadoop.metrics2.MetricsCollector,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Constructs a MetricStringBuilder with specified parameters.\n* @param parent MetricsCollector instance for collection\n* @param prefix string to prepend to metrics\n* @param separator string to separate metrics\n* @param suffix string to append to metrics\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:tuple(java.lang.String,java.lang.String)": "/**\n* Appends key-value pair to the builder with prefix and suffix.\n* @param key the key to append\n* @param value the value to append\n* @return the updated MetricStringBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:toString()": "/**\n* Returns the string representation of the object.\n* @return String representation using StringBuilder\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:parent()": "/**\n* Retrieves the parent MetricsCollector.\n* @return parent MetricsCollector instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)": "/**\n* Adds a metric with its value to the builder.\n* @param info metric information containing the name\n* @param value the value associated with the metric\n* @return updated MetricStringBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag)": "/**\n* Adds a MetricsTag and returns the updated MetricsRecordBuilder.\n* @param tag the MetricsTag to add\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String)": "/**\n* Sets the context value in the MetricsRecordBuilder.\n* @param value context string to set\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/**\n* Adds a metric tag to the builder.\n* @param info metric information containing the name\n* @param value the value associated with the metric\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric)": "/**\n* Adds a metric to the builder using its info and string representation.\n* @param metric the metric to be added\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Adds a counter metric with its value.\n* @param info metric information containing the name\n* @param value the value of the counter\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a counter metric with the specified value.\n* @param info metric information for the counter\n* @param value the counter value to be added\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Adds a gauge metric with specified value.\n* @param info metric information including name\n* @param value the gauge's integer value\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a gauge metric with its value.\n* @param info metric information containing the name\n* @param value the value of the gauge\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**\n* Adds a gauge metric with its value.\n* @param info metric information containing the name\n* @param value the gauge value\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)": "/**\n* Adds a gauge metric with its value.\n* @param info metric information containing the name\n* @param value the gauge value to be recorded\n* @return updated MetricsRecordBuilder instance\n*/"
    },
    "org.apache.hadoop.metrics2.AbstractMetric": {
        "org.apache.hadoop.metrics2.AbstractMetric:info()": "/**\n* Retrieves the MetricsInfo instance.\n* @return MetricsInfo object containing metrics data\n*/",
        "org.apache.hadoop.metrics2.AbstractMetric:toString()": "/**\n* Returns a string representation of the object with its info and value.\n* @return formatted string of object details\n*/",
        "org.apache.hadoop.metrics2.AbstractMetric:name()": "/**\n* Returns the name from the info object.\n* @return name as a String\n*/",
        "org.apache.hadoop.metrics2.AbstractMetric:description()": "/**\n* Retrieves the description from the info object.\n* @return a string containing the description\n*/",
        "org.apache.hadoop.metrics2.AbstractMetric:hashCode()": "/**\n* Computes the hash code based on info and value.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo)": "",
        "org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object)": "/**\n* Compares this AbstractMetric with another for equality.\n* @param obj object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.sink.RollingFileSystemSink": {
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:<init>()": "/**\n* Constructs a new instance of RollingFileSystemSink.\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:<init>(long,long)": "/**\n* Constructs a RollingFileSystemSink with specified flush intervals.\n* @param flushIntervalMillis time in milliseconds to flush data\n* @param flushOffsetIntervalMillis time in milliseconds for offset flush\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:setInitialFlushTime(java.util.Date)": "/**\n* Sets the initial flush time based on the current hour and roll interval.\n* @param now current date and time for calculation\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:updateFlushTime(java.util.Date)": "/**\n* Updates the next flush time based on the current time.\n* @param now current date and time for calculation\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:scheduleFlush(java.util.Date)": "/**\n* Schedules a flush operation to occur at a specified time.\n* @param when the time to execute the flush\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:extractId(java.lang.String)": "/**\n* Extracts numeric ID from the file name after the last dot.\n* @param file the file name string\n* @return extracted ID or -1 if no valid ID found\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)": "/**\n* Retrieves a non-negative integer property value.\n* @param key property name; @param defaultValue fallback if key not found\n* @return non-negative property value\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String)": "/**\n* Validates existence of a property key in the configuration.\n* @param key the property key to check\n* @throws MetricsException if the property is missing\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String)": "/**\n* Checks for errors in the output stream and throws MetricsException if found.\n* @param message error description to include in the exception\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String)": "/**\n* Throws MetricsException with a message if error is not ignored.\n* @param message detailed error description\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval()": "/**** \n* Parses and validates the roll interval, returning it in milliseconds.\n* @return roll interval in milliseconds\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)": "/**** Throws a MetricsException if error should not be ignored. \n* @param message error message \n* @param t underlying throwable cause \n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)": "/**\n* Determines the next ID to try based on existing files.\n* @param initial base path for file names\n* @param lastId last used ID to compare against\n* @return next available ID\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close()": "/**\n* Closes the output stream and checks for errors during closure.\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush()": "/**\n* Flushes the output stream if available, handling IO exceptions appropriately.\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf()": "/**\n* Loads configuration from suppliedConf or creates a new one if null.\n* @return Configuration object\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date)": "/**\n* Calculates and retrieves the current directory based on the given date.\n* @param now the reference date for calculating the current directory\n* @return Path object representing the current directory\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String)": "/**\n* Constructs a security property string based on its configuration.\n* @param property the name of the security property\n* @return formatted security property string or \"<NOT SET>\" if not found\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem)": "/**\n* Checks if appending to the base path is supported.\n* @param fs FileSystem instance to perform append operation\n* @return true if append is supported, false otherwise\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path)": "/**\n* Creates a log file with a unique name based on the initial path.\n* @param initial base path for the log file\n* @throws IOException if file creation fails unexpectedly\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path)": "/**\n* Creates or appends to a log file at the specified path.\n* @param targetFile path of the log file\n* @throws IOException if file operations fail\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir()": "/**\n* Rolls over the log directory, creating a new log file.\n* @throws IOException if directory or file operations fail\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem()": "/**\n* Retrieves the FileSystem instance based on supplied or default URI.\n* @return FileSystem instance or throws MetricsException on error\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**** Initializes configuration and security settings from provided properties. \n* @param metrics2Properties configuration properties \n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs()": "/**** Initializes the FileSystem and attempts to create the base directory. @return true if successful */",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded()": "/**\n* Rolls over the log directory if conditions are met.\n* @throws MetricsException if log directory creation fails\n*/",
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Logs metrics data to the output stream.\n* @param record MetricsRecord containing timestamp, context, tags, and metrics\n*/"
    },
    "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf": {
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setUnits(java.lang.String)": "/**\n* Sets the measurement units.\n* @param units the units to be set\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setDmax(int)": "/**\n* Sets the maximum value for dmax.\n* @param dmax maximum integer value to set\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setTmax(int)": "/**\n* Sets the maximum temperature value.\n* @param tmax maximum temperature to be set\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setSlope(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)": "/**\n* Sets the slope value.\n* @param slope the GangliaSlope to be set\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getUnits()": "/**\n* Retrieves the units as a String.\n* @return the units string\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getTmax()": "/**\n* Retrieves the maximum temperature value.\n* @return maximum temperature as an integer\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getDmax()": "/**\n* Retrieves the maximum value.\n* @return the maximum integer value (dmax)\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getSlope()": "/**\n* Retrieves the current slope value.\n* @return GangliaSlope object representing the slope\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:toString()": "/**\n* Returns a string representation of the object with unit, slope, dmax, and tmax values.\n* @return formatted string of object's properties\n*/"
    },
    "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink": {
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getGangliaConfForMetric(java.lang.String)": "/**\n* Retrieves Ganglia configuration for a specified metric name.\n* @param metricName name of the metric to fetch configuration for\n* @return GangliaConf object or default configuration if not found\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_int(int)": "/**\n* Writes an integer to the buffer in big-endian format.\n* @param i the integer to be written\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:pad()": "/**\n* Pads the buffer with zeros to align the offset to the next multiple of four.\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:emitToGangliaHosts()": "/**\n* Sends metric data to configured Ganglia hosts.\n* @throws IOException if network issues occur during sending\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:flush()": "/**\n* No operation as there is no buffered data to flush.\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getDatagramSocket()": "/**\n* Retrieves the current DatagramSocket instance.\n* @return the DatagramSocket object\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getHostName()": "/**\n* Retrieves the host name.\n* @return the host name as a String\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getMetricsServers()": "/**\n* Retrieves a list of metrics server socket addresses.\n* @return List of SocketAddress objects\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:isSupportSparseMetrics()": "/**\n* Checks if sparse metrics support is enabled.\n* @return true if sparse metrics are supported, false otherwise\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:resetBuffer()": "/**\n* Resets the buffer offset to zero.\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:setDatagramSocket(java.net.DatagramSocket)": "/**\n* Sets the DatagramSocket instance for this object.\n* @param datagramSocket the DatagramSocket to be set\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType)": "/**\n* Loads Ganglia configuration based on type.\n* @param gtype specifies the configuration type to load\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String)": "/**\n* Encodes a string to XDR format and writes it to the buffer.\n* @param s the string to encode\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**\n* Initializes GangliaSink with configuration settings and prepares networking parameters.\n* @param conf configuration settings for GangliaSink\n*/"
    },
    "org.apache.hadoop.metrics2.util.MetricsCache$Record": {
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:metricsEntrySet()": "/**\n* Returns a set of entries from the metrics map.\n* @return Set of entries with metric name and its corresponding AbstractMetric\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:getTag(java.lang.String)": "/**\n* Retrieves the tag associated with the specified key.\n* @param key the identifier for the desired tag\n* @return the tag value as a String or null if not found\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:getMetric(java.lang.String)": "/**\n* Retrieves the metric value associated with the given key.\n* @param key the identifier for the metric\n* @return the metric value or null if not found\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:getMetricInstance(java.lang.String)": "/**\n* Retrieves a metric instance by its key.\n* @param key identifier for the metric\n* @return AbstractMetric associated with the key or null if not found\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:tags()": "/**\n* Returns a set of entries representing the tags.\n* @return Set of tag entries as key-value pairs\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:metrics()": "/**\n* Returns a set of metric entries as key-value pairs.\n* @return Set of entries with metric names and their values\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:toString()": "/**\n* Returns a string representation of the object with tags and metrics.\n* @return formatted string with class name, tags, and metrics\n*/"
    },
    "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor": {
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:getType()": "/**\n* Retrieves the type of the object.\n* @return String representing the object's type\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:getSlope()": "/**\n* Retrieves the current slope value.\n* @return GangliaSlope object representing the slope\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:counter(org.apache.hadoop.metrics2.MetricsInfo,int)": "",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:counter(org.apache.hadoop.metrics2.MetricsInfo,long)": "",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)": "",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)": "",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)": "",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)": ""
    },
    "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite": {
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:<init>(java.lang.String,int)": "/**\n* Constructs a Graphite instance with server host and port.\n* @param serverHost hostname of the Graphite server\n* @param serverPort port number for the Graphite server\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:isConnected()": "/**\n* Checks if the socket is connected and not closed.\n* @return true if connected, false otherwise\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:tooManyConnectionFailures()": "/**\n* Checks if connection failures exceed the maximum allowed limit.\n* @return true if failures exceed limit, otherwise false\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:close()": "/**\n* Closes the writer and socket, handling IOExceptions during the process.\n* @throws IOException if an error occurs while closing resources\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush()": "/**\n* Flushes the output stream if the socket is connected.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect()": "/**\n* Establishes a connection to the Graphite server.\n* @throws MetricsException if already connected or on connection error\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String)": "/**\n* Sends a message if connected; connects if not.\n* @param msg message to send\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD": {
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:<init>(java.lang.String,int)": "/**\n* Initializes StatsD with server host and port.\n* @param serverHost hostname of the StatsD server\n* @param serverPort port number for StatsD server\n*/",
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:close()": "/**\n* Closes the socket and sets it to null.\n* @throws IOException if an I/O error occurs while closing the socket\n*/",
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket()": "/**** Creates a DatagramSocket and prepares a DatagramPacket. \n* @throws IOException if socket creation fails or is wrapped by NetUtils.wrapException */",
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String)": "/**\n* Sends a message via a DatagramSocket.\n* @param msg the message to be sent\n* @throws IOException if socket is not created or sending fails\n*/"
    },
    "org.apache.hadoop.metrics2.sink.FileSink": {
        "org.apache.hadoop.metrics2.sink.FileSink:flush()": "/**\n* Flushes the output stream to ensure all data is written.\n*/",
        "org.apache.hadoop.metrics2.sink.FileSink:close()": "/**\n* Closes the writer resource, releasing any associated system resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**\n* Initializes output writer based on configuration.\n* @param conf configuration containing filename\n*/",
        "org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Writes metrics record details to output.\n* @param record contains timestamp, context, tags, and metrics\n*/"
    },
    "org.apache.hadoop.metrics2.filter.AbstractPatternFilter": {
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:<init>()": "/**\n* Initializes tag pattern filters for inclusion and exclusion.\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setIncludePattern(com.google.re2j.Pattern)": "/**\n* Sets the pattern for included items.\n* @param includePattern regex pattern for inclusion\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setExcludePattern(com.google.re2j.Pattern)": "/**\n* Sets the pattern used to exclude certain items.\n* @param excludePattern the pattern to exclude items\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setIncludeTagPattern(java.lang.String,com.google.re2j.Pattern)": "/**\n* Sets a tag pattern for the specified name.\n* @param name the identifier for the tag pattern\n* @param pattern the regex pattern to include\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setExcludeTagPattern(java.lang.String,com.google.re2j.Pattern)": "/**\n* Sets a pattern for excluding tags by name.\n* @param name   the tag name to exclude\n* @param pattern the regex pattern for exclusion\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.String)": "/**\n* Determines if the given name is accepted based on whitelist and blacklist patterns.\n* @param name the name to evaluate\n* @return true if accepted, false if rejected\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag)": "/**\n* Determines if a MetricsTag is accepted based on whitelist/blacklist patterns.\n* @param tag the MetricsTag to evaluate\n* @return true if accepted, false if rejected\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable)": "/**\n* Determines if tags are accepted based on include/exclude patterns.\n* @param tags collection of MetricsTag to evaluate\n* @return true if accepted, false otherwise\n*/",
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**** Initializes patterns and tag patterns from the given configuration. \n* @param conf configuration containing include/exclude patterns and tags */"
    },
    "org.apache.hadoop.metrics2.filter.RegexFilter": {
        "org.apache.hadoop.metrics2.filter.RegexFilter:compile(java.lang.String)": "/**\n* Compiles a given regex string into a Pattern object.\n* @param s the regex string to compile\n* @return compiled Pattern object\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl": {
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:iterator()": "/**\n* Returns an iterator over MetricsRecordBuilderImpl objects.\n* @return Iterator for MetricsRecordBuilderImpl instances\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:clear()": "/**\n* Clears the contents of the rbs object.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:setRecordFilter(org.apache.hadoop.metrics2.MetricsFilter)": "/**\n* Sets the record filter for metrics collection.\n* @param rf MetricsFilter to be applied\n* @return the updated MetricsCollectorImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:setMetricFilter(org.apache.hadoop.metrics2.MetricsFilter)": "/**\n* Sets the metric filter and returns the updated MetricsCollectorImpl instance.\n* @param mf the MetricsFilter to set\n* @return the current MetricsCollectorImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo)": "/**** Adds a metrics record if accepted. \n* @param info metadata for the metrics \n* @return MetricsRecordBuilderImpl instance \n*/",
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String)": "/**\n* Adds a metrics record using the specified name.\n* @param name metric name for the record\n* @return MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords()": "/**\n* Collects non-null MetricsRecordImpl from a list of builders.\n* @return List of MetricsRecordImpl objects\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricCounterLong": {
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:value()": "/**\n* Returns the current value.\n* @return Long representing the current value\n*/",
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:visit(org.apache.hadoop.metrics2.MetricsVisitor)": "/**\n* Invokes the visitor's counter method with the current object and its value.\n* @param visitor MetricsVisitor instance to process the metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:type()": "/**\n* Returns the metric type as COUNTER.\n* @return MetricType representing the metric type\n*/",
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Initializes MetricCounterLong with MetricsInfo and a long value.\n* @param info metrics information for the counter\n* @param value initial long value for the counter\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry": {
        "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:<init>(java.lang.String,java.lang.Iterable)": "/**\n* Constructs an Entry with a name and associated metrics records.\n* @param name the source name for the entry\n* @param records iterable of MetricsRecordImpl associated with the entry\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:name()": "/**\n* Returns the name of the source.\n* @return the source name as a String\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:records()": "/**\n* Retrieves an iterable collection of MetricsRecordImpl objects.\n* @return an iterable of MetricsRecordImpl records\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsBuffer": {
        "org.apache.hadoop.metrics2.impl.MetricsBuffer:<init>(java.lang.Iterable)": "/**\n* Initializes MetricsBuffer with a collection of entries.\n* @param mutable iterable collection of MetricsBuffer.Entry objects\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsBuffer:iterator()": "/**\n* Returns an iterator over the entries in the mutable collection.\n* @return Iterator for the entries\n*/"
    },
    "org.apache.hadoop.metrics2.impl.SinkQueue": {
        "org.apache.hadoop.metrics2.impl.SinkQueue:<init>(int)": "/**\n* Initializes a SinkQueue with specified capacity.\n* @param capacity the maximum number of elements in the queue\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:enqueue(java.lang.Object)": "/**\n* Adds an element to the queue if not full.\n* @param e element to enqueue\n* @return true if added, false if queue is full\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:_dequeue()": "/**\n* Dequeues and returns the front element from the circular queue.\n* @return the dequeued element of type T\n* @throws IllegalStateException if the queue is empty\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:clearConsumerLock()": "/**\n* Clears the current consumer lock.\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:size()": "/**\n* Returns the current size of the collection.\n* @return the number of elements in the collection\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:front()": "/**\n* Retrieves the front element of the circular queue.\n* @return element at the front or null if empty\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:checkConsumer()": "/**\n* Validates if a consumer is currently processing the queue.\n* @throws ConcurrentModificationException if a consumer is active\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:setConsumerLock()": "/**\n* Sets the current thread as the consumer lock.\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:dequeue()": "/**\n* Dequeues an element, waiting if the queue is empty.\n* @return the dequeued element of type T\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:clear()": "/**\n* Clears the data array and resets size to zero.\n* @throws ConcurrentModificationException if a consumer is active\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:waitForData()": "/**\n* Waits for data availability; returns front element when ready.\n* @return front element or null if queue is empty\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)": "/**\n* Consumes data using the provided consumer and manages locks.\n* @param consumer the consumer to process the data\n* @throws InterruptedException if interrupted while waiting for data\n*/",
        "org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)": "/**\n* Consumes all elements using the provided consumer.\n* @param consumer the consumer to process each element\n* @throws InterruptedException if interrupted during consumption\n*/"
    },
    "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem": {
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:inMiniClusterMode()": "/**\n* Checks if the application is running in mini-cluster mode.\n* @return true if in mini-cluster mode, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:init(java.lang.String)": "/**\n* Initializes the MetricsSystem with the given prefix.\n* @param prefix a string used to configure the system\n* @return initialized MetricsSystem instance\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:getImpl()": "/**\n* Retrieves the MetricsSystem implementation.\n* @return MetricsSystem instance from the impl supplier\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdownInstance()": "/**\n* Shuts down the instance and clears associated mappings if it's the last shutdown.\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setImpl(org.apache.hadoop.metrics2.MetricsSystem)": "/**\n* Sets the MetricsSystem implementation and returns the previous one.\n* @param ms new MetricsSystem implementation\n* @return previous MetricsSystem implementation\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeObjectName(java.lang.String)": "/**\n* Removes an object name from the map.\n* @param name the name of the object to be removed\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSource(java.lang.String)": "/**\n* Removes a source from the map by its name.\n* @param name the name of the source to remove\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String)": "/**** Initializes the MetricsSystem with the specified prefix.  \n* @param prefix a string used to configure the system  \n* @return initialized MetricsSystem instance  \n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance()": "/**\n* Retrieves the MetricsSystem instance.\n* @return MetricsSystem implementation from INSTANCE\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown()": "/**\n* Initiates the shutdown process for the instance.\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem)": "/**\n* Sets the MetricsSystem instance and returns the previous one.\n* @param ms new MetricsSystem implementation\n* @return previous MetricsSystem implementation\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName)": "/**\n* Removes an MBean name from the instance.\n* @param name the ObjectName to be removed\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String)": "/**\n* Removes a source by its name.\n* @param name the name of the source to remove\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String)": "/**\n* Creates a new ObjectName if it doesn't already exist.\n* @param name the desired name for the ObjectName\n* @return ObjectName instance or throws MetricsException if exists\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)": "/**\n* Generates a new source name or throws an exception if it already exists.\n* @param name base name for source\n* @param dupOK flag to allow duplicates\n* @return unique source name\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String)": "/**\n* Creates a new MBean ObjectName using the provided name.\n* @param name the desired name for the MBean\n* @return ObjectName instance or throws MetricsException if exists\n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)": "/**\n* Generates a unique source name based on input parameters.\n* @param name base name for source\n* @param dupOK flag to allow duplicates\n* @return unique source name\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsSystemImpl": {
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startTimer()": "/**\n* Initializes and starts a metrics system timer if not already started.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopTimer()": "/**\n* Stops the metrics system timer if it is running.\n* Logs a warning if the timer is already stopped.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:clearConfigs()": "/**\n* Clears all configuration settings and resets the config reference to null.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)": "/**\n* Creates a proxy for the given callback to handle method invocations.\n* @param callback the callback instance to proxy\n* @return a proxy object implementing the Callback interface\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getHostname()": "/**\n* Retrieves the local hostname or returns 'localhost' on error.\n* @return local hostname as a String\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSource(java.lang.String)": "/**\n* Retrieves a MetricsSource by its name.\n* @param name the name of the MetricsSource\n* @return MetricsSource object or null if not found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSourceAdapter(java.lang.String)": "/**\n* Retrieves the MetricsSourceAdapter by its name.\n* @param name the name of the source adapter\n* @return MetricsSourceAdapter associated with the name or null if not found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSinkAdapter(java.lang.String)": "/**\n* Retrieves the MetricsSinkAdapter by its name.\n* @param name the name of the sink adapter\n* @return MetricsSinkAdapter associated with the name or null if not found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode()": "/**\n* Initializes InitMode from system property or environment variable.\n* @return InitMode based on the retrieved value or default to NORMAL\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback)": "/**\n* Registers a callback by adding its proxy to the callbacks list.\n* @param callback the callback instance to register\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)": "/**\n* Registers a callback with a given name.\n* @param name unique identifier for the callback\n* @param callback the callback instance to register\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks()": "/**\n* Stops all metrics sinks and clears the sink registry.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig()": "/**\n* Retrieves the current configuration as a string.\n* @return String representation of the current config\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem()": "/**\n* Configures the system by adding a hostname tag.\n* @return void\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)": "/**\n* Collects and records metrics about active sources and sinks.\n* @param builder MetricsCollector to store the metrics\n* @param all indicates whether to include all metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)": "/**\n* Publishes metrics to sinks, counting dropped metrics.\n* @param buffer metrics to publish\n* @param immediate true for immediate publishing, false otherwise\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans()": "/**\n* Stops all registered metrics MBeans from active sources.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)": "/**\n* Captures and logs metrics snapshot from the source adapter.\n* @param sa source adapter for metrics collection\n* @param bufferBuilder builder to store collected metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String)": "/**\n* Unregisters a source by name, stopping it and removing from various collections.\n* @param name the name of the source to unregister\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources()": "/**\n* Stops all metrics sources and clears the sources collection.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics()": "/**\n* Samples metrics from sources and returns a MetricsBuffer.\n* @return MetricsBuffer containing collected metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop()": "/**\n* Stops the metrics system, handling callbacks and clearing configurations.\n* @throws MetricsException if stopping is illegal\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent()": "/**\n* Updates logical time and publishes metrics if sinks are available.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow()": "/**\n* Publishes metrics immediately if sinks are available.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean()": "/**\n* Initializes the system MBean, registering it if not already done.\n* @throws NullPointerException if prefix is null\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown()": "/**\n* Shuts down the metrics system, clearing resources and unregistering MBeans.\n* @return true if shutdown was successful, false if already shut down\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)": "/**\n* Creates a new MetricsSinkAdapter with specified configurations.\n* @param name sink name, @param desc sink description, @param sink metrics sink object, \n* @param conf configuration settings\n* @return initialized MetricsSinkAdapter\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String)": "/**\n* Constructs MetricsSystemImpl with a specified prefix.\n* @param prefix identifier for the metrics system, can be null for default initialization\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans()": "/**\n* Starts MBeans for all metrics sources.\n* @param none\n* @return void\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)": "/**\n* Registers a metrics sink with specified name and description.\n* @param name sink name, @param desc sink description, @param sink metrics sink object\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)": "/**\n* Creates a MetricsSinkAdapter using provided name, description, and configuration.\n* @param name sink name, @param desc sink description, @param conf configuration settings\n* @return initialized MetricsSinkAdapter\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>()": "/**\n* Initializes MetricsSystemImpl with default settings.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)": "/**\n* Registers a metrics source with specified name and description.\n* @param name source identifier, @param desc source details, @param source data source\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)": "/**\n* Registers a metrics sink with a name and description.\n* @param name unique sink identifier\n* @param description details about the sink\n* @param sink the metrics sink object to register\n* @return the registered metrics sink\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks()": "/**\n* Configures and initializes sinks based on instance configurations.\n* @param none\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource()": "/**\n* Registers the system metrics source with specified configurations.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)": "/**\n* Registers a metrics source with optional name and description.\n* @param name unique identifier for the source\n* @param desc details about the source\n* @param source data source to register\n* @return the registered source object\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources()": "/**\n* Configures sources by retrieving filters and instance configs, then registers them.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String)": "/**\n* Configures the system using a specified prefix.\n* @param prefix configuration prefix for loading metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start()": "/**\n* Starts the metrics system if not already running.\n* @param prefix configuration prefix for loading metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String)": "/**\n* Initializes the metrics system with the given prefix.\n* @param prefix configuration prefix for metrics system\n* @return MetricsSystem instance for method chaining\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder": {
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:info()": "/**\n* Retrieves the MetricsInfo object.\n* @return MetricsInfo instance containing metrics data\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build()": "/**\n* Builds a MetricsSource instance or throws MetricsException for invalid conditions.\n* @return MetricsSource object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object)": "/**\n* Initializes a MetricsRegistry from the source object or creates a new one.\n* @param source the object to inspect for existing MetricsRegistry\n* @return MetricsRegistry instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)": "/**\n* Adds metrics from method annotations to the registry.\n* @param source the source object for the metrics\n* @param method the method to extract metrics from\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)": "/**** Adds MutableMetric to the source field if annotated with Metric. \n* @param source object to modify \n* @param field target field to set metric */",
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)": "/**\n* Constructs MetricsSourceBuilder with source and factory, initializes metrics registry.\n* @param source object to gather metrics from\n* @param factory mutable metrics factory for registration\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter": {
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:start()": "/**\n* Starts the sink thread and logs its initiation.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:sink()": "/**\n* Retrieves the MetricsSink instance.\n* @return MetricsSink object\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop()": "/**\n* Stops the sink thread and cleans up resources if sink is Closeable.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge()": "/**\n* Updates the queue size gauge with the current size of the queue.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)": "/**\n* Consumes metrics from buffer, filtering and sending to sink.\n* @param buffer MetricsBuffer containing entries to process\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)": "/**\n* Enqueues metrics if logicalTimeMs is a multiple of periodMs.\n* @param buffer metrics to enqueue\n* @param logicalTimeMs current logical time in milliseconds\n* @return true if enqueued or not a multiple, false if queue is full\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)": "/**\n* Attempts to enqueue metrics; returns success status.\n* @param buffer MetricsBuffer to enqueue\n* @return true if successful, false if queue full or timeout occurred\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures metrics snapshot into the builder.\n* @param rb MetricsRecordBuilder to store the snapshot\n* @param all includes all metrics if true\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue()": "/**** Publishes metrics from the queue, handling retries and errors during processing. */",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)": "/**\n* Initializes MetricsSinkAdapter with metrics and configuration parameters.\n* @param name sink name, @param description sink description, @param sink metrics sink object,\n* @param context context string, @param sourceFilter filter for source metrics,\n* @param recordFilter filter for record metrics, @param metricFilter filter for metrics,\n* @param periodMs reporting period in milliseconds, @param queueCapacity max queue size,\n* @param retryDelay initial retry delay in seconds, @param retryBackoff backoff multiplier,\n* @param retryCount number of retries\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter": {
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:name()": "/**\n* Returns the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:source()": "/**\n* Retrieves the MetricsSource instance.\n* @return MetricsSource object\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttribute(javax.management.Attribute)": "/**\n* Throws UnsupportedOperationException as metrics are read-only.\n* @param attribute the attribute to set (not used)\n* @throws UnsupportedOperationException if called\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttributes(javax.management.AttributeList)": "/**\n* Sets attributes; operation is unsupported as metrics are read-only.\n* @param attributes list of attributes to set\n* @throws UnsupportedOperationException if called\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:invoke(java.lang.String,java.lang.Object[],java.lang.String[])": "/**\n* Invokes a specified action with parameters and signature.\n* @param actionName name of the action to invoke\n* @param params parameters for the action\n* @param signature expected parameter types\n* @throws MBeanException if invocation fails\n* @throws ReflectionException for reflection-related issues\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:tagName(java.lang.String,int)": "/**\n* Constructs a tag name from a base name and record number.\n* @param name base name for the tag\n* @param recNo record number to append if greater than 0\n* @return formatted tag name as a String\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:metricName(java.lang.String,int)": "/**\n* Constructs a metric name with an optional record number suffix.\n* @param name base metric name\n* @param recNo record number to append (0 for no suffix)\n* @return formatted metric name string\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)": "/**\n* Caches attribute with a generated key from tag name and record number.\n* @param tag MetricsTag object containing name and value\n* @param recNo record number to append for key generation\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)": "/**\n* Sets attribute cache metric with formatted key.\n* @param metric metric object containing name and value\n* @param recNo record number for key formatting\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)": "/**\n* Constructs a MetricsSourceAdapter with specified parameters.\n* @param prefix identifier prefix, @param name source name, @param description details,\n* @param source data source, @param injectedTags additional tags, @param recordFilter filter for records,\n* @param metricFilter filter for metrics, @param jmxCacheTTL cache duration, @param startMBeans flag to start MBeans\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable)": "/**\n* Updates attribute cache from records and returns the count of metrics and tags processed.\n* @param lastRecs iterable of MetricsRecordImpl objects\n* @return total number of metrics and tags updated\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable)": "/**\n* Updates the info cache with new metrics records.\n* @param lastRecs iterable of metrics records to update the cache\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans()": "/**\n* Stops and unregisters the MBean if it exists.\n* @param mbeanName the ObjectName of the MBean to unregister\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)": "/**\n* Retrieves metrics records, applying filters and adding tags.\n* @param builder MetricsCollectorImpl to configure and collect records\n* @param all flag to indicate if all metrics should be retrieved\n* @return Iterable of MetricsRecordImpl objects\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop()": "/**\n* Stops the operation by unregistering MBeans.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)": "/**\n* Constructs a MetricsSourceAdapter with specified parameters.\n* @param prefix identifier prefix, @param name source name, @param description details,\n* @param source data source, @param injectedTags additional tags, @param period refresh period\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache()": "/**\n* Updates the JMX cache if the TTL has expired.\n* Resets lastRecsCleared flag to trigger metrics refresh.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String)": "/**\n* Retrieves the specified attribute value, updating the JMX cache if needed.\n* @param attribute the name of the attribute to fetch\n* @return the value of the attribute\n* @throws AttributeNotFoundException if the attribute does not exist\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[])": "/**\n* Retrieves specified attributes from the cache.\n* @param attributes array of attribute names to fetch\n* @return AttributeList containing requested attributes\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo()": "/**\n* Retrieves MBean information after updating the JMX cache.\n* @return MBeanInfo object containing the MBean's information\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans()": "/**\n* Initializes and registers an MBean if not already initialized.\n* Logs warnings and debug info during the process.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start()": "/**\n* Starts the process if MBeans are enabled.\n* @param startMBeans flag indicating whether to start MBeans\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsConfig": {
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getClassName(java.lang.String)": "/**\n* Retrieves class name based on prefix.\n* @param prefix class name prefix\n* @return full class name or null if not found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:keys()": "/**\n* Returns an iterable of keys as strings.\n* @return Iterable of key strings from the underlying data structure\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getPropertyInternal(java.lang.String)": "/**\n* Retrieves a property value by key, checking parent if not found.\n* @param key property name\n* @return property value or null if not found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)": "/**\n* Initializes MetricsConfig with configuration and a lowercase prefix.\n* @param c configuration object\n* @param prefix prefix string for metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String)": "/**\n* Creates a subset of MetricsConfig with a specified prefix.\n* @param prefix prefix string for metrics\n* @return new MetricsConfig instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader()": "/**\n* Retrieves the ClassLoader for plugins, creating it if necessary.\n* @return ClassLoader for plugin URLs or default ClassLoader if none found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration)": "/**\n* Converts Configuration to a string representation.\n* @param c the Configuration object to convert\n* @return string representation of the Configuration\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String)": "/**\n* Retrieves instance configurations filtered by type.\n* @param type configuration type to filter instances\n* @return Map of instance names to their MetricsConfig\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String)": "/**\n* Retrieves a plugin by name, initializes it, or returns null if not found.\n* @param name the name of the plugin\n* @return initialized plugin instance or null if class name is invalid\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])": "/**\n* Loads the first valid MetricsConfig from specified files.\n* @param prefix configuration prefix\n* @param fileNames array of potential configuration file names\n* @return MetricsConfig object or empty config if none found\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:toString()": "/**\n* Returns string representation of the current object.\n* @return string representation of this object\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String)": "/**\n* Retrieves a MetricsFilter based on a prefix.\n* @param prefix the prefix for filter configuration\n* @return initialized MetricsFilter or null if config is empty\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String)": "/**\n* Creates MetricsConfig by loading configuration for the given prefix.\n* @param prefix configuration prefix\n* @return MetricsConfig object from specified files\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])": "/**\n* Creates a MetricsConfig by loading from specified files.\n* @param prefix configuration prefix\n* @param fileNames potential configuration file names\n* @return MetricsConfig object or empty config if none found\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MsInfo": {
        "org.apache.hadoop.metrics2.impl.MsInfo:toString()": "/**\n* Returns a string representation of the object with name and description.\n* @return formatted string of class name, name, and description\n*/",
        "org.apache.hadoop.metrics2.impl.MsInfo:description()": ""
    },
    "org.apache.hadoop.metrics2.impl.MetricGaugeLong": {
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:value()": "/**\n* Returns the current value.\n* @return Long representing the current value\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:visit(org.apache.hadoop.metrics2.MetricsVisitor)": "/**\n* Invokes the gauge method on the MetricsVisitor with the current value.\n* @param visitor the MetricsVisitor instance to record metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:type()": "/**\n* Returns the type of metric as GAUGE.\n* @return MetricType enum representing the metric type\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Initializes MetricGaugeLong with metrics info and a long value.\n* @param info metrics information\n* @param value long metric value\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricCounterInt": {
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:value()": "/**\n* Returns the current value.\n* @return Integer representing the current value\n*/",
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:visit(org.apache.hadoop.metrics2.MetricsVisitor)": "/**\n* Invokes the visitor's counter method with the current object and its value.\n* @param visitor the MetricsVisitor instance to process the metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:type()": "/**\n* Returns the metric type as COUNTER.\n* @return MetricType enum representing the metric type\n*/",
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Initializes MetricCounterInt with MetricsInfo and an integer value.\n* @param info MetricsInfo object for metrics context\n* @param value initial integer value for the counter\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricGaugeFloat": {
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:value()": "/**\n* Returns the current value.\n* @return Float representing the current value\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:visit(org.apache.hadoop.metrics2.MetricsVisitor)": "/**\n* Visits a MetricsVisitor and reports the gauge value.\n* @param visitor the MetricsVisitor to report to\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:type()": "/**\n* Returns the type of metric as GAUGE.\n* @return MetricType representing the gauge metric\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**\n* Initializes MetricGaugeFloat with MetricsInfo and value.\n* @param info metrics information\n* @param value float value for the gauge\n*/"
    },
    "org.apache.hadoop.metrics2.util.Contracts": {
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(long,boolean,java.lang.Object)": "/**\n* Validates argument against an expression; throws exception if invalid.\n* @param arg value to check\n* @param expression condition to validate\n* @param msg error message if validation fails\n* @return the original argument if valid\n*/",
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(int,boolean,java.lang.Object)": "/**\n* Validates an argument and throws exception if expression is false.\n* @param arg the argument to validate\n* @param expression condition to check\n* @param msg message for exception if validation fails\n* @return the validated argument\n*/",
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(float,boolean,java.lang.Object)": "/**\n* Validates argument; throws exception if expression is false.\n* @param arg value to validate\n* @param expression condition to check\n* @param msg error message if validation fails\n* @return the validated argument\n*/",
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(java.lang.Object,boolean,java.lang.Object)": "/**\n* Validates an argument; throws IllegalArgumentException if expression is false.\n* @param arg the argument to validate\n* @param expression condition to check\n* @param msg message to include in exception if validation fails\n* @return the validated argument\n*/",
        "org.apache.hadoop.metrics2.util.Contracts:<init>()": "/**\n* Private constructor to prevent instantiation of the Contracts class.\n*/",
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(double,boolean,java.lang.Object)": "/**\n* Validates argument against an expression; throws exception if invalid.\n* @param arg the argument to check\n* @param expression condition to validate the argument\n* @param msg message for the exception if validation fails\n* @return the validated argument\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsRecordImpl": {
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:name()": "/**\n* Returns the name from the info object.\n* @return String representing the name\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:description()": "/**\n* Returns the description from the info object.\n* @return description string from info\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:tags()": "/**\n* Returns an unmodifiable list of MetricsTag objects.\n* @return List of MetricsTag instances\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:metrics()": "/**\n* Returns an iterable collection of AbstractMetric objects.\n* @return iterable of AbstractMetric instances\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:timestamp()": "/**\n* Returns the current timestamp value.\n* @return long representing the timestamp\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context()": "/**\n* Retrieves the context value from metrics tags or defaults if not found.\n* @return context value as a String\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)": "/**\n* Constructs a MetricsRecordImpl with specified info, timestamp, tags, and metrics.\n* @param info metrics information\n* @param timestamp record timestamp (must be positive)\n* @param tags list of metrics tags\n* @param metrics iterable of abstract metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString()": "/**\n* Returns a formatted string representation of the object with its properties.\n* @return string including timestamp, name, description, tags, and metrics\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode()": "/**\n* Computes hash code based on name, description, and tags.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object)": "/**\n* Compares this MetricsRecord to another for equality based on key attributes.\n* @param obj the object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord": {
        "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:toString()": "/**\n* Returns a string representation of the object with its properties.\n* @return formatted string including timestamp, name, description, tags, and metrics\n*/",
        "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:hashCode()": "/**\n* Computes hash code based on name, description, and tags.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:equals(java.lang.Object)": "/**\n* Compares this MetricsRecord to another object for equality.\n* @param obj the object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer": {
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:waitTillNotified(long)": "/**\n* Waits for a notification or timeout.\n* @param millisecondsToWait duration to wait before timing out\n* @return true if notified, false if interrupted or timed out\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:notifyAnyWaiters()": "/**\n* Releases a semaphore to notify any waiting threads.\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer)": "/**\n* Constructs WaitableMetricsBuffer from a MetricsBuffer instance.\n* @param metricsBuffer source MetricsBuffer to initialize from\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder": {
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:reset(java.lang.Iterable)": "/**\n* Resets the builder with new metrics records and clears attributes.\n* @param recs iterable of MetricsRecordImpl to set\n* @return MBeanInfoBuilder instance for chaining\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:getAttrName(java.lang.String)": "/**\n* Returns attribute name with record number if valid.\n* @param name base attribute name\n* @return modified attribute name with record number or base name\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)": "/**\n* Initializes MBeanInfoBuilder with name and description.\n* @param name MBean name\n* @param desc MBean description\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Creates a new MBeanAttributeInfo instance.\n* @param name attribute name, @param desc description, @param type data type\n* @return MBeanAttributeInfo object\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/**\n* Creates MBeanAttributeInfo from MetricsInfo and type.\n* @param info MetricsInfo object containing attribute details\n* @param type data type of the attribute\n* @return MBeanAttributeInfo object\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get()": "/**\n* Constructs MBeanInfo from metrics records and tags.\n* @return MBeanInfo object containing metrics attributes\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Adds a gauge attribute with a specified integer value.\n* @param info MetricsInfo for attribute details\n* @param value integer value of the gauge\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a new gauge attribute with a long value.\n* @param info MetricsInfo for attribute details\n* @param value the long value for the gauge\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**\n* Adds a gauge attribute with a float value.\n* @param info MetricsInfo for attribute details\n* @param value float value for the gauge\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)": "/**\n* Adds a new gauge attribute with specified value.\n* @param info MetricsInfo for attribute details\n* @param value the double value of the gauge\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Adds a new attribute to the counter with specified value.\n* @param info MetricsInfo for attribute details\n* @param value integer value for the counter\n*/",
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Updates attribute list with a new counter value.\n* @param info MetricsInfo for the attribute\n* @param value counter value to be added\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricGaugeDouble": {
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:value()": "/**\n* Returns the current value.\n* @return the current Double value\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:visit(org.apache.hadoop.metrics2.MetricsVisitor)": "/**\n* Visits a MetricsVisitor and updates the gauge with the current value.\n* @param visitor the MetricsVisitor to be updated\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:type()": "/**\n* Returns the metric type as GAUGE.\n* @return MetricType.GAUGE constant\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)": "/**\n* Initializes MetricGaugeDouble with MetricsInfo and a double value.\n* @param info metrics information\n* @param value the gauge's double value\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricGaugeInt": {
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:value()": "/**\n* Returns the current value.\n* @return Integer representing the current value\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:visit(org.apache.hadoop.metrics2.MetricsVisitor)": "/**\n* Invokes the gauge method on the MetricsVisitor with current value.\n* @param visitor the MetricsVisitor instance to record the gauge\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:type()": "/**\n* Returns the metric type as GAUGE.\n* @return MetricType.GAUGE constant\n*/",
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Initializes MetricGaugeInt with MetricsInfo and an integer value.\n* @param info MetricsInfo object for gauge metadata\n* @param value integer gauge value\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl": {
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:add(org.apache.hadoop.metrics2.MetricsTag)": "/**\n* Adds a MetricsTag to the record and returns the updated builder.\n* @param tag the MetricsTag to add\n* @return updated MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:add(org.apache.hadoop.metrics2.AbstractMetric)": "/**\n* Adds a metric to the record builder and returns the builder instance.\n* @param metric the metric to be added\n* @return the updated MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tags()": "/**\n* Returns an unmodifiable list of metrics tags.\n* @return unmodifiable List of MetricsTag objects\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:metrics()": "/**\n* Returns an unmodifiable list of metrics.\n* @return a list of AbstractMetric objects\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:parent()": "/**\n* Returns the parent MetricsCollector instance.\n* @return parent MetricsCollector object\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)": "/**\n* Constructs a MetricsRecordBuilder with specified filters and metadata.\n* @param parent parent MetricsCollector\n* @param info metadata for the metrics\n* @param rf record filter\n* @param mf metric filter\n* @param acceptable flag for metric acceptance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord()": "/**** Retrieves a MetricsRecordImpl if conditions are met. \n* @return MetricsRecordImpl or null if not acceptable \n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/****\n* Adds a tag to the metrics if acceptable.\n* @param info metrics information object\n* @param value tag value to be added\n* @return this MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a counter metric if acceptable and filter conditions are met.\n* @param info metrics information for the counter\n* @param value initial long value for the counter\n* @return this MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a gauge metric if acceptable and filter criteria are met.\n* @param info metrics information\n* @param value long metric value\n* @return MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Adds a counter metric if conditions are met.\n* @param info MetricsInfo for the counter\n* @param value initial value for the counter\n* @return MetricsRecordBuilderImpl for method chaining\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**** Adds a gauge metric if accepted. \n* @param info metrics information \n* @param value float value for the gauge \n* @return this MetricsRecordBuilderImpl instance \n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)": "/**\n* Adds a gauge metric if conditions are met.\n* @param info metrics information\n* @param value the gauge's double value\n* @return this MetricsRecordBuilderImpl instance\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**** Adds a gauge metric if acceptable and filter criteria are met. \n* @param info MetricsInfo for gauge metadata \n* @param value integer gauge value \n* @return MetricsRecordBuilderImpl instance for chaining \n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String)": "/**\n* Sets the context tag for metrics.\n* @param value context tag value\n* @return this MetricsRecordBuilderImpl instance\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered": {
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:<init>(org.apache.hadoop.metrics2.MetricsRecord,org.apache.hadoop.metrics2.MetricsFilter)": "/**\n* Constructs a MetricsRecordFiltered with a delegate and a filter.\n* @param delegate the MetricsRecord to be filtered\n* @param filter the MetricsFilter to apply\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:timestamp()": "/**\n* Returns the current timestamp from the delegate.\n* @return long representing the current timestamp\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:name()": "/**\n* Returns the name from the delegate object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:description()": "/**\n* Returns a description from the delegate object.\n* @return description string from the delegate\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:context()": "/**\n* Retrieves the context string from the delegate.\n* @return context string provided by the delegate\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:tags()": "/**\n* Retrieves a collection of MetricsTag objects.\n* @return Collection of MetricsTag from the delegate\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:metrics()": "/**\n* Provides filtered metrics from the delegate.\n* @return Iterable of AbstractMetric objects that meet the filter criteria\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableGaugeInt": {
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:set(int)": "/**\n* Sets the value and marks the object as changed.\n* @param value new integer value to set\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:value()": "/**\n* Retrieves the current integer value.\n* @return the current integer value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(int)": "/**\n* Increments the value by delta and marks the object as changed.\n* @param delta amount to add to the current value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(int)": "/**\n* Decreases the value by delta and marks the object as changed.\n* @param delta amount to decrease the value by\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:toString()": "/**\n* Returns the string representation of the object's value.\n* @return string representation of the value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Records a snapshot of metrics if changed or if all is true.\n* @param builder MetricsRecordBuilder to add the gauge to\n* @param all flag to indicate if all metrics should be recorded\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr()": "/**\n* Increments the value by 1 and marks the object as changed.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr()": "/**\n* Decreases the value by 1 and marks the object as changed.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Initializes MutableGaugeInt with MetricsInfo and an initial value.\n* @param info metrics information, must not be null\n* @param initValue initial integer value for the gauge\n*/"
    },
    "org.apache.hadoop.metrics2.lib.UniqueNames$Count": {
        "org.apache.hadoop.metrics2.lib.UniqueNames$Count:<init>(java.lang.String,int)": "/**\n* Initializes a Count object with a name and value.\n* @param name the name associated with the count\n* @param value the numeric value of the count\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableCounterInt": {
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(int)": "/**\n* Increments the value by delta and marks the object as changed.\n* @param delta amount to add to the current value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:value()": "/**\n* Retrieves the current integer value.\n* @return the current integer from the value object\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:incr()": "/**\n* Increments the value by 1 and marks the object as changed.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Records a snapshot of metrics if changed or if all is true.\n* @param builder MetricsRecordBuilder to record metrics\n* @param all flag to record all metrics regardless of changes\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Initializes a MutableCounterInt with metrics info and initial value.\n* @param info metrics information, must not be null\n* @param initValue initial counter value\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableQuantiles": {
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setInterval(int)": "/**\n* Sets the interval in seconds.\n* @param pIntervalSecs new interval value in seconds\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setNumInfo(org.apache.hadoop.metrics2.MetricsInfo)": "/**\n* Sets the MetricsInfo object for numInfo.\n* @param pNumInfo MetricsInfo to be set\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:getQuantiles()": "/**\n* Retrieves the array of predefined quantiles.\n* @return array of Quantile objects\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantileInfos(int)": "/**\n* Sets the quantileInfos array with specified length.\n* @param length size of the quantileInfos array\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setEstimator(org.apache.hadoop.metrics2.util.QuantileEstimator)": "/**\n* Sets the quantile estimator.\n* @param quantileEstimator the estimator to be set\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:getInterval()": "/**\n* Retrieves the current interval in seconds.\n* @return intervalSecs the current interval value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:addQuantileInfo(int,org.apache.hadoop.metrics2.MetricsInfo)": "/**\n* Adds MetricsInfo to the quantileInfos array at specified index.\n* @param i index for the quantileInfos array\n* @param info MetricsInfo object to be added\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>()": "/**\n* Constructs a new MutableQuantiles instance.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:add(long)": "/**\n* Adds a value to the estimator in a synchronized manner.\n* @param value the long value to be added\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:stop()": "/**\n* Stops the scheduled task if it exists and clears the reference.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:getEstimator()": "/**\n* Retrieves the current QuantileEstimator instance.\n* @return QuantileEstimator object used for estimation\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Takes a snapshot of metrics and updates the builder if changed or all is true.\n* @param builder MetricsRecordBuilder to store gauge values\n* @param all flag to force update regardless of changes\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)": "/**\n* Sets quantile metrics with formatted names and descriptions.\n* @param ucName prefix for metric name, uvName suffix, desc context description, lvName level name, pDecimalFormat formatter\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)": "/**\n* Initializes MutableQuantiles with specified metrics and scheduling.\n* @param name metric name, description metric description, sampleName name for samples,\n* @param valueName name for values, interval time in seconds for scheduling\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample": {
        "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:run()": "/**\n* Updates previous count and snapshot, then clears the estimator.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles)": "/**\n* Initializes RolloverSample with a parent MutableQuantiles instance.\n* @param parent the MutableQuantiles to associate with this sample\n*/"
    },
    "org.apache.hadoop.metrics2.util.SampleQuantiles": {
        "org.apache.hadoop.metrics2.util.SampleQuantiles:<init>(org.apache.hadoop.metrics2.util.Quantile[])": "/**\n* Initializes SampleQuantiles with specified quantiles.\n* @param quantiles array of Quantile objects\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:allowableError(int)": "/**\n* Calculates the minimum allowable error based on rank and quantiles.\n* @param rank the rank to evaluate for allowable error\n* @return the minimum allowable error as a double\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:getSampleCount()": "/**\n* Returns the count of samples in the collection.\n* @return number of samples as an integer\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:clear()": "/**\n* Resets count and bufferCount, and clears the samples collection.\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:getCount()": "",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:compress()": "/**\n* Compresses sample items by merging adjacent ones within allowable error limits.\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:query(double)": "/**\n* Retrieves value at specified quantile from sample data.\n* @param quantile desired quantile (0.0 to 1.0)\n* @return corresponding value from samples\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch()": "/**\n* Inserts sorted items from buffer into samples, adjusting for allowable error.\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long)": "/**\n* Inserts a value into the buffer and processes it if the buffer is full.\n* @param v value to insert into the buffer\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot()": "/**\n* Captures a snapshot of quantile values after flushing the buffer.\n* @return Map of quantiles to their corresponding Long values or null if samples are empty\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles:toString()": "/**\n* Returns a string representation of quantile samples.\n* @return formatted string of quantiles or '[no samples]' if none\n*/"
    },
    "org.apache.hadoop.metrics2.util.Quantile": {
        "org.apache.hadoop.metrics2.util.Quantile:<init>(double,double)": "/**\n* Constructs a Quantile object with specified quantile value and error margin.\n* @param quantile desired quantile value\n* @param error acceptable error margin\n*/",
        "org.apache.hadoop.metrics2.util.Quantile:equals(java.lang.Object)": "/**\n* Compares this Quantile object with another for equality.\n* @param aThat object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.metrics2.util.Quantile:hashCode()": "/**\n* Computes the hash code based on quantile and error values.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.util.Quantile:compareTo(org.apache.hadoop.metrics2.util.Quantile)": "/**\n* Compares this Quantile object with another for ordering.\n* @param other the Quantile to compare with\n* @return negative, zero, or positive integer based on comparison\n*/",
        "org.apache.hadoop.metrics2.util.Quantile:toString()": "/**\n* Returns a string representation of quantile and error as percentages.\n* @return formatted string with quantile and error values\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MetricsRegistry": {
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(org.apache.hadoop.metrics2.MetricsInfo)": "/**\n* Initializes MetricsRegistry with provided MetricsInfo.\n* @param info the MetricsInfo to associate with this registry\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:get(java.lang.String)": "/**\n* Retrieves a MutableMetric by its name.\n* @param name the name of the metric\n* @return the corresponding MutableMetric or null if not found\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:getTag(java.lang.String)": "/**\n* Retrieves a MetricsTag by its name.\n* @param name the name of the MetricsTag\n* @return MetricsTag object or null if not found\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tags()": "/**\n* Returns a collection of MetricsTag objects from the tagsMap.\n* @return Collection of MetricsTag values\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:metrics()": "/**\n* Retrieves a collection of mutable metrics from the metrics map.\n* @return Collection of MutableMetric objects\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:info()": "/**\n* Retrieves the current metrics information.\n* @return MetricsInfo object containing metrics data\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String)": "/**\n* Validates metric name for whitespace and uniqueness in metricsMap.\n* @param name the metric name to check\n* @throws MetricsException if name is invalid or already registered\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String)": "/**\n* Checks if a tag name exists in the map; throws exception if it does.\n* @param name the tag name to check\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures a snapshot of metrics and tags into the builder.\n* @param builder MetricsRecordBuilder to store the snapshot\n* @param all indicates whether to include all metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:toString()": "/**\n* Returns a string representation of the object with metrics and tags info.\n* @return formatted string of class name, metricsInfo, tags, and metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String)": "/**\n* Creates and registers new MutableRatesWithAggregation for a given metric name.\n* @param name the metric name to associate with the rates\n* @return created MutableRatesWithAggregation object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)": "/**\n* Adds a metric with a unique name to the metrics map.\n* @param name unique identifier for the metric\n* @param metric the MutableMetric to be added\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String)": "/**\n* Initializes a MetricsRegistry with the given name.\n* @param name the name of the metrics registry\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)": "/**** Adds a tag to the metrics registry. \n* @param info metrics information object \n* @param value tag value to be added \n* @param override whether to bypass tag name check \n* @return MetricsRegistry instance for chaining \n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Creates a new MutableCounterInt and registers it.\n* @param info metrics information, must not be null\n* @param iVal initial counter value\n* @return the created MutableCounterInt\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Creates a new MutableCounterLong and registers it.\n* @param info metrics info, must not be null\n* @param iVal initial counter value\n* @return the created MutableCounterLong instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Creates a new gauge metric and registers it.\n* @param info metrics information, must not be null\n* @param iVal initial long value for the gauge\n* @return MutableGaugeLong instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**\n* Creates and registers a new gauge metric.\n* @param info metrics info, must not be null\n* @param iVal initial float value for the gauge\n* @return MutableGaugeFloat instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Creates and registers a new MutableGaugeInt.\n* @param info metrics information, must not be null\n* @param iVal initial integer value for the gauge\n* @return the newly created MutableGaugeInt\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)": "/**\n* Creates and registers a new MutableStat metric.\n* @param name metric name, must be unique and valid\n* @param desc metric description\n* @param sampleName sample metric name\n* @param valueName value metric name\n* @param extended flag for extended metrics\n* @return the newly created MutableStat object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String)": "/**\n* Sets the context tag in the metrics registry.\n* @param name context tag value\n* @return MetricsRegistry instance for chaining\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)": "/**\n* Adds a metric tag using name and description.\n* @param name metric name, @param description metric description, @param value tag value, @param override bypass check\n* @return MetricsRegistry instance for chaining\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/**\n* Adds a tag to the metrics registry without overriding existing tags.\n* @param info metrics information object\n* @param value tag value to be added\n* @return MetricsRegistry instance for chaining\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)": "/**\n* Creates a new counter with specified name and description.\n* @param name metric name\n* @param desc metric description\n* @param iVal initial counter value\n* @return MutableCounterInt object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)": "/**\n* Creates a new MutableCounterLong with specified name and description.\n* @param name metric name\n* @param desc metric description\n* @param iVal initial counter value\n* @return the created MutableCounterLong instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)": "/**\n* Creates a new gauge metric with specified name and description.\n* @param name metric name\n* @param desc metric description\n* @param iVal initial long value for the gauge\n* @return MutableGaugeLong instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)": "/**\n* Creates a new gauge metric with given name and description.\n* @param name metric name\n* @param desc metric description\n* @param iVal initial float value for the gauge\n* @return MutableGaugeFloat instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)": "/**\n* Creates a new MutableGaugeInt with specified name and description.\n* @param name metric name\n* @param desc metric description\n* @param iVal initial integer value\n* @return newly created MutableGaugeInt\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)": "/**\n* Creates and registers new MutableQuantiles for metrics.\n* @param name metric name, desc description, sampleName, valueName for samples/values, interval > 0\n* @return new MutableQuantiles instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Creates a new MutableStat metric without extended flag.\n* @param name metric name, must be unique and valid\n* @param desc metric description\n* @param sampleName sample metric name\n* @param valueName value metric name\n* @return the newly created MutableStat object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)": "/**** Creates or retrieves a MutableRate metric. \n* @param name metric name, @param desc metric description, \n* @param extended flag for extended metrics, \n* @param returnExisting flag to return existing rate if present \n* @return MutableRate object \n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Adds a metric tag using name and description.\n* @param name metric name, @param description metric description, @param value tag value\n* @return MetricsRegistry instance for chaining\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)": "/**\n* Creates and registers a new MutableQuantiles instance.\n* @param name metric name, @param desc description, @param sampleName samples name,\n* @param valueName values name, @param interval positive scheduling interval\n* @return MutableQuantiles instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)": "/**\n* Creates or retrieves a MutableRate metric.\n* @param name metric name, @param desc metric description, @param extended flag for extended metrics\n* @return MutableRate object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String)": "/**\n* Creates a MutableRate metric using the given name.\n* @param name metric name\n* @return MutableRate object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)": "/**\n* Creates a MutableRate metric with default extended flag.\n* @param name metric name, @param description metric description\n* @return MutableRate object\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)": "/**\n* Adds a value to a metric or creates a new rate metric if not found.\n* @param name metric name, @param value number to add for statistics\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)": "/**\n* Creates and registers MutableRollingAverages for a metric.\n* @param name unique metric name\n* @param valueName name for the metric value\n* @return newly created MutableRollingAverages\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount": {
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getSnapshotTimeStamp()": "/**\n* Retrieves the snapshot timestamp.\n* @return long representing the snapshot time in milliseconds\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getCount()": "/**\n* Retrieves the current count value.\n* @return the current count as a long\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getSum()": "/**\n* Returns the current sum value.\n* @return the sum as a double\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:<init>(double,long,long)": "/**\n* Initializes SumAndCount with sum, count, and timestamp.\n* @param sum total value accumulated\n* @param count number of entries contributing to sum\n* @param snapshotTimeStamp timestamp of the data snapshot\n*/"
    },
    "org.apache.hadoop.metrics2.util.SampleStat": {
        "org.apache.hadoop.metrics2.util.SampleStat:total()": "/**\n* Calculates total value based on mean and number of samples.\n* @return total as a double\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:numSamples()": "/**\n* Returns the number of samples.\n* @return total number of samples as a long\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:<init>()": "/**\n* Initializes SampleStat with mean and standard deviation set to zero.\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:add(long,double)": "/**\n* Updates statistics with new samples and total value.\n* @param nSamples number of samples to add\n* @param xTotal total value of the samples\n* @return updated SampleStat object\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:mean()": "/**\n* Returns the mean value if samples exist, otherwise returns 0.0.\n* @return mean value or 0.0 if no samples are present\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:variance()": "/**\n* Calculates the sample variance.\n* @return variance value or 0.0 if insufficient samples\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:add(double)": "/**\n* Updates min-max statistics and adds a sample value.\n* @param x sample value to add\n* @return updated SampleStat object\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:min()": "/**\n* Returns the minimum value using minmax object.\n* @return minimum value as a double\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:max()": "/**\n* Returns the maximum value.\n* @return maximum value as a double\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:reset()": "/**\n* Resets sample statistics and minimum/maximum values.\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:stddev()": "/**\n* Calculates the standard deviation.\n* @return standard deviation value based on variance calculation\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)": "/**\n* Resets statistical values with provided parameters.\n* @param numSamples1 number of samples, mean1 average value, s1 standard deviation, minmax1 MinMax object\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:toString()": "/**\n* Returns a string representation of statistical sample metrics.\n* @return formatted string with samples, min, mean, stddev, and max values\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat)": "/**\n* Copies statistical values to another SampleStat object.\n* @param other target SampleStat to copy values to\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableRollingAverages": {
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:close()": "/**\n* Cancels the scheduled task and releases resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long)": "/**\n* Computes average stats for entries with sufficient samples.\n* @param minSamples minimum sample count for inclusion\n* @return map of entry names to their average values\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs()": "/**\n* Updates averages based on current snapshot data.\n* If currentSnapshot is null, no action is taken.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Records metrics snapshot if changed or all is true.\n* @param builder MetricsRecordBuilder for recording data\n* @param all flag to force recording regardless of changes\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)": "/**\n* Adds a value to inner metrics for a specified sample name.\n* @param name sample identifier\n* @param value time duration to add\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates()": "/**\n* Invokes innerMetrics to collect thread-local metrics.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String)": "/**\n* Initializes MutableRollingAverages with a metric value name.\n* @param metricValueName name for the metric value, defaults to empty if null\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)": "/**\n* Replaces the scheduled task with a new one.\n* @param windows number of time windows\n* @param interval interval between executions\n* @param timeUnit unit of the interval\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableGaugeLong": {
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:value()": "/**\n* Retrieves the current value as a long.\n* @return current value from the atomic reference\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(long)": "/**\n* Increments the value by delta and marks the object as changed.\n* @param delta amount to add to the current value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(long)": "/**\n* Decreases the value by delta and marks the object as changed.\n* @param delta amount to decrease the value by\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:set(long)": "/**\n* Sets a new value and marks the object as changed.\n* @param value the new long value to set\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:toString()": "/**\n* Returns the string representation of the object's value.\n* @return string representation of value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Records a snapshot of metrics if changed or all is true.\n* @param builder MetricsRecordBuilder to add the gauge to\n* @param all flag indicating whether to record all metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr()": "/**\n* Increments the current value by 1.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr()": "/**\n* Decreases the value by 1 and marks the object as changed.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Constructs MutableGaugeLong with initial value.\n* @param info metrics information, must not be null\n* @param initValue initial long value for the gauge\n*/"
    },
    "org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys": {
        "org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys:add(java.lang.Object,java.lang.Object)": "/**\n* Adds a value for the given key pair, creating maps as needed.\n* @param k1 first key for the map\n* @param k2 second key for the nested map\n* @return the added or existing value associated with the key pair\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableMetricsFactory": {
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric)": "/**\n* Creates a new MutableMetric for the specified field and annotation.\n* @param field the target field for the metric\n* @param annotation the metric configuration\n* @return a new MutableMetric instance\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric)": "/**\n* Creates a new MutableMetric for the specified method.\n* @param source the source object of the method\n* @param method the method to create a metric for\n* @param annotation the metric annotation details\n* @return a new MutableMetric instance\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getName(java.lang.reflect.Field)": "/**\n* Returns the capitalized name of the given field.\n* @param field the Field object to retrieve the name from\n* @return capitalized name as a String\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getName(java.lang.reflect.Method)": "/**\n* Retrieves the name of a method, capitalizing it appropriately.\n* @param method the Method object to extract the name from\n* @return capitalized method name\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)": "/**\n* Retrieves metrics information based on class and annotation.\n* @param cls class type for metrics context\n* @param annotation metrics annotation containing name and description\n* @return MetricsInfo object with metrics data\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)": "/**\n* Retrieves MetricsInfo based on Metric annotation and default name.\n* @param annotation metric annotation with name and description\n* @param defaultName fallback metric name if annotation is insufficient\n* @return MetricsInfo object based on provided information\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)": "/**\n* Retrieves MetricsInfo using Metric annotation and field's capitalized name.\n* @param annotation metric annotation for info retrieval\n* @param field the Field object for name extraction\n* @return MetricsInfo object based on provided inputs\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)": "/**\n* Retrieves MetricsInfo using Metric annotation and method name.\n* @param annotation metric annotation for information\n* @param method method to extract the name from\n* @return MetricsInfo object based on provided inputs\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)": "/**\n* Creates or retrieves a MutableMetric for a method with given details.\n* @param source the source object for the metric\n* @param method the method to create the metric for\n* @param annotation the metric annotation details\n* @param registry the registry to add the metric to\n* @return the created or retrieved MutableMetric\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)": "/**\n* Creates a MutableMetric based on field type and annotation.\n* @param field the field to create a metric for\n* @param annotation metric configuration details\n* @param registry the registry to register the metric\n* @return the created MutableMetric or throws MetricsException if unsupported\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MethodMetric$3": {
        "org.apache.hadoop.metrics2.lib.MethodMetric$3:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Takes a snapshot of metrics into the builder.\n* @param builder MetricsRecordBuilder to store the snapshot\n* @param all flag to indicate if all metrics should be included\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MetricsInfoImpl": {
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:equals(java.lang.Object)": "/**\n* Compares this MetricsInfo object with another for equality.\n* @param obj the object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:hashCode()": "/**\n* Computes the hash code based on name and description fields.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:toString()": "/**\n* Returns a string representation of the object with name and description.\n* @return formatted string of object's class name, name, and description\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:description()": "",
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:name()": "",
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)": ""
    },
    "org.apache.hadoop.metrics2.lib.MutableGaugeFloat": {
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:value()": "/**\n* Converts stored integer bits to a float value.\n* @return float representation of the integer bits\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:set(float)": "/**\n* Sets the internal value and marks the object as changed.\n* @param value the float value to set\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:compareAndSet(float,float)": "/**\n* Atomically sets the value if it matches the expected value.\n* @param expect the expected current value\n* @param update the new value to set if matched\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:toString()": "/**\n* Returns the string representation of the object's value.\n* @return string representation of value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Records a snapshot of metrics if changed or all is true.\n* @param builder MetricsRecordBuilder to collect data\n* @param all flag to force snapshot regardless of changes\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float)": "/**\n* Increments the value by delta atomically.\n* @param delta amount to add to the current value\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**\n* Constructs a MutableGaugeFloat with initial value.\n* @param info metrics info, must not be null\n* @param initValue initial float value for the gauge\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr()": "/**\n* Increments the value by 1.0 atomically.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr()": "/**\n* Decrements the value by 1 atomically.\n*/"
    },
    "org.apache.hadoop.metrics2.util.SampleStat$MinMax": {
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:add(double)": "/**\n* Updates min and max values based on the provided value.\n* @param value the number to evaluate for min and max\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:min()": "/**\n* Retrieves the minimum value.\n* @return the minimum value as a double\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:max()": "/**\n* Returns the maximum value.\n* @return the maximum value as a double\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset()": "/**\n* Resets min and max values to their default settings.\n*/",
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax)": "/**\n* Resets min and max values based on another MinMax object.\n* @param other MinMax object to copy values from\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableStat": {
        "org.apache.hadoop.metrics2.lib.MutableStat:lastStat()": "/**\n* Returns the last sample statistics based on state change.\n* @return SampleStat object representing the last statistics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:getSnapshotTimeStamp()": "/**\n* Retrieves the snapshot timestamp.\n* @return long representing the snapshot time in milliseconds\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:setExtended(boolean)": "/**\n* Sets the extended state.\n* @param extended true to enable extended mode, false to disable it\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:setUpdateTimeStamp(boolean)": "/**\n* Sets the update timestamp flag.\n* @param updateTimeStamp true to enable, false to disable timestamp updates\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)": "/**\n* Adds samples and updates interval statistics.\n* @param numSamples number of samples to add\n* @param sum total value of the samples\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax()": "/**\n* Resets min and max values to their default settings.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)": "/**\n* Initializes MutableStat with given name, description, and metrics info.\n* @param name metric name\n* @param description metric description\n* @param sampleName sample metric name\n* @param valueName value metric name\n* @param extended flag for extended metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:add(long)": "/**\n* Adds a value to statistics and updates change status.\n* @param value the number to add for statistics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**** Initializes MutableStat with name, description, sample, and value names. */",
        "org.apache.hadoop.metrics2.lib.MutableStat:toString()": "/**\n* Returns string representation of the last sample statistics.\n* @return formatted string of last sample metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures metrics snapshot to the builder if changed or all is true.\n* @param builder MetricsRecordBuilder to store metrics\n* @param all flag to capture all metrics regardless of changes\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MethodMetric": {
        "org.apache.hadoop.metrics2.lib.MethodMetric:isInt(java.lang.Class)": "/**\n* Checks if the given class type is Integer or int.\n* @param type class type to check\n* @return true if type is Integer or int, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:isLong(java.lang.Class)": "/**\n* Checks if the given class type is Long or primitive long.\n* @param type the class type to check\n* @return true if type is Long or long, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:isFloat(java.lang.Class)": "/**\n* Checks if the given class type is Float or its primitive equivalent.\n* @param type class type to check\n* @return true if type is Float or float, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:isDouble(java.lang.Class)": "/**\n* Checks if the given class type is a Double or primitive double.\n* @param type the class type to check\n* @return true if type is Double or double, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures a snapshot of metrics.\n* @param builder MetricsRecordBuilder for recording metrics\n* @param all flag to indicate if all metrics should be included\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:nameFrom(java.lang.reflect.Method)": "/**\n* Extracts and capitalizes the method name from a Method object.\n* @param method the Method object to analyze\n* @return capitalized method name, removing 'get' prefix if present\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class)": "/**\n* Creates a MutableMetric for the specified resource type.\n* @param resType the class type of the resource\n* @return a MutableMetric instance for String type\n* @throws MetricsException if resType is unsupported\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class)": "/**\n* Creates a counter metric for Integer or Long types.\n* @param type class type for the counter\n* @return MutableMetric for the specified type\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class)": "/**\n* Creates a new gauge for specified numeric type.\n* @param t class type to create a gauge for\n* @return MutableMetric instance for the type\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method)": "/**\n* Retrieves metrics information for a given method.\n* @param method the Method object to analyze\n* @return MetricsInfo object based on method name\n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type)": "/**** Creates a MutableMetric based on the specified metric type. \n* @param metricType type of metric to create \n* @return MutableMetric instance or null for unsupported types \n*/",
        "org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)": "/**\n* Initializes MethodMetric with object, method, info, and type; validates inputs.\n* @param obj the target object\n* @param method the metric method (must be parameterless)\n* @param info metadata for the metric\n* @param type the type of the metric\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableMetric": {
        "org.apache.hadoop.metrics2.lib.MutableMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Creates a snapshot using the provided MetricsRecordBuilder.\n* @param builder the MetricsRecordBuilder to record metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetric:changed()": "/**\n* Checks if the state has changed.\n* @return true if changed, false otherwise\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetric:clearChanged()": "/**\n* Resets the 'changed' flag to false, indicating no changes have occurred.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableMetric:setChanged()": "/**\n* Marks the object as changed by setting the changed flag to true.\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation": {
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:getGlobalMetrics()": "/**\n* Retrieves the global metrics map.\n* @return Map of metric names to their corresponding MutableRate objects\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)": "/**\n* Adds elapsed time to statistics for a given sample name.\n* @param name sample identifier\n* @param elapsed time duration to add\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String)": "/**\n* Adds a metric if it doesn't exist; returns the metric.\n* @param name the metric's name\n* @return MutableRate object associated with the name\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class)": "/**\n* Initializes protocol by caching it and logging its methods.\n* @param protocol the class representing the protocol\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[])": "/**\n* Initializes metrics for the given names array.\n* @param names array of metric names to initialize\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap)": "/**\n* Aggregates local stats into global metrics.\n* @param localStats map of local statistics keyed by name\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)": "/**\n* Initializes the protocol with a specified prefix.\n* @param protocol the class representing the protocol\n* @param prefix   the prefix to be assigned to the type\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Creates a snapshot of metrics, aggregating local states into global metrics.\n* @param rb MetricsRecordBuilder for recording metrics\n* @param all flag to include all metrics or not\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates()": "/**\n* Collects thread-local metrics and aggregates them into global metrics.\n*/"
    },
    "org.apache.hadoop.metrics2.source.JvmMetricsInfo": {
        "org.apache.hadoop.metrics2.source.JvmMetricsInfo:description()": "/**\n* Returns the description string.\n* @return the description of the object\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetricsInfo:toString()": "/**\n* Returns a string representation of the object with name and description.\n* @return formatted string of object details\n*/"
    },
    "org.apache.hadoop.metrics2.source.JvmMetrics": {
        "org.apache.hadoop.metrics2.source.JvmMetrics:<init>(java.lang.String,java.lang.String,boolean)": "/**\n* Initializes JvmMetrics with process name, session ID, and optional ThreadMXBean.\n* @param processName name of the process\n* @param sessionId unique session identifier\n* @param useThreadMXBean flag to use ThreadMXBean\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:getThreadUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Collects and records the usage of threads in various states.\n* @param rb MetricsRecordBuilder to store thread state counts\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:getThreadUsageFromGroup(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Records the usage statistics of threads in the current thread group.\n* @param rb MetricsRecordBuilder to store thread usage data\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:calculateMaxMemoryUsage(java.lang.management.MemoryUsage)": "/**\n* Calculates maximum memory usage in megabytes.\n* @param memHeap MemoryUsage object containing heap details\n* @return max memory in MB or MEMORY_MAX_UNLIMITED_MB if unlimited\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)": "/**\n* Registers JvmMetrics with the MetricsSystem.\n* @param ms MetricsSystem to register metrics with\n* @param jvmMetrics JvmMetrics instance to register\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Records memory usage metrics to the MetricsRecordBuilder.\n* @param rb MetricsRecordBuilder for storing memory data\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor)": "/**\n* Sets the garbage collection time monitor.\n* @param gcTimeMonitor non-null GcTimeMonitor instance to set\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded()": "/**\n* Registers JvmMetrics if not already registered in MetricsSystem.\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String)": "/**\n* Retrieves GC metrics info; caches if not present.\n* @param gcName name of the garbage collector\n* @return array of MetricsInfo for GC metrics\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton()": "/**\n* Shuts down the singleton instance and unregisters metrics.\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Collects and records garbage collection metrics in the MetricsRecordBuilder.\n* @param rb MetricsRecordBuilder to store GC metrics\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)": "/**\n* Collects JVM metrics and records them in the MetricsCollector.\n* @param collector MetricsCollector for storing JVM metrics\n* @param all flag to indicate whether to collect all metrics\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)": "/**\n* Creates and registers JvmMetrics with process and session info.\n* @param processName name of the process\n* @param sessionId unique session identifier\n* @param ms MetricsSystem instance for registration\n* @return registered JvmMetrics object\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)": "/**\n* Initializes and returns JvmMetrics singleton instance.\n* @param processName name of the process\n* @param sessionId unique session identifier\n* @return JvmMetrics instance\n*/"
    },
    "org.apache.hadoop.util.JvmPauseMonitor": {
        "org.apache.hadoop.util.JvmPauseMonitor:getNumGcWarnThresholdExceeded()": "/**\n* Retrieves the number of times GC warning threshold was exceeded.\n* @return count of GC warning threshold exceedances\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:getNumGcInfoThresholdExceeded()": "/**\n* Retrieves the count of exceeded GC info thresholds.\n* @return number of times GC info threshold was exceeded\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:getTotalGcExtraSleepTime()": "/**\n* Retrieves the total extra sleep time during garbage collection.\n* @return total extra sleep time in milliseconds\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:getGcTimes()": "/**\n* Retrieves a map of garbage collector names to their corresponding GcTimes.\n* @return Map of GC names and their GcTimes data\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:serviceStop()": "/**\n* Stops the service and interrupts the monitor thread if running.\n* @throws Exception if an error occurs during service stop\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:serviceStart()": "/**\n* Starts the monitoring service in a daemon thread.\n* @throws Exception if an error occurs during startup\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)": "/**\n* Formats a message about GC pauses and differences.\n* @param extraSleepTime pause duration in milliseconds\n* @param gcTimesAfterSleep GC times after pause\n* @param gcTimesBeforeSleep GC times before pause\n* @return formatted message string\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:<init>()": "/**** Constructs JvmPauseMonitor and initializes its state model. */",
        "org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[])": "/**\n* Main method to initialize and start JVM pause monitoring.\n* @param args command-line arguments (not used)\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes service thresholds from configuration settings.\n* @param conf the Configuration object with threshold values\n*/"
    },
    "org.apache.hadoop.util.GcTimeMonitor$GcData": {
        "org.apache.hadoop.util.GcTimeMonitor$GcData:getGcTimePercentage()": "/**\n* Retrieves the garbage collection time percentage.\n* @return gcTimePercentage as an integer value\n*/",
        "org.apache.hadoop.util.GcTimeMonitor$GcData:clone()": "/**\n* Clones the current GcData instance.\n* @return a cloned GcData object\n*/",
        "org.apache.hadoop.util.GcTimeMonitor$GcData:update(long,long,long,long,int)": "/**\n* Updates GC statistics with provided values.\n* @param inTimestamp current timestamp\n* @param inGcMonitorRunTime runtime of GC monitor\n* @param inTotalGcTime total time spent in GC\n* @param inTotalGcCount total number of GC events\n* @param inGcTimePercentage percentage of time spent in GC\n*/"
    },
    "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair": {
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:<init>(java.lang.String,long)": "/**\n* Constructs a NameValuePair with a metric name and its corresponding value.\n* @param metricName the name of the metric\n* @param value the numeric value associated with the metric\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:compareTo(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)": "/**\n* Compares this NameValuePair with another based on their values.\n* @param other another NameValuePair to compare\n* @return negative if less, positive if greater, zero if equal\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:hashCode()": "/**\n* Computes the hash code based on the object's value.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:getValue()": "/**\n* Retrieves the current value.\n* @return the current long value\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object)": "/**\n* Checks equality with another NameValuePair.\n* @param other object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.util.MBeans": {
        "org.apache.hadoop.metrics2.util.MBeans:<init>()": "/**\n* Private constructor for MBeans class to prevent instantiation.\n*/",
        "org.apache.hadoop.metrics2.util.MBeans:getMbeanNameService(javax.management.ObjectName)": "/**\n* Extracts the MBean name from the given ObjectName.\n* @param objectName the ObjectName to parse\n* @return the extracted MBean name\n* @throws IllegalArgumentException if objectName is invalid\n*/",
        "org.apache.hadoop.metrics2.util.MBeans:getMbeanNameName(javax.management.ObjectName)": "/**\n* Extracts MBean name from ObjectName.\n* @param objectName valid ObjectName instance\n* @return extracted MBean name string\n*/",
        "org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName)": "/**\n* Unregisters an MBean by its ObjectName.\n* @param mbeanName the ObjectName of the MBean to unregister\n*/",
        "org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)": "/**\n* Constructs an MBean ObjectName from service and name with additional parameters.\n* @param serviceName the service identifier\n* @param nameName the name identifier\n* @param additionalParameters key-value pairs for extra details\n* @return ObjectName instance or null if creation fails\n*/",
        "org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)": "/**\n* Registers an MBean with the specified service and name.\n* @param serviceName the service identifier\n* @param nameName the name identifier\n* @param properties key-value pairs for MBean registration\n* @param theMbean the MBean instance to register\n* @return ObjectName of the registered MBean or null if registration fails\n*/",
        "org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)": "/**\n* Registers an MBean with service and name, using empty properties.\n* @param serviceName the service identifier\n* @param nameName the name identifier\n* @param theMbean the MBean instance to register\n* @return ObjectName of the registered MBean or null if registration fails\n*/"
    },
    "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem": {
        "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem:<init>(long,int,int)": "/**\n* Constructs a SampleItem with specified value and deltas.\n* @param value the main value of the item\n* @param lowerDelta the lower delta value\n* @param delta the delta value\n*/",
        "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem:toString()": "/**\n* Returns a formatted string of value, g, and delta.\n* @return formatted string representation of the object's state\n*/"
    },
    "org.apache.hadoop.metrics2.util.Servers": {
        "org.apache.hadoop.metrics2.util.Servers:<init>()": "/**\n* Private constructor to prevent instantiation of the Servers class.\n*/",
        "org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)": "/**\n* Parses address specs into a list of InetSocketAddress.\n* @param specs comma/space-separated address strings\n* @param defaultPort port used for addresses without a specified port\n* @return List of InetSocketAddress objects\n*/"
    },
    "org.apache.hadoop.metrics2.util.MetricsCache$RecordCache": {
        "org.apache.hadoop.metrics2.util.MetricsCache$RecordCache:removeEldestEntry(java.util.Map$Entry)": "/**\n* Checks and logs if the cache exceeds the maximum allowed entries.\n* @param eldest the eldest entry in the cache\n* @return true if overflow occurs, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.util.Metrics2Util$TopN": {
        "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:<init>(int)": "/**\n* Constructs TopN instance with specified limit.\n* @param n maximum number of elements to retain\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:updateTotal(long)": "/**\n* Updates the total by adding the specified value.\n* @param value amount to add to the total\n*/",
        "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)": "/**\n* Offers a NameValuePair; updates total and manages size limit.\n* @param entry the NameValuePair to offer\n* @return true if added, false if not (size limit reached)\n*/"
    },
    "org.apache.hadoop.metrics2.util.MetricsCache": {
        "org.apache.hadoop.metrics2.util.MetricsCache:<init>(int)": "/**\n* Initializes MetricsCache with a limit on records per name.\n* @param maxRecsPerName maximum records allowed per name\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache:get(java.lang.String,java.util.Collection)": "/**\n* Retrieves a Record by name and associated metrics tags.\n* @param name the identifier for the Record\n* @param tags collection of MetricsTag to filter the Record\n* @return the corresponding Record or null if not found\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)": "/**\n* Updates or creates a Record for given MetricsRecord and tags inclusion flag.\n* @param mr MetricsRecord containing metrics and tags\n* @param includingTags flag to include tags in the Record\n* @return updated or newly created Record\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache:<init>()": "/**\n* Constructs a MetricsCache with default record limit per name.\n*/",
        "org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Updates a MetricsRecord without including tags.\n* @param mr MetricsRecord to update\n* @return updated Record\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsRecordBuilder": {
        "org.apache.hadoop.metrics2.MetricsRecordBuilder:endRecord()": "/**\n* Ends the current record and returns the MetricsCollector instance.\n* @return MetricsCollector associated with the parent\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsSystem": {
        "org.apache.hadoop.metrics2.MetricsSystem:register(java.lang.Object)": "/**\n* Registers a source object with default parameters.\n* @param source the object to register\n* @return the registered object\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsJsonBuilder": {
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:<init>(org.apache.hadoop.metrics2.MetricsCollector)": "/**\n* Initializes MetricsJsonBuilder with a parent MetricsCollector.\n* @param parent the MetricsCollector instance to associate with this builder\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:tuple(java.lang.String,java.lang.Object)": "/**\n* Adds a key-value pair to innerMetrics and returns the MetricsRecordBuilder instance.\n* @param key the metric key\n* @param value the associated metric value\n* @return the current MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:toString()": "/**\n* Converts innerMetrics to a JSON string.\n* @return JSON representation of innerMetrics or stack trace on failure\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:parent()": "/**\n* Returns the parent MetricsCollector instance.\n* @return parent MetricsCollector object\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/**\n* Tags metrics with given info and value.\n* @param info metric information\n* @param value associated metric value\n* @return MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag)": "/**** Adds a MetricsTag and returns the MetricsRecordBuilder instance. \n* @param tag the MetricsTag to add \n* @return updated MetricsRecordBuilder instance \n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric)": "/**\n* Adds a metric and returns the updated MetricsRecordBuilder.\n* @param metric the metric to be added\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String)": "/**\n* Sets the context metric with a given value.\n* @param value the context value to set\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**** Adds a counter metric with specified value. \n* @param info metric information \n* @param value counter value \n* @return updated MetricsRecordBuilder instance \n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a counter metric with specified info and value.\n* @param info metric information\n* @param value counter value to add\n* @return updated MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)": "/**\n* Adds a gauge metric with specified info and value.\n* @param info metric information, @param value gauge value\n* @return MetricsRecordBuilder instance with added gauge\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)": "/**\n* Adds a gauge metric with specified info and value.\n* @param info metric information\n* @param value gauge value to record\n* @return current MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)": "/**\n* Adds a gauge metric with specified info and value.\n* @param info metric information\n* @param value gauge value to record\n* @return MetricsRecordBuilder instance\n*/",
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)": "/**** Adds a gauge metric with specified info and value. \n* @param info metric information \n* @param value gauge value \n* @return MetricsRecordBuilder instance \n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$TestingGroups": {
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:setUserGroups(java.lang.String,java.lang.String[])": "/**\n* Associates a user with a set of groups.\n* @param user the user's identifier\n* @param groups array of group names to associate with the user\n*/",
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String)": "/**\n* Retrieves user group names.\n* @param user the username to fetch groups for\n* @return Set of group names associated with the user\n* @throws IOException if no groups are found or on cache execution error\n*/",
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String)": "/**\n* Retrieves a list of group names for a specified user.\n* @param user the username to fetch groups for\n* @return List of group names associated with the user\n* @throws IOException if no groups are found or on cache execution error\n*/",
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups)": "/**\n* Initializes TestingGroups with a Groups implementation.\n* @param underlyingImplementation the Groups instance to be used\n*/"
    },
    "org.apache.hadoop.security.NetgroupCache": {
        "org.apache.hadoop.security.NetgroupCache:getNetgroups(java.lang.String,java.util.List)": "/**\n* Retrieves netgroups for a specified user and adds them to the provided list.\n* @param user   the username whose netgroups are to be fetched\n* @param groups the list to which netgroups will be added\n*/",
        "org.apache.hadoop.security.NetgroupCache:clear()": "/**\n* Clears all entries from the user-to-netgroups map.\n*/",
        "org.apache.hadoop.security.NetgroupCache:add(java.lang.String,java.util.List)": "/**\n* Adds a group to each user in the provided list.\n* @param group the group to add\n* @param users list of users to update\n*/",
        "org.apache.hadoop.security.NetgroupCache:getGroups()": "/**\n* Collects all unique groups from user-to-netgroups mapping.\n* @return Set of unique group names\n*/",
        "org.apache.hadoop.security.NetgroupCache:getNetgroupNames()": "/**\n* Retrieves a list of unique netgroup names.\n* @return List of unique netgroup names as Strings\n*/",
        "org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String)": "/**\n* Checks if a group is cached in the unique groups set.\n* @param group name of the group to check\n* @return true if cached, false otherwise\n*/"
    },
    "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB)": "/**\n* Constructs a translator with the given RPC proxy.\n* @param rpcProxy the RPC proxy for authorization policy refresh\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy connection.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl()": "/**\n* Refreshes the service's access control list (ACL).\n* @throws IOException if an IPC call fails during execution\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)": "/**\n* Checks if a method is supported for the RefreshAuthorizationPolicy protocol.\n* @param methodName name of the method to check\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol)": "/**\n* Initializes translator with the given authorization policy implementation.\n* @param impl implementation of RefreshAuthorizationPolicyProtocol\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB:refreshServiceAcl(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto)": "/**\n* Refreshes the service ACL and returns a response.\n* @param controller RPC controller for handling requests\n* @param request contains the refresh ACL request details\n* @return RefreshServiceAclResponseProto indicating the operation's success\n*/"
    },
    "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB)": "/**\n* Constructs a translator with the given RPC proxy.\n* @param rpcProxy the proxy for RefreshUserMappingsProtocol\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy to release resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings()": "/**\n* Refreshes user-to-groups mappings via IPC call.\n* @throws IOException if the IPC call fails\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration()": "/**\n* Refreshes the super user groups configuration via IPC.\n* @throws IOException if the IPC call fails\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)": "/**\n* Checks if a method is supported for the RefreshUserMappingsProtocol.\n* @param methodName name of the method to check\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.security.RefreshUserMappingsProtocol)": "/**\n* Initializes the translator with the given implementation.\n* @param impl the RefreshUserMappingsProtocol implementation\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:refreshUserToGroupsMappings(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto)": "/**\n* Refreshes user-to-groups mappings.\n* @param controller RPC controller for request handling\n* @param request request containing mapping details\n* @return response indicating the result of the refresh\n* @throws ServiceException if an error occurs during processing\n*/",
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:refreshSuperUserGroupsConfiguration(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto)": "/**\n* Refreshes super user groups configuration.\n* @param controller RPC controller for managing calls\n* @param request configuration request details\n* @return response indicating the refresh status\n* @throws ServiceException if an error occurs during refresh\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$LoginParams": {
        "org.apache.hadoop.security.UserGroupInformation$LoginParams:<init>()": "/**\n* Initializes LoginParams with LoginParam class type.\n*/",
        "org.apache.hadoop.security.UserGroupInformation$LoginParams:put(org.apache.hadoop.security.UserGroupInformation$LoginParam,java.lang.String)": "/**\n* Adds a value for the given LoginParam if not already present.\n* @param param the login parameter key\n* @param val the value to associate with the key\n* @return the previous value or null if added\n*/",
        "org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults()": "/**** Retrieves default LoginParams using environment variables. \n* @return LoginParams object with default values \n*/"
    },
    "org.apache.hadoop.security.NullGroupsMapping": {
        "org.apache.hadoop.security.NullGroupsMapping:getGroupsSet(java.lang.String)": "/**\n* Retrieves an empty set of groups for the specified user.\n* @param user the identifier of the user\n* @return an empty Set<String>\n*/",
        "org.apache.hadoop.security.NullGroupsMapping:getGroups(java.lang.String)": "/**\n* Retrieves a list of groups for the specified user.\n* @param user the username to fetch groups for\n* @return an empty list as no groups are available\n*/",
        "org.apache.hadoop.security.NullGroupsMapping:cacheGroupsAdd(java.util.List)": "",
        "org.apache.hadoop.security.NullGroupsMapping:cacheGroupsRefresh()": ""
    },
    "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter": {
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:containsUpperCase(java.lang.Iterable)": "/**\n* Checks if any string in the iterable contains uppercase letters.\n* @param strings collection of strings to check\n* @return true if an uppercase letter is found, false otherwise\n*/",
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest)": "/**\n* Converts request parameter names to lowercase if any are uppercase.\n* @param request original HttpServletRequest\n* @return modified HttpServletRequest with lowercase parameter names\n*/",
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)": "/**\n* Retrieves proxy user configuration from filter parameters.\n* @param filterConfig contains initialization parameters\n* @return Configuration object with proxy user settings\n*/",
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes filter with proxy user configuration.\n* @param filterConfig initialization parameters for the filter\n*/",
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Filters requests for proxy user authentication and authorization.\n* @param filterChain chain of filters to apply\n* @param request HTTP request to process\n* @param response HTTP response to modify\n*/"
    },
    "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer": {
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:<init>()": "/**\n* Initializes ProxyUserAuthenticationFilter with default config prefix for Hadoop authentication.\n*/",
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration)": "/**\n* Creates a filter configuration map from given configuration.\n* @param conf Configuration object for fetching properties\n* @return Map of filter configurations including proxy user settings\n*/",
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a filter with configurations from the provided settings.\n* @param container FilterContainer to add the filter to\n* @param conf Configuration object for fetching properties\n*/"
    },
    "org.apache.hadoop.security.SaslRpcServer$QualityOfProtection": {
        "org.apache.hadoop.security.SaslRpcServer$QualityOfProtection:getSaslQop()": "/**\n* Retrieves the SASL Quality of Protection (QOP) value.\n* @return String representing the SASL QOP\n*/"
    },
    "org.apache.hadoop.security.SaslPropertiesResolver": {
        "org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress)": "/**\n* Retrieves server properties based on client address.\n* @param clientAddress the client's InetAddress\n* @return a map of server properties as key-value pairs\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress)": "/**\n* Retrieves client properties associated with the given server address.\n* @param serverAddress the server's InetAddress\n* @return a map of client properties\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getDefaultProperties()": "/**\n* Retrieves default properties as a map.\n* @return Map of default property names and values\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)": "/**\n* Retrieves server properties for a given client address and port.\n* @param clientAddress the client's InetAddress\n* @param ingressPort the port number for ingress\n* @return a map of server properties as key-value pairs\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)": "/**\n* Retrieves client properties for a specified server address and port.\n* @param serverAddress the server's InetAddress\n* @param ingressPort the port number for ingress\n* @return a map of client properties\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)": "/**\n* Retrieves SASL properties based on configuration and default QOP.\n* @param conf configuration settings\n* @param configKey key for fetching QOP values\n* @param defaultQOP fallback QOP if not found\n* @return map of SASL properties\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Configures settings from the provided Configuration object.\n* @param conf the Configuration to set properties from\n*/",
        "org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a SaslPropertiesResolver instance using the provided configuration.\n* @param conf configuration to determine the resolver class\n* @return SaslPropertiesResolver instance or null if not found\n*/"
    },
    "org.apache.hadoop.security.Credentials$SerializedFormat": {
        "org.apache.hadoop.security.Credentials$SerializedFormat:valueOf(int)": "/**\n* Retrieves SerializedFormat by index.\n* @param val index of the desired SerializedFormat\n* @return corresponding SerializedFormat object\n*/"
    },
    "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler": {
        "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:<init>(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.ipc.Server$Connection)": "/**\n* Constructs a SaslDigestCallbackHandler with specified secret manager and connection.\n* @param secretManager manages security tokens\n* @param connection server connection instance\n*/",
        "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier)": "/**\n* Retrieves and encodes a password using a token identifier.\n* @param tokenid identifier for password retrieval\n* @return encoded password as a char array\n*/",
        "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[])": "/**\n* Handles various authentication callbacks for user authorization and password retrieval.\n* @param callbacks array of Callback objects for processing authentication\n* @throws InvalidToken if token validation fails\n* @throws UnsupportedCallbackException if an unrecognized callback is encountered\n*/"
    },
    "org.apache.hadoop.security.token.SecretManager": {
        "org.apache.hadoop.security.token.SecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)": "/**\n* Retrieves password using the given identifier.\n* @param identifier token for password retrieval\n* @return byte array of the password\n*/",
        "org.apache.hadoop.security.token.SecretManager:generateSecret()": "/**\n* Generates a secure secret key using synchronized key generation.\n* @return SecretKey object for cryptographic use\n*/",
        "org.apache.hadoop.security.token.SecretManager:createPassword(byte[],javax.crypto.SecretKey)": "/**\n* Generates a password hash using HMAC with the given identifier and secret key.\n* @param identifier byte array used as input for the hash\n* @param key secret key for HMAC computation\n* @return byte array representing the hashed password\n*/",
        "org.apache.hadoop.security.token.SecretManager:createSecretKey(byte[])": "/**\n* Creates a SecretKey from the provided byte array.\n* @param key byte array representing the key\n* @return SecretKey object for the specified algorithm\n*/",
        "org.apache.hadoop.security.token.SecretManager:checkAvailableForRead()": "/**\n* Checks if the system is available for read operations.\n* @throws StandbyException if the system is not ready for reading\n*/"
    },
    "org.apache.hadoop.security.SaslRpcServer": {
        "org.apache.hadoop.security.SaslRpcServer:encodePassword(byte[])": "/**\n* Encodes a byte array password to a UTF-8 char array using Base64 encoding.\n* @param password byte array to encode\n* @return encoded password as a char array\n*/",
        "org.apache.hadoop.security.SaslRpcServer:encodeIdentifier(byte[])": "/**\n* Encodes a byte array to a Base64 string.\n* @param identifier byte array to encode\n* @return Base64-encoded string representation\n*/",
        "org.apache.hadoop.security.SaslRpcServer:decodeIdentifier(java.lang.String)": "/**\n* Decodes a Base64 encoded identifier string.\n* @param identifier Base64 encoded string to decode\n* @return byte array of decoded bytes\n*/",
        "org.apache.hadoop.security.SaslRpcServer:splitKerberosName(java.lang.String)": "/**\n* Splits a Kerberos name into components.\n* @param fullName the full Kerberos name to split\n* @return an array of components split by '/' or '@'\n*/",
        "org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the SASL factory if it is not already set.\n* @param conf configuration settings (not used directly here)\n*/",
        "org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)": "/**\n* Retrieves a token identifier from a Base64 encoded string.\n* @param id Base64 encoded identifier string\n* @param secretManager manages token identifiers\n* @return token identifier of type T\n* @throws InvalidToken if deserialization fails\n*/",
        "org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Initializes SaslRpcServer with the specified authentication method.\n* @param authMethod the authentication method used for SASL\n* @throws IOException if user information retrieval fails\n*/",
        "org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)": "/**\n* Creates a SASL server based on the connection and authentication method.\n* @param connection the connection for the SASL server\n* @param saslProperties properties for SASL configuration\n* @param secretManager manages secrets for authentication\n* @return configured SaslServer instance\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/"
    },
    "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback": {
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroups(java.lang.String)": "/**\n* Retrieves a list of groups for the specified user.\n* @param user the username to fetch groups for\n* @return List of group names associated with the user\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:cacheGroupsRefresh()": "/**\n* Refreshes cached groups by delegating to the implementation.\n* @throws IOException if an I/O error occurs during refresh\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:cacheGroupsAdd(java.util.List)": "/**\n* Adds groups to the cache.\n* @param groups list of group names to be added\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>()": "/**\n* Initializes group mapping implementation based on native code availability.\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of groups for the specified user.\n* @param user the username to fetch groups for\n* @return a set of group names associated with the user\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.GroupMappingServiceProvider": {
        "org.apache.hadoop.security.GroupMappingServiceProvider:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of groups for the specified user.\n* @param user the username to fetch groups for\n* @return a set of group names associated with the user\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.HttpCrossOriginFilterInitializer": {
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getPrefix()": "/**\n* Retrieves the prefix for cross-origin filter initialization.\n* @return String representing the prefix\n*/",
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey()": "/**\n* Constructs the enabled config key by appending a suffix to the prefix.\n* @return String representing the enabled config key\n*/",
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Extracts filter parameters from configuration by prefix.\n* @param conf configuration source\n* @param prefix key prefix to filter\n* @return map of filtered parameters without prefix\n*/",
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Cross Origin Filter if enabled in configuration.\n* @param container filter container to add the filter\n* @param conf configuration settings for filter initialization\n*/"
    },
    "org.apache.hadoop.security.FastSaslServerFactory": {
        "org.apache.hadoop.security.FastSaslServerFactory:<init>(java.util.Map)": "/**\n* Initializes FastSaslServerFactory and caches SaslServerFactory by mechanism names.\n* @param props configuration properties for mechanism retrieval\n*/",
        "org.apache.hadoop.security.FastSaslServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)": "/**\n* Creates a SaslServer for the given mechanism and protocol.\n* @param mechanism the SASL mechanism to use\n* @param protocol the protocol name\n* @param serverName the server's name\n* @param props optional properties for the server\n* @param cbh callback handler for authentication\n* @return created SaslServer or null if not found\n*/",
        "org.apache.hadoop.security.FastSaslServerFactory:getMechanismNames(java.util.Map)": "/**\n* Retrieves an array of mechanism names from the factory cache.\n* @param props properties map (unused)\n* @return array of mechanism names as Strings\n*/"
    },
    "org.apache.hadoop.security.Credentials": {
        "org.apache.hadoop.security.Credentials:<init>()": "/**\n* Default constructor for Credentials class.\n*/",
        "org.apache.hadoop.security.Credentials:getToken(org.apache.hadoop.io.Text)": "/**\n* Retrieves a token associated with the given alias.\n* @param alias identifier for the token\n* @return Token object or null if not found\n*/",
        "org.apache.hadoop.security.Credentials:getAllTokens()": "/**\n* Retrieves all tokens from the token map.\n* @return a collection of tokens stored in the map\n*/",
        "org.apache.hadoop.security.Credentials:getTokenMap()": "/**\n* Returns an unmodifiable view of the token map.\n* @return Map of Text to Token identifiers\n*/",
        "org.apache.hadoop.security.Credentials:numberOfTokens()": "/**\n* Returns the count of tokens in the token map.\n* @return number of tokens as an integer\n*/",
        "org.apache.hadoop.security.Credentials:getSecretKey(org.apache.hadoop.io.Text)": "/**\n* Retrieves the secret key associated with the given alias.\n* @param alias identifier for the secret key\n* @return byte array of the secret key or null if not found\n*/",
        "org.apache.hadoop.security.Credentials:numberOfSecretKeys()": "/**\n* Returns the count of secret keys in the map.\n* @return number of secret keys as an integer\n*/",
        "org.apache.hadoop.security.Credentials:addSecretKey(org.apache.hadoop.io.Text,byte[])": "/**\n* Adds a secret key associated with a given alias.\n* @param alias the identifier for the secret key\n* @param key the secret key to be stored\n*/",
        "org.apache.hadoop.security.Credentials:removeSecretKey(org.apache.hadoop.io.Text)": "/**\n* Removes a secret key associated with the given alias.\n* @param alias the identifier for the secret key to remove\n*/",
        "org.apache.hadoop.security.Credentials:getAllSecretKeys()": "/**\n* Retrieves all secret keys as a list.\n* @return List of secret keys from the secretKeysMap\n*/",
        "org.apache.hadoop.security.Credentials:getSecretKeyMap()": "/**\n* Returns an unmodifiable view of the secret key map.\n* @return a map of Text keys to byte array values\n*/",
        "org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)": "/**\n* Adds a token associated with an alias; updates private clones if alias exists.\n* @param alias identifier for the token\n* @param t token to be added, ignored if null\n*/",
        "org.apache.hadoop.security.Credentials:write(java.io.DataOutput)": "/**\n* Writes token and secret key data to the output stream.\n* @param out output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Credentials:readProto(java.io.DataInput)": "/**\n* Reads credentials from input stream and populates tokens and secret keys.\n* @param in data input stream for reading credentials\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)": "/**\n* Merges credentials from another instance, optionally overwriting existing entries.\n* @param other credentials to merge from\n* @param overwrite flag to indicate if existing entries should be replaced\n*/",
        "org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream)": "/**\n* Writes magic token and format to output stream.\n* @param os output stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Credentials:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and populates token and secret key maps.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials)": "/**\n* Merges credentials from another instance, replacing existing entries.\n* @param other credentials to merge from\n*/",
        "org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials)": "/**\n* Merges provided credentials without overwriting existing entries.\n* @param other credentials to merge from\n*/",
        "org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream)": "/**\n* Reads token storage from input stream and validates the format.\n* @param in DataInputStream to read token storage from\n* @throws IOException if an invalid header or format is encountered\n*/",
        "org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput)": "/**\n* Writes credentials and secret keys to output stream.\n* @param out output stream for serialized data\n* @throws IOException on write failure\n*/",
        "org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials)": "/**\n* Initializes Credentials by merging another Credentials instance.\n* @param credentials instance to merge from\n*/",
        "org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)": "/**\n* Reads credentials from a token storage file.\n* @param filename the file containing token storage\n* @param conf configuration settings\n* @return Credentials object\n* @throws IOException if reading fails\n*/",
        "org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream)": "/**\n* Writes magic token and serialized format to output stream.\n* @param os output stream for serialized data\n* @throws IOException on write failure\n*/",
        "org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)": "/**\n* Writes token storage to output stream based on the specified format.\n* @param os output stream for writing data\n* @param format serialization format to use\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream)": "/****\n* Writes token storage to output stream in the oldest format.\n* @param os output stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Reads credentials from a token storage file.\n* @param filename path to the token storage file\n* @param conf configuration for file system access\n* @return Credentials object populated from the file\n* @throws IOException if file access or reading fails\n*/",
        "org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)": "/**\n* Writes token storage to a file.\n* @param filename path to the output file\n* @param conf configuration for the filesystem\n* @param format serialization format for token storage\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Stores token data in a file using the oldest format for compatibility.\n* @param filename path to the output file\n* @param conf configuration for the filesystem\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.token.Token": {
        "org.apache.hadoop.security.token.Token:isPrivateCloneOf(org.apache.hadoop.io.Text)": "/**\n* Checks if the current instance is a private clone of the given public service.\n* @param thePublicService the public service to compare against\n* @return true if it is a private clone, false otherwise\n*/",
        "org.apache.hadoop.security.token.Token:getService()": "/**\n* Retrieves the service associated with this instance.\n* @return Text object representing the service\n*/",
        "org.apache.hadoop.security.token.Token:getKind()": "/**\n* Retrieves the 'kind' property.\n* @return the Text object representing the kind\n*/",
        "org.apache.hadoop.security.token.Token:getIdentifier()": "/**\n* Retrieves the identifier as a byte array.\n* @return byte array representing the identifier\n*/",
        "org.apache.hadoop.security.token.Token:getPassword()": "/**\n* Retrieves the stored password as a byte array.\n* @return byte array containing the password\n*/",
        "org.apache.hadoop.security.token.Token:setService(org.apache.hadoop.io.Text)": "/**\n* Sets the service to the provided Text object.\n* @param newService the new Text service to be set\n*/",
        "org.apache.hadoop.security.token.Token:isPrivate()": "/**\n* Checks if the current instance is private.\n* @return false indicating the instance is not private\n*/",
        "org.apache.hadoop.security.token.Token:getClassForIdentifier(org.apache.hadoop.io.Text)": "/**\n* Retrieves class for a given token kind.\n* @param kind token kind identifier\n* @return Class of TokenIdentifier or null if not found\n*/",
        "org.apache.hadoop.security.token.Token:addBinaryBuffer(java.lang.StringBuilder,byte[])": "/**\n* Appends hex representation of bytes to a StringBuilder buffer.\n* @param buffer StringBuilder to store hex values\n* @param bytes array of bytes to convert to hex\n*/",
        "org.apache.hadoop.security.token.Token:getRenewer()": "/**\n* Retrieves the appropriate TokenRenewer for the current token kind.\n* @return TokenRenewer or TRIVIAL_RENEWER if none found\n*/",
        "org.apache.hadoop.security.token.Token:setID(byte[])": "/**\n* Sets the identifier using the provided byte array.\n* @param bytes byte array representing the identifier\n*/",
        "org.apache.hadoop.security.token.Token:setKind(org.apache.hadoop.io.Text)": "/**\n* Sets the kind of the object and resets the renewer to null.\n* @param newKind new kind value as a Text object\n*/",
        "org.apache.hadoop.security.token.Token:setPassword(byte[])": "/**\n* Sets the user's password.\n* @param newPassword byte array containing the new password\n*/",
        "org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)": "/**\n* Initializes a Token with identifier, password, kind, and service.\n* @param identifier user identifier bytes\n* @param password user password bytes\n* @param kind type of token\n* @param service associated service\n*/",
        "org.apache.hadoop.security.token.Token:<init>()": "/**\n* Initializes a Token object with default values.\n* Sets identifier and password to empty byte arrays, and kind/service to new Text instances.\n*/",
        "org.apache.hadoop.security.token.Token:buildCacheKey()": "/**\n* Constructs a cache key using UUID from concatenated byte data.\n* @return String representation of the cache key\n*/",
        "org.apache.hadoop.security.token.Token:isManaged()": "/**\n* Checks if the current token is managed.\n* @return true if managed, false otherwise\n*/",
        "org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration)": "/**\n* Renews a token using the provided configuration.\n* @param conf configuration settings for renewal\n* @return renewed token duration in milliseconds\n*/",
        "org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration)": "/**\n* Cancels the renewal process using the provided configuration.\n* @param conf configuration settings for cancellation\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.security.token.Token:hashCode()": "/**\n* Computes the hash code for the object using its identifier.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)": "/**\n* Decodes a Base64 string and populates a Writable object.\n* @param obj Writable object to be populated\n* @param newValue Base64 encoded string to decode\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable)": "/**\n* Encodes a Writable object to a Base64 string.\n* @param obj Writable object to encode\n* @return Base64 encoded string representation\n*/",
        "org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text)": "/**\n* Clones the current token with a new associated service.\n* @param newService the new service for the cloned token\n* @return a cloned PrivateToken object\n*/",
        "org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String)": "/**\n* Decodes a Base64 string into the current Writable object.\n* @param newValue Base64 encoded string to decode\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.Token:write(java.io.DataOutput)": "/**\n* Writes identifier and password lengths and their values to output stream.\n* @param out output stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.Token:encodeToUrlString()": "/**\n* Encodes the current object to a Base64 URL string.\n* @return Base64 encoded string representation of the object\n* @throws IOException if encoding fails\n*/",
        "org.apache.hadoop.security.token.Token:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream; initializes identifier and password arrays.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token)": "/**\n* Clones a Token object from another Token.\n* @param other the Token object to clone from\n*/",
        "org.apache.hadoop.security.token.Token:equals(java.lang.Object)": "/**\n* Compares this Token object with another for equality.\n* @param right object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.token.Token:copyToken()": "/**\n* Creates a copy of the current Token object.\n* @return a new Token instance cloned from this Token\n*/",
        "org.apache.hadoop.security.token.Token:decodeIdentifier()": "/**\n* Decodes a token identifier from a byte array.\n* @return decoded TokenIdentifier or null if class not found\n*/",
        "org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder)": "/**\n* Converts identifier to string format and appends to buffer.\n* @param buffer StringBuilder to store the identifier representation\n*/",
        "org.apache.hadoop.security.token.Token:toString()": "/**\n* Returns a string representation of the object with kind, service, and identifier.\n* @return formatted string of object details\n*/"
    },
    "org.apache.hadoop.security.HadoopKerberosName": {
        "org.apache.hadoop.security.HadoopKerberosName:<init>(java.lang.String)": "/**\n* Constructs a HadoopKerberosName with the specified name.\n* @param name the Kerberos principal name\n*/",
        "org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration)": "/**** Sets security configuration based on authentication method. \n* @param conf configuration object \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[])": "/**\n* Main method to configure and print short names from Kerberos principal arguments.\n* @param args command line arguments containing Kerberos principal names\n* @throws Exception if an error occurs during configuration or processing\n*/"
    },
    "org.apache.hadoop.security.User": {
        "org.apache.hadoop.security.User:equals(java.lang.Object)": "/**\n* Compares this User object with another for equality.\n* @param o the object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.User:hashCode()": "/**\n* Returns the hash code for the object based on the full name.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.security.User:getLogin()": "/**\n* Retrieves the current LoginContext.\n* @return the current LoginContext instance\n*/",
        "org.apache.hadoop.security.User:setLogin(javax.security.auth.login.LoginContext)": "/**\n* Sets the login context for the current instance.\n* @param login the LoginContext to be set\n*/",
        "org.apache.hadoop.security.User:setLastLogin(long)": "/**\n* Sets the last login time.\n* @param time timestamp of the last login in milliseconds\n*/",
        "org.apache.hadoop.security.User:getName()": "/**\n* Returns the full name of the object.\n* @return the fullName as a String\n*/",
        "org.apache.hadoop.security.User:getAuthenticationMethod()": "/**\n* Retrieves the current authentication method.\n* @return AuthenticationMethod object representing the method\n*/",
        "org.apache.hadoop.security.User:toString()": "/**\n* Returns the string representation of the object.\n* @return fullName as a string\n*/",
        "org.apache.hadoop.security.User:getLastLogin()": "/**\n* Retrieves the timestamp of the last login.\n* @return last login time in milliseconds since epoch\n*/",
        "org.apache.hadoop.security.User:getShortName()": "/**\n* Retrieves the short name of the object.\n* @return shortName as a String\n*/",
        "org.apache.hadoop.security.User:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)": "/**\n* Sets the authentication method.\n* @param authMethod the authentication method to set\n*/",
        "org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)": "/**\n* Constructs a User with a name, authentication method, and login context.\n* @param name user's principal name\n* @param authMethod authentication method used\n* @param login login context for the user\n*/",
        "org.apache.hadoop.security.User:<init>(java.lang.String)": "/**\n* Constructs a User with a name, defaulting authMethod and loginContext to null.\n* @param name user's principal name\n*/"
    },
    "org.apache.hadoop.security.Groups$TimerToTickerAdapter": {
        "org.apache.hadoop.security.Groups$TimerToTickerAdapter:<init>(org.apache.hadoop.util.Timer)": "/**\n* Initializes the adapter with a Timer instance.\n* @param timer the Timer object to be adapted\n*/",
        "org.apache.hadoop.security.Groups$TimerToTickerAdapter:read()": "/**\n* Reads current time in nanoseconds using a monotonic clock.\n* @return time in nanoseconds\n*/"
    },
    "org.apache.hadoop.security.Groups": {
        "org.apache.hadoop.security.Groups:noGroupsForUser(java.lang.String)": "/**\n* Creates an IOException indicating no groups for the specified user.\n* @param user the username with no associated groups\n* @return IOException with an error message\n*/",
        "org.apache.hadoop.security.Groups:isNegativeCacheEnabled()": "/**\n* Checks if negative cache is enabled based on timeout value.\n* @return true if negative cache timeout is greater than 0, otherwise false\n*/",
        "org.apache.hadoop.security.Groups:getBackgroundRefreshSuccess()": "/**\n* Retrieves the count of successful background refresh operations.\n* @return long representing the number of successes\n*/",
        "org.apache.hadoop.security.Groups:getBackgroundRefreshException()": "/**\n* Retrieves the current background refresh exception count.\n* @return long representing the exception count\n*/",
        "org.apache.hadoop.security.Groups:getBackgroundRefreshQueued()": "/**\n* Retrieves the current count of queued background refresh tasks.\n* @return long representing the queued tasks count\n*/",
        "org.apache.hadoop.security.Groups:getBackgroundRefreshRunning()": "/**\n* Retrieves the current value of background refresh running status.\n* @return long value representing the running status\n*/",
        "org.apache.hadoop.security.Groups:cacheGroupsAdd(java.util.List)": "/**\n* Adds groups to the cache and logs any IO exceptions encountered.\n* @param groups list of group names to cache\n*/",
        "org.apache.hadoop.security.Groups:getNegativeCache()": "/**\n* Retrieves the current negative cache set.\n* @return Set of negative cache entries as Strings\n*/",
        "org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String)": "/**\n* Retrieves user groups, checking static mappings and negative cache.\n* @param user the username to fetch groups for\n* @return Set of group names associated with the user\n* @throws IOException if no groups are found or on cache execution error\n*/",
        "org.apache.hadoop.security.Groups:refresh()": "/**\n* Refreshes user groups cache and clears negative cache if enabled.\n*/",
        "org.apache.hadoop.security.Groups:getGroups(java.lang.String)": "/**\n* Returns an unmodifiable list of user groups.\n* @param user the username to fetch groups for\n* @return List of group names associated with the user\n* @throws IOException if no groups are found or on cache execution error\n*/",
        "org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String)": "/**\n* Retrieves an unmodifiable set of user group names.\n* @param user the username to fetch groups for\n* @return Set of group names associated with the user\n* @throws IOException if no groups are found or on cache execution error\n*/",
        "org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration)": "/**\n* Parses static user-to-group mappings from configuration.\n* @param conf Hadoop configuration object\n*/",
        "org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)": "/**\n* Initializes Groups with configuration and timer, setting caching parameters.\n* @param conf Hadoop configuration object\n* @param timer Timer instance for cache management\n*/",
        "org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Groups instance with a given configuration.\n* @param conf Hadoop configuration object\n*/",
        "org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the singleton Groups instance.\n* @param conf Hadoop configuration object\n* @return Groups object for user-to-groups mapping\n*/",
        "org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the Groups instance with the provided configuration.\n* @param conf Hadoop configuration object\n* @return Groups instance\n*/",
        "org.apache.hadoop.security.Groups:getUserToGroupsMappingService()": "/**\n* Retrieves the user-to-groups mapping service instance.\n* @return Groups object for user-to-groups mapping\n*/"
    },
    "org.apache.hadoop.security.JniBasedUnixGroupsMapping": {
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:logError(int,java.lang.String)": "/**\n* Logs an error message for a specific group ID.\n* @param groupId identifier of the group\n* @param error description of the error encountered\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsInternal(java.lang.String)": "/**\n* Retrieves groups for a specified user.\n* @param user the username to fetch groups for\n* @return an array of group names, empty if none found\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:cacheGroupsAdd(java.util.List)": "",
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:cacheGroupsRefresh()": "",
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String)": "/**\n* Retrieves a list of group names for a specified user.\n* @param user the username to fetch groups for\n* @return List of group names\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of group names for a specified user.\n* @param user the username to fetch groups for\n* @return a Set of group names\n*/"
    },
    "org.apache.hadoop.security.KDiag": {
        "org.apache.hadoop.security.KDiag:flush()": "/**\n* Flushes output streams, prioritizing 'out' or defaulting to System.out and System.err.\n*/",
        "org.apache.hadoop.security.KDiag:arg(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Formats a string with name, params, and meaning.\n* @param name the name to display\n* @param params additional parameters\n* @param meaning the meaning of the name\n* @return formatted string representation\n*/",
        "org.apache.hadoop.security.KDiag:getAndSet(java.lang.String)": "/**\n* Gets and sets a system property to true.\n* @param sysprop the name of the system property\n* @return previous value of the property as boolean\n*/",
        "org.apache.hadoop.security.KDiag:close()": "/**\n* Closes the output stream after flushing.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])": "/**\n* Prints formatted message to output stream or System.out.\n* @param format string format for the message\n* @param args values to format the message\n*/",
        "org.apache.hadoop.security.KDiag:usage()": "/**\n* Generates usage instructions for KDiag with command options.\n* @return formatted usage string for diagnostics\n*/",
        "org.apache.hadoop.security.KDiag:println()": "/**\n* Prints an empty formatted message to the output stream.\n*/",
        "org.apache.hadoop.security.KDiag:printSysprop(java.lang.String)": "/**\n* Prints system property value or UNSET if not found.\n* @param property name of the system property to retrieve\n*/",
        "org.apache.hadoop.security.KDiag:printEnv(java.lang.String)": "/**\n* Prints environment variable value or UNSET if not found.\n* @param variable name of the environment variable\n*/",
        "org.apache.hadoop.security.KDiag:dump(java.io.File)": "/**\n* Dumps file content line by line to output.\n* @param file the file to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])": "/**\n* Logs an error message with a category.\n* @param category error category\n* @param message error message format\n* @param args values for message formatting\n*/",
        "org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])": "/**\n* Logs a warning message with a specified category.\n* @param category the warning category\n* @param message the warning message format\n* @param args additional arguments for message formatting\n*/",
        "org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)": "/**\n* Constructs KDiag with configuration, output stream, keytab, and security settings.\n* @param conf configuration settings\n* @param out output stream for diagnostics\n* @param keytab file for keytab\n* @param principal user principal name\n* @param minKeyLength minimum key length\n* @param securityRequired indicates if security is needed\n*/",
        "org.apache.hadoop.security.KDiag:endln()": "/**\n* Ends the current line and prints a separator.\n*/",
        "org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])": "/**\n* Prints a formatted title surrounded by equal signs.\n* @param format string format for the title\n* @param args values to format the title\n*/",
        "org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])": "/**\n* Logs an error and throws a KerberosDiagsFailure.\n* @param category error category, @param message error message, @param args message args\n*/",
        "org.apache.hadoop.security.KDiag:<init>()": "/**\n* Default constructor for KDiag, initializes with no parameters.\n*/",
        "org.apache.hadoop.security.KDiag:printDefaultRealm()": "/**\n* Prints the default Kerberos realm and warns if none is found.\n*/",
        "org.apache.hadoop.security.KDiag:validateKrb5File()": "/**\n* Validates and locates the Kerberos configuration file, printing its path.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])": "/**\n* Verifies a condition; logs error or fails if false.\n* @param condition check condition, @param category error category, @param message error message, @param args message args\n* @return true if condition is met, false otherwise\n*/",
        "org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])": "/**\n* Logs an error and throws KerberosDiagsFailure if condition is true.\n* @param condition triggers failure logging\n* @param category error category\n* @param message error message\n* @param args message args\n*/",
        "org.apache.hadoop.security.KDiag:validateKeyLength()": "/**\n* Validates AES key length against minimum requirement and logs the maximum allowed length.\n*/",
        "org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Validates user authentication and Kerberos credentials.\n* @param messagePrefix prefix for error messages, @param user UserGroupInformation object\n*/",
        "org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)": "/**\n* Validates a file's existence, type, size, and readability.\n* @param file the file to validate\n* @param category error category for logging\n* @param text additional context for error messages\n* @return true if file is valid, false otherwise\n*/",
        "org.apache.hadoop.security.KDiag:validateShortName()": "/**\n* Validates the short name of the principal; logs warnings/errors if issues arise.\n*/",
        "org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File)": "/**\n* Analyzes a keytab file and prints its principal and entry details.\n* @param keytabFile the keytab file to examine\n* @throws IOException if an I/O error occurs during file operations\n*/",
        "org.apache.hadoop.security.KDiag:validateJAAS(boolean)": "/**\n* Validates JAAS file existence; logs details or errors.\n* @param jaasRequired indicates if JAAS file is mandatory\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.KDiag:validateNTPConf()": "/**\n* Validates NTP configuration file and prints its content if valid.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Verifies token file and logs errors if reading fails.\n* @param tokenFile the file containing token data\n* @param conf configuration settings\n* @param category error category for logging\n* @param message error message format\n* @return true if verification succeeds, false otherwise\n*/",
        "org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Dumps the count and kinds of tokens for the given user.\n* @param ugi user group information containing credentials\n*/",
        "org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration)": "/**\n* Validates Hadoop token files from system properties and configuration.\n* @param conf Hadoop configuration settings\n*/",
        "org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String)": "/**\n* Prints configuration option with its value.\n* @param option the configuration option name to print\n*/",
        "org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration)": "/**\n* Checks if the authentication method is SIMPLE.\n* @param conf configuration object\n* @return true if SIMPLE, false otherwise\n*/",
        "org.apache.hadoop.security.KDiag:validateKinitExecutable()": "/**\n* Validates the kinit executable path and prints its status.\n* Checks if it's absolute or relative, logging accordingly.\n*/",
        "org.apache.hadoop.security.KDiag:validateSasl(java.lang.String)": "/**\n* Validates SASL properties and resolves the corresponding class.\n* @param saslPropsResolverKey key for SASL properties\n*/",
        "org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Dumps user group info, including credentials and authentication details.\n* @param title the title for the output\n* @param ugi UserGroupInformation instance to display\n*/",
        "org.apache.hadoop.security.KDiag:loginFromKeytab()": "/**\n* Logs in a user from a keytab and attempts to renew the login.\n* @throws IOException if login or relogin fails\n*/",
        "org.apache.hadoop.security.KDiag:execute()": "/**\n* Executes Kerberos diagnostics and validates configurations.\n* @return true if execution is successful, throws Exception on failure.\n*/",
        "org.apache.hadoop.security.KDiag:run(java.lang.String[])": "/**\n* Parses command-line arguments and executes diagnostics.\n* @param argv command-line arguments\n* @return 0 on success, KDIAG_FAILURE on error\n*/",
        "org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])": "/**\n* Executes a tool with given configuration and arguments.\n* @param conf configuration settings for the tool\n* @param argv command-line arguments for the tool\n* @return exit status of the tool execution\n*/",
        "org.apache.hadoop.security.KDiag:main(java.lang.String[])": "/**\n* Main method to execute the application with given arguments.\n* @param argv command-line arguments for the application\n*/"
    },
    "org.apache.hadoop.security.ShellBasedIdMapping": {
        "org.apache.hadoop.security.ShellBasedIdMapping:reportDuplicateEntry(java.lang.String,java.lang.Integer,java.lang.String,java.lang.Integer,java.lang.String)": "/**\n* Logs a warning for duplicate entries with details.\n* @param header message header\n* @param key new entry key\n* @param value new entry value\n* @param ekey existing entry key\n* @param evalue existing entry value\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:parseId(java.lang.String)": "/**\n* Parses a string to an Integer.\n* @param idStr string representation of an ID\n* @return Integer parsed from the string\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:checkSupportedPlatform()": "/**\n* Checks if the current platform is supported for user and group updates.\n* @return true if supported; false otherwise, logging an error if not\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:isInteger(java.lang.String)": "/**\n* Checks if the given string can be parsed as an integer.\n* @param s the string to check\n* @return true if s is a valid integer, false otherwise\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getName2IdCmdNIX(java.lang.String,boolean)": "/**\n* Generates command to retrieve user or group ID.\n* @param name user or group name\n* @param isGrp true for group, false for user\n* @return constructed command string\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getId2NameCmdNIX(int,boolean)": "/**\n* Constructs a command to retrieve name by ID for user or group.\n* @param id unique identifier for user or group\n* @param isGrp true for group, false for user\n* @return formatted command string\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getName2IdCmdMac(java.lang.String,boolean)": "/**\n* Constructs a command to retrieve ID by name for user or group.\n* @param name the name of the user or group\n* @param isGrp true for group, false for user\n* @return command string to fetch the ID\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getId2NameCmdMac(int,boolean)": "/**\n* Constructs a command string to fetch name by ID for users or groups.\n* @param id user or group ID\n* @param isGrp true for groups, false for users\n* @return formatted command string\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps()": "/**\n* Clears user and group name maps, updating the last modification time.\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:isExpired()": "/**\n* Checks if the timeout period has expired since the last update.\n* @return true if expired, false otherwise\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)": "/**\n* Updates a BiMap from command output, handling duplicates and parsing errors.\n* @param map BiMap to update; @param mapName name of the map; @param command shell command to execute;\n* @param regex pattern for parsing output; @param staticMapping static ID mapping\n* @return true if the map was updated, false otherwise\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap()": "/**\n* Loads user map based on OS and updates last access time.\n* @throws IOException if an I/O error occurs during map loading\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap()": "/**\n* Loads the full group map into gidNameMap based on OS type.\n* @throws IOException if an error occurs during loading\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File)": "/**\n* Parses a static mapping file into user and group ID mappings.\n* @param staticMapFile the file containing the static mappings\n* @return StaticMapping object with uid and gid mappings\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps()": "/**\n* Loads full user and group maps, ensuring thread safety.\n* @throws IOException if an error occurs during loading maps\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping()": "/**\n* Updates static UID/GID mapping based on the existence and modification of the mapping file.\n* @throws IOException if an I/O error occurs during file operations\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMaps()": "/**\n* Updates user and group maps based on platform support and initialization state.\n* @throws IOException if an error occurs during map loading or updating\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)": "/**\n* Updates UID/GID mapping based on name and type (user/group).\n* @param name user or group name; @param isGrp true for group, false for user\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)": "/**\n* Updates user/group mappings based on ID; throws IOException on I/O errors.\n* @param id unique user or group ID; @param isGrp true for group, false for user\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps()": "/**\n* Checks timeout and updates user/group maps if expired.\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String)": "/**\n* Retrieves UID for a given user, updating maps if necessary.\n* @param user username to fetch UID for\n* @return UID as an integer\n* @throws IOException if user is deleted or I/O error occurs\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String)": "/**\n* Retrieves GID for a group, updating maps if necessary.\n* @param group the group name\n* @return the group ID\n* @throws IOException if the group is not found or an I/O error occurs\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)": "/**\n* Retrieves user name by UID, defaults to provided name if not found.\n* @param uid user identifier; @param unknown default name if UID is absent\n* @return user name or default if UID is not found\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)": "/**\n* Retrieves group name by ID; returns default if not found.\n* @param gid group ID; @param unknown default group name if not found\n* @return group name or default name\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String)": "/**\n* Retrieves UID for a user, falling back to hashcode on error.\n* @param user username to fetch UID for\n* @return UID as an integer\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String)": "/**\n* Retrieves GID or uses hashcode if group not found.\n* @param group the group name\n* @return group ID or its hashcode if not found\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Initializes ShellBasedIdMapping with config and constructs full map if specified.\n* @param conf configuration settings\n* @param constructFullMapAtInit flag to create full map at initialization\n* @throws IOException if an I/O error occurs during initialization\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs ShellBasedIdMapping with given configuration.\n* @param conf configuration settings\n* @throws IOException if an I/O error occurs during initialization\n*/"
    },
    "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping": {
        "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:clear()": "/**\n* Clears all entries from uidMapping and gidMapping.\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:isNonEmpty()": "/**\n* Checks if either uidMapping or gidMapping is non-empty.\n* @return true if at least one mapping is present, false otherwise\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)": "/**\n* Initializes StaticMapping with user and group ID mappings.\n* @param uidMapping user ID mappings\n* @param gidMapping group ID mappings\n*/"
    },
    "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback": {
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroups(java.lang.String)": "/**\n* Retrieves a list of groups for the specified user.\n* @param user the username to fetch groups for\n* @return List of group names associated with the user\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:cacheGroupsRefresh()": "/**\n* Refreshes the cache for groups.\n* @throws IOException if an I/O error occurs during refresh\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:cacheGroupsAdd(java.util.List)": "/**\n* Adds groups to the cache.\n* @param groups list of group names to be added\n* @throws IOException if an I/O error occurs during the operation\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>()": "/**\n* Initializes group mapping implementation based on native code availability.\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of groups for the specified user.\n* @param user the username to fetch groups for\n* @return a set of group names associated with the user\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping": {
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String)": "/**\n* Retrieves a list of users for a given netgroup.\n* @param netgroup the netgroup name, excluding the leading '@'\n* @return List of user names or an empty list if none found\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List)": "/**\n* Caches netgroups by adding users if not already cached.\n* @param groups list of group names to cache\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)": "/**\n* Retrieves user groups and netgroups for the specified username.\n* @param user the username to fetch groups for\n* @return List of group names including netgroups\n*/",
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()": "/**\n* Refreshes cached netgroup names and clears previous entries.\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod": {
        "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:getLoginAppName()": "/**\n* Retrieves the name of the login application.\n* @return loginAppName string; throws exception if not supported\n*/",
        "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:getAuthMethod()": "/**\n* Retrieves the current authentication method.\n* @return AuthMethod object representing the authentication method\n*/",
        "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Converts AuthMethod to corresponding AuthenticationMethod.\n* @param authMethod the authentication method to convert\n* @return AuthenticationMethod matching the provided AuthMethod\n*/"
    },
    "org.apache.hadoop.security.LdapGroupsMapping": {
        "org.apache.hadoop.security.LdapGroupsMapping:getRelativeDistinguishedName(java.lang.String)": "/**\n* Extracts the relative distinguished name from a given distinguished name.\n* @param distinguishedName the full distinguished name to parse\n* @return the relative distinguished name or throws NamingException if malformed\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:lookupPosixGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext)": "/**\n* Looks up a POSIX group based on attributes from a SearchResult.\n* @param result SearchResult containing user attributes\n* @param c DirContext for performing the search\n* @return NamingEnumeration of SearchResult for the group\n* @throws NamingException if attributes are missing or search fails\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:resolveCustomGroupFilterArgs(javax.naming.directory.SearchResult)": "/**\n* Resolves custom filter arguments from a SearchResult.\n* @param result the SearchResult to extract data from\n* @return array of filter arguments or null if no params\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getGroupNames(javax.naming.directory.SearchResult,java.util.Collection,java.util.Collection,boolean)": "/**\n* Retrieves group names and optionally their DNs from a search result.\n* @param groupResult result containing group attributes\n* @param groups collection to store group names\n* @param groupDNs collection to store group DNs if requested\n* @param doGetDNs flag to indicate if DNs should be retrieved\n* @throws NamingException if group attributes are missing\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:failover(int,int)": "/**\n* Checks if failover is needed based on LDAP attempts.\n* @param attemptsMadeWithSameLdap current attempt count\n* @param maxAttemptsBeforeFailover threshold for failover\n* @return true if failover occurs, otherwise false\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:extractPassword(java.lang.String)": "/**\n* Extracts password from a specified file.\n* @param pwFile path to the password file\n* @return password as a trimmed string or empty if no file\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:cacheGroupsAdd(java.util.List)": "/**\n* No operation for adding groups in this user-to-groups mapping provider.\n* @param groups list of group identifiers to add\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:cacheGroupsRefresh()": "/**\n* Refreshes cached user groups; does nothing in this implementation.\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getLdapUrls()": "/**\n* Returns an iterator for LDAP URLs.\n* @return Iterator of LDAP URL strings\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException)": "/**\n* Switches the current bind user on AuthenticationException.\n* @param e the AuthenticationException that triggered the switch\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getDirContext()": "/**\n* Creates and returns a DirContext for LDAP connectivity.\n* @return DirContext instance for LDAP operations\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)": "/**\n* Traverses group hierarchy to collect group names.\n* @param groupDNs set of group distinguished names\n* @param goUpHierarchy levels to traverse up\n* @param groups set to store collected group names\n* @throws NamingException on LDAP errors\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)": "/**\n* Retrieves group names for a user from LDAP search results.\n* @param result user SearchResult\n* @param c DirContext for LDAP operations\n* @param goUpHierarchy levels to traverse up\n* @return set of group names\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Retrieves password for alias from configuration or returns default.\n* @param config configuration source, @param alias credential alias, @param defaultPass fallback password\n* @return resolved password as String\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)": "/**\n* Retrieves user groups from LDAP, optionally traversing hierarchy.\n* @param user user identifier\n* @param goUpHierarchy levels to traverse up in group hierarchy\n* @return set of group names associated with the user\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Retrieves password for an alias from configuration or returns default.\n* @param conf configuration object, @param alias credential alias, @param defaultPass fallback password\n* @return password as String, or defaultPass if not found\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of group names for a given user with retry logic on failures.\n* @param user the user identifier\n* @return Set of group names or an empty set if not found\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration)": "/**\n* Loads SSL configuration settings from provided Configuration object.\n* @param sslConf configuration containing SSL parameters\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String)": "/**\n* Retrieves password for binding user using provided key prefix.\n* @param keyPrefix prefix for password keys\n* @return resolved password as String\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String)": "/**\n* Retrieves a list of group names for a specified user.\n* @param user the user identifier\n* @return List of group names\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers()": "/**\n* Initializes bind users from configuration.\n* Throws RuntimeException if credentials are missing.\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Configures LDAP settings from the provided Configuration object.\n* @param conf configuration containing LDAP parameters\n*/"
    },
    "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo": {
        "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:equals(java.lang.Object)": "/**\n* Compares this BindUserInfo object with another for equality.\n* @param o object to compare\n* @return true if usernames are equal; false otherwise\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:<init>(java.lang.String,java.lang.String)": "/**\n* Initializes BindUserInfo with username and password.\n* @param username user identifier\n* @param password user password\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:hashCode()": "/**\n* Returns the hash code for the username.\n* @return hash code as an integer\n*/"
    },
    "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory": {
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:setConfigurations(java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Sets SSL configurations for LdapSslSocketFactory.\n* @param newKeyStoreLocation path to the key store\n* @param newKeyStorePassword password for the key store\n* @param newTrustStoreLocation path to the trust store\n* @param newTrustStorePassword password for the trust store\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:<init>(javax.net.ssl.SSLSocketFactory)": "/**\n* Initializes LdapSslSocketFactory with a wrapped SSLSocketFactory.\n* @param wrappedSocketFactory the SSLSocketFactory to be wrapped\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getPasswordCharArray(java.lang.String)": "/**\n* Converts a password string to a character array.\n* @param password the password to convert\n* @return char array of password or null if input is null/empty\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket()": "/**\n* Creates a new socket using the socket factory.\n* @return a new Socket instance\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.lang.String,int)": "/**\n* Creates a socket connected to the specified host and port.\n* @param host the server's hostname or IP address\n* @param port the port number on the server\n* @return a Socket object for communication\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)": "/**\n* Creates a socket connected to the specified host and port.\n* @param host the server host name or IP address\n* @param port the server port number\n* @param localHost the local host address\n* @param localPort the local port number\n* @return a Socket object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.net.InetAddress,int)": "/**\n* Creates a socket connected to the specified host and port.\n* @param host the InetAddress of the remote host\n* @param port the port number to connect to\n* @return a Socket object\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)": "/**\n* Creates a socket with specified remote and local addresses and ports.\n* @param address remote InetAddress to connect to\n* @param port remote port number\n* @param localAddress local InetAddress to bind to\n* @param localPort local port number\n* @return a connected Socket instance\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)": "/**\n* Creates a KeyStore from a specified location with a password.\n* @param location path to the KeyStore file\n* @param password password for the KeyStore\n* @return KeyStore instance\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers()": "/**\n* Creates KeyManagers from a KeyStore at a specified location.\n* @return array of KeyManager or null if location is empty\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers()": "/**\n* Creates TrustManager array from a specified trust store.\n* @return TrustManager array or null if trust store location is empty\n*/",
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault()": "/**\n* Returns the default SSLSocketFactory, initializing if necessary.\n* @return SocketFactory instance for SSL connections\n*/"
    },
    "org.apache.hadoop.security.SaslOutputStream": {
        "org.apache.hadoop.security.SaslOutputStream:<init>(java.io.OutputStream,javax.security.sasl.SaslServer)": "/**\n* Initializes SaslOutputStream with specified OutputStream and SaslServer.\n* @param outStream the output stream to wrap\n* @param saslServer the SASL server for negotiation\n*/",
        "org.apache.hadoop.security.SaslOutputStream:<init>(java.io.OutputStream,javax.security.sasl.SaslClient)": "/**\n* Initializes SaslOutputStream with optional wrapping based on SASL QOP.\n* @param outStream the underlying output stream\n* @param saslClient the SASL client for negotiation\n*/",
        "org.apache.hadoop.security.SaslOutputStream:disposeSasl()": "/**\n* Disposes of the SASL client and server resources if they are not null.\n* @throws SaslException if an error occurs during disposal\n*/",
        "org.apache.hadoop.security.SaslOutputStream:flush()": "/**\n* Flushes the output stream to ensure all data is written.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)": "/**\n* Writes data to output stream, optionally wrapping it with SASL.\n* @param inBuf input byte array, @param off offset, @param len length of data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslOutputStream:close()": "/**\n* Closes the output stream and disposes of SASL resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.security.SaslOutputStream:write(int)": "/**\n* Writes a byte to the output stream, optionally wrapping it.\n* @param b byte to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslOutputStream:write(byte[])": "/**\n* Writes byte array to output stream.\n* @param b input byte array\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.SaslPlainServer$SecurityProvider": {
        "org.apache.hadoop.security.SaslPlainServer$SecurityProvider:<init>()": "/**\n* Initializes SecurityProvider with SASL PLAIN authentication support.\n*/"
    },
    "org.apache.hadoop.security.SaslPlainServer": {
        "org.apache.hadoop.security.SaslPlainServer:<init>(javax.security.auth.callback.CallbackHandler)": "/**\n* Initializes SaslPlainServer with a callback handler.\n* @param callback handler for authentication callbacks\n*/",
        "org.apache.hadoop.security.SaslPlainServer:evaluateResponse(byte[])": "/**\n* Evaluates the SASL PLAIN authentication response.\n* @param response byte array containing the response data\n* @throws SaslException if authentication fails or is invalid\n*/",
        "org.apache.hadoop.security.SaslPlainServer:throwIfNotComplete()": "/**\n* Throws an exception if authentication is not completed.\n* @throws IllegalStateException if not completed\n*/",
        "org.apache.hadoop.security.SaslPlainServer:getAuthorizationID()": "/**\n* Retrieves the authorization ID after verifying authentication completion.\n* @return authorization ID string\n*/",
        "org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String)": "/**\n* Retrieves negotiated property based on the property name.\n* @param propName the name of the property to retrieve\n* @return \"auth\" if propName is QOP, otherwise null\n*/",
        "org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)": "/**\n* Wraps outgoing data for SASL authentication.\n* @param outgoing the data to wrap\n* @param offset starting position in the data\n* @param len number of bytes to wrap\n* @throws SaslException if wrapping fails\n*/",
        "org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)": "/**\n* Unwraps a byte array for SASL PLAIN mechanism.\n* @param incoming byte array to unwrap\n* @param offset starting position in the array\n* @param len length of the data to unwrap\n* @throws SaslException if unwrapping fails\n*/"
    },
    "org.apache.hadoop.security.http.RestCsrfPreventionFilter": {
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:parseMethodsToIgnore(java.lang.String)": "/**\n* Parses comma-separated method names to ignore.\n* @param mti comma-separated method names\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:parseBrowserUserAgents(java.lang.String)": "/**\n* Parses user agent strings and stores compiled patterns in a set.\n* @param userAgents comma-separated user agent strings\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:isBrowser(java.lang.String)": "/**\n* Checks if the user agent string corresponds to a known browser.\n* @param userAgent the user agent string to evaluate\n* @return true if it matches a browser pattern, false otherwise\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes filter with custom parameters from FilterConfig.\n* @param filterConfig configuration for the filter\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction)": "/**\n* Handles HTTP interaction; checks browser and required headers.\n* @param httpInteraction the HTTP interaction to process\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)": "/**\n* Filters HTTP requests and responses, processing interactions.\n* @param request the servlet request\n* @param response the servlet response\n* @param chain the filter chain for request processing\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves filtered parameters from configuration by prefix.\n* @param conf Configuration object to filter properties from\n* @param confPrefix prefix for filtering property names\n* @return map of filtered properties without the prefix\n*/"
    },
    "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction": {
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:<init>(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain)": "/**\n* Initializes ServletFilterHttpInteraction with request, response, and filter chain.\n* @param httpRequest the HTTP request object\n* @param httpResponse the HTTP response object\n* @param chain the filter chain for request processing\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:getHeader(java.lang.String)": "/**\n* Retrieves the value of the specified HTTP header.\n* @param header name of the HTTP header to retrieve\n* @return value of the header or null if not present\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:getMethod()": "/**\n* Retrieves the HTTP method of the current request.\n* @return String representing the HTTP method (e.g., GET, POST)\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:proceed()": "/**\n* Executes the filter chain for the HTTP request and response.\n* @throws IOException if an I/O error occurs\n* @throws ServletException if a servlet-related error occurs\n*/",
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:sendError(int,java.lang.String)": "/**\n* Sends an error response with specified code and message.\n* @param code HTTP status code\n* @param message error description\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.http.CrossOriginFilter": {
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedOrigins(javax.servlet.FilterConfig)": "/**\n* Initializes allowed origins from filter configuration.\n* @param filterConfig configuration for filter parameters\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeMaxAge(javax.servlet.FilterConfig)": "/**\n* Initializes maxAge from filterConfig or sets to default if not specified.\n* @param filterConfig configuration containing initialization parameters\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:destroy()": "/**\n* Clears all allowed methods, headers, and origins.\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:encodeHeader(java.lang.String)": "/**\n* Encodes HTTP header to prevent response splitting.\n* @param header the header string to encode\n* @return sanitized header or null if input is null\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:isCrossOrigin(java.lang.String)": "/**\n* Checks if the origins list is not null.\n* @param originsList a string of origins\n* @return true if originsList is not null, false otherwise\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:areOriginsAllowed(java.lang.String)": "/**\n* Checks if provided origins are allowed based on configured rules.\n* @param originsList space-separated origins to validate\n* @return true if any origin is allowed, false otherwise\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:isMethodAllowed(java.lang.String)": "/**\n* Checks if the given method is allowed for access control.\n* @param accessControlRequestMethod the method to check\n* @return true if allowed or if method is null; otherwise false\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:areHeadersAllowed(java.lang.String)": "/**\n* Checks if the specified headers are allowed.\n* @param accessControlRequestHeaders comma-separated headers\n* @return true if headers are allowed, false otherwise\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:getAllowedMethodsHeader()": "/**\n* Joins allowed HTTP methods into a single header string.\n* @return Comma-separated string of allowed methods\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:getAllowedHeadersHeader()": "/**\n* Joins allowed headers into a comma-separated string.\n* @return comma-separated string of allowed headers\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig)": "/**\n* Initializes allowed HTTP methods from filter config or defaults.\n* @param filterConfig configuration for filter initialization\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Validates CORS headers and configures the response accordingly.\n* @param req HTTP request object\n* @param res HTTP response object\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig)": "/**\n* Initializes allowed headers from filter config or uses default.\n* @param filterConfig configuration for filter initialization\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)": "/**\n* Filters requests and responses, applying CORS validation before proceeding.\n* @param req HTTP request object\n* @param res HTTP response object\n*/",
        "org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes filter configuration for HTTP methods, headers, origins, and max age.\n* @param filterConfig configuration for filter initialization\n*/"
    },
    "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper": {
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addHeader(java.lang.String,java.lang.String)": "/**\n* Adds a header unless it is X_FRAME_OPTIONS.\n* @param name the header name\n* @param value the header value\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setHeader(java.lang.String,java.lang.String)": "/**\n* Sets HTTP header if not X-Frame-Options to prevent overwriting.\n* @param name  header name\n* @param value header value\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setDateHeader(java.lang.String,long)": "/**\n* Sets a date header if not overwriting X_FRAME_OPTIONS.\n* @param name header name\n* @param date header date value in milliseconds\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addDateHeader(java.lang.String,long)": "/**\n* Adds a date header unless it's the X-Frame-Options header.\n* @param name the name of the header\n* @param date the date value to set for the header\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setIntHeader(java.lang.String,int)": "/**\n* Sets an integer header if not overwriting X_FRAME_OPTIONS.\n* @param name header name\n* @param value integer value to set\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addIntHeader(java.lang.String,int)": "/**\n* Adds an integer header if not X_FRAME_OPTIONS.\n* @param name header name\n* @param value integer value to set\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:containsHeader(java.lang.String)": "/**\n* Checks if a specific header is present.\n* @param name the header name to check\n* @return true if header exists, false otherwise\n*/"
    },
    "org.apache.hadoop.security.http.XFrameOptionsFilter": {
        "org.apache.hadoop.security.http.XFrameOptionsFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)": "/**\n* Filters HTTP response to set X-Frame-Options header.\n* @param req the ServletRequest object\n* @param res the ServletResponse object\n* @param chain the FilterChain to continue processing\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes filter configuration with custom options.\n* @param config filter configuration containing initialization parameters\n*/",
        "org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves filtered parameters from configuration by prefix.\n* @param conf configuration object\n* @param confPrefix prefix to filter properties\n* @return map of filtered parameters\n*/"
    },
    "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException": {
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:<init>(java.lang.String)": "/**\n* Constructs a PartialGroupNameException with a detailed message.\n* @param message error description for the exception\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PartialGroupNameException with a message and cause.\n* @param message error message\n* @param err underlying cause of the exception\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:toString()": "/**\n* Returns a string representation of the exception with its message.\n* @return formatted exception message\n*/"
    },
    "org.apache.hadoop.security.SecurityUtil$StandardHostResolver": {
        "org.apache.hadoop.security.SecurityUtil$StandardHostResolver:getByName(java.lang.String)": "/**\n* Resolves the hostname to an InetAddress.\n* @param host the hostname to resolve\n* @return InetAddress of the specified host\n* @throws UnknownHostException if the host cannot be resolved\n*/"
    },
    "org.apache.hadoop.security.UGIExceptionMessages": {
        "org.apache.hadoop.security.UGIExceptionMessages:<init>()": "/**\n* Private constructor for UGIExceptionMessages class, preventing instantiation.\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext": {
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:<init>(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)": "/**\n* Initializes HadoopLoginContext with application name and configuration.\n* @param appName name of the application\n* @param subject security subject for authentication\n* @param conf Hadoop configuration settings\n* @throws LoginException if authentication fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:isLoginSuccess()": "/**\n* Checks if the user is currently logged in.\n* @return true if logged in, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getSubjectLock()": "/**\n* Retrieves the subject's private credentials or locks the context if subject is null.\n* @return Object representing private credentials or the context itself\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getConfiguration()": "/**\n* Retrieves the current Hadoop configuration.\n* @return HadoopConfiguration object containing the configuration\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getAppName()": "/**\n* Returns the application name.\n* @return the name of the application as a String\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login()": "/**\n* Handles user login, tracking success and failure metrics.\n* @throws LoginException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout()": "/**\n* Logs out the current user if logged in.\n* @throws LoginException if logout fails\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Renew": {
        "org.apache.hadoop.security.token.DtUtilShell$Renew:validate()": "/**\n* Validates the presence of the alias for renewal.\n* @return true if alias is present; false otherwise\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Renew:getUsage()": "/**\n* Returns the usage information for renewal.\n* @return a string containing the renewal usage details\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Renew:execute()": "/**\n* Executes token renewal for each file in the tokenFiles list.\n* @throws Exception if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Edit": {
        "org.apache.hadoop.security.token.DtUtilShell$Edit:validate()": "/**\n* Validates required fields for the edit command.\n* @return true if validation passes; false otherwise\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Edit:getUsage()": "/**\n* Returns the usage information for editing.\n* @return String containing the edit usage details\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Edit:execute()": "/**\n* Executes token aliasing for each file in the tokenFiles collection.\n* @throws Exception if an error occurs during processing\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Remove": {
        "org.apache.hadoop.security.token.DtUtilShell$Remove:validate()": "/**\n* Validates the presence of the alias flag.\n* @return true if alias is present; false otherwise\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Remove:getUsage()": "/**\n* Returns usage information based on cancel state.\n* @return usage string for cancel or remove action\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Remove:execute()": "/**\n* Executes token removal from specified files.\n* @throws Exception if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.security.token.SecretManager$InvalidToken": {
        "org.apache.hadoop.security.token.SecretManager$InvalidToken:<init>(java.lang.String)": "/**\n* Constructs an InvalidToken exception with a message.\n* @param msg the detail message for the exception\n*/"
    },
    "org.apache.hadoop.security.token.Token$TrivialRenewer": {
        "org.apache.hadoop.security.token.Token$TrivialRenewer:getKind()": "/**\n* Returns null for the kind of text.\n* @return always null\n*/",
        "org.apache.hadoop.security.token.Token$TrivialRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)": "/**\n* Throws exception for unsupported token renewal.\n* @param token the token to renew\n* @param conf configuration settings\n* @throws UnsupportedOperationException if renewal is attempted\n*/",
        "org.apache.hadoop.security.token.Token$TrivialRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)": "/**\n* Throws exception as token cancellation is unsupported for the given token type.\n* @param token the token to cancel\n* @param conf configuration settings\n* @throws UnsupportedOperationException if cancellation is not supported\n*/",
        "org.apache.hadoop.security.token.Token$TrivialRenewer:isManaged(org.apache.hadoop.security.token.Token)": "/**\n* Checks if the given token is managed.\n* @param token the token to check\n* @return false, indicating the token is not managed\n*/",
        "org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text)": "/**\n* Compares provided Text kind with the current object's kind.\n* @param kind Text object to compare\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics": {
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackDuration(java.lang.String,long)": "/**\n* Tracks duration statistics for a given key.\n* @param key identifier for the duration tracking\n* @param count duration value to track\n* @return DurationTracker object with updated statistics\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>()": "/**\n* Initializes metrics for DelegationTokenSecretManager.\n* Configures IOStatistics and creates a MetricsRegistry.\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create()": "/**\n* Creates and registers DelegationTokenSecretManagerMetrics.\n* @return registered DelegationTokenSecretManagerMetrics instance\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)": "/**\n* Tracks the duration of an invocation and updates the metric.\n* @param invocation the invocation to track\n* @param statistic key for tracking duration\n* @param metric rate metric to update\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)": "/**\n* Tracks the storage of a token and updates the corresponding metric.\n* @param invocation the invocation to track\n* @throws IOException if an I/O error occurs during tracking\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)": "/**** Tracks the update token invocation duration. \n* @param invocation the invocation to track \n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)": "/**\n* Tracks the removal of a token.\n* @param invocation the invocation to track\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier": {
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getSequenceNumber()": "/**\n* Retrieves the current sequence number.\n* @return the current sequence number as an integer\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getMasterKeyId()": "/**\n* Retrieves the master key identifier.\n* @return master key ID as an integer\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setIssueDate(long)": "/**\n* Sets the issue date for the object.\n* @param issueDate timestamp representing the issue date\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setMaxDate(long)": "/**\n* Sets the maximum date value.\n* @param maxDate the maximum date as a long timestamp\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setMasterKeyId(int)": "/**\n* Sets the master key identifier.\n* @param newId new master key identifier\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setSequenceNumber(int)": "/**\n* Sets the sequence number.\n* @param seqNum the sequence number to set\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getRealUser()": "/**\n* Retrieves the real user object.\n* @return Text object representing the real user\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getMaxDate()": "/**\n* Retrieves the maximum date value.\n* @return maxDate as a long timestamp\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getRenewer()": "/**\n* Retrieves the renewer object.\n* @return Text object representing the renewer\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)": "/**\n* Checks if two objects are equal, handling null values appropriately.\n* @param a first object to compare\n* @param b second object to compare\n* @return true if both objects are equal or both are null, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:toString()": "/**\n* Returns a string representation of the object with key details.\n* @return formatted string with owner, renewer, realUser, and dates\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:toStringStable()": "/**\n* Returns a stable string representation of the object's properties.\n* @return formatted string of owner, renewer, realUser, issueDate, maxDate, sequenceNumber, and masterKeyId\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getIssueDate()": "/**\n* Retrieves the issue date of the item.\n* @return long representing the issue date\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getOwner()": "/**\n* Retrieves the owner property.\n* @return Text object representing the owner\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:hashCode()": "/**\n* Returns the hash code based on the sequence number.\n* @return integer hash code of the object\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text)": "/**\n* Sets the owner property; initializes with a new Text if null.\n* @param owner Text object to set as owner\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text)": "/**\n* Sets the realUser; creates a new Text if input is null.\n* @param realUser Text object or null to create a new one\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object)": "/**\n* Compares this object to another for equality.\n* @param obj object to compare with this instance\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput)": "/**\n* Serializes object data to output stream.\n* @param out output stream for serialized data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)": "/**\n* Sets the renewer Text; initializes if null, else processes renewer name.\n* @param renewer the Text object representing the renewer\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput)": "/**\n* Serializes object data, ensuring fields do not exceed maximum length.\n* @param out output stream for serialized data\n* @throws IOException if a field is too long or an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream, validates version, and populates object attributes.\n* @param in DataInput stream to read from\n* @throws IOException if version is unknown or read fails\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)": "/**\n* Initializes a delegation token identifier with owner, renewer, and real user.\n* @param owner owner of the token\n* @param renewer entity renewing the token\n* @param realUser actual user associated with the token\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>()": "/**\n* Constructs an AbstractDelegationTokenIdentifier with null values for owner, renewer, and real user.\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser()": "/**\n* Retrieves UserGroupInformation based on owner and realUser fields.\n* @return UserGroupInformation instance or null if owner is invalid\n*/"
    },
    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager": {
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:<init>(long,long,long,long)": "/**\n* Initializes the token secret manager with specified intervals.\n* @param delegationKeyUpdateInterval key update interval\n* @param delegationTokenMaxLifetime max lifetime of tokens\n* @param delegationTokenRenewInterval renew interval for tokens\n* @param delegationTokenRemoverScanInterval scan interval for token removal\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:setCurrentKeyId(int)": "/**\n* Sets the current key ID.\n* @param keyId the new key identifier to set\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:setDelegationTokenSeqNum(int)": "/**\n* Sets the delegation token sequence number.\n* @param seqNum the new sequence number to set\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCurrentTokensSize()": "/**\n* Returns the size of the current tokens collection.\n* @return number of tokens in the collection\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCurrentKeyId()": "/**\n* Retrieves the current key identifier.\n* @return current key ID as an integer\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getAllKeys()": "/**\n* Retrieves all DelegationKey objects.\n* @return array of DelegationKey objects\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getDelegationKey(int)": "/**\n* Retrieves a DelegationKey by its identifier.\n* @param keyId unique identifier for the DelegationKey\n* @return DelegationKey associated with the keyId or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Stores a new master key for delegation.\n* @param key the DelegationKey to be stored\n* @throws IOException if an I/O error occurs during storage\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves DelegationTokenInformation for a given token identifier.\n* @param ident the token identifier\n* @return DelegationTokenInformation or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)": "/**\n* Stores a new token with its renewal date.\n* @param ident TokenIdent object representing the token\n* @param renewDate timestamp for token renewal\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)": "/**\n* Updates the stored token with a new renewal date.\n* @param ident Token identifier to update\n* @param renewDate New renewal date in milliseconds\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getDelegationTokenSeqNum()": "/**\n* Retrieves the current delegation token sequence number.\n* @return current delegation token sequence number\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves tracking ID if tracking is enabled.\n* @param ident TokenIdent object containing tracking info\n* @return Tracking ID string or null if tracking is disabled\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:incrementCurrentKeyId()": "/**\n* Increments and returns the current key ID in a thread-safe manner.\n* @return the incremented current key ID\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Logs the update of a master key.\n* @param key the DelegationKey to log\n* @throws IOException if an I/O error occurs during logging\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Removes the stored master key.\n* @param key the DelegationKey to be removed\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:incrementDelegationTokenSeqNum()": "/**\n* Increments and returns the delegation token sequence number.\n* @return updated sequence number\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCandidateTokensForCleanup()": "/**\n* Retrieves current tokens eligible for cleanup.\n* @return map of token identifiers to their information\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Logs expiration of a token identified by TokenIdent.\n* @param ident the token identifier to log\n* @throws IOException if an I/O error occurs during logging\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Removes a stored token identified by the given TokenIdent.\n* @param ident the TokenIdent of the token to be removed\n* @throws IOException if an I/O error occurs during removal\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:stopThreads()": "/**\n* Stops the token remover thread and waits for its termination.\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getMetrics()": "/**\n* Retrieves the metrics for the Delegation Token Secret Manager.\n* @return DelegationTokenSecretManagerMetrics instance\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRenewInterval()": "/**\n* Retrieves the token renewal interval.\n* @return token renewal interval in milliseconds\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:isRunning()": "/**\n* Checks if the process is currently running.\n* @return true if running, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[])": "/**\n* Creates a SecretKey from the provided byte array.\n* @param key byte array representing the key\n* @return SecretKey object for the specified algorithm\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Formats the TokenIdent ID as a string.\n* @param id TokenIdent to format\n* @return formatted string representation of the TokenIdent\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset()": "/**\n* Resets key ID and clears all token lists.\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Updates the delegation key in the storage.\n* @param key the DelegationKey to update\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Adds a DelegationKey if not running; updates current key ID if necessary.\n* @param key the DelegationKey to add\n* @throws IOException if adding key while running\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Stores a delegation key and updates the master key store.\n* @param key the DelegationKey to be stored\n* @throws IOException if an I/O error occurs during storage\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys()": "/**\n* Removes expired keys from the collection and cleans up associated master keys.\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves tracking ID for the given token identifier.\n* @param identifier the token identifier\n* @return tracking ID as a String or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Removes expired stored token identified by TokenIdent.\n* @param ident the TokenIdent of the token to be removed\n* @throws IOException if an I/O error occurs during removal\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int)": "/**\n* Retrieves top N token owners based on their statistics.\n* @param n maximum number of token owners to return\n* @return list of NameValuePair representing top token owners\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Validates a token and retrieves its information.\n* @param identifier token identifier\n* @return DelegationTokenInformation if valid; throws InvalidToken otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection)": "/**\n* Logs and removes expired tokens from storage.\n* @param expiredTokens collection of expired token identifiers\n* @throws IOException if an I/O error occurs during logging or removal\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey()": "/**\n* Updates the current master key for delegation tokens and logs the change.\n* @throws IOException if an I/O error occurs during logging or storage\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves the password for a given token identifier.\n* @param identifier token identifier\n* @return byte array representing the password\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads()": "/**\n* Starts the token remover thread after validating the state.\n* @throws IOException if an I/O error occurs during key update\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey()": "/**** Rolls the master key, updates its expiry, and manages delegation keys. */",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])": "/**\n* Verifies token by comparing provided password with stored password.\n* @param identifier token identifier\n* @param password byte array to validate against stored password\n* @throws InvalidToken if the password does not match\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves the real owner's name from the token identifier.\n* @param id TokenIdent object containing user info\n* @return real owner's name as a String\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)": "/**\n* Decodes a TokenIdent from a given token.\n* @param token the token containing the identifier\n* @return decoded TokenIdent or null if class not found\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Updates token ownership statistics for a given token identifier.\n* @param id TokenIdent object for which stats are updated\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Removes or decrements token stats for the real owner.\n* @param id TokenIdent containing user info\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Creates a password for the given TokenIdent and stores the associated token information.\n* @param identifier TokenIdent for which the password is created\n* @return byte array representing the generated password\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Renews a token if valid; throws exceptions for invalid cases.\n* @param token the token to renew\n* @param renewer the entity requesting the renewal\n* @return the new expiration time in milliseconds\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Cancels a token if authorized and returns its identifier.\n* @param token the token to cancel\n* @param canceller the user requesting the cancellation\n* @return TokenIdent of the canceled token\n* @throws IOException if an I/O error occurs or authorization fails\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)": "/**\n* Adds a persisted delegation token if not running.\n* @param identifier the token identifier\n* @param renewDate timestamp for renewal\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats()": "/**\n* Synchronizes token ownership statistics by clearing and updating from current tokens.\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken()": "/**\n* Removes expired tokens and logs them.\n* @throws IOException if an I/O error occurs during logging or removal\n*/"
    },
    "org.apache.hadoop.security.token.delegation.DelegationKey": {
        "org.apache.hadoop.security.token.delegation.DelegationKey:getKeyId()": "/**\n* Retrieves the key identifier.\n* @return the keyId value\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:setExpiryDate(long)": "/**\n* Sets the expiry date.\n* @param expiryDate the new expiry date in milliseconds since epoch\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:getExpiryDate()": "/**\n* Retrieves the expiry date.\n* @return long representing the expiry date in milliseconds\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param right object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,byte[])": "/**\n* Constructs a DelegationKey with specified ID, expiry date, and encoded key bytes.\n* @param keyId unique identifier for the key\n* @param expiryDate expiration timestamp\n* @param encodedKey byte array representing the key, must not exceed MAX_KEY_LEN\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:hashCode()": "/**\n* Computes hash code for the object based on expiryDate, keyBytes, and keyId.\n* @return integer hash code value\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)": "/**\n* Constructs a DelegationKey with specified ID, expiry date, and optional SecretKey.\n* @param keyId unique identifier for the key\n* @param expiryDate expiration timestamp\n* @param key optional SecretKey to derive encoded key bytes\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput)": "/**\n* Serializes object data to a DataOutput stream.\n* @param out output stream for serialized data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:getKey()": "/**\n* Retrieves a SecretKey from keyBytes or returns null if keyBytes is empty.\n* @return SecretKey object or null if keyBytes is invalid\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:<init>()": "/**\n* Initializes a DelegationKey with default values (0, 0L, null).\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput stream, initializing keyId, expiryDate, and keyBytes.\n* @param in DataInput stream to read fields from\n* @throws IOException if an error occurs during reading\n*/"
    },
    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation": {
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getRenewDate()": "/**\n* Retrieves the renewal date.\n* @return the renewal date as a long value\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[],java.lang.String)": "/**\n* Constructs DelegationTokenInformation with renew date, password, and tracking ID.\n* @param renewDate timestamp for renewal\n* @param password byte array for authentication\n* @param trackingId identifier for tracking the token\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getPassword()": "/**\n* Retrieves the stored password as a byte array.\n* @return byte array representing the password\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getTrackingId()": "/**\n* Retrieves the tracking ID.\n* @return the current tracking ID as a String\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])": "/**\n* Constructs DelegationTokenInformation with renew date and password.\n* @param renewDate timestamp for renewal\n* @param password byte array for authentication\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput)": "/**\n* Writes object data to DataOutput stream.\n* @param out output stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>()": "/**\n* Default constructor for DelegationTokenInformation.\n* Initializes with default values (0, null).\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput stream into object attributes.\n* @param in DataInput stream for reading fields\n*/"
    },
    "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache": {
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:<init>(long,long,java.util.function.Function)": "/**\n* Initializes a loading cache with expiration and size limits.\n* @param cacheExpirationMs cache entry expiration time in milliseconds\n* @param maximumCacheSize maximum number of entries in the cache\n* @param singleEntryFunction function to load cache entries\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:size()": "/**\n* Returns the number of entries in the internal loading cache.\n* @return the size of the cache as an integer\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:containsKey(java.lang.Object)": "/**\n* Checks if the cache contains a specified key.\n* @param key the key to check in the cache\n* @return true if the key exists; false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:containsValue(java.lang.Object)": "/**\n* Checks if the collection contains the specified value.\n* @param value the value to check for presence\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:get(java.lang.Object)": "/**\n* Retrieves a value by key from the cache.\n* @param key the key to look up\n* @return the associated value or null if not found or an error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:put(java.lang.Object,java.lang.Object)": "/**\n* Stores a key-value pair and retrieves the stored value.\n* @param key the key to store the value under\n* @param value the value to be associated with the key\n* @return the stored value associated with the key\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:remove(java.lang.Object)": "/**\n* Removes and returns the value associated with the specified key.\n* @param key the key whose associated value is to be removed\n* @return the removed value or null if key was not present\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:putAll(java.util.Map)": "/**\n* Inserts all mappings from the specified map into the cache.\n* @param m a map of key-value pairs to insert\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:clear()": "/**\n* Clears all entries from the internal loading cache.\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:keySet()": "/**\n* Returns a set of keys from the internal loading cache.\n* @return Set of keys contained in the cache\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:values()": "/**\n* Retrieves all values from the internal loading cache.\n* @return Collection of values stored in the cache\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:entrySet()": "/**\n* Returns a set of entries from the internal loading cache.\n* @return Set of key-value pairs in the cache\n*/",
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty()": "/**\n* Checks if the cache is empty.\n* @return true if cache size is 0, otherwise false\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:setAuthHandlerClass(java.util.Properties)": "/**\n* Sets the authentication handler class based on configuration properties.\n* @param props configuration properties\n* @throws ServletException if AUTH_TYPE is missing or invalid\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:setHandlerAuthMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Sets the authentication method for the handler.\n* @param authMethod the authentication method to set\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getDoAs(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves the 'doAs' parameter from the HTTP request query string.\n* @param request the HTTP request object\n* @return the value of 'doAs' or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getHttpUserGroupInformationInContext()": "/**\n* Retrieves the current UserGroupInformation from the thread-local storage.\n* @return UserGroupInformation object for the current context\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)": "/**\n* Retrieves configuration properties and sets the auth handler class.\n* @param configPrefix prefix for configuration keys\n* @param filterConfig filter configuration object\n* @return Properties object containing configuration settings\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)": "/**\n* Initializes the authentication handler and sets the CuratorFramework.\n* @param authHandlerClassName name of the auth handler class\n* @param filterConfig configuration for the filter\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)": "/**\n* Constructs Configuration with proxyuser settings from FilterConfig.\n* @param filterConfig contains initialization parameters\n* @return Configuration object with proxyuser properties\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig)": "/**\n* Initializes authentication handler and configures delegation token settings.\n* @param filterConfig initialization parameters for the filter\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Filters HTTP requests for user authentication and authorization.\n* @param filterChain chain of filters for request processing\n* @param request HTTP request object\n* @param response HTTP response object\n*/"
    },
    "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager": {
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:setCurator(org.apache.curator.framework.CuratorFramework)": "/**\n* Sets the current CuratorFramework instance.\n* @param curator the CuratorFramework to set\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getCurator()": "/**\n* Retrieves the CuratorFramework instance from thread-local storage.\n* @return CuratorFramework instance\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrSharedCount(org.apache.curator.framework.recipes.shared.SharedCount,int)": "/**\n* Increments shared count by batch size until successful.\n* @param sharedCount the SharedCount object to increment\n* @param batchSize the amount to add to the count\n* @return the updated count value\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createPersistentNode(java.lang.String)": "/**\n* Creates a persistent znode at the specified path.\n* @param nodePath the path where the znode will be created\n* @throws Exception if the znode cannot be created\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean)": "/**\n* Loads data from ZK cache based on cache type (token or key).\n* @param isTokenCache flag to select token or key cache loading\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyRemoved(java.lang.String)": "/**\n* Removes a key from allKeys based on the last segment of the path.\n* @param path the string path containing the key identifier\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationTokenSeqNum()": "/**\n* Retrieves the current delegation token sequence number.\n* @return the current sequence number as an integer\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:setDelegationTokenSeqNum(int)": "/**\n* Sets the delegation token sequence number.\n* @param seqNum the sequence number to set\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getCurrentKeyId()": "/**\n* Retrieves the current key ID from the key ID sequence counter.\n* @return current key ID as an integer\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getNodePath(java.lang.String,java.lang.String)": "/**\n* Constructs a node path by combining root and nodeName.\n* @param root the base path\n* @param nodeName the name of the node\n* @return the full node path as a String\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromMemory(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves DelegationTokenInformation from memory using TokenIdent.\n* @param ident identifier for the token\n* @return DelegationTokenInformation or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:isTokenWatcherEnabled()": "/**\n* Checks if the token watcher feature is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads()": "/**\n* Stops all threads and closes associated resources, handling exceptions silently.\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum()": "/**\n* Increments the delegation token sequence number and fetches a new range if exhausted.\n* @return updated sequence number\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId()": "/**\n* Increments the current key ID and returns the updated count.\n* @return updated key ID count after increment\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Removes the stored master key from ZooKeeper.\n* @param key the DelegationKey to be removed\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)": "/**\n* Adds or updates a delegation key in Zookeeper.\n* @param key the DelegationKey to store\n* @param isUpdate true if updating an existing key\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Stores a new delegation key.\n* @param key the DelegationKey to store\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Updates an existing delegation key.\n* @param key the DelegationKey to update\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[])": "/**\n* Processes and updates a DelegationKey from byte array data.\n* @param data byte array containing key information\n* @throws IOException if reading data fails\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int)": "/**\n* Retrieves a DelegationKey from Zookeeper by keyId.\n* @param keyId identifier for the DelegationKey\n* @return DelegationKey object or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads()": "/***************************************************************************************\n* Initializes and starts Zookeeper threads and caches based on client type.\n* @throws IOException if starting ZK client or creating nodes fails\n***************************************************************************************/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[])": "/**\n* Processes token data for add/update; returns identifier or null if read fails.\n* @param data byte array containing token information\n* @return TokenIdent object or null if reading fails\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData)": "/**\n* Processes removed token from ChildData and updates currentTokens.\n* @param data ChildData containing token information\n* @throws IOException if reading token fails\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)": "/**\n* Retrieves DelegationTokenInformation from Zookeeper node.\n* @param nodePath path to the ZK node\n* @param quiet suppresses error logging if true\n* @return DelegationTokenInformation or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int)": "/**\n* Retrieves a DelegationKey by keyId, caching it if fetched from Zookeeper.\n* @param keyId identifier for the DelegationKey\n* @return DelegationKey object or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)": "/**\n* Retrieves DelegationTokenInformation using a token identifier.\n* @param ident token identifier for fetching info\n* @param quiet suppresses error logging if true\n* @return DelegationTokenInformation or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves DelegationTokenInformation using a token identifier.\n* @param ident token identifier for fetching info\n* @return DelegationTokenInformation or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)": "/**\n* Removes a stored token if it hasn't been renewed; checks ZK if specified.\n* @param ident token identifier for the stored token\n* @param checkAgainstZkBeforeDeletion flag to verify renewal status before deletion\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves DelegationTokenInformation for a given token identifier.\n* @param ident token identifier for fetching info\n* @return DelegationTokenInformation or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Syncs local cache with ZK based on token identifier.\n* @param ident token identifier for fetching DelegationTokenInformation\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Removes a stored token using its identifier.\n* @param ident token identifier for the stored token\n* @throws IOException if an I/O error occurs during removal\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Cancels a token and syncs local cache with ZK.\n* @param token the token to cancel\n* @param canceller the user requesting cancellation\n* @return TokenIdent of the canceled token\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Creates a CuratorFramework client for Zookeeper with provided configuration and namespace.\n* @param conf configuration settings for Zookeeper connection\n* @param namespace the namespace for the Zookeeper client\n* @return CuratorFramework instance\n*/",
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ZKDelegationTokenSecretManager with configuration parameters.\n* @param conf configuration settings for token management\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:<init>(org.apache.hadoop.security.authentication.server.AuthenticationHandler)": "/**\n* Initializes the DelegationTokenAuthenticationHandler with a specified AuthenticationHandler.\n* @param handler the AuthenticationHandler to be used for authentication\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initJsonFactory(java.util.Properties)": "/**\n* Initializes JsonFactory based on configuration properties.\n* @param config properties containing JSON mapper features\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getType()": "/**\n* Returns the authentication type.\n* @return String representing the authentication type\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getAuthHandler()": "/**\n* Retrieves the current authentication handler.\n* @return AuthenticationHandler instance\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getTokenManager()": "/**\n* Retrieves the DelegationTokenManager instance.\n* @return DelegationTokenManager object\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest)": "/**\n* Checks if the request is a management operation.\n* @param request the HttpServletRequest object\n* @return true if it's a management operation, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves the delegation token from the request headers or parameters.\n* @param request the HttpServletRequest object\n* @return delegation token string or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)": "/**\n* Sets the external delegation token secret manager.\n* @param secretManager new secret manager instance\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy()": "/**\n* Stops token and auth handlers, cleaning up resources.\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token)": "/**\n* Converts a Token to a JSON representation.\n* @param token the Token to encode\n* @return a Map containing the encoded Token in JSON format\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Authenticates user using delegation token from request.\n* @param request HTTP request containing the token\n* @param response HTTP response for error handling\n* @return AuthenticationToken or null if authentication fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties)": "/**\n* Initializes the token manager with configuration properties.\n* @param config properties for token management settings\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties)": "/**\n* Initializes the servlet with configuration properties.\n* @param config properties for servlet initialization\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles management operations for delegation tokens.\n* @param token authentication token, can be null\n* @param request HTTP request object\n* @param response HTTP response object\n* @return true if request continues, false otherwise\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler": {
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:getTokenTypes()": "/**\n* Retrieves token types from the authentication handler.\n* @return Collection of token type strings\n*/",
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>()": "/**\n* Initializes MultiSchemeDelegationTokenAuthenticationHandler with a MultiSchemeAuthenticationHandler.\n*/",
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Authenticates user via delegation token in request.\n* @param request HTTP request containing the token\n* @param response HTTP response for error handling\n* @return AuthenticationToken or null if authentication fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties)": "/**\n* Initializes servlet with authentication schemes from config.\n* @param config properties for servlet initialization\n* @throws ServletException if initialization fails\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:setConnectionConfigurator(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)": "/**\n* Sets the connection configurator for the authenticator.\n* @param configurator the ConnectionConfigurator to be set\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:<init>(org.apache.hadoop.security.authentication.client.Authenticator)": "/**\n* Initializes DelegationTokenAuthenticator with the given Authenticator.\n* @param authenticator instance used for authentication\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)": "/**\n* Checks for the presence of a delegation token in the given URL or token.\n* @param url the URL to check for a delegation token\n* @param token the authenticated URL token\n* @return true if a delegation token exists, false otherwise\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)": "/**\n* Performs delegation token operations via HTTP.\n* @param url target URL for the operation\n* @param token authentication token\n* @param operation type of delegation operation\n* @param renewer user renewing the token\n* @param dToken delegation token to use\n* @param hasResponse indicates if a response is expected\n* @param doAsUser user to act as during operation\n* @return response map or null if no response\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)": "/**\n* Retrieves a delegation token from a specified URL.\n* @param url the target URL for token retrieval\n* @param token authentication token\n* @param renewer user renewing the token\n* @param doAsUser user to act as during operation\n* @return the retrieved delegation token\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Renews a delegation token via HTTP.\n* @param url target URL for the operation\n* @param token authentication token\n* @param dToken delegation token to renew\n* @param doAsUser user to act as during operation\n* @return renewed token expiration time\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Cancels a delegation token via HTTP.\n* @param url target URL for the operation\n* @param token authentication token\n* @param dToken delegation token to cancel\n* @param doAsUser user to act as during operation\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)": "/**\n* Retrieves a delegation token using the specified URL and renewer.\n* @param url the target URL for token retrieval\n* @param token authentication token\n* @param renewer user renewing the token\n* @return delegation token\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)": "/**\n* Renews a delegation token via HTTP.\n* @param url target URL for the operation\n* @param token authentication token\n* @param dToken delegation token to renew\n* @return renewed token expiration time\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)": "/**\n* Cancels a delegation token using the specified URL and authentication token.\n* @param url target URL for the operation\n* @param token authentication token\n* @param dToken delegation token to cancel\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)": "/**\n* Authenticates using a delegation token or renews TGT if absent.\n* @param url the URL to authenticate\n* @param token the authenticated URL token\n* @throws IOException if an I/O error occurs\n* @throws AuthenticationException if authentication fails\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:augmentURL(java.net.URL,java.util.Map)": "/**\n* Augments a URL with query parameters.\n* @param url base URL to augment\n* @param params map of query parameters\n* @return augmented URL with parameters added\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:useQueryStringForDelegationToken()": "/**\n* Returns the flag indicating if query string is used for delegation token.\n* @return true if query string is used, otherwise false\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)": "/**\n* Retrieves a delegation token for the specified URL and token.\n* @param url the resource URL\n* @param token the original token\n* @param renewer the user requesting renewal\n* @param doAsUser the user to act as\n* @return the delegation token or null on failure\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)": "/**\n* Renews a delegation token via the specified URL.\n* @param url the service URL for token renewal\n* @param token the token to renew\n* @param doAsUser user to act as during renewal\n* @return new expiration time in milliseconds\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)": "/**\n* Cancels a delegation token for a given URL and user.\n* @param url the URL for the token cancellation\n* @param token the token to be cancelled\n* @param doAsUser user to act on behalf of\n* @throws IOException if cancellation fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)": "/**\n* Obtains a DelegationTokenAuthenticator, creating one if null.\n* @param dta existing authenticator or null to create a new one\n* @param connConfigurator connection configurator for the authenticator\n* @return DelegationTokenAuthenticator instance\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)": "/**\n* Retrieves a delegation token for the specified URL and token.\n* @param url the resource URL\n* @param token the original token\n* @param renewer the user requesting renewal\n* @return delegation token or null on failure\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)": "/**\n* Renews a delegation token using the specified URL.\n* @param url the service URL for token renewal\n* @param token the token to renew\n* @return new expiration time in milliseconds\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)": "/**\n* Cancels a delegation token for a specified URL.\n* @param url the URL for token cancellation\n* @param token the token to be cancelled\n* @throws IOException if cancellation fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)": "/**\n* Constructs DelegationTokenAuthenticatedURL using provided authenticator and configurator.\n* @param authenticator existing or new DelegationTokenAuthenticator\n* @param connConfigurator connection configurator for the authenticator\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>()": "/**\n* Initializes DelegationTokenAuthenticatedURL with default null parameters.\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator)": "/**\n* Constructs DelegationTokenAuthenticatedURL with the specified authenticator.\n* @param authenticator DelegationTokenAuthenticator for authentication\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)": "/**\n* Constructs DelegationTokenAuthenticatedURL with a connection configurator.\n* @param connConfigurator connection configurator for the authenticator\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)": "/**\n* Selects a delegation token for the given URL and credentials.\n* @param url the service URL\n* @param creds user credentials containing tokens\n* @return the delegation Token or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)": "/**\n* Opens an HTTP connection with optional delegation token and proxy user parameters.\n* @param url the URL to connect to\n* @param token the authentication token\n* @param doAs the user to impersonate, or null\n* @return HttpURLConnection object for the specified URL\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)": "/**\n* Opens an HTTP connection with optional authentication token.\n* @param url the URL to connect to\n* @param token the authentication token\n* @return HttpURLConnection object\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)": "/**\n* Opens an HTTP connection with optional authentication token.\n* @param url the URL to connect to\n* @param token the authentication token\n* @return HttpURLConnection object\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator$1": {
        "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator$1:<init>()": "/**\n* Constructs a KerberosDelegationTokenAuthenticator with a fallback to PseudoDelegationTokenAuthenticator.\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator$1": {
        "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator$1:<init>()": "/**\n* Constructs a PseudoDelegationTokenAuthenticator with a custom user name retrieval.\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)": "/**\n* Decodes a delegation token identifier from a given token.\n* @param token the token to decode\n* @return DelegationTokenIdentifier object\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier()": "/**\n* Creates a DelegationTokenIdentifier with the specified token kind.\n* @return DelegationTokenIdentifier instance\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)": "/**\n* Initializes the DelegationTokenSecretManager with configuration parameters.\n* @param conf configuration settings\n* @param tokenKind type of token being managed\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.ServletUtils": {
        "org.apache.hadoop.security.token.delegation.web.ServletUtils:getParameter(javax.servlet.http.HttpServletRequest,java.lang.String)": "/**\n* Retrieves the value of a specified query parameter from the request.\n* @param request the HttpServletRequest object\n* @param name the parameter name to look for\n* @return the parameter value or null if not found\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:getHttpMethod()": "/**\n* Retrieves the HTTP method used.\n* @return String representing the HTTP method\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:requiresKerberosCredentials()": "/**\n* Checks if Kerberos credentials are required.\n* @return true if credentials are needed, false otherwise\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token:getDelegationToken()": "/**\n* Retrieves the delegation token.\n* @return Token containing the delegation information\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token:setDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Sets the delegation token for authentication.\n* @param delegationToken the token to be set\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)": "/**\n* Decodes a delegation token identifier from the given token.\n* @param token the token to decode\n* @return DelegationTokenIdentifier object\n* @throws IOException if decoding fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier()": "/**\n* Creates a DelegationTokenIdentifier with a specified token kind.\n* @return DelegationTokenIdentifier instance\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)": "/**\n* Constructs ZKSecretManager with configuration and token kind.\n* @param conf configuration settings for token management\n* @param tokenKind type of token being managed\n*/"
    },
    "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager": {
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationTokenSeqNum()": "/**\n* Retrieves the delegation token sequence number.\n* @return sequence number or throws RuntimeException on SQL error\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:setDelegationTokenSeqNum(int)": "/**\n* Sets the delegation token sequence number.\n* @param seqNum the new sequence number to set\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:incrementDelegationTokenSeqNum()": "/**\n* Increments and returns the delegation token sequence number.\n* @return updated sequence number after increment\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCurrentKeyId()": "/**\n* Retrieves the current key ID from the database.\n* @return current key ID as an integer\n* @throws RuntimeException if SQL query fails\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:setCurrentKeyId(int)": "/**\n* Sets the current key ID and handles potential SQL exceptions.\n* @param keyId the new key identifier to set\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:incrementCurrentKeyId()": "/**\n* Increments the current key ID by 1.\n* @return updated key ID after incrementing\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Removes a stored token using its identifier.\n* @param ident the TokenIdent object containing token info\n* @throws IOException if an I/O error occurs during removal\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Removes the stored master key using its key identifier.\n* @param key DelegationKey object to be removed\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Stores a DelegationKey in SQL and local cache.\n* @param key the DelegationKey to be stored\n* @throws IOException if storage fails\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)": "/**\n* Updates the delegation key in storage and local cache.\n* @param key the DelegationKey to update\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[])": "/**\n* Creates DelegationTokenInformation from byte array.\n* @param tokenInfoBytes byte array containing token info\n* @return DelegationTokenInformation object\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int)": "/**\n* Retrieves a DelegationKey by its ID from cache or SQL database.\n* @param keyId unique identifier for the DelegationKey\n* @return DelegationKey object or null if not found\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[])": "/**\n* Creates TokenIdent from byte array.\n* @param tokenIdentBytes byte array to read identifier from\n* @return TokenIdent object\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Retrieves DelegationTokenInformation from SQL using TokenIdent.\n* @param ident identifier for the token\n* @return DelegationTokenInformation object\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup()": "/**\n* Retrieves candidate tokens for cleanup from the database.\n* @return map of TokenIdent to DelegationTokenInformation\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)": "/**\n* Removes expired stored token if not renewed by another router.\n* @param ident the TokenIdent object for the token to remove\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Cancels a token and returns its identifier.\n* @param token the token to cancel\n* @param canceller the user requesting cancellation\n* @return TokenIdent of the canceled token\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)": "/**** Initializes SQLDelegationTokenSecretManager with configuration parameters. */"
    },
    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector": {
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:<init>(org.apache.hadoop.io.Text)": "/**\n* Constructs a DelegationTokenSelector with a specified kind name.\n* @param kindName the type of delegation token\n*/",
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)": "/**\n* Selects a token matching the service and kind.\n* @param service the service to match\n* @param tokens collection of tokens to search\n* @return matching Token or null if not found\n*/"
    },
    "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory": {
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)": "/**\n* Constructs a HadoopZookeeperFactory with security configurations.\n* @param zkPrincipal Zookeeper principal name\n* @param kerberosPrincipal Kerberos principal name\n* @param kerberosKeytab Kerberos keytab file path\n* @param sslEnabled flag for SSL usage\n* @param truststoreKeystore Truststore and keystore configuration\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:isJaasConfigurationSet(org.apache.zookeeper.client.ZKClientConfig)": "/**\n* Checks if JAAS configuration is set for the given ZK client configuration.\n* @param zkClientConfig configuration object for ZK client\n* @return true if configuration is set; false otherwise\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Constructs a HadoopZookeeperFactory with default SSL configuration.\n* @param zkPrincipal Zookeeper principal name\n* @param kerberosPrincipal Kerberos principal name\n* @param kerberosKeytab Kerberos keytab file path\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String)": "/**\n* Constructs a HadoopZookeeperFactory with specified Zookeeper principal.\n* @param zkPrincipal Zookeeper principal name\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig)": "/**\n* Configures JAAS with Kerberos principal and keytab.\n* @param zkClientConfig ZK client configuration to update\n* @throws IOException if configuration fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)": "/**\n* Creates a new ZooKeeper instance with optional SSL and JAAS configuration.\n* @param connectString connection string for ZooKeeper\n* @param sessionTimeout session timeout in milliseconds\n* @param watcher watcher for ZooKeeper events\n* @param canBeReadOnly flag for read-only mode\n* @param zkClientConfig configuration for ZooKeeper client\n* @return new ZooKeeper instance\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)": "/**\n* Creates a new ZooKeeper instance with a default client configuration.\n* @param connectString connection string for ZooKeeper\n* @param sessionTimeout session timeout in milliseconds\n* @param watcher watcher for ZooKeeper events\n* @param canBeReadOnly flag for read-only mode\n* @return new ZooKeeper instance\n*/"
    },
    "org.apache.hadoop.security.token.DelegationTokenIssuer": {
        "org.apache.hadoop.security.token.DelegationTokenIssuer:getAdditionalTokenIssuers()": "/**\n* Retrieves additional token issuers.\n* @return array of DelegationTokenIssuer, or null if none available\n*/",
        "org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)": "/**\n* Collects delegation tokens from issuer and its children.\n* @param issuer the token issuer\n* @param renewer the renewer identifier\n* @param credentials the credentials to update\n* @param tokens list to collect tokens\n*/",
        "org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)": "/**\n* Adds delegation tokens to credentials.\n* @param renewer identifier for the token renewer\n* @param credentials credentials to update or create new\n* @return array of collected Token objects\n*/"
    },
    "org.apache.hadoop.security.token.DtFileOperations": {
        "org.apache.hadoop.security.token.DtFileOperations:<init>()": "/**\n* Private constructor to prevent instantiation of the DtFileOperations class.\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:stripPrefix(java.lang.String)": "/**\n* Removes HTTP/HTTPS prefix from the given URL.\n* @param u the URL string to process\n* @return URL without the HTTP/HTTPS prefix\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:formatDate(long)": "/**\n* Formats a timestamp into a short date-time string.\n* @param date timestamp in milliseconds\n* @return formatted date-time string\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File)": "/**\n* Converts a File object to a Path representation.\n* @param f the File to convert\n* @return a Path object representing the file's URI\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)": "/**\n* Checks if the alias matches the service of the given token.\n* @param token the Token object to check\n* @param alias the Text alias to compare, may be null\n* @return true if alias is null or matches the token's service\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)": "/**\n* Checks if the service matches the fetched service name or is null with a valid URL.\n* @param fetcher retrieves service name\n* @param service service to compare\n* @param url URL to validate against service name\n* @return true if matched, false otherwise\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)": "/**\n* Prints token credentials matching the given alias to the specified output stream.\n* @param creds the Credentials object containing tokens\n* @param alias the Text alias to match against tokens\n* @param out the PrintStream to output the formatted credentials\n* @throws IOException if an I/O error occurs during output\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)": "/**\n* Prints token file details and matching credentials.\n* @param tokenFile the file containing token data\n* @param alias the Text alias to match credentials\n* @param conf configuration settings\n* @param out the PrintStream for output\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)": "/**\n* Writes credentials to a file in specified format.\n* @param f file to write credentials to\n* @param format desired serialization format\n* @param creds credentials to be written\n* @param conf configuration for the filesystem\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves and writes delegation tokens to a specified file.\n* @param tokenFile file to store tokens, @param fileFormat format for serialization,\n* @param alias token alias, @param service service to fetch tokens for,\n* @param url service URL, @param renewer renewer identifier, @param conf configuration settings\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)": "/**\n* Updates tokens in a file, aliasing specified service with a new alias.\n* @param tokenFile file containing tokens\n* @param fileFormat format for writing tokens\n* @param alias new identifier for the service\n* @param service original service to alias\n* @param conf configuration settings\n* @throws Exception if an error occurs during processing\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Appends tokens from files to new credentials and writes to the last file.\n* @param tokenFiles list of token files to read from\n* @param fileFormat desired format for output credentials\n* @param conf configuration settings\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)": "/**\n* Removes a token from file; cancels it if specified.\n* @param cancel indicates whether to cancel the token\n* @param tokenFile file containing tokens\n* @param fileFormat format for token serialization\n* @param alias token identifier to remove\n* @param conf configuration settings\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if interrupted\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)": "/**\n* Renews tokens from a file and writes updated credentials back.\n* @param tokenFile file containing tokens to renew\n* @param fileFormat desired format for writing credentials\n* @param alias optional alias to match tokens\n* @param conf configuration settings for token renewal\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the renewal is interrupted\n*/",
        "org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Imports a token from a file and writes updated credentials.\n* @param tokenFile file containing the token\n* @param fileFormat format for serialization\n* @param alias identifier for the token service\n* @param base64 Base64 string representing the token\n* @param conf configuration settings\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Get": {
        "org.apache.hadoop.security.token.DtUtilShell$Get:isGenericUrl()": "/**\n* Checks if the URL uses HTTP or HTTPS protocol.\n* @return true if URL starts with HTTP/HTTPS, false otherwise\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Get:getUsage()": "/**\n* Returns the usage string for the current context.\n* @return usage string constant\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Get:validate()": "/**\n* Validates service URL format and presence.\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Get:execute()": "/**\n* Executes the token retrieval process and writes to the specified file.\n* @throws Exception if an error occurs during token file operations\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Print": {
        "org.apache.hadoop.security.token.DtUtilShell$Print:getUsage()": "/**\n* Returns the usage information for the command.\n* @return a string containing usage details\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Print:execute()": "/**\n* Executes file processing for token files.\n* @throws Exception if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Append": {
        "org.apache.hadoop.security.token.DtUtilShell$Append:getUsage()": "/**\n* Returns the usage information for the application.\n* @return a string containing the usage details\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Append:execute()": "/**\n* Executes token file operations by appending tokens to credentials.\n* @throws Exception if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell$Import": {
        "org.apache.hadoop.security.token.DtUtilShell$Import:getUsage()": "/**\n* Returns the usage information for the import feature.\n* @return String containing the usage details\n*/",
        "org.apache.hadoop.security.token.DtUtilShell$Import:execute()": "/**\n* Executes the import of a token file with specified parameters.\n* @throws Exception if an error occurs during the import process\n*/"
    },
    "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream": {
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:readNextRpcPacket()": "/**\n* Reads and decodes the next RPC packet; throws IOException on failure.\n*/",
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)": "/**\n* Reads bytes into a buffer from an RPC message.\n* @param buf byte array for storing read data\n* @param off offset to start writing in buf\n* @param len maximum number of bytes to read\n* @return number of bytes actually read\n*/",
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read()": "/**\n* Reads a single byte from the input stream.\n* @return byte value read, or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[])": "/**\n* Reads bytes into the buffer from an RPC message.\n* @param b byte array for storing read data\n* @return number of bytes actually read\n*/"
    },
    "org.apache.hadoop.security.SaslRpcClient": {
        "org.apache.hadoop.security.SaslRpcClient:getNegotiatedProperty(java.lang.String)": "/**\n* Retrieves negotiated property from SASL client by key.\n* @param key property identifier\n* @return property value or null if SASL client is not initialized\n*/",
        "org.apache.hadoop.security.SaslRpcClient:createSaslReply(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])": "/**\n* Creates an RpcSaslProto reply with the given state and response token.\n* @param state the current SASL state\n* @param responseToken the token for the SASL response, may be null\n* @return a builder for RpcSaslProto\n*/",
        "org.apache.hadoop.security.SaslRpcClient:saslEvaluateToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto,boolean)": "/**\n* Evaluates SASL response token and checks server completion status.\n* @param saslResponse server response containing SASL token\n* @param serverIsDone indicates if the server has finished the handshake\n* @return evaluated SASL token or null if no token is present\n*/",
        "org.apache.hadoop.security.SaslRpcClient:useWrap()": "/**\n* Determines if SASL wrapping is applicable based on QOP.\n* @return true if QOP is set and not 'auth', false otherwise\n*/",
        "org.apache.hadoop.security.SaslRpcClient:dispose()": "/**\n* Disposes of the SASL client resources.\n* @throws SaslException if an error occurs during disposal\n*/",
        "org.apache.hadoop.security.SaslRpcClient:getAuthMethod()": "/**\n* Retrieves the current authentication method.\n* @return AuthMethod instance representing the authentication method\n*/",
        "org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)": "/**\n* Validates authentication type against known methods.\n* @param authType the authentication type to validate\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream)": "/**\n* Returns an InputStream, optionally wrapped if SASL wrapping is applicable.\n* @param in the original InputStream\n* @return wrapped InputStream or original if not applicable\n*/",
        "org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream)": "/**\n* Returns an OutputStream, possibly wrapped based on SASL negotiation.\n* @param out the original OutputStream\n* @return a wrapped OutputStream if applicable\n*/",
        "org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)": "/**\n* Retrieves a server token based on authentication type.\n* @param authType the authentication mechanism\n* @return Token object or null if no token is supported\n*/",
        "org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)": "/**\n* Sends a SASL message to the output stream.\n* @param out the output stream to send the message to\n* @param message the SASL message to be sent\n* @throws IOException if an I/O error occurs during sending\n*/",
        "org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)": "/**\n* Retrieves the server principal for the given authentication type.\n* @param authType the authentication type to derive the principal\n* @return server principal string or throws IllegalArgumentException if invalid\n*/",
        "org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)": "/**\n* Creates a SaslClient for the specified authentication type.\n* @param authType the SASL authentication type\n* @return SaslClient instance or null if creation fails\n*/",
        "org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a SaslRpcClient with user info, protocol, server address, and configuration.\n* @param ugi user group information\n* @param protocol protocol class for RPC\n* @param serverAddr address of the server\n* @param conf configuration for SASL properties\n*/",
        "org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List)": "/**\n* Selects a valid SaslAuth from a list for client authentication.\n* @param authTypes list of available SaslAuth types\n* @return selected SaslAuth or null if none is valid\n*/",
        "org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams)": "/**\n* Handles SASL connection negotiation and returns the authentication method used.\n* @param ipcStreams streams for IPC communication\n* @return AuthMethod representing the negotiated authentication method\n*/"
    },
    "org.apache.hadoop.security.SaslRpcServer$AuthMethod": {
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:getMechanismName()": "/**\n* Retrieves the name of the mechanism.\n* @return the name of the mechanism as a String\n*/",
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:valueOf(byte)": "/**\n* Retrieves AuthMethod by byte code.\n* @param code byte representing the AuthMethod\n* @return corresponding AuthMethod or null if invalid\n*/",
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:write(java.io.DataOutput)": "/**\n* Writes the code to the given DataOutput stream.\n* @param out output stream to write data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput)": "/**\n* Reads AuthMethod from DataInput stream.\n* @param in input stream to read byte code from\n* @return corresponding AuthMethod or null if invalid\n*/"
    },
    "org.apache.hadoop.security.SecurityUtil": {
        "org.apache.hadoop.security.SecurityUtil:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves TokenInfo for a given protocol and configuration.\n* @param protocol the protocol class\n* @param conf the configuration settings\n* @return TokenInfo object or null if not found\n*/",
        "org.apache.hadoop.security.SecurityUtil:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves KerberosInfo for a given protocol and configuration.\n* @param protocol the protocol class\n* @param conf the configuration settings\n* @return KerberosInfo object or null if not found\n*/",
        "org.apache.hadoop.security.SecurityUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the SecurityUtil class.\n*/",
        "org.apache.hadoop.security.SecurityUtil:setTokenServiceUseIp(boolean)": "/**\n* Sets the flag for using IP in token service configuration.\n* @param flag true to use IP, false for hostname resolution\n*/",
        "org.apache.hadoop.security.SecurityUtil:isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal)": "/**\n* Checks if the given principal is a TGS principal.\n* @param principal KerberosPrincipal to evaluate\n* @return true if it is a TGS principal, false otherwise\n*/",
        "org.apache.hadoop.security.SecurityUtil:getComponents(java.lang.String)": "/**\n* Splits the principalConfig string into components by '/' or '@'.\n* @param principalConfig input string to split\n* @return array of components or null if input is null\n*/",
        "org.apache.hadoop.security.SecurityUtil:validateSslConfiguration(org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)": "/**\n* Validates SSL configuration parameters in TruststoreKeystore.\n* @param truststoreKeystore contains keystore and truststore details\n* @throws ConfigurationException if any parameter is empty\n*/",
        "org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)": "/**\n* Executes a privileged action as a specified user.\n* @param ugi user information for privilege context\n* @param action action to execute with privileges\n* @return result of the privileged action\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String)": "/**\n* Extracts the host from a Kerberos principal name.\n* @param principalName the Kerberos principal name\n* @return the hostname extracted from the principal\n*/",
        "org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket)": "/**\n* Checks if the given Kerberos ticket is an original TGT.\n* @param ticket KerberosTicket to evaluate\n* @return true if it's an original TGT, false otherwise\n*/",
        "org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)": "/**\n* Sets SSL configuration for ZooKeeper client.\n* @param zkClientConfig configuration object for ZooKeeper client\n* @param truststoreKeystore contains keystore and truststore details\n* @param x509Util utility for X.509 certificate management\n* @throws ConfigurationException if SSL parameters are invalid\n*/",
        "org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)": "/**\n* Configures SSL for ZooKeeper client.\n* @param zkClientConfig ZooKeeper client configuration\n* @param truststoreKeystore contains keystore and truststore details\n* @throws ConfigurationException if SSL parameters are invalid\n*/",
        "org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress)": "/**\n* Builds a Text token service from an InetSocketAddress.\n* @param addr address containing host and port\n* @return Text object formatted as \"host:port\"\n*/",
        "org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)": "/**\n* Configures a token service using the provided address.\n* @param token the token to be configured, may be null\n* @param addr the address for building the service\n*/",
        "org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String)": "/**** Retrieves InetAddress for a given hostname and logs slow lookups. \n* @param hostname the name to resolve\n* @return InetAddress corresponding to the hostname\n*/",
        "org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)": "/**\n* Builds a service name from URI and default port.\n* @param uri the URI to extract authority from\n* @param defPort the default port if not specified\n* @return formatted service name or null if authority is absent\n*/",
        "org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token)": "/**\n* Retrieves InetSocketAddress for the service associated with the given token.\n* @param token the Token object containing service information\n* @return InetSocketAddress for the service\n*/",
        "org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI)": "/**\n* Builds a Text token service from a URI.\n* @param uri the URI containing host and port information\n* @return Text object formatted as \"host:port\"\n*/",
        "org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)": "/**\n* Sets the authentication method in the configuration.\n* @param authenticationMethod method to set, defaults to SIMPLE if null\n* @param conf configuration object to update\n*/",
        "org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the local hostname using configuration settings.\n* @param conf optional configuration object\n* @return local hostname as a String\n*/",
        "org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the client principal for a given protocol and configuration.\n* @param protocol the protocol class\n* @param conf the configuration settings\n* @return client principal or null if not found\n*/",
        "org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the authentication method from configuration.\n* @param conf configuration object\n* @return AuthenticationMethod enum value\n*/",
        "org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)": "/**\n* Constructs a formatted string using hostname and components.\n* @param components array of string components\n* @param hostname optional hostname to format\n* @return formatted string combining components and hostname\n*/",
        "org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)": "/**\n* Retrieves formatted server principal from config and hostname.\n* @param principalConfig input string to parse\n* @param hostname optional hostname for formatting\n* @return formatted server principal or original config if invalid\n*/",
        "org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)": "/**\n* Returns formatted server principal from config using the provided address.\n* @param principalConfig input string to process\n* @param addr client InetAddress for hostname resolution\n* @return formatted server principal or original config if invalid\n*/",
        "org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration)": "/**\n* Configures security settings from the provided Configuration object.\n* @param conf configuration settings to apply\n*/",
        "org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves ZKAuthInfo list from configuration using the specified key.\n* @param conf configuration object\n* @param configKey key to fetch password\n* @return List of ZKAuthInfo or empty if not found\n*/",
        "org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration)": "/**\n* Updates the application configuration.\n* @param conf configuration settings to apply\n*/",
        "org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction)": "/**\n* Executes action as the logged-in user if security is enabled.\n* @param action the privileged action to execute\n* @return result of the action execution\n*/",
        "org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction)": "/**\n* Executes a privileged action as the logged-in user.\n* @param action action to execute with privileges\n* @return result of the privileged action\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction)": "/**\n* Executes a privileged action as the current user.\n* @param action action to execute with privileges\n* @return result of the privileged action\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Logs in a user using keytab and configuration settings.\n* @param conf configuration containing keytab and username keys\n* @param keytabFileKey key for keytab file in configuration\n* @param userNameKey key for username in configuration\n* @param hostname optional hostname for principal formatting\n* @throws IOException if keytab is missing or login fails\n*/",
        "org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Authenticates a user using keytab and configuration settings.\n* @param conf configuration with keytab and username keys\n* @param keytabFileKey key for keytab file in configuration\n* @param userNameKey key for username in configuration\n*/"
    },
    "org.apache.hadoop.ipc.RpcWritable$Buffer": {
        "org.apache.hadoop.ipc.RpcWritable$Buffer:remaining()": "/**\n* Returns the number of remaining elements in the buffer.\n* @return count of remaining elements\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:<init>(java.nio.ByteBuffer)": "/**\n* Initializes Buffer with a ByteBuffer instance.\n* @param bb ByteBuffer to be associated with this Buffer\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:<init>()": "/**\n* Constructs a new Buffer instance.\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:readFrom(java.nio.ByteBuffer)": "/**\n* Reads and slices a ByteBuffer, returning the current instance.\n* @param bb ByteBuffer to read from\n* @return current instance cast to type T\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:getByteBuffer()": "/**\n* Returns the ByteBuffer instance.\n* @return ByteBuffer object\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer)": "/**\n* Wraps a ByteBuffer in a Buffer instance.\n* @param bb ByteBuffer to be wrapped\n* @return Buffer object associated with the ByteBuffer\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer)": "/**\n* Writes byte buffer contents to the response buffer.\n* @param out ResponseBuffer to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object)": "/**\n* Retrieves a value from a ByteBuffer after wrapping it.\n* @param value the value to be wrapped and read\n* @return the read value of type T\n*/",
        "org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/****\n* Creates a new instance of the specified class and configures it.\n* @param valueClass the class type to instantiate\n* @param conf configuration object for setup\n* @return the configured instance of type T\n*/"
    },
    "org.apache.hadoop.security.FastSaslClientFactory": {
        "org.apache.hadoop.security.FastSaslClientFactory:<init>(java.util.Map)": "/**\n* Initializes FastSaslClientFactory with available SASL client factories.\n* @param props configuration properties for mechanism retrieval\n*/",
        "org.apache.hadoop.security.FastSaslClientFactory:getMechanismNames(java.util.Map)": "/**\n* Retrieves an array of mechanism names from the factory cache.\n* @param props properties map (unused)\n* @return array of mechanism names\n*/",
        "org.apache.hadoop.security.FastSaslClientFactory:createSaslClient(java.lang.String[],java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)": "/**\n* Creates a SaslClient using specified mechanisms and parameters.\n* @param mechanisms supported SASL mechanisms\n* @param authorizationId optional authorization ID\n* @param protocol protocol name\n* @param serverName server name\n* @param props additional properties\n* @param cbh callback handler for authentication\n* @return SaslClient instance or null if not created\n*/"
    },
    "org.apache.hadoop.security.IngressPortBasedResolver": {
        "org.apache.hadoop.security.IngressPortBasedResolver:getServerProperties(java.net.InetAddress,int)": "/**\n* Retrieves server properties based on client address and port.\n* @param clientAddress client's InetAddress\n* @param ingressPort port number for property mapping\n* @return Map of server properties or default if port is unconfigured\n*/",
        "org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Configures settings based on the provided Configuration object.\n* @param conf Configuration to retrieve port settings\n*/"
    },
    "org.apache.hadoop.security.ProviderUtils": {
        "org.apache.hadoop.security.ProviderUtils:<init>()": "/**\n* Private constructor to prevent instantiation for utility class compliance.\n*/",
        "org.apache.hadoop.security.ProviderUtils:nestURIForLocalJavaKeyStoreProvider(java.net.URI)": "/**\n* Creates a nested URI for a local Java KeyStore provider.\n* @param localFile URI of the local file\n* @return Nested URI for the KeyStore provider\n* @throws URISyntaxException if the URI is malformed\n*/",
        "org.apache.hadoop.security.ProviderUtils:locatePassword(java.lang.String,java.lang.String)": "/**\n* Retrieves password from environment variable or file.\n* @param envWithPass environment variable name\n* @param fileWithPass file path for password if env not set\n* @return char array of password or null if not found\n*/",
        "org.apache.hadoop.security.ProviderUtils:noPasswordInstruction(java.lang.String,java.lang.String)": "/**\n* Generates instructions for no-password setup using environment and file keys.\n* @param envKey key for the environment variable\n* @param fileKey key for the configuration file entry\n* @return formatted instruction string\n*/",
        "org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)": "/**\n* Constructs a no-password warning message.\n* @param envKey key for the environment variable\n* @param fileKey key for the configuration file entry\n* @return formatted warning string\n*/",
        "org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)": "/**\n* Constructs a no-password error message.\n* @param envKey key for the environment variable\n* @param fileKey key for the configuration file entry\n* @return formatted no-password error message\n*/",
        "org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI)": "/**\n* Converts a nested URI to a Path object.\n* @param nestedUri the URI to unnest\n* @return Path object representing the un-nested URI\n*/",
        "org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)": "/**\n* Excludes incompatible credential providers from the configuration.\n* @param config the original Configuration object\n* @param fileSystemClass class of the FileSystem to exclude\n* @return updated Configuration with filtered providers\n*/"
    },
    "org.apache.hadoop.security.SaslInputStream": {
        "org.apache.hadoop.security.SaslInputStream:unsignedBytesToInt(byte[])": "/**\n* Converts a 4-byte array to an unsigned integer.\n* @param buf byte array of length 4\n* @return resulting unsigned integer value\n*/",
        "org.apache.hadoop.security.SaslInputStream:disposeSasl()": "/**\n* Disposes of the SASL client and server resources if they are not null.\n* @throws SaslException if an error occurs during disposal\n*/",
        "org.apache.hadoop.security.SaslInputStream:<init>(java.io.InputStream,javax.security.sasl.SaslServer)": "/**\n* Initializes SaslInputStream with an input stream and a SaslServer instance.\n* @param inStream the input stream to read data from\n* @param saslServer the SaslServer for authentication negotiation\n*/",
        "org.apache.hadoop.security.SaslInputStream:<init>(java.io.InputStream,javax.security.sasl.SaslClient)": "/**\n* Initializes SaslInputStream with input stream and SaslClient.\n* @param inStream input stream for data\n* @param saslClient client for SASL authentication\n*/",
        "org.apache.hadoop.security.SaslInputStream:skip(long)": "/**\n* Skips over n bytes in the input stream.\n* @param n number of bytes to skip\n* @return actual number of bytes skipped\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslInputStream:available()": "/**\n* Returns the number of bytes available to read.\n* @return number of available bytes from the input stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslInputStream:readMoreData()": "/**\n* Reads data from the input stream and unwraps it using SASL.\n* @return length of unwrapped data or -1 if end of stream reached\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslInputStream:close()": "/**\n* Closes the stream and disposes of SASL resources.\n* @throws IOException if an error occurs during closing\n*/",
        "org.apache.hadoop.security.SaslInputStream:read()": "/**\n* Reads a byte from the input stream, handling wrapping if enabled.\n* @return byte value or -1 if end of stream reached\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)": "/**\n* Reads bytes into an array, handling wrapping if enabled.\n* @param b byte array to fill, @param off offset in array, @param len number of bytes to read\n* @return number of bytes read or -1 if end of stream reached\n*/",
        "org.apache.hadoop.security.SaslInputStream:read(byte[])": "/**\n* Reads bytes into the provided array.\n* @param b byte array to fill\n* @return number of bytes read or -1 if end of stream reached\n*/",
        "org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer)": "/**\n* Reads bytes into ByteBuffer, updating its position.\n* @param dst destination ByteBuffer\n* @return number of bytes read or -1 if end of stream reached\n*/"
    },
    "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler": {
        "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[])": "/**\n* Processes SASL callbacks to set username, password, and realm.\n* @param callbacks array of Callback objects to handle\n* @throws UnsupportedCallbackException if a callback type is unrecognized\n*/",
        "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token)": "/**** Initializes SaslClientCallbackHandler with encoded username and password. \n* @param token authentication token containing user credentials\n*/"
    },
    "org.apache.hadoop.security.AnnotatedSecurityInfo": {
        "org.apache.hadoop.security.AnnotatedSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves KerberosInfo annotation from the specified protocol class.\n* @param protocol the class to inspect for KerberosInfo\n* @param conf configuration settings (unused)\n* @return KerberosInfo annotation or null if not present\n*/",
        "org.apache.hadoop.security.AnnotatedSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves TokenInfo annotation from the specified protocol class.\n* @param protocol the class to inspect for TokenInfo\n* @param conf configuration settings (unused)\n* @return TokenInfo annotation or null if not present\n*/"
    },
    "org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream": {
        "org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream:write(byte[],int,int)": "/**\n* Wraps and sends a byte array as a SASL message.\n* @param buf byte array to wrap, off offset, len length of data\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.KerberosAuthException": {
        "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String)": "/**\n* Constructs a KerberosAuthException with a specified message.\n* @param msg error message associated with the exception\n*/",
        "org.apache.hadoop.security.KerberosAuthException:setUser(java.lang.String)": "/**\n* Sets the user identifier.\n* @param u the user identifier to set\n*/",
        "org.apache.hadoop.security.KerberosAuthException:setKeytabFile(java.lang.String)": "/**\n* Sets the keytab file path.\n* @param k the path of the keytab file to set\n*/",
        "org.apache.hadoop.security.KerberosAuthException:setPrincipal(java.lang.String)": "/**\n* Sets the principal value.\n* @param p the principal string to be set\n*/",
        "org.apache.hadoop.security.KerberosAuthException:setTicketCacheFile(java.lang.String)": "/**\n* Sets the ticket cache file path.\n* @param t the path to the ticket cache file\n*/",
        "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.Throwable)": "/**\n* Constructs a KerberosAuthException with the specified cause.\n* @param cause the underlying reason for the exception\n*/",
        "org.apache.hadoop.security.KerberosAuthException:getMessage()": "/**\n* Constructs a detailed message based on various attributes.\n* @return concatenated message string\n*/",
        "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a KerberosAuthException with a message and cause.\n* @param initialMsg the detail message\n* @param cause the underlying reason for the exception\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration": {
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getParameters()": "/**\n* Retrieves the current login parameters.\n* @return LoginParams object containing login details\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:<init>(org.apache.hadoop.security.UserGroupInformation$LoginParams)": "/**\n* Initializes HadoopConfiguration with provided login parameters.\n* @param params login parameters for configuration\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:prependFileAuthority(java.lang.String)": "/**\n* Prepends 'file://' to the path if not already present.\n* @param keytabPath the original file path\n* @return modified path with 'file://' prefix\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry()": "/**\n* Constructs a Kerberos AppConfigurationEntry based on parameters.\n* @return AppConfigurationEntry for Kerberos authentication settings\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String)": "/**\n* Retrieves app configuration entries based on the app name.\n* @param appName name of the application\n* @return array of AppConfigurationEntry objects\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$RealUser": {
        "org.apache.hadoop.security.UserGroupInformation$RealUser:<init>(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Constructs a RealUser with specified user group information.\n* @param realUser user group information for the real user\n*/",
        "org.apache.hadoop.security.UserGroupInformation$RealUser:getRealUser()": "/**\n* Retrieves the real user information.\n* @return UserGroupInformation of the real user\n*/",
        "org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode()": "/**\n* Returns the hash code for this object based on realUser's identity.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object)": "/**\n* Compares this RealUser object to another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.UserGroupInformation$RealUser:getName()": "/**\n* Returns the name of the user.\n* @return user's full name as a String\n*/",
        "org.apache.hadoop.security.UserGroupInformation$RealUser:toString()": "/**\n* Returns a string representation of the real user.\n* @return formatted user details from realUser\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:<init>()": "/**\n* Protected constructor for AbstractVerifier class.\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.lang.String[],java.lang.String[])": "/**\n* Validates SSL certificate for the given host.\n* @param host the hostname to check\n* @param cns array of common names\n* @param subjectAlts array of subject alternative names\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isIP4Address(java.lang.String)": "/**\n* Checks if the given string is a valid IPv4 address.\n* @param cn the string to validate\n* @return true if valid IPv4, false otherwise\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:acceptableCountryWildcard(java.lang.String)": "/**\n* Validates if the country wildcard format is acceptable.\n* @param cn country string to validate\n* @return true if valid, false if invalid based on criteria\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:countDots(java.lang.String)": "/**\n* Counts the number of dots in the given string.\n* @param s input string to search for dots\n* @return total count of dot characters\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String)": "/**\n* Checks if the given host is a localhost address.\n* @param host the host string to check\n* @return true if host is localhost, false otherwise\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)": "/**\n* Validates host against certificate's CNs and DNS Subject Alternative Names.\n* @param host array of hostnames to check\n* @param cert the X509Certificate to validate against\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)": "/**\n* Validates hostnames against CNs and subjectAlt names for SSL certificates.\n* @param hosts array of hostnames to check\n* @param cns array of common names from the certificate\n* @param subjectAlts array of subject alternative names\n* @param ie6 flag for IE6 compatibility\n* @param strictWithSubDomains flag for strict subdomain matching\n* @throws SSLException if validation fails\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)": "/**\n* Verifies host against SSL session's peer certificate.\n* @param host hostname to validate\n* @param session SSLSession containing peer certificate\n* @return true if valid, false if an exception occurs\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)": "/**\n* Validates a single hostname against the provided X509Certificate.\n* @param host the hostname to check\n* @param cert the X509Certificate for validation\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)": "/**\n* Validates SSL session and checks host against the peer's certificate.\n* @param host array of hostnames to verify\n* @param ssl SSLSocket to obtain the session and certificates\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)": "/**\n* Validates SSL session for a single hostname.\n* @param host hostname to verify\n* @param ssl SSLSocket to obtain the session and certificates\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates:getCNs(java.security.cert.X509Certificate)": "/**\n* Extracts Common Names (CN) from an X.509 certificate.\n* @param cert the X509Certificate to parse\n* @return array of CN strings or null if none found\n*/",
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates:getDNSSubjectAlts(java.security.cert.X509Certificate)": "/**\n* Retrieves DNS Subject Alternative Names from an X509 certificate.\n* @param cert the X509Certificate to extract names from\n* @return array of DNS names or null if none found\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$4": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String[],java.lang.String[],java.lang.String[])": "/**\n* Validates SSL hostnames against provided hosts, CNs, and subject alternative names.\n* @param hosts array of hostnames to check\n* @param cns array of common names for validation\n* @param subjectAlts array of subject alternative names\n* @throws SSLException if validation fails\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$1": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String[],java.lang.String[],java.lang.String[])": "/**\n* Validates SSL certificates against provided hosts and names.\n* @param hosts array of hostnames to check\n* @param cns array of common names for validation\n* @param subjectAlts array of subject alternative names\n* @throws SSLException if validation fails\n*/"
    },
    "org.apache.hadoop.security.ssl.ReloadingX509TrustManager": {
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadTrustManager(java.nio.file.Path)": "/**\n* Loads an X509TrustManager from a specified truststore file.\n* @param path the path to the truststore file\n* @return X509TrustManager instance or null if not found\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:checkClientTrusted(java.security.cert.X509Certificate[],java.lang.String)": "/**\n* Validates client certificate chain against trust manager.\n* @param chain array of X509 certificates\n* @param authType authentication type\n* @throws CertificateException if validation fails\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:checkServerTrusted(java.security.cert.X509Certificate[],java.lang.String)": "/**\n* Validates server certificate chain and authentication type.\n* @param chain array of X509 certificates to validate\n* @param authType the authentication type used\n* @throws CertificateException if validation fails\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:getAcceptedIssuers()": "/**\n* Retrieves accepted issuer certificates from the trust manager.\n* @return array of X509Certificate or an empty array if none\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Initializes ReloadingX509TrustManager with type, location, and password.\n* @param type the type of trust manager\n* @param location the path to the truststore file\n* @param password the truststore password\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path)": "/**\n* Loads and sets an X509TrustManager from a truststore file.\n* @param path the path to the truststore file\n* @return ReloadingX509TrustManager instance\n*/"
    },
    "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory": {
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:destroy()": "/**\n* Cleans up resources by canceling timers and nullifying trust managers.\n*/",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getConf()": "",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getKeyManagers()": "",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getTrustManagers()": "",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:setConf(org.apache.hadoop.conf.Configuration)": "",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)": "/**\n* Formats a template string with the lowercase mode value.\n* @param mode SSLFactory.Mode to be converted to string\n* @param template the template string to format\n* @return formatted string with mode value\n*/",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**** Retrieves password from configuration or returns default. \n* @param conf configuration source \n* @param alias credential identifier \n* @param defaultPass fallback password \n* @return resolved password as String \n*/",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)": "/**\n* Creates trust managers from configuration settings.\n* @param mode SSLFactory.Mode for trust store configuration\n* @param truststoreType type of trust store\n* @param truststoreLocation location of the trust store\n* @param storesReloadInterval interval for reloading trust store\n*/",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)": "/**\n* Creates key managers from configuration for SSL.\n* @param mode SSLFactory.Mode for keystore settings\n* @param keystoreType type of the keystore\n* @param storesReloadInterval interval for reloading keystore\n*/",
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode)": "/**** Initializes SSL settings based on mode, including key and trust managers. */"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$2": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String[],java.lang.String[],java.lang.String[])": "/**\n* Validates SSL hostnames against provided hosts and subject alternative names.\n* @param hosts array of hostnames to check\n* @param cns array of common names to validate\n* @param subjectAlts array of subject alternative names\n* @throws SSLException if validation fails\n*/"
    },
    "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager": {
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadKeyManager(java.nio.file.Path)": "/**\n* Loads an X509ExtendedKeyManager from a specified KeyStore file.\n* @param path location of the KeyStore file\n* @return X509ExtendedKeyManager or null if not found\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseEngineClientAlias(java.lang.String[],java.security.Principal[],javax.net.ssl.SSLEngine)": "/**\n* Chooses the client alias for the SSL engine.\n* @param strings array of possible aliases\n* @param principals array of certificate principals\n* @param sslEngine the SSL engine in use\n* @return selected client alias as a String\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseEngineServerAlias(java.lang.String,java.security.Principal[],javax.net.ssl.SSLEngine)": "/**\n* Chooses server alias for SSL engine.\n* @param s requested server name\n* @param principals array of authentication principals\n* @param sslEngine SSL engine context\n* @return selected server alias as a String\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getClientAliases(java.lang.String,java.security.Principal[])": "/**\n* Retrieves client aliases for a specified key type.\n* @param s key type as a String\n* @param principals array of Principal objects\n* @return array of client aliases as Strings\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseClientAlias(java.lang.String[],java.security.Principal[],java.net.Socket)": "/**\n* Selects a client alias based on provided strings and principals.\n* @param strings array of alias options\n* @param principals array of principal identities\n* @param socket the socket for the connection\n* @return selected client alias as a String\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getServerAliases(java.lang.String,java.security.Principal[])": "/**\n* Retrieves server aliases for a given hostname and principals.\n* @param s hostname to look up aliases for\n* @param principals array of Principal objects for authentication\n* @return array of server aliases associated with the hostname\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseServerAlias(java.lang.String,java.security.Principal[],java.net.Socket)": "/**\n* Chooses a server alias based on input parameters.\n* @param s server name; @param principals array of Principal objects; @param socket network socket\n* @return selected server alias as a String\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getCertificateChain(java.lang.String)": "/**\n* Retrieves the certificate chain for the given alias.\n* @param s the alias for the certificate\n* @return array of X509Certificate or null if not found\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getPrivateKey(java.lang.String)": "/**\n* Retrieves the private key associated with the given identifier.\n* @param s unique identifier for the private key\n* @return PrivateKey object or null if not found\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Initializes ReloadingX509KeystoreManager with type, location, and passwords.\n* @param type keystore type, @param location path to keystore, \n* @param storePassword keystore password, @param keyPassword key password\n*/",
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path)": "/****\n* Loads the X509 key manager from a KeyStore file.\n* @param path location of the KeyStore file\n* @return ReloadingX509KeystoreManager instance\n*/"
    },
    "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory": {
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:resetDefaultFactory()": "/**\n* Resets the default SSL Socket Factory for testing purposes.\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:alterCipherList(java.lang.String[])": "/**\n* Filters out GCM mode ciphers from the provided list.\n* @param defaultCiphers array of supported cipher suites\n* @return array of ciphers excluding GCM mode ones\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:bindToOpenSSLProvider()": "/**\n* Registers OpenSSL provider and initializes SSLContext.\n* @throws NoSuchAlgorithmException if SSL algorithm is not available\n* @throws KeyManagementException if key management fails\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:getDefaultCipherSuites()": "/**\n* Returns a clone of the default cipher suites array.\n* @return array of default cipher suite strings\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:getSupportedCipherSuites()": "/**\n* Returns a clone of the supported cipher suites.\n* @return array of supported cipher suite strings\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:configureSocket(java.net.Socket)": "/**\n* Configures the SSL socket with specified cipher suites.\n* @param socket the SSLSocket to configure\n* @return the configured SSLSocket\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)": "/**\n* Initializes SSLContext based on preferred channel mode.\n* @param preferredChannelMode mode for SSL channel configuration\n* @throws NoSuchAlgorithmException if SSL algorithm is unavailable\n* @throws KeyManagementException if key management fails\n* @throws IOException if an unknown channel mode is provided\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket()": "/**\n* Creates and configures an SSL socket.\n* @return configured SSLSocket\n* @throws IOException if socket creation fails\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)": "/**\n* Creates and configures an SSL socket.\n* @param s existing socket, host target host, port target port, autoClose flag for socket closure\n* @return configured SSLSocket\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)": "/**\n* Creates and configures an SSL socket.\n* @param address destination InetAddress\n* @param port destination port number\n* @param localAddress local InetAddress\n* @param localPort local port number\n* @return configured SSLSocket\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)": "/**\n* Creates and configures an SSL socket.\n* @param host the server's hostname\n* @param port the server's port number\n* @param localHost local address for the socket\n* @param localPort local port for the socket\n* @return configured SSLSocket\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)": "/**\n* Creates and configures an SSL socket for the given host and port.\n* @param host the InetAddress of the remote host\n* @param port the port number on the remote host\n* @return configured SSLSocket\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)": "/**\n* Creates and configures an SSL socket for the specified host and port.\n* @param host the server host name\n* @param port the server port number\n* @return configured SSLSocket\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)": "/**\n* Initializes DelegatingSSLSocketFactory with SSL configuration.\n* @param preferredChannelMode mode for SSL channel setup\n* @throws IOException if SSL context initialization fails\n*/",
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)": "/**\n* Initializes the default SSL socket factory.\n* @param preferredMode mode for SSL channel setup\n* @throws IOException if SSL context initialization fails\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$3": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String[],java.lang.String[],java.lang.String[])": "/**\n* Validates SSL hosts, common names, and subject alternative names.\n* @param hosts array of hostnames to check\n* @param cns array of common names for SSL certificate\n* @param subjectAlts array of subject alternative names\n* @throws SSLException if validation fails\n*/"
    },
    "org.apache.hadoop.conf.Configured": {
        "org.apache.hadoop.conf.Configured:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf the Configuration to set\n*/",
        "org.apache.hadoop.conf.Configured:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Configured object with the given configuration.\n* @param conf the Configuration to set\n*/",
        "org.apache.hadoop.conf.Configured:<init>()": "/**\n* Default constructor for Configured, initializes with null configuration.\n*/"
    },
    "org.apache.hadoop.security.ShellBasedUnixGroupsMapping": {
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolveFullGroupNames(java.lang.String)": "/**\n* Splits a string of group names into a set of unique names.\n* @param groupNames comma-separated group names\n* @return Set of unique group names\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:cacheGroupsAdd(java.util.List)": "",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:cacheGroupsRefresh()": "",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)": "/**\n* Parses group names and IDs, returning resolvable group names.\n* @param groupNames comma-separated group names\n* @param groupIDs comma-separated group IDs\n* @return Set of resolvable group names\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)": "/**\n* Handles shell executor timeout and logs a warning message.\n* @param executor the ShellCommandExecutor instance\n* @param user the user for whom the command was executed\n* @return true if timed out, false otherwise\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String)": "/**\n* Retrieves shell command to fetch user groups for a specified username.\n* @param userName the username to query groups for\n* @return command array for executing in the shell\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String)": "/**\n* Retrieves group IDs for a specified user.\n* @param userName the username to get group IDs for\n* @return array of group IDs for the user\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String)": "/**\n* Creates a ShellCommandExecutor for fetching user groups.\n* @param userName the username to query groups for\n* @return ShellCommandExecutor instance configured for the user\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String)": "/**\n* Creates a ShellCommandExecutor for retrieving group IDs of a user.\n* @param userName the username to get group IDs for\n* @return ShellCommandExecutor instance\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Resolves partial group names for a user, throwing exceptions for errors.\n* @param userName the username to resolve group names for\n* @param errMessage error message for exceptions\n* @param groupNames comma-separated group names to resolve\n* @return Set of resolvable group names\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String)": "/**** Retrieves Unix groups for a specified user. \n* @param user the username to fetch groups for \n* @return Set of user groups or an empty set on error \n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String)": "/**\n* Retrieves a list of groups for a specified user.\n* @param userName the username to fetch groups for\n* @return List of group names\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of groups for the specified user.\n* @param userName the username to fetch groups for\n* @return Set of user groups\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and initializes timeout from specified key.\n* @param conf the Configuration object to set\n*/"
    },
    "org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler": {
        "org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler:handle(javax.security.auth.callback.Callback[])": "/**\n* Processes SASL GSSAPI callbacks for authorization.\n* @param callbacks array of Callback objects\n* @throws UnsupportedCallbackException if an unrecognized callback is encountered\n*/"
    },
    "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory": {
        "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:getMechanismNames(java.util.Map)": "/**\n* Retrieves mechanism names based on properties.\n* @param props configuration properties map\n* @return array of mechanism names, default is {\"PLAIN\"} if conditions met\n*/",
        "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)": "/**\n* Creates a SaslServer for the specified mechanism.\n* @param mechanism authentication mechanism\n* @param protocol protocol used\n* @param serverName name of the server\n* @param props additional properties\n* @param cbh callback handler for authentication\n* @return SaslServer or null if mechanism is not \"PLAIN\"\n*/"
    },
    "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver": {
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getInetAddressByName(java.lang.String)": "/**\n* Retrieves InetAddress for the specified host.\n* @param host hostname to resolve\n* @return InetAddress object for the host\n* @throws UnknownHostException if the host cannot be resolved\n*/",
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:setSearchDomains(java.lang.String[])": "/**\n* Sets the search domains from a variable number of string arguments.\n* @param domains array of domain strings to set\n*/",
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String)": "/**\n* Resolves a fully qualified hostname to its InetAddress.\n* @param host the hostname to resolve\n* @return InetAddress object or null if resolution fails\n*/",
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String)": "/**\n* Resolves hostname to InetAddress, searching through domains if unqualified.\n* @param host the hostname to resolve\n* @return InetAddress object or null if resolution fails\n*/",
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String)": "/**\n* Resolves a hostname to its InetAddress or throws UnknownHostException if not found.\n* @param host the hostname to resolve\n* @return InetAddress object for the given hostname\n*/"
    },
    "org.apache.hadoop.security.CompositeGroupsMapping": {
        "org.apache.hadoop.security.CompositeGroupsMapping:getGroups(java.lang.String)": "/**\n* Retrieves unique groups for a specified user.\n* @param user the username to fetch groups for\n* @return a list of unique group names\n*/",
        "org.apache.hadoop.security.CompositeGroupsMapping:getConf()": "",
        "org.apache.hadoop.security.CompositeGroupsMapping:cacheGroupsAdd(java.util.List)": "",
        "org.apache.hadoop.security.CompositeGroupsMapping:cacheGroupsRefresh()": "",
        "org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of groups for the specified user.\n* @param user the username to fetch groups for\n* @return a set of group names or empty if none found\n*/",
        "org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String)": "/**\n* Prepares a Configuration object for a specified provider.\n* @param providerName the name of the provider\n* @return Configuration with filtered properties for the provider\n*/",
        "org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)": "/**\n* Adds a mapping provider to the list.\n* @param providerName name of the provider\n* @param providerClass class type of the provider\n*/",
        "org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders()": "/**\n* Loads mapping providers from configuration.\n* Retrieves provider names and their classes, logs errors if invalid.\n*/",
        "org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and loads mapping providers.\n* @param conf Configuration object to set and read properties\n*/"
    },
    "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap": {
        "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(java.util.Map)": "/**\n* Initializes PassThroughMap with entries from the provided mapping.\n* @param mapping key-value pairs to populate the map\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:get(java.lang.Object)": "/**\n* Retrieves value by key or returns the key if not found.\n* @param key the key to search for\n* @return value associated with key or the key itself\n*/",
        "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>()": "/**\n* Constructs a PassThroughMap with an empty mapping.\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule": {
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:getCanonicalUser(java.lang.Class)": "/**\n* Retrieves the first canonical user of the specified type.\n* @param cls the class type of the user to retrieve\n* @return the user of type T or null if none found\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:login()": "/**\n* Performs user login and returns success status.\n* @return true if login is successful\n* @throws LoginException if login fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:logout()": "/**\n* Logs out the user and returns success status.\n* @return true if logout is successful\n* @throws LoginException if logout fails\n*/",
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit()": "/**\n* Commits user login and updates subject with the authenticated user.\n* @return true if login is successful, throws LoginException if failed\n*/"
    },
    "org.apache.hadoop.security.Groups$GroupCacheLoader": {
        "org.apache.hadoop.security.Groups$GroupCacheLoader:reload(java.lang.String,java.util.Set)": "/**\n* Reloads groups asynchronously; returns a future with the updated set of group keys.\n* @param key identifier for the group to reload\n* @param oldValue current set of group keys\n* @return ListenableFuture containing the updated set of group keys\n*/",
        "org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String)": "/**\n* Retrieves user groups and logs latency if above threshold.\n* @param user username to fetch groups for\n* @return Set of group names associated with the user\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String)": "/**\n* Loads user groups, annotates tracing, and handles empty results.\n* @param user the username to fetch groups for\n* @return Set of group names associated with the user\n* @throws Exception if an error occurs during loading\n*/"
    },
    "org.apache.hadoop.security.authorize.ServiceAuthorizationManager": {
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getHostKey(java.lang.String)": "/**\n* Retrieves host key from service key by removing the suffix after the last dot.\n* @param serviceKey the original service key\n* @return modified host key or original if no dot exists\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsWithAcls()": "/**\n* Retrieves the set of protocol classes associated with ACLs.\n* @return Set of protocol Class objects\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsAcls(java.lang.Class)": "/**\n* Retrieves the first AccessControlList for the given class.\n* @param className the class for which ACLs are retrieved\n* @return the first AccessControlList associated with the class\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsBlockedAcls(java.lang.Class)": "/**\n* Retrieves blocked access control list for a given class.\n* @param className the class for which ACLs are retrieved\n* @return AccessControlList associated with the class\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsWithMachineLists()": "/**\n* Retrieves a set of protocol classes with associated machine lists.\n* @return Set of protocol classes\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsMachineList(java.lang.Class)": "/**\n* Retrieves the first MachineList for the specified class.\n* @param className the class to lookup in protocolToMachineLists\n* @return the corresponding MachineList object\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsBlockedMachineList(java.lang.Class)": "/**\n* Retrieves blocked machine list for specified protocol class.\n* @param className protocol class to fetch the machine list for\n* @return MachineList associated with the protocol class\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)": "/**\n* Refreshes ACLs and machine lists from the loaded configuration.\n* @param conf configuration settings\n* @param provider policy provider for services\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)": "/**\n* Authorizes user access based on protocol, ACLs, and host address.\n* @param user user group information\n* @param protocol the protocol class\n* @param conf configuration settings\n* @param addr client InetAddress\n* @throws AuthorizationException if access is denied\n*/",
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)": "/**\n* Refreshes configuration with policy file settings.\n* @param conf original Configuration object\n* @param provider policy provider for services\n*/"
    },
    "org.apache.hadoop.security.authorize.Service": {
        "org.apache.hadoop.security.authorize.Service:getServiceKey()": "/**\n* Retrieves the service key.\n* @return the service key as a String\n*/",
        "org.apache.hadoop.security.authorize.Service:getProtocol()": "/**\n* Returns the protocol class type.\n* @return Class<?> representing the protocol\n*/",
        "org.apache.hadoop.security.authorize.Service:<init>(java.lang.String,java.lang.Class)": "/**\n* Constructs a Service with a specified key and protocol class.\n* @param key unique identifier for the service\n* @param protocol class type of the service protocol\n*/"
    },
    "org.apache.hadoop.security.authorize.AccessControlList": {
        "org.apache.hadoop.security.authorize.AccessControlList:isWildCardACLValue(java.lang.String)": "/**\n* Checks if the ACL string is a wildcard value.\n* @param aclString the ACL string to evaluate\n* @return true if aclString is a wildcard, otherwise false\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:isAllAllowed()": "/**\n* Checks if all permissions are allowed.\n* @return true if all permissions are allowed, false otherwise\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:toString()": "/**\n* Returns a string representation of user access permissions.\n* @return description of allowed users and groups\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:getString(java.util.Collection)": "/**\n* Concatenates strings from a collection into a single comma-separated string.\n* @param strings collection of strings to concatenate\n* @return concatenated string result\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:getGroups()": "/**\n* Retrieves a collection of group names.\n* @return Collection of group names as strings\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String)": "/**\n* Adds a user if not a wildcard ACL and permissions allow it.\n* @param user the user to be added\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String)": "/**\n* Adds a group if not a wildcard and permissions allow it.\n* @param group name of the group to add\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String)": "/**\n* Removes a user if not a wildcard and permissions allow.\n* @param user the username to be removed\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String)": "/**\n* Removes a group if not a wildcard and all permissions are not allowed.\n* @param group the group to remove\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:getUsersString()": "/**\n* Retrieves a comma-separated string of user names.\n* @return concatenated user names from the users collection\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:getGroupsString()": "/**\n* Retrieves a comma-separated string of group names.\n* @return concatenated string of group names\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[])": "/**\n* Builds ACL from user group strings, populating users and groups sets.\n* @param userGroupStrings array of user and group strings\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:getAclString()": "/**\n* Constructs an ACL string based on user and group permissions.\n* @return ACL string or '*' if all access is allowed\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput)": "/**\n* Writes ACL string to output stream.\n* @param out output stream for writing ACL data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput)": "/**\n* Reads fields from input and builds ACL from the retrieved string.\n* @param in input stream to read from\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:<init>()": "/**\n* Constructs an AccessControlList object.\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String)": "/**\n* Constructs AccessControlList from a space-separated ACL string.\n* @param aclString space-separated ACL representation\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs AccessControlList using specified users and groups.\n* @param users comma-separated user identifiers\n* @param groups comma-separated group identifiers\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Checks if a user is in the allowed list or their groups.\n* @param ugi user group information\n* @return true if user is allowed, false otherwise\n*/",
        "org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Checks if a user is allowed based on their group information.\n* @param ugi user group information\n* @return true if the user is allowed, false otherwise\n*/"
    },
    "org.apache.hadoop.security.authorize.AccessControlList$1": {
        "org.apache.hadoop.security.authorize.AccessControlList$1:<init>()": "/**\n* Initializes a new instance of AccessControlList.\n*/"
    },
    "org.apache.hadoop.security.authorize.AuthorizationException": {
        "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(java.io.PrintStream)": "/**\n* Prints the object to the specified PrintStream without stack trace details.\n* @param s the PrintStream to output the object representation\n*/",
        "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(java.io.PrintWriter)": "/**\n* Prints the object representation to the provided PrintWriter.\n* @param s PrintWriter to output the object representation\n*/",
        "org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String)": "/**\n* Constructs an AuthorizationException with a detail message.\n* @param message detail message for the exception\n*/",
        "org.apache.hadoop.security.authorize.AuthorizationException:<init>()": "/**\n* Constructs an AuthorizationException with a default message.\n*/",
        "org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable)": "/**\n* Constructs an AuthorizationException with a specified cause.\n* @param cause the underlying reason for the exception\n*/",
        "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace()": "/**\n* Prints the stack trace to the standard error stream.\n*/"
    },
    "org.apache.hadoop.security.authorize.DefaultImpersonationProvider": {
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf configuration to be set\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getAclKey(java.lang.String)": "/**\n* Retrieves the ACL key by removing the last segment after the dot.\n* @param key the original key string\n* @return the modified key string without the last segment\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserIpConfKey(java.lang.String)": "/**\n* Constructs the proxy superuser IP configuration key.\n* @param userName the username for which the key is generated\n* @return the configuration key as a String\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserUserConfKey(java.lang.String)": "/**\n* Constructs a configuration key for a proxy superuser.\n* @param userName the name of the user\n* @return the configuration key as a String\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserGroupConfKey(java.lang.String)": "/**\n* Generates configuration key for proxy superuser group.\n* @param userName the name of the user\n* @return concatenated configuration key string\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getConf()": "",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups()": "/**\n* Retrieves proxy groups mapped to user ACLs.\n* @return Map of proxy group names to their corresponding group collections\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts()": "/**\n* Retrieves a map of proxy hosts and their associated entry collections.\n* @return map of proxy host names to collections of entry strings\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String)": "/**\n* Initializes proxy user ACLs and host lists from the configuration prefix.\n* @param configurationPrefix base configuration key prefix\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider()": "/**\n* Retrieves a singleton DefaultImpersonationProvider instance.\n* @return DefaultImpersonationProvider instance\n*/",
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)": "/**\n* Authorizes user impersonation based on ACL and remote IP.\n* @param user user attempting impersonation\n* @param remoteAddress IP address of the connection\n* @throws AuthorizationException if unauthorized\n*/"
    },
    "org.apache.hadoop.util.MachineList": {
        "org.apache.hadoop.util.MachineList:includes(java.net.InetAddress)": "/**\n* Checks if the given IP address is included in the allowed addresses or ranges.\n* @param address IP address to check for inclusion\n* @return true if included, false otherwise\n*/",
        "org.apache.hadoop.util.MachineList:getCollection()": "/**\n* Retrieves the collection of entries.\n* @return a collection of entry strings\n*/",
        "org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)": "/**\n* Initializes MachineList with host entries and address factory.\n* @param hostEntries collection of host names or CIDR notations\n* @param addressFactory factory to resolve host names to IP addresses\n*/",
        "org.apache.hadoop.util.MachineList:includes(java.lang.String)": "/**\n* Checks if the given IP address is included in allowed addresses.\n* @param ipAddress the IP address to check\n* @return true if included, false otherwise\n*/",
        "org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)": "/**\n* Constructs MachineList from host entries and address factory.\n* @param hostEntries comma-separated host names or CIDR notations\n* @param addressFactory factory to resolve host names to IP addresses\n*/",
        "org.apache.hadoop.util.MachineList:<init>(java.util.Collection)": "/****\n* Constructs a MachineList with host entries using a default address factory.\n* @param hostEntries collection of host names or CIDR notations\n*/",
        "org.apache.hadoop.util.MachineList:<init>(java.lang.String)": "/**\n* Constructs MachineList from host entries using default address factory.\n* @param hostEntries comma-separated host names or CIDR notations\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell$ListCommand": {
        "org.apache.hadoop.security.alias.CredentialShell$ListCommand:validate()": "/**\n* Validates if the credential provider is available.\n* @return true if provider is not null, false otherwise\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$ListCommand:execute()": "/**\n* Lists aliases from the CredentialProvider and handles IO exceptions.\n* @throws IOException if an error occurs while fetching aliases\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$ListCommand:getUsage()": "/**\n* Retrieves usage information as a formatted string.\n* @return formatted usage and description text\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry": {
        "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:<init>(java.lang.String,char[])": "/**\n* Initializes a CredentialEntry with an alias and credential.\n* @param alias unique identifier for the credential\n* @param credential secret key as a char array\n*/",
        "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:getCredential()": "/**\n* Retrieves the credential as a character array.\n* @return char[] representing the credential\n*/",
        "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:toString()": "/**\n* Returns a string representation of the object with alias and credential.\n* @return formatted string of alias and credential or \"null\" if credential is null\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialProvider": {
        "org.apache.hadoop.security.alias.CredentialProvider:needsPassword()": "/**\n* Checks if a password is required.\n* @return false indicating no password is needed\n*/",
        "org.apache.hadoop.security.alias.CredentialProvider:noPasswordError()": "/**\n* Returns null to indicate a no-password error condition.\n* @return null\n*/",
        "org.apache.hadoop.security.alias.CredentialProvider:noPasswordWarning()": "/**\n* Returns a warning message for missing password.\n* @return null indicating no warning message available\n*/",
        "org.apache.hadoop.security.alias.CredentialProvider:isTransient()": "/**\n* Checks if the object is transient.\n* @return false indicating the object is not transient\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell": {
        "org.apache.hadoop.security.alias.CredentialShell:getPasswordReader()": "/**\n* Retrieves the singleton instance of PasswordReader.\n* @return PasswordReader instance\n*/",
        "org.apache.hadoop.security.alias.CredentialShell:getCommandUsage()": "/**\n* Generates command usage information as a formatted string.\n* @return formatted command usage details\n*/",
        "org.apache.hadoop.security.alias.CredentialShell:promptForCredential()": "/**\n* Prompts user for matching passwords and returns the valid password.\n* @return char array of the entered password\n* @throws IOException if no console is available for input\n*/",
        "org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[])": "/**\n* Initializes command-line arguments and sets subcommands.\n* @param args command-line arguments\n* @return status code indicating success or failure\n*/",
        "org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[])": "/**\n* Executes the CredentialShell tool with provided arguments.\n* @param args command-line arguments for the tool\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell$PasswordReader": {
        "org.apache.hadoop.security.alias.CredentialShell$PasswordReader:readPassword(java.lang.String)": "/**\n* Reads a password from the console with a prompt.\n* @param prompt message to display for input\n* @return char array containing the entered password\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$PasswordReader:format(java.lang.String)": "/**\n* Formats and prints the given message to the console.\n* @param message the message to format and display\n*/"
    },
    "org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory": {
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a CredentialProvider based on the given URI and configuration.\n* @param providerName URI of the provider\n* @param conf configuration settings\n* @return CredentialProvider instance or null if scheme doesn't match\n*/"
    },
    "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider$Factory": {
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a CredentialProvider based on the given URI and configuration.\n* @param providerName URI specifying the provider scheme\n* @param conf configuration settings\n* @return CredentialProvider or null if the scheme does not match\n*/"
    },
    "org.apache.hadoop.security.alias.UserProvider$Factory": {
        "org.apache.hadoop.security.alias.UserProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a CredentialProvider based on the URI scheme.\n* @param providerName URI of the provider\n* @param conf configuration settings\n* @return CredentialProvider or null if scheme does not match\n*/"
    },
    "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider$Factory": {
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a CredentialProvider based on the URI scheme.\n* @param providerName URI for the provider\n* @param conf configuration settings\n* @return CredentialProvider or null if scheme does not match\n*/"
    },
    "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider": {
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPath()": "/**\n* Retrieves the current file path.\n* @return Path object representing the file path\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:bytesToChars(byte[])": "/**\n* Converts byte array to character array using UTF-8 encoding.\n* @param bytes input byte array\n* @return character array representation of the input bytes\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:deleteCredentialEntry(java.lang.String)": "/**\n* Deletes a credential entry by name if it exists.\n* @param name the name of the credential to delete\n* @throws IOException if the credential does not exist or an error occurs\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:flush()": "/**\n* Saves the keystore if changes occurred, handling IO exceptions.\n* @throws IOException if storage fails due to algorithm, key, or certificate issues\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:toString()": "/**\n* Returns the string representation of the URI.\n* @return string form of the URI object\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getKeyStore()": "/**\n* Retrieves the KeyStore instance.\n* @return KeyStore object containing cryptographic keys\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPassword()": "/**\n* Retrieves the stored password as a character array.\n* @return char array representing the password\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getReadLock()": "/**\n* Retrieves the read lock instance.\n* @return Lock object representing the read lock\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getUri()": "/**\n* Retrieves the URI instance.\n* @return the URI object\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getWriteLock()": "/**\n* Retrieves the write lock instance.\n* @return Lock object representing the write lock\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:isChanged()": "/**\n* Checks if the state has changed.\n* @return true if changed, false otherwise\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setChanged(boolean)": "/**\n* Sets the changed state of the object.\n* @param chg true if changed, false otherwise\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setPassword(char[])": "/**\n* Sets the user's password.\n* @param pass character array representing the new password\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setPath(org.apache.hadoop.fs.Path)": "/**\n* Sets the path to the specified Path object.\n* @param p the Path to be set\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setReadLock(java.util.concurrent.locks.Lock)": "/**\n* Sets the read lock to the specified Lock object.\n* @param rl the Lock object to be set as the read lock\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setWriteLock(java.util.concurrent.locks.Lock)": "/**\n* Sets the write lock for the current instance.\n* @param wl the Lock object to be set as the write lock\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])": "/**\n* Sets a credential in the key store.\n* @param alias unique identifier for the credential\n* @param material secret key as a char array\n* @return CredentialEntry object with the alias and material\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString()": "/**\n* Retrieves the file path as a string.\n* @return String representation of the file path\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning()": "/****\n* Generates a no-password warning message.\n* @return formatted warning string based on env and file keys\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError()": "/**\n* Generates a no-password error message.\n* @return formatted no-password error message\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])": "/**\n* Creates a credential entry if alias doesn't exist.\n* @param alias unique identifier for the credential\n* @param credential secret key as a char array\n* @return CredentialEntry object\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String)": "/**\n* Retrieves a CredentialEntry by alias.\n* @param alias unique identifier for the credential\n* @return CredentialEntry or null if not found\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases()": "/**\n* Retrieves a list of aliases from the key store.\n* @return List of alias strings\n* @throws IOException if an error occurs while accessing the key store\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI)": "/**\n* Initializes the file system with the provided keystore URI.\n* @param keystoreUri URI of the keystore to initialize\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore()": "/**\n* Locates and initializes a KeyStore with a password from environment or file.\n* @throws IOException if keystore creation or loading fails\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword()": "/**\n* Checks if a password is needed based on environment variable and file settings.\n* @return true if password is not found, false otherwise\n*/",
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a key store provider with URI and configuration.\n* @param uri the URI of the keystore\n* @param conf the configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell$Command": {
        "org.apache.hadoop.security.alias.CredentialShell$Command:printProviderWritten()": "/**\n* Prints a message indicating the provider was updated.\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$Command:doHelp()": "/**\n* Displays help information and usage for commands.\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider()": "/**\n* Warns if the provider is transient.\n* Checks provider state and prints a warning if true.\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider()": "/**\n* Retrieves a CredentialProvider based on configuration and transient status.\n* @return CredentialProvider or null if none available\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand": {
        "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:execute()": "/**\n* Executes credential deletion or displays help.\n* @throws IOException if deletion fails or provider issues occur\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:getUsage()": "/**\n* Returns usage information as a formatted string.\n* @return concatenated usage and description strings\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate()": "/**** Validates alias and prompts for confirmation before deletion. \n* @return true if valid or confirmed, false otherwise \n*/"
    },
    "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$Factory": {
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a CredentialProvider based on the given URI and configuration.\n* @param providerName URI of the provider\n* @param conf configuration settings\n* @return CredentialProvider or null if the scheme does not match\n*/"
    },
    "org.apache.hadoop.security.alias.LocalKeyStoreProvider": {
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:getOutputStreamForKeystore()": "/**\n* Retrieves an OutputStream for the keystore file.\n* @return OutputStream for writing to the keystore\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:keystoreExists()": "/**\n* Checks if the keystore file exists and is non-empty.\n* @return true if the file exists and has content, false otherwise\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:getInputStreamForFile()": "/**\n* Retrieves an InputStream for the specified file.\n* @return InputStream for the file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:modeToPosixFilePermission(int)": "/**\n* Converts an integer file mode to a set of POSIX file permissions.\n* @param mode integer representing file permissions\n* @return Set of PosixFilePermission based on the mode\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String)": "/**\n* Creates file permissions from octal string.\n* @param perms octal string representing file permissions\n* @throws IOException if the permissions mode is invalid\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions()": "/**\n* Stashes original file permissions for potential keystore rewrite.\n* @throws IOException if an I/O error occurs while accessing file permissions\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI)": "/****\n* Initializes the file system with the given URI.\n* @param uri URI of the file system to initialize\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush()": "/**\n* Flushes output and resets file permissions based on the operating system.\n* @throws IOException if permission change fails\n*/",
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a LocalKeyStoreProvider with specified URI and configuration.\n* @param uri the URI of the keystore\n* @param conf the configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.util.ZKUtil": {
        "org.apache.hadoop.util.ZKUtil:resolveConfIndirection(java.lang.String)": "/**\n* Resolves configuration indirection by reading from a file path.\n* @param valInConf input string, may be a path prefixed with '@'\n* @return resolved string or null if input is null\n*/",
        "org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String)": "/**\n* Converts a permission string to a bitmask of permissions.\n* @param permString string representing permissions (e.g., \"rwd\")\n* @return bitmask integer representing combined permissions\n*/",
        "org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String)": "/**\n* Parses ACLs from a string and returns a list of ACL objects.\n* @param aclString string representation of ACLs\n* @return List of ACL objects\n* @throws BadAclFormatException if the format is invalid\n*/",
        "org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String)": "/**\n* Parses authentication string into ZKAuthInfo list.\n* @param authString formatted auth string\n* @return List of ZKAuthInfo objects\n*/"
    },
    "org.apache.hadoop.security.WhitelistBasedResolver": {
        "org.apache.hadoop.security.WhitelistBasedResolver:getSaslProperties(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves SASL properties using the provided configuration.\n* @param conf configuration settings\n* @return map of SASL properties\n*/",
        "org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress)": "/**\n* Retrieves server properties based on client IP address.\n* @param clientAddress client's InetAddress\n* @return properties map based on whitelist check\n*/",
        "org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String)": "/**\n* Retrieves server properties based on client IP address.\n* @param clientAddress client's IP address as a string\n* @return properties map or default if address is null\n*/",
        "org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Configures SASL settings and initializes IP whitelist.\n* @param conf configuration settings for SASL properties\n*/"
    },
    "org.apache.hadoop.util.CombinedIPWhiteList": {
        "org.apache.hadoop.util.CombinedIPWhiteList:isIn(java.lang.String)": "/**\n* Checks if the given IP address is in the local or defined network lists.\n* @param ipAddress the IP address to check\n* @return true if in network lists, false otherwise\n*/",
        "org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)": "/**\n* Initializes CombinedIPWhiteList with fixed and optional variable IP lists.\n* @param fixedWhiteListFile path to fixed IP list file\n* @param variableWhiteListFile path to variable IP list file or null\n* @param cacheExpiryInSeconds duration for variable list cache expiry\n*/"
    },
    "org.apache.hadoop.crypto.UnsupportedCodecException": {
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>()": "/**\n* Constructs a new UnsupportedCodecException with no detail message.\n*/",
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.String)": "/**\n* Constructs an UnsupportedCodecException with a specified message.\n* @param message detailed error message\n*/",
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an UnsupportedCodecException with a message and cause.\n* @param message error message\n* @param cause underlying cause of the exception\n*/",
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.Throwable)": "/**\n* Constructs an UnsupportedCodecException with the specified cause.\n* @param cause the underlying reason for the exception\n*/"
    },
    "org.apache.hadoop.crypto.OpensslCipher$AlgMode": {
        "org.apache.hadoop.crypto.OpensslCipher$AlgMode:get(java.lang.String,java.lang.String)": "/**\n* Retrieves the ordinal of a specified algorithm and mode.\n* @param algorithm the algorithm name\n* @param mode the mode associated with the algorithm\n* @return ordinal of the algorithm-mode combination\n* @throws NoSuchAlgorithmException if the combination is unsupported\n*/"
    },
    "org.apache.hadoop.crypto.OpensslCipher": {
        "org.apache.hadoop.crypto.OpensslCipher:getLoadingFailureReason()": "/**\n* Retrieves the reason for loading failure.\n* @return String representing the loading failure reason\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:init(int,byte[],byte[])": "/**\n* Initializes the context with specified mode, key, and IV.\n* @param mode operation mode for initialization\n* @param key encryption key\n* @param iv initialization vector\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:<init>(long,int,int,long)": "/**\n* Initializes an OpenSSL cipher with specified parameters.\n* @param context cipher context handle\n* @param alg cipher algorithm identifier\n* @param padding padding scheme identifier\n* @param engine engine handle for cryptographic operations\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:clean()": "/**\n* Resets context and engine to zero after cleaning resources.\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:checkState()": "/**\n* Validates that the context is not zero; throws exception if invalid.\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String)": "/**\n* Parses a transformation string into algorithm, mode, and padding.\n* @param transformation the transformation string to tokenize\n* @return Transform object with parsed components\n* @throws NoSuchAlgorithmException if input is null or format is invalid\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:finalize()": "/**\n* Cleans resources before object finalization.\n* @throws Throwable if cleanup fails during finalization\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Updates output buffer from input buffer; requires direct buffers.\n* @param input source buffer, must be direct\n* @param output target buffer, must be direct\n* @return number of bytes written to output\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer)": "/**\n* Completes the encryption process and writes to a direct ByteBuffer.\n* @param output direct ByteBuffer for output\n* @return number of bytes written to the output\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)": "/**\n* Creates an OpensslCipher instance from transformation and engine ID.\n* @param transformation cipher parameters string\n* @param engineId optional engine identifier\n* @return OpensslCipher object\n* @throws NoSuchAlgorithmException if the algorithm is invalid\n* @throws NoSuchPaddingException if the padding is unsupported\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite)": "/**\n* Checks if the given CipherSuite is supported.\n* @param suite the CipherSuite to validate\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String)": "/**\n* Creates an OpensslCipher instance from transformation.\n* @param transformation cipher parameters string\n* @return OpensslCipher object\n* @throws NoSuchAlgorithmException if the algorithm is invalid\n* @throws NoSuchPaddingException if the padding is unsupported\n*/"
    },
    "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec": {
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:getCipherSuite()": "/**\n* Returns the configured cipher suite for encryption.\n* @return CipherSuite.SM4_CTR_NOPADDING constant\n*/",
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:getLogger()": "/**\n* Returns the logger instance for the class.\n* @return Logger object used for logging\n*/",
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])": "/**\n* Calculates the initialization vector (IV) using provided bytes and counter.\n* @param initIV initial IV bytes\n* @param counter counter value for IV calculation\n* @param iv output IV array\n*/",
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>()": "/**\n* Initializes codec, throwing exceptions if loading fails or SM4 CTR is unsupported.\n*/",
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor()": "/**\n* Creates an Encryptor instance for encryption using OpenSSL.\n* @return Encryptor object for encryption\n* @throws GeneralSecurityException if initialization fails\n*/",
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor()": "/**\n* Creates a Decryptor using OpenSSL with specified mode and cipher suite.\n* @return Decryptor instance for decryption\n*/",
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and initializes engine ID.\n* @param conf configuration to apply for the engine ID\n*/"
    },
    "org.apache.hadoop.crypto.CryptoStreamUtils": {
        "org.apache.hadoop.crypto.CryptoStreamUtils:getInputStreamOffset(java.io.InputStream)": "/**\n* Retrieves the current offset of the input stream.\n* @param in the InputStream to check\n* @return current position if Seekable, otherwise 0\n*/",
        "org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)": "/**\n* Validates and adjusts buffer size based on codec's block size.\n* @param codec the CryptoCodec used for block size calculation\n* @param bufferSize the initial buffer size to validate and adjust\n* @return adjusted buffer size\n*/",
        "org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer)": "/**\n* Frees the given ByteBuffer if unmapping is supported.\n* @param buffer the ByteBuffer to be freed\n*/",
        "org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec)": "/**\n* Validates the codec's cipher suite; throws exception if invalid.\n* @param codec the CryptoCodec to validate\n*/",
        "org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the buffer size from configuration settings.\n* @param conf configuration object\n* @return buffer size as an integer\n*/"
    },
    "org.apache.hadoop.crypto.CryptoInputStream": {
        "org.apache.hadoop.crypto.CryptoInputStream:getDecryptor()": "/**\n* Retrieves a Decryptor from the pool or creates a new one if unavailable.\n* @return Decryptor instance\n* @throws IOException if creation fails due to security issues\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:checkStream()": "/**\n* Validates if the stream is open; throws IOException if closed.\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getTmpBuf()": "/**\n* Retrieves a temporary byte buffer, initializing it if necessary.\n* @return byte array representing the temporary buffer\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getBuffer()": "/**\n* Retrieves a ByteBuffer from the pool or allocates a new one if empty.\n* @return a ByteBuffer instance\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:returnBuffer(java.nio.ByteBuffer)": "/**\n* Returns a ByteBuffer to the pool after clearing it.\n* @param buf the ByteBuffer to return, or null to do nothing\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:returnDecryptor(org.apache.hadoop.crypto.Decryptor)": "/**\n* Returns a Decryptor to the pool if not null.\n* @param decryptor the Decryptor to return\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:reset()": "/**\n* Resets the input stream; always throws IOException indicating not supported.\n* @throws IOException if mark/reset is not supported\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:releaseBuffer(java.nio.ByteBuffer)": "/**\n* Releases the specified ByteBuffer if supported by the input object.\n* @param buffer the ByteBuffer to release\n* @throws UnsupportedOperationException if input does not support buffer release\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:setReadahead(java.lang.Long)": "/**\n* Sets the readahead caching strategy if supported.\n* @param readahead the readahead value to set\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if readahead cannot be set\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:setDropBehind(java.lang.Boolean)": "/**\n* Sets the drop-behind caching setting for the input stream.\n* @param dropCache true to enable, false to disable caching\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if the stream does not support this operation\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getFileDescriptor()": "/**\n* Retrieves the file descriptor from the input stream.\n* @return FileDescriptor or null if not applicable\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:cleanDecryptorPool()": "/**\n* Clears the decryptor pool to free up resources.\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getCounter(long)": "/**\n* Calculates the counter value based on position and algorithm block size.\n* @param position the input position to calculate the counter\n* @return the computed counter value\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getPadding(long)": "/**\n* Calculates padding based on the given position.\n* @param position the current position in the data stream\n* @return padding byte for the specified position\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String)": "/**\n* Checks if the given capability is supported.\n* @param capability the capability to check\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)": "/**\n* Decrypts data from inBuffer to outBuffer using the specified decryptor.\n* @param decryptor the decryptor used for decryption\n* @param inBuffer input buffer containing encrypted data\n* @param outBuffer output buffer for decrypted data\n* @param padding number of padding bytes to skip in the output\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getPos()": "/**\n* Returns the current position in the stream.\n* @return long position offset from the stream\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:available()": "/**\n* Returns the number of available bytes to read.\n* @return total available bytes from input and output buffers\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer)": "/**\n* Reads data from the underlying stream into the provided ByteBuffer.\n* @param inBuffer buffer to store read data\n* @return number of bytes read, or -1 if end of stream\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics using the input object.\n* @return IOStatistics object or null if input is invalid\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])": "/**\n* Updates the decryptor with a new initialization vector based on position.\n* @param decryptor the Decryptor to initialize\n* @param position the input position for counter calculation\n* @param iv the byte array for initialization vector\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool()": "/**\n* Cleans up the buffer pool by freeing each ByteBuffer.\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])": "/**\n* Handles decryption context and calculates padding.\n* @param decryptor the Decryptor instance\n* @param inBuffer input ByteBuffer for data\n* @param position current position in the data stream\n* @param iv initialization vector for decryption\n* @return padding byte for the specified position\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long)": "/**\n* Resets stream offset and buffers, updating decryptor and calculating padding.\n* @param offset new stream offset value\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:freeBuffers()": "/**\n* Frees input and output buffers, and cleans up the buffer pool.\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:unbuffer()": "/**\n* Unbuffers resources and cleans up buffer and decryptor pools.\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)": "/**\n* Reads bytes into the provided array, decrypting data from the stream.\n* @param b byte array to store read data\n* @param off offset in the array to start storing data\n* @param len maximum number of bytes to read\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)": "/**\n* Decrypts data from buffer using a specified position and updates the output buffer.\n* @param position data stream position for decryption\n* @param buffer byte array for input and output data\n* @param offset starting index in the buffer\n* @param length amount of data to decrypt\n* @throws IOException if decryption fails\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)": "/**\n* Decrypts data from a ByteBuffer using a specified file position and updates the buffer.\n* @param filePosition position in the file for decryption\n* @param buf ByteBuffer containing encrypted data\n* @param length number of bytes to decrypt\n* @param start starting position in the buffer\n* @throws IOException if decryption fails\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)": "/**\n* Decrypts data from a ByteBuffer and writes to an output buffer.\n* @param buf input buffer with encrypted data\n* @param length number of bytes to decrypt\n* @param start starting position in the input buffer\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)": "/**\n* Initializes CryptoInputStream with codec and buffer settings.\n* @param in input stream, @param codec encryption codec, @param bufferSize size of buffer, \n* @param key encryption key, @param iv initialization vector, @param streamOffset stream position\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:seek(long)": "/**\n* Seeks to a specified position in the stream.\n* @param pos target position in the stream\n* @throws IOException if an I/O error occurs or stream is not seekable\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:skip(long)": "/**\n* Skips specified bytes in the stream, updating position and offset.\n* @param n number of bytes to skip; must be non-negative\n* @return total bytes skipped\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long)": "/**\n* Seeks to a new source position in the stream.\n* @param targetPos new position to seek to; must be non-negative\n* @return true if successful, otherwise throws IOException\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:close()": "/**\n* Closes the resource, freeing buffers and marking it as closed.\n* @throws IOException if an error occurs during closing\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:read()": "/**\n* Reads a single byte, returning -1 if end of stream is reached.\n* @return byte value or -1 if end of stream\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)": "/**\n* Reads data from a position in the stream and decrypts it.\n* @param position data stream position to read from\n* @param buffer byte array for input and output data\n* @param offset starting index in the buffer\n* @param length amount of data to read\n* @return number of bytes read\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)": "/**\n* Reads data from a specified position into a buffer.\n* @param position data stream position to read from\n* @param buffer byte array for input and output data\n* @param offset starting index in the buffer\n* @param length amount of data to read\n* @throws IOException if stream is closed or read fails\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)": "/**\n* Reads bytes from a position into a ByteBuffer and decrypts if successful.\n* @param position the file position to read from\n* @param buf the ByteBuffer to store read data\n* @return number of bytes read, or 0 if none\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)": "/**\n* Reads data into a ByteBuffer from a specified position and decrypts if necessary.\n* @param position file position to read from\n* @param buf ByteBuffer to store the read data\n* @throws IOException if stream is closed or read fails\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer)": "/**\n* Reads bytes from a ByteBuffer, decrypting if necessary.\n* @param buf the ByteBuffer to fill with read data\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)": "/**\n* Reads decrypted data into a ByteBuffer.\n* @param bufferPool buffer pool for allocation\n* @param maxLength maximum bytes to read\n* @param opts read options\n* @return ByteBuffer containing decrypted data\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])": "/**\n* Constructs CryptoInputStream with specified codec and buffer settings.\n* @param in input stream, @param codec encryption codec, @param bufferSize size of buffer,\n* @param key encryption key, @param iv initialization vector\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])": "/**\n* Reads data from a specified position into the provided buffer.\n* @param position data stream position to read from\n* @param buffer byte array for input and output data\n* @throws IOException if stream is closed or read fails\n*/",
        "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])": "/**\n* Constructs CryptoInputStream with specified input, codec, key, and IV.\n* @param in input stream, @param codec encryption codec, @param key encryption key, @param iv initialization vector\n*/"
    },
    "org.apache.hadoop.crypto.CryptoOutputStream": {
        "org.apache.hadoop.crypto.CryptoOutputStream:checkStream()": "/**\n* Validates if the stream is open; throws IOException if closed.\n* @throws IOException if the stream is closed\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:getTmpBuf()": "/**\n* Retrieves a temporary byte buffer, initializing it if not already created.\n* @return byte array of temporary buffer\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:setDropBehind(java.lang.Boolean)": "/**\n* Sets drop-behind caching for the output stream.\n* @param dropCache true to enable, false to disable\n* @throws IOException if an I/O error occurs\n* @throws UnsupportedOperationException if the operation is not supported\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor()": "/**\n* Updates the encryptor's state using stream offset and initializes it with IV.\n* @throws IOException if an I/O error occurs during initialization\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics()": "/**\n* Retrieves IOStatistics from the output object.\n* @return IOStatistics object or null if invalid output\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:encrypt()": "/**\n* Encrypts data from inBuffer to outBuffer, handling padding and stream offsets.\n* @throws IOException if an I/O error occurs during encryption\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String)": "/**\n* Checks if the OutputStream has a specific capability.\n* @param capability the capability name to verify\n* @return true if the capability exists, false otherwise\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers()": "/**\n* Frees input and output buffers to release resources.\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)": "/**\n* Initializes CryptoOutputStream with encryption parameters and buffers.\n* @param out output stream, @param codec encryption codec, @param bufferSize buffer size,\n* @param key encryption key, @param iv initialization vector, @param streamOffset offset,\n* @param closeOutputStream flag to close output stream\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)": "/**\n* Writes data from byte array to buffer, encrypting when necessary.\n* @param b byte array to write from, @param off start offset, @param len number of bytes to write\n* @throws IOException if the stream is closed or an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:flush()": "/**\n* Flushes the output stream after encrypting data.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)": "/**\n* Constructs CryptoOutputStream with specified encryption parameters.\n* @param out output stream, @param codec encryption codec, @param bufferSize buffer size,\n* @param key encryption key, @param iv initialization vector, @param streamOffset offset\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:write(int)": "/**\n* Writes a single byte to the output, using a buffer.\n* @param b the byte to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:close()": "/**\n* Closes the stream, flushing data and releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:hflush()": "/**\n* Flushes output stream and syncs if it's Syncable.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:hsync()": "/**\n* Synchronizes the output stream and flushes data.\n* @throws IOException if an I/O error occurs during flushing\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])": "/**\n* Initializes CryptoOutputStream with encryption parameters.\n* @param out output stream, @param codec encryption codec, \n* @param bufferSize buffer size, @param key encryption key, \n* @param iv initialization vector\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)": "/**\n* Constructs CryptoOutputStream with encryption parameters.\n* @param out output stream, @param codec encryption codec, @param key encryption key,\n* @param iv initialization vector, @param streamOffset offset, @param closeOutputStream flag\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)": "/**\n* Initializes CryptoOutputStream with encryption parameters.\n* @param out output stream, @param codec encryption codec, \n* @param key encryption key, @param iv initialization vector,\n* @param streamOffset offset for the stream\n*/",
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])": "/**** Initializes CryptoOutputStream with encryption parameters. \n* @param out output stream, @param codec encryption codec, \n* @param key encryption key, @param iv initialization vector \n*/"
    },
    "org.apache.hadoop.crypto.OpensslCipher$Padding": {
        "org.apache.hadoop.crypto.OpensslCipher$Padding:get(java.lang.String)": "/**\n* Retrieves the ordinal of the specified padding type.\n* @param padding name of the padding type\n* @return ordinal of the padding type\n* @throws NoSuchPaddingException if padding type is unsupported\n*/"
    },
    "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec": {
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:<init>()": "/**\n* Constructs a new instance of JceSm4CtrCryptoCodec.\n*/",
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:getCipherSuite()": "/**\n* Returns the SM4 CTR No Padding cipher suite.\n* @return CipherSuite instance for SM4 CTR No Padding\n*/",
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:getLogger()": "/**\n* Retrieves the logger instance.\n* @return Logger object used for logging\n*/",
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])": "/**\n* Calculates the IV using initial bytes and counter, delegating to super with block size.\n* @param initIV initial IV bytes; must match blockSize\n* @param counter counter for IV calculation\n* @param iv output IV array; must match blockSize\n*/",
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor()": "/**\n* Creates an Encryptor instance for SM4 encryption.\n* @return Encryptor object for encryption operations\n*/",
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor()": "/**\n* Creates a Decryptor using SM4 cipher in decrypt mode.\n* @return Decryptor instance for decryption\n*/"
    },
    "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher": {
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Processes input data with cryptographic cipher and outputs to outBuffer.\n* @param inBuffer input data buffer\n* @param outBuffer output data buffer\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:isContextReset()": "/**\n* Checks if the context has been reset.\n* @return true if context is reset, false otherwise\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)": "/**\n* Initializes JceCtrCipher with mode, provider, suite, and name.\n* @param mode operation mode; @param provider security provider; \n* @param suite cipher suite; @param name cipher name\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Encrypts data from inBuffer and writes to outBuffer.\n* @param inBuffer input data buffer\n* @param outBuffer output data buffer\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decrypts input data and writes the result to output buffer.\n* @param inBuffer encrypted data buffer\n* @param outBuffer buffer for decrypted output data\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])": "/**\n* Initializes cipher with key and IV.\n* @param key byte array for the secret key\n* @param iv byte array for the initialization vector\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.crypto.OpensslCtrCryptoCodec": {
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:generateSecureRandom(byte[])": "/**\n* Fills the provided byte array with secure random bytes.\n* @param bytes array to be filled with random data\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getEngineId()": "/**\n* Retrieves the engine ID.\n* @return engineId as a String\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getRandom()": "/**\n* Returns the Random instance used for generating random values.\n* @return Random object for random number generation\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setEngineId(java.lang.String)": "/**\n* Sets the engine identifier.\n* @param engineId unique identifier for the engine\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setRandom(java.util.Random)": "/**\n* Sets the Random instance for generating random values.\n* @param random Random object to be assigned\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close()": "/**\n* Closes the random resource if it is Closeable and logs any exceptions.\n* @throws IOException if an I/O error occurs during closure\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)": "/**\n* Calculates the initialization vector (IV) based on input and counter.\n* @param initIV initial IV bytes, must match blockSize\n* @param counter counter value for IV calculation\n* @param iv output IV array, must match blockSize\n* @param blockSize size of the IV in bytes\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and initializes random number generator.\n* @param conf configuration to apply for random generator\n*/"
    },
    "org.apache.hadoop.crypto.CryptoProtocolVersion": {
        "org.apache.hadoop.crypto.CryptoProtocolVersion:getVersion()": "/**\n* Retrieves the current version number.\n* @return the version as an integer\n*/",
        "org.apache.hadoop.crypto.CryptoProtocolVersion:setUnknownValue(int)": "/**\n* Sets the value of unknownValue.\n* @param unknown the value to set for unknownValue\n*/",
        "org.apache.hadoop.crypto.CryptoProtocolVersion:getUnknownValue()": "/**\n* Returns the value of unknownValue.\n* @return the current unknownValue as an integer\n*/",
        "org.apache.hadoop.crypto.CryptoProtocolVersion:toString()": "/**\n* Returns a string representation of CryptoProtocolVersion details.\n* @return formatted string of description, version, and unknownValue\n*/",
        "org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion)": "/**\n* Checks if the given protocol version is supported.\n* @param version the CryptoProtocolVersion to check\n* @return true if supported, false if unknown or not found\n*/"
    },
    "org.apache.hadoop.crypto.JceCtrCryptoCodec": {
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:setProvider(java.lang.String)": "/**\n* Sets the provider name.\n* @param provider the name of the provider to set\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:generateSecureRandom(byte[])": "/**\n* Fills the provided byte array with secure random bytes.\n* @param bytes array to be filled with random data\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:close()": "/**\n* Closes the resource, releasing any associated system resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:getProvider()": "/**\n* Retrieves the provider name.\n* @return provider name as a String\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)": "/**\n* Calculates the initialization vector (IV) based on input parameters.\n* @param initIV initial IV bytes; must match blockSize\n* @param counter counter for IV calculation\n* @param iv output IV array; must match blockSize\n* @param blockSize size of the IV in bytes\n*/",
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Configures security settings using the provided Configuration object.\n* @param conf configuration settings for security\n*/"
    },
    "org.apache.hadoop.crypto.JceAesCtrCryptoCodec": {
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:<init>()": "/**\n* Constructs a new instance of JceAesCtrCryptoCodec.\n*/",
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:getCipherSuite()": "/**\n* Returns the cipher suite used for encryption.\n* @return CipherSuite constant for AES in CTR mode with no padding\n*/",
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:getLogger()": "/**\n* Returns the logger instance for the class.\n* @return Logger instance used for logging\n*/",
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])": "/**\n* Computes the initialization vector (IV) using provided parameters.\n* @param initIV initial IV bytes; must match blockSize\n* @param counter counter for IV calculation\n* @param iv output IV array; must match blockSize\n*/",
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor()": "/**\n* Creates an Encryptor instance for AES encryption.\n* @return Encryptor object configured for AES mode\n*/",
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor()": "/**\n* Creates a decryptor using AES in CTR mode.\n* @return Decryptor instance for decryption operations\n*/"
    },
    "org.apache.hadoop.crypto.CipherOption": {
        "org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite,byte[],byte[],byte[],byte[])": "/**\n* Initializes CipherOption with specified cipher suite and keys/IVs.\n* @param suite encryption cipher suite\n* @param inKey input encryption key\n* @param inIv input initialization vector\n* @param outKey output encryption key\n* @param outIv output initialization vector\n*/",
        "org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite)": "/**** Initializes CipherOption with specified cipher suite. \n* @param suite encryption cipher suite \n*/"
    },
    "org.apache.hadoop.crypto.OpensslCipher$Transform": {
        "org.apache.hadoop.crypto.OpensslCipher$Transform:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Initializes a Transform object with algorithm, mode, and padding.\n* @param alg the encryption algorithm\n* @param mode the operation mode\n* @param padding the padding scheme used\n*/"
    },
    "org.apache.hadoop.crypto.random.OpensslSecureRandom": {
        "org.apache.hadoop.crypto.random.OpensslSecureRandom:<init>()": "/**\n* Initializes OpensslSecureRandom; falls back to Java SecureRandom if OpenSSL is not supported.\n*/",
        "org.apache.hadoop.crypto.random.OpensslSecureRandom:nextBytes(byte[])": "/**\n* Fills the byte array with random bytes.\n* @param bytes array to be filled with random data\n*/",
        "org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int)": "/**\n* Generates an integer from random bytes based on the specified bit count.\n* @param numBits number of bits to generate (0-32)\n* @return generated integer value\n*/"
    },
    "org.apache.hadoop.crypto.random.OsSecureRandom": {
        "org.apache.hadoop.crypto.random.OsSecureRandom:<init>()": "/**\n* Constructs an instance of OsSecureRandom.\n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:getConf()": "/**\n* Retrieves the current configuration object.\n* @return Configuration instance representing the current settings\n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:close()": "/**\n* Closes the stream and logs any exceptions during cleanup.\n* @param stream the stream to be closed\n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int)": "/**** Fills the reservoir from an InputStream if space is insufficient. \n* @param min minimum space required in the reservoir \n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:finalize()": "/**\n* Finalizes the object and closes the stream.\n* @throws Throwable if an error occurs during finalization\n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[])": "/**\n* Fills byte array with random bytes from the reservoir.\n* @param bytes array to be filled with random bytes\n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:next(int)": "/**\n* Generates the next integer with specified bit count.\n* @param nbits number of bits to generate (up to 32)\n* @return generated integer value\n*/",
        "org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration and updates the random device path.\n* @param conf the configuration object to set\n*/"
    },
    "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec": {
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:getCipherSuite()": "/**\n* Returns the cipher suite used for encryption.\n* @return CipherSuite constant for AES with CTR mode and no padding\n*/",
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:getLogger()": "/**\n* Returns the logger instance for this class.\n* @return Logger object used for logging\n*/",
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>()": "/**\n* Initializes the codec and checks for loading failures.\n* @throws RuntimeException if loading fails with a reason\n*/",
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])": "/**\n* Computes the initialization vector (IV) using the provided initial IV and counter.\n* @param initIV initial IV bytes, must match block size\n* @param counter counter value for IV calculation\n* @param iv output IV array, must match block size\n*/",
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor()": "/**\n* Creates an Encryptor instance using AES CTR mode.\n* @return Encryptor object configured for encryption\n* @throws GeneralSecurityException for security-related issues\n*/",
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor()": "/**\n* Creates a decryptor using OpenSSL CTR mode.\n* @return Decryptor instance for decryption\n* @throws GeneralSecurityException for security-related issues\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion": {
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:<init>(java.lang.String,java.lang.String,byte[])": "/**\n* Constructs a KeyVersion instance with specified name, version, and material.\n* @param name identifier for the key\n* @param versionName version of the key\n* @param material byte array containing key material\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:toString()": "/**\n* Returns a string representation of the object with version and material in hex.\n* @return formatted string of versionName and material bytes\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:equals(java.lang.Object)": "/**\n* Compares this KeyVersion object with another for equality.\n* @param rhs object to compare with this instance\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:hashCode()": "/**\n* Computes the hash code based on name, versionName, and material array.\n* @return calculated hash code as an integer\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getMaterial()": "/**\n* Retrieves the material data as a byte array.\n* @return byte array representing the material\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getName()": "/**\n* Retrieves the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getVersionName()": "/**\n* Retrieves the current version name.\n* @return String representing the version name\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension": {
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension)": "/**\n* Initializes KeyProviderCryptoExtension with specified key provider and crypto extension.\n* @param keyProvider the key provider to use\n* @param extension the crypto extension for additional functionality\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:warmUpEncryptedKeys(java.lang.String[])": "/**\n* Warms up encrypted keys specified by their names.\n* @param keyNames array of key names to warm up\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:generateEncryptedKey(java.lang.String)": "/**\n* Generates an encrypted key using the specified encryption key name.\n* @param encryptionKeyName name of the encryption key\n* @return EncryptedKeyVersion object\n* @throws IOException if an I/O error occurs\n* @throws GeneralSecurityException if a security error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Decrypts the given encrypted key version.\n* @param encryptedKey the encrypted key to decrypt\n* @return the decrypted KeyVersion object\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Reencrypts the given encrypted key version.\n* @param ekv the encrypted key version to reencrypt\n* @return the reencrypted EncryptedKeyVersion\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:drain(java.lang.String)": "/**\n* Drains resources associated with the specified key name.\n* @param keyName identifier for the resource to drain\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:reencryptEncryptedKeys(java.util.List)": "/**\n* Reencrypts a list of encrypted key versions.\n* @param ekvs list of EncryptedKeyVersion objects to reencrypt\n* @throws IOException if an I/O error occurs\n* @throws GeneralSecurityException if a security error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider)": "/**\n* Creates KeyProviderCryptoExtension from a KeyProvider.\n* @param keyProvider the key provider for the extension\n* @return KeyProviderCryptoExtension instance\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close()": "/**\n* Closes the key provider if it's not null or the current instance.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderExtension": {
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyProvider()": "/**\n* Retrieves the KeyProvider instance.\n* @return KeyProvider object associated with this instance\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyVersion(java.lang.String)": "/**\n* Retrieves the KeyVersion associated with the specified version name.\n* @param versionName the name of the key version\n* @return KeyVersion object corresponding to the version name\n* @throws IOException if an I/O error occurs during retrieval\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeys()": "/**\n* Retrieves a list of keys from the key provider.\n* @return List of keys as strings\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyVersions(java.lang.String)": "/**\n* Retrieves key versions by name.\n* @param name the name of the key\n* @return a list of KeyVersion objects\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getMetadata(java.lang.String)": "/**\n* Retrieves metadata for a given name.\n* @param name the identifier for the metadata\n* @return Metadata object associated with the name\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a new key with specified name and material.\n* @param name key name, @param material key data, @param options creation options\n* @return KeyVersion object representing the created key\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:deleteKey(java.lang.String)": "/**\n* Deletes a key by its name.\n* @param name the name of the key to delete\n* @throws IOException if an I/O error occurs during deletion\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String,byte[])": "/**\n* Rolls a new version of a key with specified material.\n* @param name the key name\n* @param material the byte array for the new key version\n* @return KeyVersion object representing the new version\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:flush()": "/**\n* Flushes the key provider to ensure all data is written.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:toString()": "/**\n* Returns a string representation of the object with its class name and key provider.\n* @return formatted string of class name and key provider\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getExtension()": "/**\n* Retrieves the current extension.\n* @return the extension of type E\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[])": "/**\n* Retrieves metadata for specified keys.\n* @param names variable-length array of key names\n* @return array of Metadata objects corresponding to the keys\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String)": "/**\n* Invalidates the cache for the specified name.\n* @param name the identifier for the cache to invalidate\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient()": "/**\n* Determines if the object is transient by checking the key provider's status.\n* @return true if transient, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String)": "/**\n* Retrieves the current key version by name.\n* @param name base name for the key\n* @return KeyVersion object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a cryptographic key with the given name and options.\n* @param name key name\n* @param options configuration options for key generation\n* @return KeyVersion object representing the created key\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String)": "/**\n* Rolls a new key version using the specified name.\n* @param name the key name\n* @return KeyVersion object\n* @throws NoSuchAlgorithmException if the algorithm is not found\n* @throws IOException if metadata is missing\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProvider": {
        "org.apache.hadoop.crypto.key.KeyProvider:close()": "/**\n* Closes the resource; does nothing and may throw IOException.\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:getKeysMetadata(java.lang.String[])": "/**\n* Retrieves metadata for specified keys.\n* @param names variable-length array of key names\n* @return array of Metadata objects corresponding to the keys\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:buildVersionName(java.lang.String,int)": "/**\n* Constructs a versioned name string.\n* @param name base name string\n* @param version version number\n* @return formatted string in \"name@version\" format\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:getAlgorithm(java.lang.String)": "/**\n* Extracts the algorithm name from a cipher string.\n* @param cipher the cipher string to parse\n* @return the algorithm name or the full cipher if no '/' is found\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:getBaseName(java.lang.String)": "/**\n* Extracts the base name from a version string.\n* @param versionName the version string containing a '@' character\n* @return base name before the '@' or throws IOException if not found\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:findProvider(java.util.List,java.lang.String)": "/**\n* Finds a KeyProvider by key name from a list.\n* @param providerList list of KeyProviders\n* @param keyName name of the key to search for\n* @return KeyProvider with the specified key\n* @throws IOException if no KeyProvider is found\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:invalidateCache(java.lang.String)": "/**\n* Invalidates the cache for the specified name.\n* @param name the identifier for the cache to invalidate\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object containing settings\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:needsPassword()": "/**\n* Determines if a password is required.\n* @return false indicating no password is needed\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:noPasswordError()": "/**\n* Returns null indicating a no password error.\n* @return null to signify absence of a password\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:noPasswordWarning()": "/**\n* Returns a warning message for missing password.\n* @return always null\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:isTransient()": "/**\n* Indicates if the object is transient.\n* @return false, as the object is not transient\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String)": "/**\n* Retrieves the current key version by name.\n* @param name base name for the key\n* @return KeyVersion object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)": "/**\n* Generates a cryptographic key of specified size and algorithm.\n* @param size key size in bits\n* @param algorithm cipher algorithm name\n* @return byte array representing the generated key\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a cryptographic key using specified name and options.\n* @param name key name\n* @param options configuration options for key generation\n* @return KeyVersion object representing the created key\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String)": "/**\n* Rolls a new key version using specified name.\n* @param name the key name\n* @return KeyVersion object\n* @throws NoSuchAlgorithmException if the algorithm is not found\n* @throws IOException if metadata is missing\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes KeyProvider with configuration and sets security properties.\n* @param conf Configuration object for key management\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration)": "/**\n* Creates an Options instance using the provided configuration.\n* @param conf configuration object for Options initialization\n* @return Options object initialized with the given configuration\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProvider$Metadata": {
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:<init>(byte[])": "/**\n* Initializes Metadata from byte array JSON.\n* @param bytes JSON byte array representing metadata\n* @throws IOException if reading from the input fails\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)": "/**\n* Constructs a Metadata object with encryption details and attributes.\n* @param cipher encryption algorithm name\n* @param bitLength key size in bits\n* @param description metadata description\n* @param attributes optional key-value pairs\n* @param created creation timestamp\n* @param versions number of versions available\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:serialize()": "/**\n* Serializes object to JSON byte array.\n* @return byte array representation of the object\n* @throws IOException if an I/O error occurs during serialization\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getVersions()": "/**\n* Retrieves the current version count.\n* @return the number of versions\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getBitLength()": "/**\n* Retrieves the bit length value.\n* @return the current bit length as an integer\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:addVersion()": "/**\n* Increments and returns the current version number.\n* @return the updated version number\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:toString()": "/**\n* Returns a string representation of the objects metadata.\n* @return formatted metadata string including cipher, length, description, and attributes\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getAttributes()": "/**\n* Returns a map of attributes or an empty map if none exist.\n* @return Map of attributes\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getAlgorithm()": "/**\n* Retrieves the algorithm name from the cipher string.\n* @return algorithm name or the full cipher if no '/' is present\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getCipher()": "/**\n* Retrieves the current cipher string.\n* @return the cipher string\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getCreated()": "/**\n* Retrieves the creation date of the object.\n* @return Date representing the creation timestamp\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getDescription()": "/**\n* Retrieves the description string.\n* @return the description of the object\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProvider$Options": {
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getBitLength()": "/**\n* Returns the bit length of the current object.\n* @return the number of bits used to represent the value\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getCipher()": "/**\n* Retrieves the current cipher string.\n* @return the cipher string\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getDescription()": "/**\n* Retrieves the description of the object.\n* @return String representing the object's description\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getAttributes()": "/**\n* Returns a map of attributes or an empty map if none exist.\n* @return Map of attribute key-value pairs\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setBitLength(int)": "/**\n* Sets the bit length and returns the updated Options instance.\n* @param bitLength the desired bit length\n* @return the updated Options object\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setCipher(java.lang.String)": "/**\n* Sets the cipher and returns the current Options instance.\n* @param cipher the cipher string to set\n* @return the updated Options instance\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setDescription(java.lang.String)": "/**\n* Sets the description and returns the updated Options instance.\n* @param description the new description to set\n* @return the updated Options object\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setAttributes(java.util.Map)": "/**\n* Sets attributes from a map and returns the updated Options object.\n* @param attributes map of string key-value pairs\n* @return updated Options instance\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:toString()": "/**\n* Returns a string representation of the Options object.\n* @return formatted string with cipher, bitLength, description, and attributes\n*/",
        "org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Options with configuration values for cipher and bit length.\n* @param conf configuration object containing settings\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell": {
        "org.apache.hadoop.crypto.key.KeyShell:getCommandUsage()": "/**\n* Generates command usage information as a formatted string.\n* @return formatted string of command usages and descriptions\n*/",
        "org.apache.hadoop.crypto.key.KeyShell:prettifyException(java.lang.Exception)": "/**\n* Formats exception message by class name and first line of localized message.\n* @param e the Exception to format\n* @return formatted string representation of the exception\n*/",
        "org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception)": "/**\n* Prints error message for command execution failure.\n* @param e the Exception encountered during execution\n*/",
        "org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[])": "/**\n* Initializes options and parses command line arguments.\n* @param args command line arguments\n* @return 0 on success, 1 for help or error\n*/",
        "org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[])": "/**\n* Executes the KeyShell tool with provided arguments.\n* @param args command-line arguments for the tool\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand": {
        "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute()": "/**\n* Deletes a key from the KeyProvider if conditions are met.\n* @throws IOException if deletion fails or provider error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:getUsage()": "/**\n* Retrieves usage information string.\n* @return concatenated usage and description strings\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate()": "/**\n* Validates key deletion with user confirmation and checks key provider existence.\n* @return true if valid, false otherwise\n*/"
    },
    "org.apache.hadoop.crypto.key.JavaKeyStoreProvider": {
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:isBadorWrongPassword(java.io.IOException)": "/**\n* Determines if the IOException indicates a bad or wrong password.\n* @param ioe the IOException to analyze\n* @return true if the password is incorrect, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:renameOrFail(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory; throws IOException if unsuccessful.\n* @param src source path to rename\n* @param dest destination path for the new name\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeys()": "/**\n* Retrieves a list of key aliases excluding metadata keys.\n* @return List of key aliases\n* @throws IOException if key retrieval fails\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getMetadata(java.lang.String)": "/**\n* Retrieves metadata for a given key name from the keystore.\n* @param name the key name to fetch metadata for\n* @return Metadata object or null if not found\n* @throws IOException if an error occurs during retrieval\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:toString()": "/**\n* Returns the string representation of the URI object.\n* @return string representation of the URI\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String)": "/**\n* Retrieves a KeyVersion by its version name.\n* @param versionName the name of the key version\n* @return KeyVersion object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)": "/**\n* Stores a key entry in the keystore and returns a KeyVersion instance.\n* @param name identifier for the key\n* @param versionName version of the key\n* @param material byte array containing key material\n* @param cipher algorithm used for the key\n* @return KeyVersion object representing the stored key\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Cleans up old and new paths by renaming and deleting.\n* @param newPath new file path to rename\n* @param oldPath old file path to delete\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path)": "/**\n* Backs up the current path to an old path.\n* @param oldPath destination for the backup\n* @return true if successful, false if the file was not found\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)": "/**\n* Reverts to an old file path if it existed.\n* @param oldPath the previous file path to revert to\n* @param fileExisted indicates if the file was present\n* @throws IOException if renaming fails\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String)": "/**\n* Deletes a key and its versions from the keystore.\n* @param name the key name to delete\n* @throws IOException if the key does not exist or removal fails\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning()": "/**\n* Generates a no-password warning message.\n* @return formatted warning string based on environment and file keys\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError()": "/**\n* Generates a no-password error message for keystore configuration.\n* @return formatted no-password error message\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String)": "/**\n* Retrieves key versions for a specified key name.\n* @param name the key name to fetch versions for\n* @return list of KeyVersion objects\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a key with specified name and material.\n* @param name key identifier; must be lowercase\n* @param material byte array containing key material\n* @return KeyVersion object representing the created key\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])": "/**\n* Rolls a new key version, validating length and storing it in the keystore.\n* @param name key identifier\n* @param material byte array containing key material\n* @return KeyVersion object for the new key version\n* @throws IOException if key not found or length mismatch\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path)": "/**\n* Constructs a new Path by appending \"_NEW\" to the original Path.\n* @param path the original Path object\n* @return a new Path object with \"_NEW\" appended\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path)": "/**\n* Constructs a new Path by appending \"_OLD\" to the string representation of the given Path.\n* @param path the original Path object\n* @return a new Path object with \"_OLD\" suffix\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword()": "/**** Checks if a password is needed for the keystore.\\n@return true if password is absent, false otherwise */",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider)": "/**\n* Clones a JavaKeyStoreProvider instance.\n* @param other source JavaKeyStoreProvider to copy from\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])": "/**\n* Loads file permissions from a specified path using a password.\n* @param p file path to load permissions from\n* @param password password for keyStore access\n* @return FsPermission object for the file\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Loads file permissions from a path, using a backup if the primary fails.\n* @param path primary file path; @param backupPath backup file path\n* @return FsPermission object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Loads permissions from a path and deletes a specified file.\n* @param pathToLoad path to load permissions from\n* @param pathToDelete path of the file to delete\n* @return FsPermission object loaded from the path\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path)": "/**\n* Resets the key store state by clearing cache and reloading from a specified path.\n* @param path the path to load the key store from\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Attempts to load file permissions from specified paths, creating a default if absent.\n* @param oldPath path to the old file\n* @param newPath path to the new file\n* @return FsPermission object for the file permissions\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore()": "/****\n* Locates keystore and manages password and permissions.\n* @throws IOException if keystore loading fails or inconsistencies found\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path)": "/**\n* Writes the keystore to a new path.\n* @param newPath destination path for the keystore\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush()": "/**\n* Flushes updates to the keystore, handling backups and renaming paths.\n* @throws IOException if an I/O error occurs during the process\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes JavaKeyStoreProvider with URI and configuration.\n* @param uri keystore URI\n* @param conf configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$Factory": {
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a KeyProvider based on the URI scheme.\n* @param providerName URI of the provider\n* @param conf configuration settings\n* @return KeyProvider instance or null if scheme is unsupported\n*/"
    },
    "org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension": {
        "org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)": "/**\n* Initializes caches for key versions and metadata with timeouts.\n* @param prov KeyProvider for retrieving key data\n* @param keyTimeoutMillis Timeout for key caches\n* @param currKeyTimeoutMillis Timeout for current key cache\n*/"
    },
    "org.apache.hadoop.crypto.key.CachingKeyProvider": {
        "org.apache.hadoop.crypto.key.CachingKeyProvider:getCurrentKey(java.lang.String)": "/**\n* Retrieves the current key by name.\n* @param name the key identifier\n* @return KeyVersion or null if not found; throws IOException on error\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:getKeyVersion(java.lang.String)": "/**\n* Retrieves the KeyVersion by its name.\n* @param versionName the name of the KeyVersion\n* @return KeyVersion object or null if not found\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:deleteKey(java.lang.String)": "/**\n* Deletes a key by name and invalidates related caches.\n* @param name the name of the key to delete\n* @throws IOException if an I/O error occurs during deletion\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:getMetadata(java.lang.String)": "/**\n* Retrieves metadata by name, handling key not found and IO exceptions.\n* @param name the name of the metadata\n* @return Metadata object or null if not found\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)": "/**\n* Constructs a CachingKeyProvider with specified timeouts for key caching.\n* @param keyProvider source for key data\n* @param keyTimeoutMillis timeout for key caches\n* @param currKeyTimeoutMillis timeout for current key cache\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String)": "/**\n* Invalidates caches for the specified key name, including all versions.\n* @param name the identifier for the cache to invalidate\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])": "/**\n* Rolls a new version for the specified key and invalidates its cache.\n* @param name the identifier for the key\n* @param material the data for the new version\n* @return KeyVersion of the rolled version\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String)": "/**\n* Rolls a new key version and invalidates its cache.\n* @param name the key name for versioning\n* @return KeyVersion object\n* @throws NoSuchAlgorithmException if the algorithm is not found\n* @throws IOException if metadata is missing\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension": {
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider)": "/**\n* Initializes DefaultCryptoExtension with a KeyProvider.\n* @param keyProvider provides cryptographic keys\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:drain(java.lang.String)": "/**\n* No operation method for draining cache by key name.\n* @param keyName the name of the key to drain (ignored)\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:warmUpEncryptedKeys(java.lang.String[])": "/**\n* No operation method for warming up encrypted keys; does nothing.\n* @param keyNames variable number of key names to warm up\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])": "/**\n* Generates an EncryptedKeyVersion from provided key and IV.\n* @param encryptor encryption utility\n* @param encryptionKey key version for encryption\n* @param key raw key data\n* @param iv initialization vector for encryption\n* @return EncryptedKeyVersion object\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Decrypts an encrypted key using a specified decryptor and key versions.\n* @param decryptor used for decryption process\n* @param encryptionKey key for decryption\n* @param encryptedKeyVersion contains the encrypted key data\n* @return decrypted KeyVersion instance\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String)": "/**** Generates an encrypted key version using the specified encryption key name. \n* @param encryptionKeyName name of the encryption key\n* @return EncryptedKeyVersion object\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List)": "/**\n* Re-encrypts a list of EncryptedKeyVersion objects.\n* @param ekvs list of encrypted key versions to re-encrypt\n* @throws IOException if an I/O error occurs\n* @throws GeneralSecurityException if a security error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Decrypts the provided encrypted key version.\n* @param encryptedKeyVersion the encrypted key to decrypt\n* @return decrypted KeyVersion instance\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Reencrypts an EncryptedKeyVersion using the current key version.\n* @param ekv the EncryptedKeyVersion to reencrypt\n* @return new EncryptedKeyVersion after re-encryption\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion": {
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:deriveIV(byte[])": "/**\n* Derives an IV by flipping bits of the given encrypted key IV.\n* @param encryptedKeyIV input byte array for transformation\n* @return derived byte array representing the new IV\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$KeyVersion)": "/**\n* Constructs an EncryptedKeyVersion with specified parameters.\n* @param keyName name of the encryption key\n* @param encryptionKeyVersionName version name of the encryption key\n* @param encryptedKeyIv initialization vector for encryption\n* @param encryptedKeyVersion associated key version object\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptionKeyName()": "/**\n* Retrieves the name of the encryption key.\n* @return the name of the encryption key as a String\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptedKeyVersion()": "/**\n* Retrieves the current encrypted key version.\n* @return KeyVersion object representing the encrypted key version\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptedKeyIv()": "/**\n* Retrieves the encrypted initialization vector.\n* @return byte array representing the encrypted IV\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptionKeyVersionName()": "/**\n* Retrieves the encryption key version name.\n* @return String representing the encryption key version name\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])": "/**\n* Creates an EncryptedKeyVersion for decryption.\n* @param keyName name of the encryption key\n* @param encryptionKeyVersionName version name of the encryption key\n* @param encryptedKeyIv initialization vector for encryption\n* @param encryptedKeyMaterial encrypted key material\n* @return EncryptedKeyVersion object\n*/"
    },
    "org.apache.hadoop.crypto.key.UserProvider$Factory": {
        "org.apache.hadoop.crypto.key.UserProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a KeyProvider based on the given URI and configuration.\n* @param providerName URI for the provider\n* @param conf configuration settings\n* @return KeyProvider instance or null if scheme does not match\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension": {
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension)": "/**\n* Constructs KeyProviderDelegationTokenExtension with specified key provider and extensions.\n* @param keyProvider the key provider instance\n* @param extensions delegation token extensions\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getCanonicalServiceName()": "/**\n* Retrieves the canonical service name from the extension.\n* @return canonical service name as a String\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getDelegationToken(java.lang.String)": "/**\n* Retrieves a delegation token for the specified renewer.\n* @param renewer the entity requesting the token\n* @return a Token object associated with the renewer\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider)": "/**\n* Creates KeyProviderDelegationTokenExtension from provided key provider.\n* @param keyProvider the key provider instance\n* @return KeyProviderDelegationTokenExtension object\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$WrapperException": {
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$WrapperException:<init>(java.lang.Throwable)": "/**\n* Constructs a WrapperException with the specified cause.\n* @param cause the throwable that caused this exception\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:toString()": "/**\n* Returns a string representation of the KMSClientProvider with its URL.\n* @return formatted string including KMS URL\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createURL(java.lang.String,java.lang.String,java.lang.String,java.util.Map)": "/**\n* Constructs a URL from given collection, resource, and parameters.\n* @param collection base collection path\n* @param resource specific resource path\n* @param subResource optional sub-resource path\n* @param parameters query parameters for the URL\n* @return constructed URL\n* @throws IOException if URL construction fails\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeySets(java.lang.String[])": "/**\n* Creates batches of key names, limited to 1500 characters each.\n* @param keyNames array of key names\n* @return list of String arrays containing batched key names\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createAuthenticatedURL()": "/**\n* Creates an authenticated URL for delegation token selection.\n* @return DelegationTokenAuthenticatedURL instance\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKMSUrl()": "/**\n* Retrieves the KMS URL as a string.\n* @return the KMS URL in string format\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:setClientTokenProvider(org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension)": "/**\n* Sets the client token provider.\n* @param provider delegation token extension provider\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:flush()": "/**\n* No operation for flush; no local state to clear for client or server.\n* @throws IOException if an I/O error occurs (not applicable here)\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection)": "/**\n* Configures SSL settings for the given HttpURLConnection.\n* @param conn HttpURLConnection to configure\n* @return configured HttpURLConnection instance\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)": "/**\n* Serializes an object to JSON and writes it to an output stream.\n* @param obj the object to serialize\n* @param os the output stream for JSON data\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[])": "/**\n* Initializes queues for encrypted keys and handles potential IOExceptions.\n* @param keyNames variable number of encrypted key names\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:close()": "/**\n* Closes resources, shutting down the queue and destroying the SSL factory.\n* @throws IOException if shutdown fails\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Generates a DelegationTokenAuthenticatedURL.Token from a given Token.\n* @param dToken the input token to generate the delegation token from\n* @return a DelegationTokenAuthenticatedURL.Token instance\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path)": "/**\n* Creates a service URL from a given path.\n* @param path file system path to convert\n* @return constructed service URL\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI)": "/**\n* Creates a Text object from a URI, excluding the fragment if present.\n* @param uri the URI to process\n* @return Text representation of the URI\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName()": "/**\n* Returns the canonical service name as a String.\n* @return String representation of the canonical service\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String)": "/**\n* Retrieves the size of the encryption key queue.\n* @param keyName the key for which to get the queue size\n* @return size of the encryption key queue\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String)": "/**\n* Clears tasks associated with the specified key.\n* @param keyName the key for identifying tasks to clear\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)": "/**\n* Selects a delegation token for the given service from credentials.\n* @param creds user credentials containing tokens\n* @param service service identifier for the token\n* @return selected Token object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String)": "/**\n* Generates an encrypted key version for the given key name.\n* @param encryptionKeyName identifier for the encryption key\n* @return EncryptedKeyVersion object\n* @throws IOException if an I/O error occurs\n* @throws GeneralSecurityException if a security error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI)": "/**\n* Extracts KMS path from a URI.\n* @param uri the URI to extract the path from\n* @return Path object representing the KMS path\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials)": "/**\n* Selects a delegation token from credentials for specific services.\n* @param creds user credentials containing tokens\n* @return selected Token object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Checks if KMS delegation token exists in user's credentials.\n* @param ugi user group information\n* @return true if token is found, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser()": "/**\n* Retrieves short username if current user is authenticated via proxy.\n* @return short username or null if not using proxy authentication\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi()": "/**\n* Retrieves the effective UserGroupInformation for the current user.\n* @return UserGroupInformation object representing the user\n* @throws IOException if an error occurs while fetching user info\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)": "/**\n* Creates and configures an HTTP connection.\n* @param url the target URL for the connection\n* @param method HTTP method (GET, POST, etc.)\n* @return configured HttpURLConnection instance\n* @throws IOException if a connection error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String)": "/**\n* Retrieves a new delegation token for the specified renewer.\n* @param renewer the entity requesting the token\n* @return Token object or throws IOException if creation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Renews a delegation token using the provided token.\n* @param dToken the token to renew\n* @return renewed token expiration time\n* @throws IOException if an error occurs during renewal\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Cancels a delegation token using the provided token.\n* @param dToken the token to be cancelled\n* @throws IOException if cancellation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)": "/**\n* Executes an HTTP call with JSON output and handles authentication retries.\n* @param conn configured HTTP connection\n* @param jsonOutput JSON data to send\n* @param expectedResponse expected HTTP status code\n* @param klass class type for response deserialization\n* @param authRetryCount number of auth retries allowed\n* @return deserialized response object or null\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)": "/**\n* Executes an HTTP call with JSON output and handles retries.\n* @param conn configured HTTP connection\n* @param jsonOutput JSON data to send\n* @param expectedResponse expected HTTP status code\n* @param klass class type for response deserialization\n* @return deserialized response object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String)": "/**\n* Retrieves KeyVersion by version name.\n* @param versionName the name of the version\n* @return KeyVersion object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String)": "/**\n* Retrieves the current KeyVersion by name.\n* @param name the key name\n* @return KeyVersion object or null if not found\n* @throws IOException if a network error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys()": "/**\n* Retrieves a list of key names from the KMS REST API.\n* @return List of key names as strings\n* @throws IOException if an error occurs during the process\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[])": "/**\n* Retrieves metadata for provided key names.\n* @param keyNames variable-length array of key names\n* @return array of Metadata objects\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a KeyVersion object from provided parameters.\n* @param name key name, @param material key material, @param options configuration options\n* @return KeyVersion object or null if creation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String)": "/**\n* Invalidates server and local cache for the specified resource name.\n* @param name resource name to invalidate\n* @throws IOException if a connection error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Decrypts an encrypted key version and returns the KeyVersion object.\n* @param encryptedKeyVersion contains details for decryption\n* @return KeyVersion object representing the decrypted key\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Re-encrypts the provided EncryptedKeyVersion.\n* @param ekv the EncryptedKeyVersion to re-encrypt\n* @return new EncryptedKeyVersion after re-encryption\n* @throws IOException if a connection error occurs\n* @throws GeneralSecurityException if a security issue arises\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List)": "/**\n* Re-encrypts a list of encrypted keys and updates them.\n* @param ekvs list of EncryptedKeyVersion objects to re-encrypt\n* @throws IOException if a connection error occurs\n* @throws GeneralSecurityException if a security issue arises\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String)": "/**\n* Retrieves a list of KeyVersion objects for the specified name.\n* @param name resource name to fetch versions for\n* @return List of KeyVersion objects or null if no versions exist\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String)": "/**\n* Retrieves metadata by name.\n* @param name the identifier for the metadata\n* @return Metadata object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String)": "/**\n* Deletes a key by name from the key management system.\n* @param name the name of the key to delete\n* @throws IOException if an error occurs during the deletion process\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a KeyVersion using the provided name and options.\n* @param name key name, @param options configuration options\n* @return KeyVersion object or null if creation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a KeyVersion from name and material.\n* @param name key name, @param material key material, @param options config options\n* @return KeyVersion object\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])": "/**\n* Rolls a new key version and updates cache.\n* @param name resource name for the key\n* @param material key material data\n* @return KeyVersion object for the new key version\n* @throws NoSuchAlgorithmException if algorithm not found\n* @throws IOException if connection error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes KMSClientProvider with URI and configuration.\n* @param uri KMS URI for service location\n* @param conf configuration settings for client\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String)": "/**\n* Rolls a new key version using the specified name.\n* @param name resource name for the key\n* @return KeyVersion object for the new key version\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])": "/**\n* Rolls a new key version from provided material.\n* @param name resource name for the key\n* @param material key material data\n* @return KeyVersion object\n* @throws IOException if connection error occurs\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator:<init>(int,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)": "/**\n* Initializes TimeoutConnConfigurator with a timeout and connection configurator.\n* @param timeout connection timeout duration\n* @param cc connection configurator instance\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator:configure(java.net.HttpURLConnection)": "/**\n* Configures the given HttpURLConnection with timeouts.\n* @param conn HttpURLConnection to configure\n* @return configured HttpURLConnection\n*/"
    },
    "org.apache.hadoop.util.KMSUtil": {
        "org.apache.hadoop.util.KMSUtil:checkNotNull(java.lang.Object,java.lang.String)": "/**\n* Validates that the object is not null.\n* @param o object to check\n* @param name name of the parameter\n* @return the non-null object\n* @throws IllegalArgumentException if the object is null\n*/",
        "org.apache.hadoop.util.KMSUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the KMSUtil class.\n*/",
        "org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion)": "/**\n* Converts KeyVersion to a JSON map.\n* @param keyVersion the KeyVersion object to convert\n* @return Map containing JSON representation of KeyVersion\n*/",
        "org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)": "/**\n* Validates that the string is not null or empty.\n* @param s string to check\n* @param name name of the parameter\n* @return the non-empty string\n* @throws IllegalArgumentException if the string is null or empty\n*/",
        "org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)": "/**\n* Creates a KeyProvider from a URI and configuration.\n* @param conf configuration settings\n* @param providerUri resource URI for the KeyProvider\n* @return KeyProvider instance\n* @throws IOException if instantiation fails or provider is transient\n*/",
        "org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map)": "/**\n* Parses a JSON map to create a KeyVersion object.\n* @param valueMap map containing key details\n* @return KeyVersion object or null if map is empty\n*/",
        "org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map)": "/**\n* Parses JSON metadata from a map.\n* @param valueMap key-value pairs for metadata fields\n* @return Metadata object or null if map is empty\n*/",
        "org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Converts EncryptedKeyVersion to a JSON map.\n* @param encryptedKeyVersion the EncryptedKeyVersion object to convert\n* @return Map containing JSON representation of the encrypted key version\n*/",
        "org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)": "/**\n* Parses JSON to create an EncryptedKeyVersion object.\n* @param keyName name of the encryption key\n* @param valueMap map containing encryption details\n* @return constructed EncryptedKeyVersion instance\n*/",
        "org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)": "/**\n* Parses a list of JSON objects into EncryptedKeyVersion instances.\n* @param keyName name of the encryption key\n* @param valueList list of JSON objects\n* @return list of EncryptedKeyVersion instances\n*/",
        "org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Retrieves a URI from configuration by key name.\n* @param conf configuration object\n* @param configKeyName key to fetch the URI\n* @return URI object or null if not set\n*/",
        "org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Creates a KeyProvider from configuration and key name.\n* @param conf configuration settings\n* @param configKeyName key to fetch the URI\n* @return KeyProvider instance or null if URI not found\n*/",
        "org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration)": "/****\n* Retrieves the key provider URI from configuration.\n* @param conf configuration object\n* @return URI object or null if not set\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.ValueQueue": {
        "org.apache.hadoop.crypto.key.kms.ValueQueue:initializeQueuesForKeys(java.lang.String[])": "/**\n* Initializes queues for the specified key names.\n* @param keyNames variable number of key names to initialize queues for\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:shutdown()": "/**\n* Immediately shuts down the executor, stopping all actively executing tasks.\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:indexFor(java.lang.String)": "/**\n* Computes index for a given key using its hash code.\n* @param keyName the key for which to compute the index\n* @return computed index as an integer\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)": "",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)": "/**\n* Submits a refill task for a specified key to a queue.\n* @param keyName identifier for the task\n* @param keyQueue queue to refill\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String)": "/**\n* Retrieves a ReadWriteLock for the specified key.\n* @param keyName the key for which to obtain the lock\n* @return ReadWriteLock associated with the key\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)": "/**\n* Initializes ValueQueue with specified parameters.\n* @param numValues total values in the queue\n* @param lowWaterMark threshold for low water mark\n* @param expiry duration before expiration\n* @param numFillerThreads number of threads for filling\n* @param fetcher refiller for queue values\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String)": "/**\n* Acquires a read lock for the specified key.\n* @param keyName the key for which to obtain the read lock\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String)": "/**\n* Releases the read lock associated with the specified key.\n* @param keyName the key for which to release the read lock\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String)": "/**\n* Releases the write lock associated with the specified key.\n* @param keyName the key for which to release the lock\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String)": "/**\n* Acquires a write lock for the specified key.\n* @param keyName the key for which to obtain the write lock\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String)": "/**\n* Retrieves the size of the queue associated with the given key.\n* @param keyName the key for which to get the queue size\n* @return size of the queue or 0 if not present\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)": "/**\n* Retrieves up to 'num' elements from the queue for 'keyName'.\n* @param keyName identifier for the queue\n* @param num maximum number of elements to retrieve\n* @return list of retrieved elements\n* @throws IOException if an I/O error occurs\n* @throws ExecutionException if task execution fails\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String)": "/**\n* Clears and removes all tasks associated with the given key.\n* @param keyName the key for identifying tasks to clear\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String)": "/**\n* Retrieves the next element from the queue for the given key.\n* @param keyName identifier for the queue\n* @return the next element or null if not found\n* @throws IOException if an I/O error occurs\n* @throws ExecutionException if task execution fails\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue": {
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:put(java.lang.Runnable)": "/**\n* Adds a Runnable to the queue if not already in progress.\n* @param e the Runnable to add\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:take()": "/**\n* Retrieves and removes a Runnable from the queue.\n* @return Runnable object or null if queue is empty\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:poll(long,java.util.concurrent.TimeUnit)": "/**\n* Polls for a Runnable task with a timeout.\n* @param timeout maximum wait time for a task\n* @param unit time unit for the timeout\n* @return Runnable task or null if none available\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String)": "/**\n* Deletes a NamedRunnable by name and cancels it if found.\n* @param name the name of the Runnable to delete\n* @return the deleted NamedRunnable or null if not found\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable": {
        "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:cancel()": "/**\n* Marks the operation as canceled.\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:<init>(java.lang.String)": "/**\n* Initializes a NamedRunnable with the specified key name.\n* @param keyName the name to assign to this runnable\n*/",
        "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:isCanceled()": "/**\n* Checks if the operation is canceled.\n* @return true if canceled, false otherwise\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSDelegationToken": {
        "org.apache.hadoop.crypto.key.kms.KMSDelegationToken:<init>()": "/**\n* Private constructor for KMSDelegationToken class.\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider": {
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:shuffle(org.apache.hadoop.crypto.key.kms.KMSClientProvider[])": "/**\n* Shuffles the array of KMSClientProvider instances.\n* @param providers array of KMSClientProvider to shuffle\n* @return shuffled array of KMSClientProvider\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getProviders()": "/**\n* Retrieves an array of KMSClientProvider instances.\n* @return array of KMSClientProvider objects\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:nextIdx()": "/**\n* Computes the next index in a circular manner.\n* @return the current index before updating to the next\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush()": "/**\n* Flushes all KMS client providers, logging errors encountered during the process.\n* @throws IOException if an I/O error occurs (not applicable here)\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[])": "/**\n* Warms up encrypted keys using configured providers.\n* @param keyNames variable number of encrypted key names\n* @throws IOException if key warming fails for all providers\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close()": "/**\n* Closes all KMS client providers, logging errors for any that fail.\n* @throws IOException if closing a provider fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName()": "/**\n* Retrieves the canonical service name as a String.\n* @return String representation of the canonical service\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String)": "/**\n* Clears tasks for each provider using the specified key.\n* @param keyName the key for identifying tasks to clear\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials)": "/**\n* Selects a delegation token from credentials for specific services.\n* @param creds user credentials containing tokens\n* @return selected Token object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)": "/**\n* Executes a callable operation on KMS providers with failover support.\n* @param op callable operation to execute\n* @param currPos starting index for provider selection\n* @param isIdempotent indicates if the operation can be retried safely\n* @return result of the operation\n* @throws IOException if no providers are configured or operation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes LoadBalancingKMSClientProvider with URI, providers, and configuration settings.\n* @param uri token service URI\n* @param providers array of KMSClientProvider instances\n* @param seed random seed for shuffling providers\n* @param conf configuration object\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String)": "/**\n* Retrieves a delegation token for the specified renewer.\n* @param renewer identifier for the token renewer\n* @return Token object containing the delegation token\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Renews a delegation token using a KMS provider.\n* @param token the token to renew\n* @return renewed token's expiration time\n* @throws IOException if renewal fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Cancels a delegation token using KMS provider.\n* @param token the delegation token to cancel\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String)": "/**\n* Generates an encrypted key using the specified encryption key name.\n* @param encryptionKeyName name of the encryption key\n* @return EncryptedKeyVersion object\n* @throws IOException or GeneralSecurityException on failure\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Decrypts an encrypted key version.\n* @param encryptedKeyVersion the key version to decrypt\n* @return decrypted KeyVersion object\n* @throws IOException on I/O errors\n* @throws GeneralSecurityException on decryption failures\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)": "/**\n* Re-encrypts the given key version using KMS providers.\n* @param ekv the EncryptedKeyVersion to re-encrypt\n* @return the re-encrypted EncryptedKeyVersion\n* @throws IOException if a provider error occurs\n* @throws GeneralSecurityException if a security issue arises\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List)": "/**\n* Re-encrypts a list of encrypted keys.\n* @param ekvs list of encrypted key versions to re-encrypt\n* @throws IOException if an I/O error occurs\n* @throws GeneralSecurityException if a security error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String)": "/**\n* Retrieves the KeyVersion by its name.\n* @param versionName the name of the key version\n* @return KeyVersion object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys()": "/**\n* Retrieves a list of keys from KMS providers.\n* @return List of keys or throws IOException on failure\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[])": "/**\n* Retrieves metadata for specified keys.\n* @param names array of key names\n* @return array of Metadata objects\n* @throws IOException if retrieval fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String)": "/**\n* Retrieves key versions by name.\n* @param name the name of the key\n* @return list of KeyVersion objects\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String)": "/**\n* Retrieves the current key by name.\n* @param name the identifier for the key\n* @return KeyVersion object representing the current key\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String)": "/**\n* Retrieves metadata by name.\n* @param name the name for which metadata is requested\n* @return Metadata object or null if not found\n* @throws IOException if retrieval fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a new key with specified name and material.\n* @param name key name, @param material key material, @param options creation options\n* @return KeyVersion object representing the created key\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a key with the specified name and options.\n* @param name the name of the key\n* @param options additional options for key creation\n* @return KeyVersion object representing the created key\n* @throws NoSuchAlgorithmException if the algorithm is not found\n* @throws IOException for I/O errors during key creation\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)": "/**\n* Constructs LoadBalancingKMSClientProvider with URI, providers, and current time.\n* @param providerUri service URI\n* @param providers array of KMSClientProvider instances\n* @param conf configuration object\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs LoadBalancingKMSClientProvider for testing.\n* @param providers array of KMSClientProvider instances\n* @param seed random seed for shuffling providers\n* @param conf configuration object\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String)": "/**\n* Invalidates cache for all KMS clients using the specified key name.\n* @param keyName name of the resource to invalidate\n* @throws IOException if a connection error occurs\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String)": "/**\n* Deletes a key and invalidates its cache.\n* @param name the name of the key to delete\n* @throws IOException if the operation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])": "/**\n* Rolls a new version of a key and invalidates its cache.\n* @param name key name to roll version for\n* @param material new key material\n* @return KeyVersion of the new key version\n* @throws IOException if operation fails\n*/",
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String)": "/**\n* Rolls a new version of the key identified by name.\n* @param name the key name to roll a new version for\n* @return KeyVersion of the newly created key version\n* @throws NoSuchAlgorithmException if the algorithm is not supported\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell$RollCommand": {
        "org.apache.hadoop.crypto.key.KeyShell$RollCommand:validate()": "/**\n* Validates key provider and key name.\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$RollCommand:getUsage()": "/**\n* Returns usage information including description.\n* @return formatted usage string with details\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute()": "/**\n* Executes key version rolling and handles potential exceptions.\n* @throws NoSuchAlgorithmException if the algorithm is not found\n* @throws IOException if metadata is missing\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell$Command": {
        "org.apache.hadoop.crypto.key.KeyShell$Command:printProviderWritten()": "/**\n* Prints a message indicating the provider has been updated.\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider()": "/**\n* Warns if the provider is transient.\n* Logs a warning message if provider.isTransient() returns true.\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider()": "/**\n* Retrieves a KeyProvider based on user input or availability.\n* @return KeyProvider or null if none are valid\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderFactory": {
        "org.apache.hadoop.crypto.key.KeyProviderFactory:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a KeyProvider based on the given URI and configuration.\n* @param uri the resource URI for the KeyProvider\n* @param conf configuration settings for the provider\n* @return KeyProvider instance or null if not found\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a list of KeyProviders from configuration paths.\n* @param conf configuration settings\n* @return list of KeyProvider instances\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand": {
        "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:validate()": "/**\n* Validates key provider and key name.\n* @return true if both are valid, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:getUsage()": "/**\n* Returns usage information and description.\n* @return a string containing usage and description details\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute()": "/**\n* Executes cache invalidation for the specified key in the KeyProvider.\n* @throws NoSuchAlgorithmException if an algorithm is not found\n* @throws IOException if cache invalidation fails\n*/"
    },
    "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata": {
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:<init>(org.apache.hadoop.crypto.key.KeyProvider$Metadata)": "/**\n* Initializes KeyMetadata with provided Metadata object.\n* @param meta Metadata to be associated with the KeyMetadata\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream)": "/**\n* Reads object data from stream and initializes metadata.\n* @param in input stream to read object data\n* @throws IOException if reading fails\n* @throws ClassNotFoundException if class not found\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream)": "/**\n* Writes serialized metadata to output stream.\n* @param out output stream to write serialized data\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm()": "/**\n* Returns the algorithm used for encryption.\n* @return the cipher string from metadata\n*/"
    },
    "org.apache.hadoop.ipc.CallerContext$Builder": {
        "org.apache.hadoop.ipc.CallerContext$Builder:isValid(java.lang.String)": "/**\n* Checks if the given field is non-null and not empty.\n* @param field string to validate\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:checkFieldSeparator(java.lang.String)": "/**\n* Validates the field separator against illegal separators.\n* @param separator the field separator to check\n* @throws IllegalArgumentException if the separator is illegal\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:setSignature(byte[])": "/**\n* Sets the signature for the builder.\n* @param signature byte array representing the signature\n* @return the current Builder instance\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:getContext()": "/**\n* Returns the string representation of the context if available.\n* @return context string or null if empty\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:build()": "/**\n* Constructs a new CallerContext instance.\n* @return a new CallerContext object\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:getSignature()": "/**\n* Retrieves the current signature as a byte array.\n* @return byte array representing the signature\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String)": "/**\n* Appends a valid field to the builder's string.\n* @param field the string to append\n* @return the Builder instance for chaining\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)": "/**\n* Appends key-value pair to a StringBuilder if both are valid.\n* @param key the key to append\n* @param value the value to append\n* @return this Builder instance\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)": "/**\n* Appends key-value pair if key is absent; validates inputs.\n* @param key the key to append\n* @param value the value associated with the key\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a Builder with context and field separator.\n* @param context string to append if valid\n* @param separator field separator to validate\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String)": "/**\n* Initializes Builder with context and default field separator.\n* @param context string to append if valid\n*/",
        "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Builder with context and default field separator from configuration.\n* @param context string to append if valid\n* @param conf configuration object to retrieve separator\n*/"
    },
    "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB)": "/**\n* Initializes the translator with the given RPC proxy.\n* @param rpcProxy the RPC proxy for communication\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto)": "/**\n* Unpacks data from a proto to create a RefreshResponse.\n* @param proto contains response data\n* @return constructed RefreshResponse object\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy to release resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto)": "/**\n* Unpacks responses from a collection proto.\n* @param collection contains response data\n* @return list of RefreshResponse objects\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])": "/**\n* Refreshes data using identifier and arguments.\n* @param identifier unique identifier for the refresh request\n* @param args array of arguments for the refresh\n* @return collection of RefreshResponse objects\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)": "/**\n* Checks if a method is supported for the GenericRefreshProtocol.\n* @param methodName name of the method to check\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.RefreshResponse": {
        "org.apache.hadoop.ipc.RefreshResponse:<init>(int,java.lang.String)": "/**\n* Constructs a RefreshResponse with a return code and message.\n* @param returnCode operation status code\n* @param message descriptive message for the response\n*/",
        "org.apache.hadoop.ipc.RefreshResponse:setSenderName(java.lang.String)": "/**\n* Sets the sender's name.\n* @param name the name of the sender\n*/",
        "org.apache.hadoop.ipc.RefreshResponse:getReturnCode()": "/**\n* Retrieves the current return code.\n* @return the return code as an integer\n*/",
        "org.apache.hadoop.ipc.RefreshResponse:getMessage()": "/**\n* Retrieves the current message.\n* @return the message string\n*/",
        "org.apache.hadoop.ipc.RefreshResponse:getSenderName()": "/**\n* Retrieves the name of the sender.\n* @return senderName as a String\n*/",
        "org.apache.hadoop.ipc.RefreshResponse:toString()": "/**\n* Returns a string representation of the object including sender and message.\n* @return formatted string with sender, message, and return code\n*/",
        "org.apache.hadoop.ipc.RefreshResponse:successResponse()": "/**** \n* Creates a successful RefreshResponse.\n* @return RefreshResponse with success code and message\n*/"
    },
    "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB)": "/**\n* Initializes the translator with the given RPC proxy.\n* @param rpcProxy the RPC proxy for call queue protocol\n*/",
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy connection.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue()": "/**\n* Refreshes the call queue via an IPC call.\n* @throws IOException if the IPC call fails\n*/",
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)": "/**\n* Checks if a method is supported for the RefreshCallQueue protocol.\n* @param methodName name of the method to check\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.RefreshCallQueueProtocol)": "/**\n* Initializes the RefreshCallQueueProtocolServerSideTranslatorPB with the given implementation.\n* @param impl the RefreshCallQueueProtocol implementation to be used\n*/",
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB:refreshCallQueue(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto)": "/**\n* Refreshes the call queue and returns a response.\n* @param controller RPC controller for managing calls\n* @param request request data for refreshing the queue\n* @return RefreshCallQueueResponseProto response object\n*/"
    },
    "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.GenericRefreshProtocol)": "/**\n* Constructs a GenericRefreshProtocolServerSideTranslatorPB with the given implementation.\n* @param impl instance of GenericRefreshProtocol to be used\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection)": "/**\n* Packs a collection of RefreshResponse into a GenericRefreshResponseCollectionProto.\n* @param responses collection of RefreshResponse objects\n* @return built GenericRefreshResponseCollectionProto\n*/",
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)": "/**\n* Refreshes responses based on the request and packs them.\n* @param controller RPC controller\n* @param request contains identifier and arguments\n* @return packed GenericRefreshResponseCollectionProto\n*/"
    },
    "org.apache.hadoop.ipc.StandbyException": {
        "org.apache.hadoop.ipc.StandbyException:<init>(java.lang.String)": "/**\n* Constructs a StandbyException with the specified message.\n* @param msg detailed message for the exception\n*/"
    },
    "org.apache.hadoop.ipc.ClientId": {
        "org.apache.hadoop.ipc.ClientId:getClientId()": "/**\n* Generates a random client ID as a byte array.\n* @return byte array representing the client ID\n*/",
        "org.apache.hadoop.ipc.ClientId:getMsb(byte[])": "/**\n* Computes the most significant bits from a byte array.\n* @param clientId byte array representing the client ID\n* @return long value of the most significant bits\n*/",
        "org.apache.hadoop.ipc.ClientId:getLsb(byte[])": "/**\n* Computes the least significant bits from the given byte array.\n* @param clientId array of bytes representing client ID\n* @return long value of the least significant bits\n*/",
        "org.apache.hadoop.ipc.ClientId:toBytes(java.lang.String)": "/**\n* Converts a UUID string to a byte array.\n* @param id UUID string representation\n* @return byte array of the UUID, or empty if input is null/empty\n*/",
        "org.apache.hadoop.ipc.ClientId:toString(byte[])": "/**** Converts a byte array to its UUID string representation. \n* @param clientId byte array representing the client ID \n* @return UUID string or empty string for null/empty input \n*/"
    },
    "org.apache.hadoop.ipc.CallQueueManager": {
        "org.apache.hadoop.ipc.CallQueueManager:createScheduler(java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Creates an instance of a configurable RpcScheduler.\n* @param theClass class type of the scheduler\n* @param priorityLevels number of priority levels\n* @param ns namespace for the scheduler\n* @param conf configuration settings\n* @return instance of T or throws RuntimeException if construction fails\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:createCallQueueInstance(java.lang.Class,int,int,java.lang.String,int[],org.apache.hadoop.conf.Configuration)": "/**\n* Creates an instance of a BlockingQueue with specified parameters.\n* @param theClass class type of the BlockingQueue\n* @param priorityLevels number of priority levels\n* @param maxLen maximum length of the queue\n* @param ns namespace for the queue\n* @param capacityWeights weights for capacity distribution\n* @param conf configuration settings\n* @return an instance of BlockingQueue of type T\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:<init>(java.util.concurrent.BlockingQueue,org.apache.hadoop.ipc.RpcScheduler,boolean,boolean)": "/**\n* Initializes CallQueueManager with specified parameters.\n* @param queue blocking queue for task management\n* @param scheduler RPC scheduler for task execution\n* @param clientBackOffEnabled flag for client backoff\n* @param serverFailOverEnabled flag for server failover\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:shouldBackOff(org.apache.hadoop.ipc.Schedulable)": "/**\n* Determines if the scheduler should back off for the given schedulable entity.\n* @param e the schedulable entity to evaluate\n* @return true if backoff is required, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)": "/**\n* Retrieves the priority level of a schedulable entity.\n* @param e the schedulable entity\n* @return the priority level as an integer\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:isClientBackoffEnabled()": "/**\n* Checks if client backoff is enabled.\n* @return true if backoff is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:throwBackoff()": "/**\n* Throws CallQueueOverflowException based on server failover state.\n* @throws IllegalStateException if server failover is not enabled\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:offer(org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the collection.\n* @param e element to be added\n* @return true if the element was added successfully\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)": "/**\n* Attempts to add an element with a timeout.\n* @param e element to add; @param timeout max wait time; @param unit time unit\n* @return true if added, false if timeout occurred\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:peek()": "/**\n* Retrieves the element at the front without removing it.\n* @return the front element or null if empty\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:poll()": "/**\n* Retrieves and removes the head element, or returns null if empty.\n* @return head element or null if the collection is empty\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:poll(long,java.util.concurrent.TimeUnit)": "/**\n* Polls for an element with a timeout.\n* @param timeout maximum time to wait\n* @param unit time unit for the timeout\n* @return element or null if none available within timeout\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:take()": "/**\n* Retrieves an element, waiting if necessary.\n* @return an element of type E, or null if interrupted\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:size()": "/**\n* Returns the size of the referenced collection.\n* @return the number of elements in the collection\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:remainingCapacity()": "/**\n* Returns the remaining capacity of the referenced object.\n* @return int indicating the remaining capacity\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:getDefaultQueueCapacityWeights(int)": "/**\n* Returns an array of default weights for queue capacities.\n* @param priorityLevels number of priority levels\n* @return array of weights initialized to 1 for each level\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:queueIsReallyEmpty(java.util.concurrent.BlockingQueue)": "/**\n* Checks if the queue is empty after specified intervals.\n* @param q the BlockingQueue to check\n* @return true if empty, false if not or interrupted\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:stringRepr(java.lang.Object)": "/**\n* Returns a string representation of the object's class and hash code.\n* @param o the object to represent\n* @return string in the format \"ClassName@hashcode\"\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:drainTo(java.util.Collection)": "/**\n* Drains elements to the specified collection.\n* @param c collection to receive drained elements\n* @return number of elements drained\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:drainTo(java.util.Collection,int)": "/**\n* Drains elements to the specified collection up to a maximum limit.\n* @param c collection to drain elements into\n* @param maxElements maximum number of elements to drain\n* @return number of elements drained\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:iterator()": "/**\n* Returns an iterator over elements from the referenced collection.\n* @return Iterator for the collection elements\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:convertQueueClass(java.lang.Class,java.lang.Class)": "/**\n* Converts a queue class to a BlockingQueue type with specified element type.\n* @param queueClass the class of the queue to convert\n* @param elementClass the class of the elements in the queue\n* @return converted BlockingQueue class\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:convertSchedulerClass(java.lang.Class)": "/**\n* Converts a class to a RpcScheduler type.\n* @param schedulerClass the class to convert\n* @return Class of RpcScheduler type\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:setClientBackoffEnabled(boolean)": "/**\n* Sets the client backoff enabled state.\n* @param value true to enable, false to disable backoff\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabled()": "/**\n* Checks if server failover is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue()": "/**\n* Checks if server failover is enabled for the queue.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)": "/**\n* Adds an element if backoff is not required.\n* @param e element to add\n* @param checkBackoff flag to evaluate backoff status\n* @return true if added successfully, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object)": "/**\n* Adds an element to the collection using a reference.\n* @param e element to be added\n* @return true if the element was added successfully\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)": "/**\n* Attempts to add an element with a timeout.\n* @param e element to add; @param timeout max wait time; @param unit time unit\n* @return true if added, false if timeout occurred\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable)": "/**\n* Puts an element if client backoff is not enabled or not required.\n* @param e element to add\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the collection.\n* @param e element to add\n* @return true if added successfully, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)": "/**\n* Sets the priority level for a user in the scheduler.\n* @param user user group information\n* @param priority desired priority level\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object)": "/**\n* Adds an element if client backoff is not required.\n* @param e element to add\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object)": "/**\n* Adds an element to the collection.\n* @param e element to add\n* @return result of internal add operation\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)": "/**\n* Adds response time for a schedulable task.\n* @param name identifier for the call\n* @param e the schedulable object with priority level\n* @param details processing details containing timing information\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Returns priority level for a user in a DecayRpcScheduler.\n* @param user user group information\n* @return priority level as an integer or 0 if not using DecayRpcScheduler\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses the number of levels from configuration settings.\n* @param ns namespace for configuration keys\n* @param conf configuration object\n* @return number of levels, must be at least 1\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Determines if server failover is enabled for a given namespace.\n* @param namespace the namespace to check\n* @param conf configuration object containing properties\n* @return true if enabled, otherwise default value\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses capacity weights from configuration or defaults.\n* @param priorityLevels number of priority levels\n* @param ns namespace for configuration\n* @param conf configuration object\n* @return array of capacity weights\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes CallQueueManager with specified scheduler and queue configurations.\n* @param backingClass class type of the backing queue\n* @param schedulerClass class type of the scheduler\n* @param clientBackOffEnabled flag for client backoff\n* @param maxQueueSize maximum size of the queue\n* @param namespace configuration namespace\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Swaps the current queue and scheduler with new instances based on configuration.\n* @param schedulerClass class type of the new RpcScheduler\n* @param queueClassToUse class type of the new BlockingQueue\n* @param maxSize maximum size for the new queue\n* @param ns namespace for configuration\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.ipc.FairCallQueue": {
        "org.apache.hadoop.ipc.FairCallQueue:isServerFailOverEnabled()": "/**\n* Checks if server failover is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:signalNotEmpty()": "/**\n* Signals that a resource is no longer empty by releasing a semaphore.\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:peek()": "/**\n* Retrieves the first element from the first non-empty queue.\n* @return the element or null if all queues are empty\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:size()": "/**\n* Returns the number of available permits in the semaphore.\n* @return number of available permits as an integer\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:iterator()": "/**\n* Returns an iterator over elements of type E.\n* @throws NotImplementedException if method is not implemented\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection,int)": "/**\n* Drains up to maxElements from queues to the collection.\n* @param c collection to drain elements into\n* @param maxElements maximum number of elements to drain\n* @return number of elements successfully drained\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:remainingCapacity()": "/**\n* Calculates total remaining capacity across all queues.\n* @return total remaining capacity as an integer\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:getQueueSizes()": "/**\n* Returns an array of sizes for each queue.\n* @return array of integers representing queue sizes\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:getOverflowedCalls()": "/**\n* Retrieves an array of overflowed call counts from all queues.\n* @return array of long representing overflowed call counts\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the specified priority queue and signals non-emptiness.\n* @param priority the priority level of the queue\n* @param e the element to be added\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the queue based on priority.\n* @param priority the queue's priority level\n* @param e the element to add\n* @return true if added successfully, false otherwise\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)": "/**\n* Adds an element with timeout; signals if added successfully.\n* @param e element to add, @param timeout max wait time, @param unit time unit\n* @return true if added, false if timeout occurs\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the priority queue and signals if not empty.\n* @param e element to be added with a priority level\n* @return true if added successfully, false otherwise\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection)": "/**\n* Drains all elements from the queue to the specified collection.\n* @param c collection to drain elements into\n* @return number of elements successfully drained\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)": "/**\n* Attempts to add an element to queues based on priority.\n* @param priority starting priority level\n* @param e element to add\n* @param includeLast whether to include the last priority\n* @return true if added, false otherwise\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)": "/**\n* Adds an element to a priority queue with a timeout.\n* @param e element to add, @param timeout max wait time, @param unit time unit\n* @return true if added, false if timeout occurs\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object)": "/**\n* Adds an element to the appropriate priority queue and signals if not empty.\n* @param e element to be added\n* @return true if added successfully, false otherwise\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the queue based on its priority.\n* @param e element to add\n* @return true if added successfully\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable)": "/**\n* Adds an element to the appropriate priority queue.\n* @param e element to add, must implement getPriorityLevel\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:removeNextElement()": "/**\n* Removes and returns the next available element from the queue.\n* @return the next element or blocks until one is available\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object)": "/**\n* Adds an element to the queue; throws exception on overflow.\n* @param e element to add\n* @return true if added successfully\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object)": "/**\n* Adds an element to a priority queue based on its priority level.\n* @param e element to add, must implement getPriorityLevel\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:take()": "/**\n* Retrieves and removes the next element, blocking if necessary.\n* @return the next element from the queue\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)": "/**\n* Retrieves and removes the next element or returns null if timeout expires.\n* @param timeout maximum wait time\n* @param unit time unit for the timeout\n* @return the next element or null if timeout occurs\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:poll()": "/**\n* Retrieves and removes the next element if available; returns null if not.\n* @return the next element or null if semaphore acquisition fails\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FairCallQueue with specified priority levels and capacity.\n* @param priorityLevels number of priority levels, must be > 0\n* @param capacity total capacity of the queue\n* @param ns namespace for metrics\n* @param capacityWeights weights for each queue's capacity\n* @param serverFailOverEnabled enables server failover\n* @param conf configuration for queue settings\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs FairCallQueue with specified priority levels and capacity.\n* @param priorityLevels number of priority levels, must be > 0\n* @param capacity total capacity of the queue\n* @param ns namespace for metrics\n* @param conf configuration for queue settings\n*/",
        "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a FairCallQueue with specified parameters.\n* @param priorityLevels number of priority levels\n* @param capacity total capacity of the queue\n* @param ns namespace for metrics\n* @param serverFailOverEnabled enables server failover\n* @param conf configuration for queue settings\n*/"
    },
    "org.apache.hadoop.ipc.ProtocolProxy": {
        "org.apache.hadoop.ipc.ProtocolProxy:getProxy()": "/**\n* Retrieves the current proxy instance.\n* @return the proxy of type T\n*/",
        "org.apache.hadoop.ipc.ProtocolProxy:<init>(java.lang.Class,java.lang.Object,boolean)": "/**\n* Initializes a ProtocolProxy with protocol type, proxy instance, and server method check flag.\n* @param protocol the class type of the protocol\n* @param proxy the proxy instance for the protocol\n* @param supportServerMethodCheck flag to enable server method checks\n*/",
        "org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method)": "/**\n* Fetches methods from the server and checks protocol version consistency.\n* @param method the Method object to fetch server methods for\n* @throws IOException if an I/O error occurs during fetching\n*/",
        "org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])": "/**\n* Checks if a method is supported by the server.\n* @param methodName name of the method to check\n* @param parameterTypes types of the method parameters\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.Client$ConnectionId": {
        "org.apache.hadoop.ipc.Client$ConnectionId:getAddress()": "/**\n* Retrieves the InetSocketAddress of the current instance.\n* @return InetSocketAddress object representing the address\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getTicket()": "/**\n* Retrieves the current UserGroupInformation ticket.\n* @return UserGroupInformation object representing the ticket\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getRpcTimeout()": "/**\n* Retrieves the RPC timeout value.\n* @return the current RPC timeout in milliseconds\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getRetryPolicy()": "/**\n* Retrieves the current connection retry policy.\n* @return RetryPolicy instance for connection retries\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getMaxIdleTime()": "/**\n* Retrieves the maximum idle time value.\n* @return maximum idle time in milliseconds\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getMaxRetriesOnSasl()": "/**\n* Retrieves the maximum number of retries for SASL authentication.\n* @return maximum retries count as an integer\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getMaxRetriesOnSocketTimeouts()": "/**\n* Returns the maximum number of retries for socket timeouts.\n* @return maximum retries on socket timeouts\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getTcpNoDelay()": "/**\n* Returns the TCP_NODELAY option status.\n* @return true if TCP_NODELAY is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getTcpLowLatency()": "/**\n* Retrieves the current TCP low latency setting.\n* @return true if low latency is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getDoPing()": "/**\n* Returns the current ping status.\n* @return true if ping is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getPingInterval()": "/**\n* Retrieves the current ping interval value.\n* @return the ping interval in milliseconds\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getProtocol()": "/**\n* Retrieves the protocol class type.\n* @return Class<?> representing the protocol\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:setAddress(java.net.InetSocketAddress)": "/**\n* Sets the address if host and port match the current address.\n* @param address new InetSocketAddress to set\n* @throws IllegalArgumentException if host or port does not match\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:isEqual(java.lang.Object,java.lang.Object)": "/**\n* Checks if two objects are equal, handling nulls appropriately.\n* @param a first object to compare\n* @param b second object to compare\n* @return true if both are equal or both are null, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:toString()": "/**\n* Returns the string representation of the address.\n* @return String representation of the address object\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:hashCode()": "/**\n* Computes the hash code for the object based on connection parameters.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object)": "/**\n* Compares this ConnectionId with another object for equality.\n* @param obj object to compare with this ConnectionId\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a ConnectionId with given parameters and configuration settings.\n* @param address connection address, protocol class, user ticket, timeout, retry policy, and config\n*/",
        "org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a ConnectionId using provided parameters and config.\n* @param addr connection address, protocol class, user ticket, timeout, retry policy, and config\n* @return ConnectionId instance\n*/"
    },
    "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper": {
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:<init>(org.apache.hadoop.thirdparty.protobuf.Message)": "/**\n* Initializes ProtobufWrapper with a Message object.\n* @param message the Message to wrap\n*/",
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:readFrom(java.nio.ByteBuffer)": "/**\n* Deserializes a protocol buffer message from a ByteBuffer.\n* @param bb ByteBuffer containing the serialized message\n* @return Deserialized message of type T\n*/",
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:getMessage()": "/**\n* Retrieves the current message object.\n* @return the Message object\n*/",
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer)": "/**\n* Writes serialized message to ResponseBuffer.\n* @param out output buffer for the serialized message\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.ipc.CallerContext": {
        "org.apache.hadoop.ipc.CallerContext:getSignature()": "/**\n* Returns a copy of the signature byte array.\n* @return byte array copy of signature or null if not set\n*/",
        "org.apache.hadoop.ipc.CallerContext:isContextValid()": "/**\n* Checks if the context is not null and not empty.\n* @return true if context is valid, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallerContext:hashCode()": "/**\n* Computes the hash code based on the context object.\n* @return integer hash code value\n*/",
        "org.apache.hadoop.ipc.CallerContext:equals(java.lang.Object)": "/**\n* Compares this object with another for equality.\n* @param obj object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.ipc.CallerContext:getCurrent()": "/**\n* Retrieves the current CallerContext from the holder.\n* @return CallerContext associated with the current thread\n*/",
        "org.apache.hadoop.ipc.CallerContext:setCurrent(org.apache.hadoop.ipc.CallerContext)": "/**\n* Sets the current CallerContext for the CurrentCallerContextHolder.\n* @param callerContext the CallerContext to be set\n*/",
        "org.apache.hadoop.ipc.CallerContext:getContext()": "/**\n* Retrieves the current context string.\n* @return the context string\n*/",
        "org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder)": "/**\n* Initializes CallerContext with context and signature from Builder.\n* @param builder Builder instance containing context and signature\n*/",
        "org.apache.hadoop.ipc.CallerContext:toString()": "/**\n* Returns a string representation of the object.\n* @return formatted string or empty if context is invalid\n*/"
    },
    "org.apache.hadoop.ipc.IpcException": {
        "org.apache.hadoop.ipc.IpcException:<init>(java.lang.String)": "/**\n* Constructs an IpcException with the specified error message.\n* @param err error message for the exception\n*/"
    },
    "org.apache.hadoop.ipc.Client$IpcStreams": {
        "org.apache.hadoop.ipc.Client$IpcStreams:sendRequest(byte[])": "/**\n* Sends a byte array to an output stream.\n* @param buf data to be sent\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:flush()": "/**\n* Flushes the output stream to ensure all data is written.\n* @throws IOException if an I/O error occurs during flush\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:setInputStream(java.io.InputStream)": "/**\n* Sets the input stream, ensuring it's wrapped in a DataInputStream.\n* @param is the InputStream to set\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:setOutputStream(java.io.OutputStream)": "/**\n* Sets the output stream, wrapping it in DataOutputStream if necessary.\n* @param os the OutputStream to set\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:close()": "/**\n* Closes input and output streams to release resources.\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient)": "/**\n* Configures SASL client streams for input and output.\n* @param client the SaslRpcClient used for stream setup\n* @throws IOException if stream operations fail\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:readResponse()": "/**\n* Reads and validates RPC response from input stream.\n* @return ByteBuffer containing the response data\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)": "/**\n* Initializes IpcStreams with socket and maximum response length.\n* @param socket the Socket for input/output operations\n* @param maxResponseLength the maximum length for responses\n*/"
    },
    "org.apache.hadoop.ipc.Server$RpcKindMapValue": {
        "org.apache.hadoop.ipc.Server$RpcKindMapValue:<init>(java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)": "/**\n* Initializes RpcKindMapValue with request wrapper class and invoker.\n* @param rpcRequestWrapperClass class type for RPC request wrapper\n* @param rpcInvoker instance to invoke RPC calls\n*/"
    },
    "org.apache.hadoop.ipc.WritableRpcEngine$Invocation": {
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>()": "/**\n* Default constructor for Invocation, used during deserialization.\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:toString()": "/**\n* Returns a string representation of the object with method details.\n* @return formatted string with method name, parameters, and versions\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getProtocolVersion()": "/**\n* Retrieves the current protocol version.\n* @return the protocol version as a long value\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getRpcVersion()": "/**\n* Retrieves the current RPC version.\n* @return the RPC version as a long value\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getMethodName()": "/**\n* Retrieves the name of the current method.\n* @return name of the method as a String\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getParameterClasses()": "/**\n* Returns an array of parameter classes.\n* @return array of Class objects representing parameter types\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getParameters()": "/**\n* Retrieves the parameters array.\n* @return an array of parameters\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getConf()": "",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:setConf(org.apache.hadoop.conf.Configuration)": "",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])": "/**\n* Initializes Invocation with method details and RPC versioning.\n* @param method method to invoke\n* @param parameters arguments for the method\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput)": "/**\n* Writes protocol data to DataOutput stream.\n* @param out DataOutput stream to write to\n* @throws IOException if writing fails\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream to initialize object state.\n* @param in input data stream\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.ipc.ExternalCall": {
        "org.apache.hadoop.ipc.ExternalCall:waitForCompletion()": "/**\n* Waits until the completion condition is met.\n* @throws InterruptedException if the waiting thread is interrupted\n*/",
        "org.apache.hadoop.ipc.ExternalCall:isDone()": "/**\n* Checks if the task is completed.\n* @return true if done, false otherwise\n*/",
        "org.apache.hadoop.ipc.ExternalCall:run()": "/**\n* Executes an action and handles response; returns null.\n* @throws IOException if an I/O error occurs during execution\n*/",
        "org.apache.hadoop.ipc.ExternalCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)": "/**\n* Handles the response by setting the error and notifying completion.\n* @param t Throwable representing the error encountered\n* @param status RpcStatusProto containing the RPC status information\n*/",
        "org.apache.hadoop.ipc.ExternalCall:get()": "/**\n* Retrieves the result after waiting for completion.\n* @return result of type T\n* @throws InterruptedException if waiting is interrupted\n* @throws ExecutionException if an error occurred during computation\n*/",
        "org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction)": "/**\n* Initializes ExternalCall with a privileged action.\n* @param action the privileged action to be executed\n*/"
    },
    "org.apache.hadoop.ipc.Server$Call": {
        "org.apache.hadoop.ipc.Server$Call:isCallCoordinated()": "/**\n* Checks if the call is coordinated.\n* @return true if coordinated, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Call:getClientStateId()": "/**\n* Retrieves the client state ID.\n* @return client state identifier as a long value\n*/",
        "org.apache.hadoop.ipc.Server$Call:isResponseDeferred()": "/**\n* Checks if the response is deferred.\n* @return true if response is deferred, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Call:getDetailedMetricsName()": "/**\n* Retrieves the detailed metrics name.\n* @return String representing the detailed metrics name\n*/",
        "org.apache.hadoop.ipc.Server$Call:getRemoteUser()": "/**\n* Retrieves the remote user information.\n* @return UserGroupInformation object or null if not available\n*/",
        "org.apache.hadoop.ipc.Server$Call:getProcessingDetails()": "/**\n* Retrieves the current processing details.\n* @return ProcessingDetails object containing details\n*/",
        "org.apache.hadoop.ipc.Server$Call:isOpen()": "/**\n* Checks if the resource is currently open.\n* @return true if open, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Call:run()": "/**\n* Executes a task and returns null.\n* @return always returns null\n*/",
        "org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)": "/**\n* Handles response processing for RPC errors.\n* @param t Throwable representing the error\n* @param proto RpcStatusProto containing response status\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server$Call:setDetailedMetricsName(java.lang.String)": "/**\n* Sets the detailed metrics name.\n* @param name the new name for detailed metrics\n*/",
        "org.apache.hadoop.ipc.Server$Call:toString()": "/**\n* Returns a string representation of the call with its ID and retry count.\n* @return formatted string of call ID and retry count\n*/",
        "org.apache.hadoop.ipc.Server$Call:deferResponse()": "/**\n* Marks the response as deferred for later processing.\n*/",
        "org.apache.hadoop.ipc.Server$Call:setDeferredResponse(org.apache.hadoop.io.Writable)": "/**\n* Sets the deferred response for processing.\n* @param response the Writable response to be deferred\n*/",
        "org.apache.hadoop.ipc.Server$Call:setDeferredError(java.lang.Throwable)": "/**\n* Sets a deferred error for later handling.\n* @param t the Throwable to be deferred\n*/",
        "org.apache.hadoop.ipc.Server$Call:getHostInetAddress()": "/**\n* Retrieves the InetAddress for the host.\n* @return InetAddress object or null if not available\n*/",
        "org.apache.hadoop.ipc.Server$Call:postponeResponse()": "/**\n* Increments response wait count and asserts response has not been sent.\n*/",
        "org.apache.hadoop.ipc.Server$Call:getRemotePort()": "/**\n* Returns the remote port number.\n* @return the remote port as an integer\n*/",
        "org.apache.hadoop.ipc.Server$Call:getProtocol()": "/**\n* Retrieves the protocol as a String.\n* @return always returns null\n*/",
        "org.apache.hadoop.ipc.Server$Call:getPriorityLevel()": "/**\n* Retrieves the priority level of the current object.\n* @return int representing the priority level\n*/",
        "org.apache.hadoop.ipc.Server$Call:getCallerContext()": "/**\n* Retrieves the current CallerContext instance.\n* @return CallerContext object representing the caller's context\n*/",
        "org.apache.hadoop.ipc.Server$Call:getFederatedNamespaceState()": "/**\n* Retrieves the current state of the federated namespace.\n* @return ByteString representing the federated namespace state\n*/",
        "org.apache.hadoop.ipc.Server$Call:getTimestampNanos()": "/**\n* Retrieves the timestamp in nanoseconds.\n* @return timestamp in nanoseconds as a long\n*/",
        "org.apache.hadoop.ipc.Server$Call:markCallCoordinated(boolean)": "/**\n* Sets the call coordination status.\n* @param flag true to mark as coordinated, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Call:setClientStateId(long)": "/**\n* Sets the client state identifier.\n* @param stateId the new client state ID to be set\n*/",
        "org.apache.hadoop.ipc.Server$Call:setFederatedNamespaceState(org.apache.hadoop.thirdparty.protobuf.ByteString)": "/**\n* Sets the state of the federated namespace.\n* @param federatedNamespaceState the state to be set\n*/",
        "org.apache.hadoop.ipc.Server$Call:setPriorityLevel(int)": "/**\n* Sets the priority level.\n* @param priorityLevel the new priority level to set\n*/",
        "org.apache.hadoop.ipc.Server$Call:getUserGroupInformation()": "/**\n* Retrieves user group information from remote source.\n* @return UserGroupInformation object or null if not available\n*/",
        "org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable)": "/**\n* Handles fatal RPC errors by invoking response processing.\n* @param t Throwable representing the error\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)": "/**\n* Constructs a Call object with specified parameters.\n* @param id unique call identifier\n* @param retryCount number of retries for the call\n* @param kind type of RPC call\n* @param clientId identifier for the client\n* @param span tracing information for the call\n* @param callerContext context of the caller\n*/",
        "org.apache.hadoop.ipc.Server$Call:getHostAddress()": "/**\n* Retrieves the host's IP address as a string.\n* @return host IP address or null if not available\n*/",
        "org.apache.hadoop.ipc.Server$Call:sendResponse()": "/**\n* Sends response if no more waits are pending.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable)": "/**** Aborts the response if not already sent. \n* @param t Throwable error to handle\n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call)": "/**\n* Clones a Call object by copying its properties.\n* @param call the Call object to be cloned\n*/",
        "org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])": "/**\n* Initializes a Call object with specified ID and retry count.\n* @param id unique call identifier\n* @param retryCount number of retries for the call\n*/",
        "org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])": "/**\n* Constructs a Call object for testing with specified parameters.\n* @param id unique call identifier\n* @param retryCount number of retries for the call\n*/",
        "org.apache.hadoop.ipc.Server$Call:<init>()": "/**\n* Default constructor for Call, initializes with invalid ID and retry count.\n*/"
    },
    "org.apache.hadoop.ipc.RpcServerException": {
        "org.apache.hadoop.ipc.RpcServerException:getRpcStatusProto()": "/**\n* Retrieves the current RPC status.\n* @return RpcStatusProto indicating the error state\n*/",
        "org.apache.hadoop.ipc.RpcServerException:getRpcErrorCodeProto()": "/**\n* Retrieves the RPC error code for server errors.\n* @return RpcErrorCodeProto representing server error\n*/",
        "org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String)": "/**\n* Constructs an RpcServerException with a specified error message.\n* @param message descriptive error message for the exception\n*/",
        "org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an RpcServerException with a message and cause.\n* @param message error description\n* @param cause underlying throwable cause\n*/"
    },
    "org.apache.hadoop.ipc.Schedulable": {
        "org.apache.hadoop.ipc.Schedulable:getCallerContext()": "/**\n* Throws UnsupportedOperationException for invalid operations.\n* @return never returns a valid CallerContext\n*/"
    },
    "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask": {
        "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:run()": "/**\n* Executes the decay process for current costs or cancels the timer if scheduler is null.\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)": "/**\n* Initializes DecayTask with a scheduler and timer.\n* @param scheduler decay scheduler reference\n* @param timer timer for task execution\n*/"
    },
    "org.apache.hadoop.ipc.ProtocolSignature$ProtocolSigFingerprint": {
        "org.apache.hadoop.ipc.ProtocolSignature$ProtocolSigFingerprint:<init>(org.apache.hadoop.ipc.ProtocolSignature,int)": "/**\n* Initializes ProtocolSigFingerprint with a signature and its corresponding fingerprint.\n* @param sig ProtocolSignature object to be assigned\n* @param fingerprint integer representing the fingerprint value\n*/"
    },
    "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException": {
        "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException:<init>(java.io.IOException,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)": "/**\n* Constructs an exception for queue overflow with IO and RPC status details.\n* @param ioe the original IOException\n* @param status the associated RpcStatusProto\n*/",
        "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException:getCause()": "/**\n* Retrieves the cause of the exception as an IOException.\n* @return IOException that caused this exception\n*/"
    },
    "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey": {
        "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:<init>(java.net.InetSocketAddress,java.lang.String,java.lang.String)": "/**\n* Constructs a ProtoSigCacheKey with server address, protocol, and RPC kind.\n* @param addr server address as InetSocketAddress\n* @param p protocol string\n* @param rk RPC kind string\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:hashCode()": "/**\n* Computes hash code for the object based on serverAddress, protocol, and rpcKind.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:equals(java.lang.Object)": "/**\n* Compares this object to another for equality.\n* @param other object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.ProtocolSignature": {
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method)": "/**\n* Generates a fingerprint hash for a given method.\n* @param method the Method object to generate a fingerprint for\n* @return an integer hash code representing the method's signature\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:<init>(long,int[])": "/**\n* Constructs a ProtocolSignature with a version and method hashcodes.\n* @param version protocol version number\n* @param methodHashcodes array of method hashcodes\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getMethods()": "/**\n* Retrieves the array of method identifiers.\n* @return array of method identifiers\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:<init>()": "/**\n* Default constructor for ProtocolSignature class.\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:readFields(java.io.DataInput)": "/**\n* Reads fields from a DataInput stream, populating version and methods array.\n* @param in the DataInput stream to read from\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:write(java.io.DataOutput)": "/**\n* Writes object data to output stream.\n* @param out DataOutput stream for writing data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(int[])": "/**\n* Computes a fingerprint from sorted hashcodes.\n* @param hashcodes array of hashcodes to process\n* @return hash code of the sorted array\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:resetCache()": "/**\n* Clears the protocol fingerprint cache for testing purposes.\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getVersion()": "/**\n* Retrieves the current version number.\n* @return the version as a long value\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[])": "/**\n* Generates fingerprint hashes for an array of methods.\n* @param methods array of Method objects\n* @return array of integer hash codes or null if input is null\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[])": "/**\n* Computes a fingerprint for an array of methods.\n* @param methods array of Method objects\n* @return hash code of the sorted array of fingerprints\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)": "/**\n* Retrieves or computes the protocol signature fingerprint.\n* @param protocol the protocol class\n* @param serverVersion the server version number\n* @return ProtocolSigFingerprint object\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)": "/**\n* Retrieves the protocol signature based on client hash and server version.\n* @param clientMethodsHashCode hash of client methods\n* @param serverVersion server's protocol version\n* @param protocol protocol class type\n* @return ProtocolSignature object indicating match or server signature\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)": "/**\n* Retrieves the protocol signature by name and version.\n* @param protocolName the name of the protocol class\n* @param version the server version number\n* @return ProtocolSignature object\n*/",
        "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)": "/**\n* Retrieves protocol signature based on server and client versioning.\n* @param server the server's versioned protocol\n* @param protocol the protocol class name\n* @param clientVersion client's protocol version\n* @param clientMethodsHash client's methods hash code\n* @return ProtocolSignature object indicating match\n*/"
    },
    "org.apache.hadoop.ipc.RpcClientUtil": {
        "org.apache.hadoop.ipc.RpcClientUtil:methodToTraceString(java.lang.reflect.Method)": "/**\n* Traces the enclosing class and method name.\n* @param method the Method object to trace\n* @return formatted string \"ClassName#methodName\"\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:toTraceName(java.lang.String)": "/**\n* Converts a full class name to a trace name format.\n* @param fullName the full class name\n* @return formatted trace name as a String\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)": "/**\n* Stores a mapping of version signatures by creating a cache key.\n* @param addr server address, protocol, and RPC kind for the cache key\n* @param map mapping of version signatures\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)": "/**\n* Retrieves version signatures for a given server and protocol.\n* @param addr server address as InetSocketAddress\n* @param protocol protocol string\n* @param rpcKind RPC kind string\n* @return map of version signatures or null if not found\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List)": "/**\n* Converts a list of ProtocolSignatureProto to a map of ProtocolSignature by version.\n* @param protoList list of ProtocolSignatureProto objects\n* @return Map of version to ProtocolSignature objects\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)": "/**\n* Checks if a method exists in the given version's protocol signature.\n* @param methodHash hash of the method to check\n* @param version version of the protocol\n* @param versionMap mapping of versions to protocol signatures\n* @return true if method exists, false otherwise\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a ProtocolMetaInfoPB proxy for RPC communication.\n* @param proxy the RPC proxy object\n* @param conf configuration settings\n* @return ProtocolMetaInfoPB instance\n*/",
        "org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)": "/**\n* Checks if a method is supported for a given RPC version and protocol.\n* @param rpcProxy RPC proxy object\n* @param protocol protocol class type\n* @param rpcKind RPC kind\n* @param version protocol version\n* @param methodName name of the method to check\n* @return true if the method is supported, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.RefreshRegistry": {
        "org.apache.hadoop.ipc.RefreshRegistry:<init>()": "/**\n* Initializes the RefreshRegistry with an empty handler table.\n*/",
        "org.apache.hadoop.ipc.RefreshRegistry:register(java.lang.String,org.apache.hadoop.ipc.RefreshHandler)": "/**\n* Registers a RefreshHandler with a unique identifier.\n* @param identifier unique identifier for the handler\n* @param handler the RefreshHandler to register\n*/",
        "org.apache.hadoop.ipc.RefreshRegistry:unregister(java.lang.String,org.apache.hadoop.ipc.RefreshHandler)": "/**\n* Unregisters a handler for a given identifier.\n* @param identifier unique identifier for the handler\n* @param handler the RefreshHandler to unregister\n* @return true if unregistration was successful, false otherwise\n*/",
        "org.apache.hadoop.ipc.RefreshRegistry:unregisterAll(java.lang.String)": "/**\n* Unregisters all handlers associated with the given identifier.\n* @param identifier unique identifier for handlers to remove\n*/",
        "org.apache.hadoop.ipc.RefreshRegistry:handlerName(org.apache.hadoop.ipc.RefreshHandler)": "/**\n* Returns the class name and hash code of the RefreshHandler.\n* @param h the RefreshHandler instance\n* @return formatted string of class name and hash code\n*/",
        "org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])": "/**\n* Dispatches refresh requests to handlers by identifier and arguments.\n* @param identifier unique identifier for the handler\n* @param args array of arguments for the handler\n* @return collection of RefreshResponse objects from handlers\n*/"
    },
    "org.apache.hadoop.ipc.Server$AuthProtocol": {
        "org.apache.hadoop.ipc.Server$AuthProtocol:valueOf(int)": "/**\n* Retrieves AuthProtocol by call ID.\n* @param callId identifier for the authentication protocol\n* @return AuthProtocol object or null if not found\n*/"
    },
    "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics": {
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:getQueueName(int)": "/**\n* Constructs a queue name based on the given priority.\n* @param priority the priority level for the queue\n* @return formatted queue name as a String\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:getProcessingName(int)": "/**\n* Constructs a processing name string based on priority.\n* @param priority the priority level for processing\n* @return formatted processing name string\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown()": "/**\n* Shuts down the metrics source by unregistering it.\n* @param name the name of the metrics source to unregister\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)": "/**\n* Adds queue time for a specified priority level.\n* @param priority the priority level index\n* @param queueTime duration to add to the queue statistics\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)": "/**\n* Adds processing time for a specific priority level.\n* @param priority index for processing level\n* @param processingTime duration to add for that level\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String)": "/**\n* Initializes metrics for DecayRpcScheduler with namespace.\n* @param ns namespace for the metrics registry\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int)": "/**\n* Initializes RPC stats for specified priority levels.\n* @param numLevels number of priority levels to initialize\n*/",
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String)": "/**\n* Creates and registers detailed metrics for DecayRpcScheduler.\n* @param ns namespace for the metrics\n* @return registered DecayRpcSchedulerDetailedMetrics instance\n*/"
    },
    "org.apache.hadoop.ipc.RetryCache": {
        "org.apache.hadoop.ipc.RetryCache:getCacheName()": "/**\n* Retrieves the name of the cache.\n* @return the cache name as a String\n*/",
        "org.apache.hadoop.ipc.RetryCache:lock()": "/**\n* Acquires the lock to ensure thread safety.\n*/",
        "org.apache.hadoop.ipc.RetryCache:unlock()": "/**\n* Unlocks the associated lock object.\n*/",
        "org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload,boolean,java.lang.Object)": "/**\n* Sets the state of a cache entry with a payload and success flag.\n* @param e cache entry to update, @param success operation success status, @param payload data to store\n*/",
        "org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)": "/**\n* Determines if a retry should be skipped based on RPC status and identifiers.\n* @param clientId client identifier for the request\n* @param callId identifier for the call\n* @return true if retry should be skipped, false otherwise\n*/",
        "org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)": "/**\n* Sets the state of a CacheEntry based on success status.\n* @param e CacheEntry to update; @param success completion status\n*/",
        "org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)": "/**\n* Creates a new CacheEntry with specified expiration and identifiers.\n* @param expirationTime duration until expiration in nanoseconds\n* @param clientId byte array representing the client ID (must be 16 bytes)\n* @param callId identifier for the call\n* @return newly created CacheEntry object\n*/",
        "org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter()": "/**\n* Increments the cache cleared counter via metrics.\n* @return void\n*/",
        "org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)": "/**\n* Creates a new CacheEntryWithPayload.\n* @param payload associated payload object\n* @param expirationTime duration until expiration\n* @param clientId byte array representing client ID\n* @param callId identifier for the call\n* @return CacheEntryWithPayload instance\n*/",
        "org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache)": "/**\n* Clears the specified RetryCache if not null.\n* @param cache the RetryCache to clear\n*/",
        "org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)": "/**\n* Constructs a RetryCache with specified name, capacity percentage, and expiration time.\n* @param cacheName name of the cache, @param percentage capacity fraction, @param expirationTime lifespan\n*/",
        "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry)": "/**\n* Waits for a cache entry to complete or adds a new entry if not present.\n* @param newEntry the CacheEntry to add or wait for\n* @return the completed CacheEntry\n*/",
        "org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)": "/**\n* Adds a new cache entry for the given client ID and call ID.\n* @param clientId byte array representing the client ID\n* @param callId identifier for the call\n*/",
        "org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)": "/**\n* Adds a cache entry with payload and updates metrics.\n* @param clientId client identifier\n* @param callId call identifier\n* @param payload data associated with the entry\n*/",
        "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)": "/**\n* Waits for a cache entry or returns null if retry is skipped.\n* @param cache the RetryCache to use\n* @param clientId client identifier for the request\n* @param callId identifier for the call\n* @return completed CacheEntry or null if skipped\n*/",
        "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)": "/**** \n* Waits for cache entry completion or returns null if skip condition met.\n* @param cache the RetryCache to check\n* @param payload the associated payload object\n* @param clientId client identifier for the request\n* @param callId identifier for the call\n* @return CacheEntryWithPayload or null if skipped\n*/"
    },
    "org.apache.hadoop.ipc.Server": {
        "org.apache.hadoop.ipc.Server:getServerName()": "/**\n* Retrieves the name of the server.\n* @return serverName as a String\n*/",
        "org.apache.hadoop.ipc.Server:getNumInProcessHandler()": "/**\n* Retrieves the current count of handlers in process.\n* @return number of handlers currently in process\n*/",
        "org.apache.hadoop.ipc.Server:getTotalRequests()": "/**\n* Returns the total number of requests processed.\n* @return total number of requests as a long\n*/",
        "org.apache.hadoop.ipc.Server:getTotalRequestsPerSecond()": "/**\n* Retrieves the total number of requests processed per second.\n* @return long representing total requests per second\n*/",
        "org.apache.hadoop.ipc.Server:getPort()": "/**\n* Retrieves the current port number.\n* @return the port number as an integer\n*/",
        "org.apache.hadoop.ipc.Server:isRpcInvocation()": "/**\n* Checks if the current call is an RPC invocation.\n* @return true if an RPC call is active, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:setTracer(org.apache.hadoop.tracing.Tracer)": "/**\n* Sets the tracer instance for this object.\n* @param t Tracer instance to be assigned\n*/",
        "org.apache.hadoop.ipc.Server:getMaxIdleTime()": "/**\n* Retrieves the maximum idle time for connections.\n* @return maximum idle time in milliseconds\n*/",
        "org.apache.hadoop.ipc.Server:getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)": "/**\n* Retrieves RpcInvoker associated with the specified RpcKind.\n* @param rpcKind type of RPC to fetch the invoker for\n* @return corresponding RpcInvoker or null if not found\n*/",
        "org.apache.hadoop.ipc.Server:get()": "/**\n* Retrieves the singleton instance of the Server.\n* @return the Server instance\n*/",
        "org.apache.hadoop.ipc.Server:getCallId()": "/**\n* Retrieves the current call ID or returns an invalid ID if none exists.\n* @return current call ID or INVALID_CALL_ID if no call is active\n*/",
        "org.apache.hadoop.ipc.Server:getCallRetryCount()": "/**\n* Retrieves the current call's retry count.\n* @return int retry count or INVALID_RETRY_COUNT if no call exists\n*/",
        "org.apache.hadoop.ipc.Server:getClientId()": "/**\n* Retrieves the client ID; returns a dummy ID if not available.\n* @return byte array of client ID or dummy client ID\n*/",
        "org.apache.hadoop.ipc.Server:setPurgeIntervalNanos(int)": "/**\n* Sets the purge interval in nanoseconds.\n* @param purgeInterval interval in minutes; defaults if non-positive\n*/",
        "org.apache.hadoop.ipc.Server:getLogSlowRPCThresholdTime()": "/**\n* Retrieves the threshold time for slow RPC logging.\n* @return long value representing the threshold time in milliseconds\n*/",
        "org.apache.hadoop.ipc.Server:isLogSlowRPC()": "/**\n* Checks if slow RPC logging is enabled.\n* @return true if slow RPC logging is active, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:getHandlers()": "/**\n* Returns an iterable list of thread handlers for testing purposes.\n* @return Iterable of Thread handlers\n*/",
        "org.apache.hadoop.ipc.Server:getQueueClassPrefix()": "/**\n* Constructs the queue class prefix using IPC_NAMESPACE and port.\n* @return concatenated queue class prefix string\n*/",
        "org.apache.hadoop.ipc.Server:setLogSlowRPC(boolean)": "/**\n* Sets the flag for logging slow RPC calls.\n* @param logSlowRPCFlag true to enable logging, false to disable\n*/",
        "org.apache.hadoop.ipc.Server:getDelimitedLength(org.apache.hadoop.thirdparty.protobuf.Message)": "/**\n* Calculates total serialized size of the message.\n* @param message the Message object to measure\n* @return total length including size overhead\n*/",
        "org.apache.hadoop.ipc.Server:start()": "/**\n* Initializes and starts main and auxiliary listeners, and handler threads.\n*/",
        "org.apache.hadoop.ipc.Server:shutdownMetricsUpdaterExecutor()": "/**\n* Shuts down the metrics updater executor service gracefully.\n*/",
        "org.apache.hadoop.ipc.Server:join()": "/**\n* Waits for the thread to finish if currently running.\n* @throws InterruptedException if the thread is interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.Server:call(org.apache.hadoop.io.Writable,long)": "/**\n* Deprecated: Calls the RPC with a writable parameter and receive time.\n* @param param input writable data\n* @param receiveTime timestamp of the received data\n* @return Writable result from the RPC call\n*/",
        "org.apache.hadoop.ipc.Server:getConf()": "/**\n* Retrieves the current configuration object.\n* @return Configuration instance representing the current settings\n*/",
        "org.apache.hadoop.ipc.Server:channelIO(java.nio.channels.ReadableByteChannel,java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)": "/**\n* Transfers data between channels using a ByteBuffer.\n* @param readCh channel for reading, or null for writing\n* @param writeCh channel for writing\n* @param buf buffer for data transfer\n* @return number of bytes transferred\n*/",
        "org.apache.hadoop.ipc.Server:getCallQueue()": "/**\n* Retrieves the current call queue.\n* @return CallQueueManager containing Call objects\n*/",
        "org.apache.hadoop.ipc.Server:getMaxQueueSize()": "/**\n* Retrieves the maximum queue size.\n* @return the maximum size of the queue\n*/",
        "org.apache.hadoop.ipc.Server:getNumReaders()": "/**\n* Returns the number of active readers.\n* @return count of active reader threads\n*/",
        "org.apache.hadoop.ipc.Server:getPurgeIntervalNanos()": "/**\n* Retrieves the purge interval in nanoseconds.\n* @return purge interval as a long value in nanoseconds\n*/",
        "org.apache.hadoop.ipc.Server:getRpcDetailedMetrics()": "/**\n* Retrieves detailed RPC metrics.\n* @return RpcDetailedMetrics object containing metrics data\n*/",
        "org.apache.hadoop.ipc.Server:getRpcMetrics()": "/**\n* Retrieves the RpcMetrics instance.\n* @return RpcMetrics object containing RPC metrics data\n*/",
        "org.apache.hadoop.ipc.Server:getServiceAuthorizationManager()": "/**\n* Retrieves the ServiceAuthorizationManager instance.\n* @return ServiceAuthorizationManager object\n*/",
        "org.apache.hadoop.ipc.Server:setAlignmentContext(org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Sets the alignment context for the current instance.\n* @param alignmentContext the context to be set\n*/",
        "org.apache.hadoop.ipc.Server:setCallQueue(org.apache.hadoop.ipc.CallQueueManager)": "/**\n* Sets the call queue for managing calls.\n* @param callQueue the CallQueueManager instance to set\n*/",
        "org.apache.hadoop.ipc.Server:setRpcRequestClass(java.lang.Class)": "/**\n* Sets the RPC request class type.\n* @param rpcRequestClass class type of the RPC request\n*/",
        "org.apache.hadoop.ipc.Server:setSocketSendBufSize(int)": "/**\n* Sets the socket send buffer size.\n* @param size the desired buffer size in bytes\n*/",
        "org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)": "/**\n* Retrieves the priority level of a schedulable entity.\n* @param e the schedulable entity\n* @return priority level as an integer\n*/",
        "org.apache.hadoop.ipc.Server:isClientBackoffEnabled()": "/**\n* Checks if client backoff is enabled by querying the call queue.\n* @return true if backoff is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:getCallQueueLen()": "/**\n* Returns the number of elements in the call queue.\n* @return the length of the call queue\n*/",
        "org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)": "/**\n* Registers an RPC protocol engine with its request wrapper class and invoker.\n* @param rpcKind the type of RPC protocol\n* @param rpcRequestWrapperClass class for the RPC request wrapper\n* @param rpcInvoker instance to invoke RPC calls\n*/",
        "org.apache.hadoop.ipc.Server:getRemoteUser()": "/**\n* Retrieves the remote user information or null if unavailable.\n* @return UserGroupInformation object or null\n*/",
        "org.apache.hadoop.ipc.Server:getNumDroppedConnections()": "/**\n* Retrieves the number of dropped connections.\n* @return count of dropped connections as a long\n*/",
        "org.apache.hadoop.ipc.Server:getNumOpenConnections()": "/**\n* Retrieves the number of open connections.\n* @return current size of open connections as an integer\n*/",
        "org.apache.hadoop.ipc.Server:getConnections()": "/**\n* Retrieves an array of Connection objects from the connection manager.\n* @return array of Connection objects\n*/",
        "org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[])": "/**\n* Adds exception classes for terse logging.\n* @param exceptionClass variable number of exception classes\n*/",
        "org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[])": "/**\n* Adds exception classes to the suppressed logging list.\n* @param exceptionClass variable number of exception classes\n*/",
        "org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)": "/**\n* Logs exceptions based on severity and suppression rules.\n* @param logger Logger instance for logging messages\n* @param e Throwable exception to log\n* @param call Call context for logging details\n*/",
        "org.apache.hadoop.ipc.Server:getListenerAddress()": "/**\n* Retrieves the listener's local socket address.\n* @return InetSocketAddress of the listener\n*/",
        "org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses()": "/**\n* Retrieves all auxiliary listener socket addresses.\n* @return Set of InetSocketAddress for auxiliary listeners\n*/",
        "org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)": "/**\n* Prepares response for old version fatal errors.\n* @param response output stream for the response\n* @param call RPC call details\n* @param rv writable object for additional data\n* @param errorClass class of the error\n* @param error error message\n*/",
        "org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto)": "/**\n* Retrieves RPC request wrapper class based on RpcKindProto.\n* @param rpcKind RpcKindProto to determine the wrapper class\n* @return Class of the writable RPC request or null if not found\n*/",
        "org.apache.hadoop.ipc.Server:getRemoteIp()": "/**\n* Retrieves the remote IP address from the current call.\n* @return InetAddress of the host or null if no call exists\n*/",
        "org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)": "/**\n* Retrieves RpcInvoker for the specified RPC kind.\n* @param rpcKind type of RPC to fetch the invoker for\n* @return corresponding RpcInvoker or null if not found\n*/",
        "org.apache.hadoop.ipc.Server:getRemotePort()": "/**\n* Retrieves the remote port number or returns 0 if no active call exists.\n* @return remote port as an integer\n*/",
        "org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP()": "/**\n* Retrieves established QOP for the auxiliary port if connected.\n* @return established QOP as a String or null if not applicable\n*/",
        "org.apache.hadoop.ipc.Server:getProtocol()": "/**\n* Retrieves the protocol from the current call.\n* @return protocol as a String or null if no current call exists\n*/",
        "org.apache.hadoop.ipc.Server:getPriorityLevel()": "/**\n* Retrieves the priority level of the current call.\n* @return int representing the priority level or 0 if no call exists\n*/",
        "org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long)": "/**\n* Sets the threshold time for logging slow RPCs.\n* @param logSlowRPCThresholdMs time in milliseconds to convert\n*/",
        "org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean)": "/**\n* Sets the client backoff enabled state.\n* @param value true to enable, false to disable backoff\n*/",
        "org.apache.hadoop.ipc.Server:addAuxiliaryListener(int)": "/**\n* Adds an auxiliary listener for the specified port.\n* @param auxiliaryPort port number for the listener\n* @throws IOException if the port is already in use\n*/",
        "org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)": "/**\n* Prepares a byte array response for a Protobuf message.\n* @param header the RpcResponseHeaderProto to serialize\n* @param rv the writable object containing the Protobuf message\n* @return byte array containing the serialized response\n*/",
        "org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser()": "/**\n* Returns JSON string of open connections per user.\n* @return JSON representation or null on error\n*/",
        "org.apache.hadoop.ipc.Server:isServerFailOverEnabled()": "/**\n* Checks if server failover is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)": "/**\n* Writes data to a channel and increments sent bytes metric.\n* @param channel channel to write data to\n* @param buffer buffer containing data to write\n* @return number of bytes written\n*/",
        "org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)": "/**\n* Reads data from a channel into a buffer and updates received byte metrics.\n* @param channel the channel to read from\n* @param buffer the buffer to store read data\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue()": "/**\n* Checks if server failover is enabled for the queue.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:getRemoteAddress()": "/**\n* Retrieves the remote IP address as a string.\n* @return IP address string or null if not available\n*/",
        "org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves protocol class by name, caching for future use.\n* @param protocolName fully qualified protocol class name\n* @param conf configuration for class retrieval\n* @return Class object of the protocol\n* @throws ClassNotFoundException if class is not found\n*/",
        "org.apache.hadoop.ipc.Server:stop()": "/**\n* Stops the server and interrupts all associated listeners and handlers.\n*/",
        "org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)": "/**\n* Logs slow RPC calls exceeding a threshold based on processing time.\n* @param methodName name of the RPC method\n* @param call details of the RPC call\n* @param details processing metrics for the RPC call\n*/",
        "org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection)": "/**\n* Closes the specified connection using the connection manager.\n* @param connection the connection to be closed\n*/",
        "org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)": "/**** Queues a call, blocking if specified, and updates processing details. \n* @param call the Call object to queue \n* @param blocking true to block on queue full, false to add without blocking \n* @throws IOException if an I/O error occurs \n* @throws InterruptedException if interrupted while waiting \n*/",
        "org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)": "/**\n* Sets the priority level for a user in the scheduler.\n* @param ugi user group information\n* @param priority desired priority level\n*/",
        "org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)": "/**** Prepares response data for writing to a buffer. \n* @param header response header \n* @param rv optional writable object \n* @return byte array representation of the response */",
        "org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call)": "/**\n* Queues a call and blocks until space is available.\n* @param call the Call object to queue\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)": "/**\n* Prepares and sets the RPC response based on the writable object type.\n* @param call the RPC call context\n* @param header the response header\n* @param rv the writable object for response data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)": "/**\n* Updates deferred metrics with processing time for a given sample name.\n* @param name sample identifier\n* @param processingTime time duration in milliseconds\n*/",
        "org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)": "/**\n* Updates metrics for an RPC call based on processing details and completion time.\n* @param call the RPC call object containing timing information\n* @param processingStartTimeNanos start time of processing in nanoseconds\n* @param connDropped indicates if the connection was dropped during processing\n*/",
        "org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call)": "/**\n* Queues a call and handles exceptions during the process.\n* @param call the Call object to queue\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Retrieves priority level for a user.\n* @param ugi user group information\n* @return integer priority level from callQueue\n*/",
        "org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)": "/**\n* Sets up the RPC response based on the call status and error details.\n* @param call the RPC call context\n* @param status the status of the RPC call\n* @param erCode error code for the response\n* @param rv writable object for response data\n* @param errorClass name of the error class\n* @param error error message\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall)": "/**\n* Wraps RPC response with SASL token if SASL server is present.\n* @param call the RPC call context\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a list of authentication methods based on configuration and secret manager.\n* @param secretManager the secret manager for authentication\n* @param conf configuration settings for authentication\n* @return List of AuthMethod objects\n*/",
        "org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Binds a ServerSocket to an address with specified backlog and port range.\n* @param socket the ServerSocket to bind\n* @param address the InetSocketAddress to bind to\n* @param backlog the maximum number of queued connections\n* @param conf configuration object for range settings\n* @param rangeConf property name for integer range configuration\n* @throws IOException if binding fails\n*/",
        "org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Checks if client backoff is enabled based on the configuration.\n* @param prefix configuration prefix\n* @param conf configuration object\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)": "/**\n* Checks if client backoff is enabled for given namespace and port.\n* @param namespace configuration namespace\n* @param port network port number\n* @param conf configuration object\n* @return true if backoff is enabled, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the queue class for calls from configuration.\n* @param prefix configuration prefix, @param conf configuration object\n* @return Class of BlockingQueue for calls\n*/",
        "org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the BlockingQueue class based on namespace and port from configuration.\n* @param namespace the namespace for the queue\n* @param port the port number for the queue\n* @param conf the configuration object\n* @return Class of the BlockingQueue for calls\n*/",
        "org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the RpcScheduler class based on configuration settings.\n* @param prefix configuration prefix, @param conf configuration object\n* @return Class of RpcScheduler or default if not found\n*/",
        "org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the RpcScheduler class based on namespace, port, and configuration.\n* @param namespace the namespace for the scheduler\n* @param port the port number\n* @param conf configuration object containing scheduler settings\n* @return Class of RpcScheduler type or default if not found\n*/",
        "org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)": "/**\n* Binds a ServerSocket to a specified address with a default configuration.\n* @param socket the ServerSocket to bind\n* @param address the InetSocketAddress to bind to\n* @param backlog the maximum number of queued connections\n* @throws IOException if binding fails\n*/",
        "org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration)": "/**\n* Refreshes the call queue with new configuration settings.\n* @param conf configuration settings for the queue\n*/",
        "org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)": "/**\n* Refreshes service ACLs using the provided configuration and policy provider.\n* @param conf configuration settings\n* @param provider policy provider for services\n*/",
        "org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)": "/**\n* Authorizes user access based on protocol and client address.\n* @param user user group information\n* @param protocolName fully qualified protocol class name\n* @param addr client InetAddress\n* @throws AuthorizationException if authorization fails\n*/",
        "org.apache.hadoop.ipc.Server:doKerberosRelogin()": "/**\n* Handles Kerberos relogin if the user is not authenticated.\n* @throws IOException if relogin fails\n*/",
        "org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List)": "/**\n* Builds a negotiate response based on available authentication methods.\n* @param authMethods list of supported authentication methods\n* @return RpcSaslProto containing negotiation state and methods\n*/",
        "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)": "/**\n* Initializes the server with configuration and parameters.\n* @param bindAddress address to bind the server\n* @param port port number for the server\n* @param rpcRequestClass class type for RPC requests\n* @param handlerCount number of handler threads\n* @param numReaders number of read threads\n* @param queueSizePerHandler size of the queue per handler\n* @param conf configuration settings\n* @param serverName name of the server\n* @param secretManager secret manager for security\n* @param portRangeConfig config for port range\n*/",
        "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a Server instance with specified parameters.\n* @param bindAddress address to bind the server\n* @param port port number for the server\n* @param paramClass class type for RPC requests\n* @param handlerCount number of handler threads\n*/",
        "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)": "/**\n* Constructs a Server instance with specified configuration parameters.\n* @param bindAddress server binding address\n* @param port server port number\n* @param rpcRequestClass class for RPC requests\n* @param handlerCount number of handler threads\n* @param numReaders number of read threads\n* @param queueSizePerHandler size of queue per handler\n* @param conf configuration settings\n* @param serverName name of the server\n* @param secretManager security secret manager\n*/",
        "org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)": "/**\n* Refreshes service ACLs using provided configuration and policy provider.\n* @param conf configuration settings for the service\n* @param provider policy provider for authorization\n*/"
    },
    "org.apache.hadoop.ipc.RpcScheduler": {
        "org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,int,int,int)": "/**\n* Deprecated method for adding response time; use the updated version instead.\n* @param name identifier for the response\n* @param priorityLevel level of priority\n* @param queueTime time spent in queue\n* @param processingTime time taken for processing\n*/",
        "org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)": "/**** Adds response time using deprecated method for backward compatibility. \n* @param callName identifier for the call\n* @param schedulable the schedulable object with priority level\n* @param details processing details containing timing information\n*/"
    },
    "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer": {
        "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:<init>(java.lang.String,long)": "/**\n* Constructs a ProtoNameVer instance with protocol and version.\n* @param protocol the protocol name\n* @param ver the version number\n*/",
        "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:equals(java.lang.Object)": "/**\n* Compares this ProtoNameVer with another object for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:hashCode()": "/**\n* Computes the hash code based on protocol and version.\n* @return computed hash code as an integer\n*/"
    },
    "org.apache.hadoop.ipc.RPC$Server": {
        "org.apache.hadoop.ipc.RPC$Server:getProtocolImplMap(org.apache.hadoop.ipc.RPC$RpcKind)": "/**\n* Retrieves a map of protocol implementations for a given RPC kind.\n* @param rpcKind the type of RPC to retrieve implementations for\n* @return a map of ProtoNameVer to ProtoClassProtoImpl\n*/",
        "org.apache.hadoop.ipc.RPC$Server:serverNameFromClass(java.lang.Class)": "/**\n* Extracts server name from class name.\n* @param clazz the Class object to extract the server name from\n* @return extracted server name as a String\n*/",
        "org.apache.hadoop.ipc.RPC$Server:call(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String,org.apache.hadoop.io.Writable,long)": "/**\n* Invokes the server RPC with specified parameters.\n* @param rpcKind type of RPC to be called\n* @param protocol communication protocol\n* @param rpcRequest request data to send\n* @param receiveTime timestamp of when the request was received\n* @return Writable response from the RPC call\n*/",
        "org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)": "/**\n* Retrieves supported protocol versions for a given RPC kind and protocol name.\n* @param rpcKind type of RPC\n* @param protocolName name of the protocol\n* @return array of VerProtocolImpl or null if none found\n*/",
        "org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)": "/**\n* Retrieves the highest supported protocol version for a given RPC kind and name.\n* @param rpcKind the type of RPC to check\n* @param protocolName the name of the protocol to find\n* @return VerProtocolImpl object or null if not found\n*/",
        "org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)": "/**\n* Registers a protocol and its implementation for a specific RPC kind.\n* @param rpcKind the type of RPC\n* @param protocolClass the protocol class\n* @param protocolImpl the implementation of the protocol\n*/",
        "org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)": "/**\n* Adds a protocol implementation for a specified RPC kind.\n* @param rpcKind the type of RPC\n* @param protocolClass the protocol class\n* @param protocolImpl the implementation of the protocol\n* @return the current Server instance\n*/",
        "org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration)": "/**** Initializes protocol meta information using the provided configuration. \n* @param conf configuration object for setting up the protocol engine \n*/",
        "org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)": "/**\n* Constructs a Server instance with specified parameters and initializes protocol info.\n* @param bindAddress address to bind the server\n* @param port port number for the server\n* @param paramClass class type for RPC requests\n* @param handlerCount number of handler threads\n* @param numReaders number of read threads\n* @param queueSizePerHandler size of the queue per handler\n* @param conf configuration settings\n* @param serverName name of the server\n* @param secretManager secret manager for security\n* @param portRangeConfig config for port range\n*/"
    },
    "org.apache.hadoop.ipc.ProcessingDetails": {
        "org.apache.hadoop.ipc.ProcessingDetails:<init>(java.util.concurrent.TimeUnit)": "/**\n* Initializes ProcessingDetails with the specified time unit.\n* @param timeUnit the time unit for processing details\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing)": "/**\n* Retrieves timing value for the specified type, ensuring non-negative return.\n* @param type the Timing enum to fetch the value for\n* @return the timing value or 0 if negative\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long)": "/**\n* Sets the timing value for the specified type.\n* @param type the timing category\n* @param value the timing value to set\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:add(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)": "/**\n* Adds a value to the specified timing type.\n* @param type timing category\n* @param value amount to add\n* @param timeUnit unit of the value\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:setReturnStatus(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)": "/**\n* Sets the return status.\n* @param status the RpcStatusProto to be set as return status\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:getReturnStatus()": "/**\n* Retrieves the current RPC status.\n* @return RpcStatusProto object representing the status\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)": "/**\n* Converts timing value to specified time unit.\n* @param type the Timing enum to fetch the value for\n* @param timeUnit the TimeUnit to convert to\n* @return converted timing value in the specified time unit\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:toString()": "/**\n* Constructs a string representation of Timing values and their corresponding times.\n* @return formatted string of Timing names and their time values\n*/",
        "org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)": "/**\n* Sets timing value for a specified type in the given time unit.\n* @param type the timing category\n* @param value the timing value to set\n* @param timeUnit the time unit for the value\n*/"
    },
    "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy": {
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:setDelegate(org.apache.hadoop.ipc.DecayRpcScheduler)": "/**\n* Sets a weak reference to the DecayRpcScheduler delegate.\n* @param obj the DecayRpcScheduler instance to reference\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:removeInstance(java.lang.String)": "/**\n* Removes an instance from the MetricsProxy by namespace.\n* @param namespace identifier for the instance to remove\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary()": "/**\n* Retrieves scheduling decision summary or indicates no active scheduler.\n* @return JSON string of decisions or error message if no scheduler is active\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount()": "/**\n* Retrieves the count of unique identities from the scheduler.\n* @return count of unique identities or -1 if scheduler is null\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume()": "/**\n* Gets total call volume or -1 if scheduler is unavailable.\n* @return total call volume as a long value\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime()": "/**\n* Retrieves average response times, defaulting if scheduler is unavailable.\n* @return array of average response times\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow()": "/**\n* Retrieves response times from the last time window.\n* @return long array of response times or default if scheduler is null\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String)": "/**\n* Registers metrics with a specified namespace.\n* @param namespace unique identifier for the metrics source\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary()": "/**\n* Retrieves call volume summary or indicates no active scheduler.\n* @return JSON summary of call volumes or error message\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String)": "/**\n* Unregisters a metrics source by namespace.\n* @param namespace the namespace for the metrics source\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)": "/**\n* Retrieves metrics using the provided collector if the scheduler is available.\n* @param collector MetricsCollector to gather metrics\n* @param all flag to indicate whether to collect all metrics\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)": "/**\n* Initializes MetricsProxy with namespace and levels, setting delegate and registering metrics.\n* @param namespace unique identifier for the metrics source\n* @param numLevels number of response time levels\n* @param drs DecayRpcScheduler instance to delegate to\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)": "/**\n* Retrieves or creates a MetricsProxy instance.\n* @param namespace unique identifier for the metrics source\n* @param numLevels number of response time levels\n* @param drs DecayRpcScheduler instance to delegate to\n* @return MetricsProxy instance associated with the namespace\n*/"
    },
    "org.apache.hadoop.ipc.DecayRpcScheduler": {
        "org.apache.hadoop.ipc.DecayRpcScheduler:getSchedulingDecisionSummary()": "/**\n* Retrieves a summary of scheduling decisions as a JSON string.\n* @return JSON representation of decisions or error message if serialization fails\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getUniqueIdentityCount()": "/**\n* Returns the count of unique identities.\n* @return number of unique identities in callCosts\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalCallVolume()": "/**\n* Retrieves the total call volume.\n* @return total call volume as a long value\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getAverageResponseTime()": "/**\n* Returns an array of average response times from the last window.\n* @return array of average response times\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getResponseTimeCountInLastWindow()": "/**\n* Returns an array of response times from the last time window.\n* @return long array of response times\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getDefaultThresholds(int)": "/**\n* Generates default thresholds for a specified number of levels.\n* @param numLevels the number of levels to calculate thresholds for\n* @return an array of threshold values\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getDefaultBackOffResponseTimeThresholds(int)": "/**\n* Generates default backoff response time thresholds.\n* @param numLevels number of levels for thresholds\n* @return array of long values representing thresholds\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:isServiceUser(java.lang.String)": "/**\n* Checks if the given username is a service user.\n* @param userName the username to check\n* @return true if userName is a service user, false otherwise\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:updateAverageResponseTime(boolean)": "/**\n* Updates average response time for each level, with optional decay.\n* @param enableDecay if true, applies decay to the last average\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getIdentity(org.apache.hadoop.ipc.Schedulable)": "/**\n* Retrieves identity string for a Schedulable object.\n* @param obj the Schedulable object to get identity for\n* @return identity string or a default unknown identity if not found\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:newSchedulable(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Creates a new Schedulable instance with specified user group information.\n* @param ugi user group information for the Schedulable\n* @return a Schedulable object with default priority level\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getCallCostSnapshot()": "/**\n* Retrieves an unmodifiable snapshot of call costs.\n* @return a map of call identifiers to their costs\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalCallSnapshot()": "/**\n* Retrieves the total decayed call cost.\n* @return long representing the total call snapshot value\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalRawCallVolume()": "/**\n* Retrieves the total raw call volume.\n* @return total raw call volume as a long\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalServiceUserCallVolume()": "/**\n* Retrieves the total call volume for service users.\n* @return total call volume as a long value\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalServiceUserRawCallVolume()": "/**\n* Retrieves the total raw call volume for service users.\n* @return total raw call volume as a long value\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getDecayedCallCosts()": "/**\n* Calculates and returns decayed call costs for users.\n* @return Map of users to their respective decayed costs\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)": "/**\n* Adds cost for a given identity, creating it if absent.\n* @param identity user identifier\n* @param costDelta amount to increment the cost\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)": "/**\n* Computes priority level based on cost and user identity.\n* @param cost cost value to determine priority\n* @param identity user identity object\n* @return priority level as an integer\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)": "/**\n* Sets the priority level for a user group.\n* @param ugi user group information\n* @param priority desired priority level\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary()": "/**\n* Generates a JSON summary of call volumes.\n* @return JSON string of call costs or error message on failure\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds unique identity count to the metrics record.\n* @param rb MetricsRecordBuilder to add the counter to\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds decayed call volume to the metrics record.\n* @param rb MetricsRecordBuilder to update with decayed call volume\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds raw call volume metric to the record builder.\n* @param rb MetricsRecordBuilder to add the counter to\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds decayed call volume metric for service users.\n* @param rb MetricsRecordBuilder to store the metric\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds service user raw call volume to the metrics record.\n* @param rb MetricsRecordBuilder to update with call volume\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds completed call volume gauges by priority.\n* @param rb MetricsRecordBuilder to record the gauges\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds average response time gauges per priority to the MetricsRecordBuilder.\n* @param rb MetricsRecordBuilder to add gauges to\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int)": "/**\n* Retrieves the top N callers based on their call costs.\n* @param n number of top callers to return\n* @return TopN object containing the top callers\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)": "/**\n* Determines if backoff is needed based on response times and priority level.\n* @param obj Schedulable object containing priority level\n* @return true if backoff is required, false otherwise\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache()": "/**\n* Recomputes and updates the schedule cache based on call costs.\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object)": "/**\n* Retrieves priority level from cache or computes it if not available.\n* @param identity user identity object\n* @return priority level as an integer\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder)": "/**\n* Adds summary of top N callers to MetricsRecordBuilder.\n* @param rb MetricsRecordBuilder to collect caller metrics\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts()": "/**\n* Decays current call costs and updates totals for service users.\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)": "/**\n* Computes the priority level for a Schedulable object.\n* @param obj the Schedulable object\n* @return priority level as an integer, non-negative\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Retrieves the priority level for a user based on their group information.\n* @param ugi user group information\n* @return priority level as an integer\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:stop()": "/**\n* Stops metrics collection by unregistering sources and shutting down instances.\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)": "/**\n* Collects and records various metrics using the provided MetricsCollector.\n* @param collector MetricsCollector to gather metrics\n* @param all flag to indicate whether to collect all metrics\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay()": "/**\n* Forces decay of current costs for service users.\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)": "/**\n* Records response time metrics for a scheduled call.\n* @param callName identifier for the call\n* @param schedulable task with priority level\n* @param details processing metrics details\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses service user names from configuration.\n* @param ns namespace for the property key\n* @param conf configuration object containing properties\n* @return Set of service user names or empty set if none found\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses decay period in milliseconds from configuration.\n* @param ns namespace for property keys\n* @param conf configuration object\n* @return decay period in milliseconds\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses a decay factor from configuration.\n* @param ns namespace for the property keys\n* @param conf configuration source\n* @return decay factor between 0 and 1\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses backoff setting from configuration based on namespace.\n* @param ns namespace for the property\n* @param conf configuration object\n* @return Boolean value indicating backoff setting\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)": "/**\n* Parses threshold percentages from configuration and converts to decimals.\n* @param ns namespace for configuration keys\n* @param conf configuration object\n* @param numLevels expected number of levels\n* @return array of decimal thresholds\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)": "/**\n* Parses backoff response time thresholds from configuration.\n* @param ns namespace for the configuration\n* @param conf configuration object\n* @param numLevels expected number of thresholds\n* @return array of response time thresholds\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses and returns the CostProvider based on the namespace and configuration.\n* @param ns namespace string for the provider\n* @param conf configuration object containing provider details\n* @return CostProvider instance or DefaultCostProvider if none found\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Parses and retrieves the first IdentityProvider from configuration.\n* @param ns namespace for identity provider lookup\n* @param conf configuration object containing provider details\n* @return the first IdentityProvider or UserIdentityProvider if none found\n*/",
        "org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes DecayRpcScheduler with configuration and metrics.\n* @param numLevels number of priority levels\n* @param ns namespace for configuration\n* @param conf configuration object\n*/"
    },
    "org.apache.hadoop.ipc.ProtocolSignature$1": {
        "org.apache.hadoop.ipc.ProtocolSignature$1:<init>()": "/**\n* Constructs a new ProtocolSignature instance.\n*/"
    },
    "org.apache.hadoop.ipc.Server$ConnectionManager": {
        "org.apache.hadoop.ipc.Server$ConnectionManager:add(org.apache.hadoop.ipc.Server$Connection)": "/**\n* Adds a connection to the set and increments count if successful.\n* @param connection the connection to add\n* @return true if added, false if already present\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:remove(org.apache.hadoop.ipc.Server$Connection)": "/**\n* Removes a connection and decrements the count if successful.\n* @param connection the connection to be removed\n* @return true if removed, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:incrUserConnections(java.lang.String)": "/**\n* Increments the connection count for a specified user.\n* @param user the username whose connections are to be incremented\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:decrUserConnections(java.lang.String)": "/**\n* Decreases the connection count for a specified user.\n* @param user the identifier of the user whose connections are decremented\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:getDroppedConnections()": "/**\n* Retrieves the count of dropped connections.\n* @return number of dropped connections as a long\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:size()": "/**\n* Returns the current size count.\n* @return the current size as an integer\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:toArray()": "/**\n* Converts the connections list to an array of Connection objects.\n* @return array of Connection objects\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:scheduleIdleScanTask()": "/**\n* Schedules a recurring task to scan for idle resources.\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:stopIdleScan()": "/**\n* Stops the idle scan timer.\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:getUserToConnectionsMap()": "/**\n* Retrieves a map of users to their connection counts.\n* @return Map with user names as keys and connection counts as values\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:isFull()": "/**\n* Checks if the connection pool is full.\n* @return true if full, false if not or maxConnections <= 0\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan()": "/**\n* Initiates the process of scanning for idle resources.\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection)": "/**\n* Closes a connection if it exists and decrements user connections.\n* @param connection the connection to be closed\n* @return true if the connection was removed, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)": "/**\n* Registers a new connection if the pool is not full.\n* @param channel socket channel for the connection\n* @param ingressPort port number for incoming traffic\n* @param isOnAuxiliaryPort indicates if it's an auxiliary port\n* @return Connection object or null if the pool is full\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean)": "/**\n* Closes idle connections based on last contact time.\n* @param scanAll whether to scan all connections regardless of count\n*/",
        "org.apache.hadoop.ipc.Server$ConnectionManager:closeAll()": "/**\n* Closes all active connections by iterating over a copied list.\n*/"
    },
    "org.apache.hadoop.ipc.Server$Connection": {
        "org.apache.hadoop.ipc.Server$Connection:getLastContact()": "/**\n* Retrieves the timestamp of the last contact.\n* @return last contact time in milliseconds since epoch\n*/",
        "org.apache.hadoop.ipc.Server$Connection:setLastContact(long)": "/**\n* Sets the last contact timestamp.\n* @param lastContact time of last contact in milliseconds\n*/",
        "org.apache.hadoop.ipc.Server$Connection:shouldClose()": "/**\n* Indicates if the resource should be closed.\n* @return true if the resource should be closed, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getHostInetAddress()": "/**\n* Retrieves the InetAddress of the host.\n* @return InetAddress object representing the host's address\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getRemotePort()": "/**\n* Retrieves the remote port number.\n* @return the remote port as an integer\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getServer()": "/**\n* Returns the current Server instance.\n* @return the Server object associated with this instance\n*/",
        "org.apache.hadoop.ipc.Server$Connection:toString()": "/**\n* Returns a string representation of the host address and remote port.\n* @return formatted string \"hostAddress:remotePort\"\n*/",
        "org.apache.hadoop.ipc.Server$Connection:isIdle()": "/**\n* Checks if the current state is idle based on RPC count.\n* @return true if idle, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Connection:decRpcCount()": "/**\n* Decrements the RPC count atomically.\n*/",
        "org.apache.hadoop.ipc.Server$Connection:incRpcCount()": "/**\n* Increments the RPC count atomically.\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getTrueCause(java.io.IOException)": "/**\n* Retrieves the true cause of an IOException, prioritizing specific exception types.\n* @param e the IOException to analyze\n* @return the first matching cause or the original IOException if none found\n*/",
        "org.apache.hadoop.ipc.Server$Connection:disposeSasl()": "/**\n* Disposes of the SASL server instance, handling exceptions and nullifying reference.\n*/",
        "org.apache.hadoop.ipc.Server$Connection:buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])": "/**\n* Builds an RpcSaslProto response with the given state and optional reply token.\n* @param state current SASL state\n* @param replyToken optional byte array for the response token\n* @return constructed RpcSaslProto object\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getHostAddress()": "/**\n* Retrieves the host address.\n* @return the host address as a String\n*/",
        "org.apache.hadoop.ipc.Server$Connection:setServiceClass(int)": "/**\n* Sets the service class value.\n* @param serviceClass integer representing the service class\n*/",
        "org.apache.hadoop.ipc.Server$Connection:isOnAuxiliaryPort()": "/**\n* Checks if the device is connected to the auxiliary port.\n* @return true if connected, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getEstablishedQOP()": "/**\n* Retrieves the established Quality of Protection (QOP) value.\n* @return establishedQOP as a String\n*/",
        "org.apache.hadoop.ipc.Server$Connection:setShouldClose()": "/**\n* Sets shouldClose to true and returns the updated value.\n* @return true after setting shouldClose\n*/",
        "org.apache.hadoop.ipc.Server$Connection:switchToSimple()": "/**\n* Switches authentication protocol to simple by disabling SASL.\n*/",
        "org.apache.hadoop.ipc.Server$Connection:close()": "/**\n* Closes resources and disposes of SASL server, logging exceptions during cleanup.\n*/",
        "org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)": "/**\n* Processes SASL token and returns the response.\n* @param saslMessage input SASL message containing the token\n* @return RpcSaslProto response based on token evaluation\n*/",
        "org.apache.hadoop.ipc.Server$Connection:checkDataLength(int)": "/**\n* Validates data length and throws IOException for invalid values.\n* @param dataLength length of the data to check\n*/",
        "org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall)": "/**\n* Sends response for the given RPC call.\n* @param call the RPC call to respond to\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String)": "/****\n* Retrieves UserGroupInformation based on authorization ID.\n* @param authorizedId the ID used for authorization\n* @return UserGroupInformation instance\n* @throws InvalidToken if token deserialization fails\n* @throws AccessControlException if username retrieval fails\n*/",
        "org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto)": "/**** Checks RPC headers for validity and throws exceptions for invalid cases. \n* @param header the RPC request header to validate\n* @throws RpcServerException if the header is invalid\n*/",
        "org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**** Retrieves a message from a buffer and handles deserialization errors. \n* @param message the message to decode \n* @param buffer the buffer containing the message data \n* @return the decoded message of type T \n* @throws RpcServerException if deserialization fails \n*/",
        "org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message)": "/**\n* Handles SASL reply by setting up and sending a response.\n* @param message the message to be wrapped and sent\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception)": "/**\n* Handles SASL reply on authentication failure.\n* @param ioe the exception indicating the authentication error\n* @throws IOException if an I/O error occurs during response processing\n*/",
        "org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int)": "/****  \n* Prepares and sends a version mismatch response based on client version.  \n* @param clientVersion the version of the client making the request  \n* @throws IOException if an I/O error occurs during response setup or sending  \n*/",
        "org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse()": "/**\n* Sets up and sends an HTTP request response over IPC.\n* @throws IOException if an I/O error occurs during setup or sending\n*/",
        "org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**\n* Processes an RPC request, validates, and queues it for execution.\n* @param header RPC request header containing metadata\n* @param buffer buffer for reading the RPC request data\n* @throws RpcServerException if RPC processing fails\n* @throws InterruptedException if the thread is interrupted\n*/",
        "org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int)": "/**** Initializes authentication context based on authType. \n* @param authType type of authentication method \n* @return AuthProtocol object \n* @throws IOException if an error occurs during initialization \n*/",
        "org.apache.hadoop.ipc.Server$Connection:authorizeConnection()": "/**\n* Authorizes user connection based on authentication method and host address.\n* @throws RpcServerException if authorization fails\n*/",
        "org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**\n* Processes the connection context and manages user authentication.\n* @param buffer the buffer containing connection context data\n* @throws RpcServerException if connection context is invalid or unauthorized\n*/",
        "org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Creates a SASL server using the specified authentication method.\n* @param authMethod the authentication method for SASL\n* @return configured SaslServer instance\n*/",
        "org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse()": "/**\n* Builds SASL negotiate response with initial challenge if TOKEN auth is enabled.\n* @return RpcSaslProto negotiate message with challenge\n*/",
        "org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)": "/**\n* Processes SASL messages and returns a response based on the current state.\n* @param saslMessage input SASL message\n* @return RpcSaslProto response based on the message processing\n*/",
        "org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)": "/**\n* Processes SASL messages and manages authentication flow.\n* @param saslMessage input SASL message\n* @throws RpcServerException if RPC header is invalid\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**\n* Reads and processes SASL messages from a buffer.\n* @param buffer input buffer containing the SASL message\n* @throws RpcServerException for RPC header issues\n* @throws IOException for I/O errors\n* @throws InterruptedException if interrupted\n*/",
        "org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**** Processes out-of-band RPC requests based on the call ID and buffer data. */",
        "org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer)": "/**\n* Processes a single RPC request from a ByteBuffer.\n* @param bb ByteBuffer containing the RPC request data\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the thread is interrupted\n*/",
        "org.apache.hadoop.ipc.Server$Connection:readAndProcess()": "/**\n* Reads and processes RPC requests; returns count of bytes read.\n* @return number of bytes read or -1 if closed or error occurs\n*/",
        "org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[])": "/**\n* Unwraps and processes RPCs from the input byte array.\n* @param inBuf byte array containing the input data\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the thread is interrupted\n*/"
    },
    "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy": {
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:setDelegate(org.apache.hadoop.ipc.FairCallQueue)": "/**\n* Sets a delegate FairCallQueue and increments the revision number.\n* @param obj the FairCallQueue to set as the delegate\n*/",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getCallQueue()": "/**\n* Retrieves the FairCallQueue from a weak reference.\n* @return FairCallQueue or null if reference is cleared\n*/",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getRevision()": "",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes()": "/**\n* Retrieves sizes of queues from the FairCallQueue.\n* @return array of integers representing queue sizes or empty if not available\n*/",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls()": "/**\n* Retrieves overflowed call counts from the current call queue.\n* @return array of long representing overflowed call counts\n*/",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)": "/**\n* Collects and records metrics for the FairCallQueue.\n* @param collector the MetricsCollector for recording metrics\n* @param all flag to determine if all metrics should be collected\n*/",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String)": "/**\n* Initializes MetricsProxy with a namespace and registers MBeans.\n* @param namespace identifier for the MBean registration\n*/",
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String)": "/**\n* Retrieves or creates a MetricsProxy instance for the given namespace.\n* @param namespace identifier for the MBean registration\n* @return MetricsProxy instance associated with the namespace\n*/"
    },
    "org.apache.hadoop.ipc.Server$ExceptionsHandler": {
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:addTerseLoggingExceptions(java.lang.Class[])": "/**\n* Adds exception classes to terse logging set.\n* @param exceptionClass variable number of exception classes\n*/",
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:addSuppressedLoggingExceptions(java.lang.Class[])": "/**\n* Adds exception classes to the suppressed logging list.\n* @param exceptionClass variable number of exception classes\n*/",
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:isTerseLog(java.lang.Class)": "/**\n* Checks if the class is in the list of terse exceptions.\n* @param t class to check\n* @return true if terse, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:isSuppressedLog(java.lang.Class)": "/**\n* Checks if a class is in the suppressed log list.\n* @param t class to check for suppression\n* @return true if suppressed, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl": {
        "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl:<init>(java.lang.Class,java.lang.Object)": "/**\n* Initializes ProtoClassProtoImpl with protocol class and implementation.\n* @param protocolClass the class type of the protocol\n* @param protocolImpl the implementation of the protocol\n*/",
        "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl:isShadedPBImpl()": "/**\n* Checks if the shadedPBImpl flag is set.\n* @return true if shadedPBImpl is true, otherwise false\n*/"
    },
    "org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl": {
        "org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl:<init>(long,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)": "/**\n* Constructs a VerProtocolImpl with specified version and protocol target.\n* @param ver the version number\n* @param protocolTarget the target protocol implementation\n*/"
    },
    "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB": {
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.RPC$Server)": "/**\n* Initializes the ProtocolMetaInfoServerSideTranslatorPB with a server instance.\n* @param server RPC.Server instance for handling requests\n*/",
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)": "/**\n* Retrieves protocol versions for a given RPC kind and protocol class.\n* @param rpcKind type of RPC\n* @param protocol name of the protocol class\n* @return array of protocol versions or null if none found\n*/",
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)": "/**\n* Retrieves supported protocol versions for a given RPC request.\n* @param controller RPC controller for handling calls\n* @param request contains the protocol name\n* @return response with protocol versions\n*/",
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)": "/**\n* Retrieves protocol signatures based on RPC kind and protocol request.\n* @param controller RPC controller\n* @param request contains protocol and RPC kind info\n* @return GetProtocolSignatureResponseProto object\n*/"
    },
    "org.apache.hadoop.ipc.RpcException": {
        "org.apache.hadoop.ipc.RpcException:<init>(java.lang.String)": "/**\n* Constructs an RpcException with a specified error message.\n* @param message descriptive error message for the exception\n*/",
        "org.apache.hadoop.ipc.RpcException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an RpcException with a message and cause.\n* @param message error description\n* @param cause underlying throwable cause\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequestHeader(java.lang.reflect.Method)": "/**\n* Constructs an RPC request header from the given method.\n* @param method the method to extract information from\n* @return constructed RequestHeaderProto object\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnProtoType(java.lang.reflect.Method)": "/**\n* Retrieves or creates a default Message prototype for a given method.\n* @param method the method to get the return prototype from\n* @return Message prototype object\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getConnectionId()": "",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)": "/**\n* Constructs an RPC request with a header and message.\n* @param method the RPC method to construct the header\n* @param theRequest the message content for the request\n* @return Writable object representing the RPC request\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close()": "/**\n* Closes the client if not already closed, stopping it and releasing resources.\n* @throws IOException if an error occurs while stopping the client\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**\n* Retrieves a Message based on the given method and buffer.\n* @param method the method to derive the return Message from\n* @param buf the buffer to read the Message value\n* @return the constructed Message object\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes an RPC method and handles tracing, exceptions, and response retrieval.\n* @param proxy the proxy instance, @param method the invoked method, @param args method arguments\n* @return Message object containing the response\n* @throws ServiceException for invalid parameters or RPC errors\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Initializes an Invoker with protocol and connection details.\n* @param protocol protocol class type\n* @param connId connection identifier\n* @param conf configuration settings\n* @param factory socket factory for connections\n* @param alignmentContext context for alignment\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Constructs an Invoker with protocol and connection details.\n* @param protocol protocol class type\n* @param addr connection address\n* @param ticket user authentication info\n* @param conf configuration settings\n* @param factory socket factory for connections\n* @param rpcTimeout timeout for RPC calls\n* @param connectionRetryPolicy retry policy for connections\n* @param fallbackToSimpleAuth indicates fallback to simple auth\n* @param alignmentContext context for alignment\n*/"
    },
    "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer": {
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:setCapacity(int)": "/**\n* Sets the buffer capacity, adjusting for framing bytes.\n* @param capacity new buffer size excluding framing bytes\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:setSize(int)": "/**\n* Sets the size in the first four bytes of the buffer.\n* @param size the size value to be stored\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:capacity()": "/**\n* Calculates the available capacity of the buffer.\n* @return the capacity of the buffer after subtracting framing bytes\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset()": "/**\n* Resets the count to FRAMING_BYTES and size to zero.\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int)": "/**\n* Initializes FramedBuffer with specified capacity and resets state.\n* @param capacity buffer capacity excluding framing bytes\n*/"
    },
    "org.apache.hadoop.ipc.ProxyCombiner": {
        "org.apache.hadoop.ipc.ProxyCombiner:<init>()": "/**\n* Private constructor for ProxyCombiner to prevent instantiation.\n*/",
        "org.apache.hadoop.ipc.ProxyCombiner:combine(java.lang.Class,java.lang.Object[])": "/**\n* Combines multiple proxy objects into a single proxy of the specified interface.\n* @param combinedProxyInterface interface for the combined proxy\n* @param proxies array of proxy objects to combine\n* @return combined proxy instance of type T\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufHelper": {
        "org.apache.hadoop.ipc.ProtobufHelper:<init>()": "/**\n* Private constructor to prevent instantiation of the ProtobufHelper class.\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(com.google.protobuf.ServiceException)": "/**\n* Retrieves an IOException from a ServiceException.\n* @param se the ServiceException to extract the IOException from\n* @return IOException corresponding to the ServiceException\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException)": "/**\n* Converts ServiceException to IOException using helper method.\n* @param se ServiceException to convert\n* @return IOException derived from the ServiceException\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String)": "/**\n* Retrieves a ByteString from cache using the provided key.\n* @param key the string key for the ByteString\n* @return the corresponding ByteString object\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[])": "/**\n* Returns ByteString from a byte array, optimizing for singleton usage.\n* @param bytes input byte array\n* @return ByteString representation of the input bytes\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto)": "/**\n* Converts TokenProto to a Token instance.\n* @param tokenProto the TokenProto containing token data\n* @return constructed Token object\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text)": "/**\n* Retrieves a ByteString from a Text key.\n* @param key the Text key for fetching ByteString\n* @return corresponding ByteString from cache or newly created\n*/",
        "org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token)": "/**\n* Converts a Token object to its TokenProto representation.\n* @param tok the Token object to convert\n* @return TokenProto built from the Token data\n*/"
    },
    "org.apache.hadoop.ipc.internal.ShadedProtobufHelper": {
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException)": "/**\n* Converts ServiceException to IOException, preserving the cause if it's an IOException.\n* @param se ServiceException to convert\n* @return IOException based on the cause of the ServiceException\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(java.lang.String)": "/**\n* Retrieves or creates a ByteString from a cache using the provided key.\n* @param key the string key for the ByteString\n* @return the corresponding ByteString object\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getByteString(byte[])": "/**\n* Converts byte array to ByteString, returning EMPTY for zero-length array.\n* @param bytes input byte array\n* @return ByteString representation of the input bytes\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:<init>()": "/**\n* Private constructor to prevent instantiation of utility class with static methods.\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall)": "/**\n* Executes an IPC call and handles exceptions.\n* @param call the IpcCall to execute\n* @return result of the IPC call\n* @throws IOException if a ServiceException occurs\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto)": "/**\n* Creates a Token from a TokenProto object.\n* @param tokenProto the TokenProto containing token data\n* @return constructed Token instance\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text)": "/**\n* Retrieves or creates a ByteString from a Text key.\n* @param key the Text key for fetching or generating ByteString\n* @return the corresponding ByteString from cache or newly created\n*/",
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token)": "/**\n* Converts a Token object to a TokenProto representation.\n* @param tok the Token object to convert\n* @return TokenProto built from the Token data\n*/"
    },
    "org.apache.hadoop.ipc.RetryCache$CacheEntry": {
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:completed(boolean)": "/**\n* Updates state to SUCCESS or FAILED and notifies waiting threads.\n* @param success indicates completion status\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:equals(java.lang.Object)": "/**\n* Compares this CacheEntry to another object for equality.\n* @param obj the object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(long)": "/**\n* Computes the hash code for a given long value.\n* @param value the long value to hash\n* @return the computed hash code as an int\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:toString()": "/**\n* Returns a string representation of the object in UUID:callId:state format.\n* @return formatted string of UUID, callId, and state\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:getExpirationTime()": "/**\n* Retrieves the expiration time.\n* @return the expiration time in milliseconds\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:getNext()": "/**\n* Retrieves the next linked element.\n* @return next LinkedElement or null if it is the last element\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:isSuccess()": "/**\n* Checks if the current state indicates success.\n* @return true if state is SUCCESS, otherwise false\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:setExpirationTime(long)": "/**\n* Sets the expiration time, but does not modify it.\n* @param timeNano expiration time in nanoseconds\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:setNext(org.apache.hadoop.util.LightWeightGSet$LinkedElement)": "/**\n* Sets the next LinkedElement in the chain.\n* @param next the next LinkedElement to set\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)": "/**\n* Constructs a CacheEntry with validated clientId, callId, and expirationTime.\n* @param clientId byte array representing the client ID (must be 16 bytes)\n* @param callId identifier for the call\n* @param expirationTime time until the entry expires\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode()": "/**\n* Computes the hash code for the object using client IDs and call ID.\n* @return computed hash code as an int\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)": "/**\n* Constructs a CacheEntry with success state.\n* @param clientId byte array representing the client ID\n* @param callId identifier for the call\n* @param expirationTime time until the entry expires\n* @param success indicates if the operation was successful\n*/"
    },
    "org.apache.hadoop.ipc.Client$Connection$2": {
        "org.apache.hadoop.ipc.Client$Connection$2:run()": "/**\n* Executes the main thread for handling RPC responses and managing connections.\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:getRequestHeader()": "/**\n* Retrieves the request header, initializing it if necessary.\n* @return RequestHeaderProto object or null if not set\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:writeTo(org.apache.hadoop.ipc.ResponseBuffer)": "/**\n* Writes request header and payload to the response buffer.\n* @param out the ResponseBuffer to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>()": "/**\n* Constructs a new RpcProtobufRequest instance.\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)": "/**\n* Initializes RpcProtobufRequest with header and payload.\n* @param header request metadata\n* @param payload message content\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString()": "/**\n* Returns a string representation of the object.\n* @return formatted string of class and method names\n*/"
    },
    "org.apache.hadoop.ipc.DefaultRpcScheduler": {
        "org.apache.hadoop.ipc.DefaultRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes DefaultRpcScheduler with specified priority levels and configuration.\n* @param priorityLevels number of priority levels for scheduling\n* @param namespace identifier for the namespace\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.ipc.DefaultRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)": "",
        "org.apache.hadoop.ipc.DefaultRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)": "",
        "org.apache.hadoop.ipc.DefaultRpcScheduler:stop()": ""
    },
    "org.apache.hadoop.ipc.WritableRpcEngine": {
        "org.apache.hadoop.ipc.WritableRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Retrieves a protocol meta info proxy, but throws UnsupportedOperationException.\n* @param connId connection identifier\n* @param conf configuration settings\n* @param factory socket factory for connections\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:initialize()": "/**\n* Initializes the RPC server protocol engine.\n* Registers the writable RPC engine and sets initialization flag.\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized()": "/**\n* Ensures the RPC server protocol engine is initialized.\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a Client instance using the provided configuration.\n* @param conf configuration settings for the Client\n* @return Client object from cache or newly created\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates a proxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance wrapping the proxy\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Retrieves a proxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance wrapping the proxy\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)": "/**** Retrieves a proxy for the specified protocol type. \n* @param protocol the protocol class type \n* @return ProtocolProxy instance wrapping the proxy \n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates and returns an RPC server with specified parameters.\n* @return RPC.Server instance configured with provided settings\n*/"
    },
    "org.apache.hadoop.ipc.AsyncCallLimitExceededException": {
        "org.apache.hadoop.ipc.AsyncCallLimitExceededException:<init>(java.lang.String)": "/**\n* Constructs an AsyncCallLimitExceededException with a specified message.\n* @param message detail message for the exception\n*/"
    },
    "org.apache.hadoop.ipc.Client$Call": {
        "org.apache.hadoop.ipc.Client$Call:setAlignmentContext(org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Sets the alignment context.\n* @param ac the AlignmentContext to be set\n*/",
        "org.apache.hadoop.ipc.Client$Call:getRpcResponse()": "/**\n* Retrieves the RPC response.\n* @return Writable object representing the RPC response\n*/",
        "org.apache.hadoop.ipc.Client$Call:toString()": "/**\n* Returns a string representation of the object with class name and ID.\n* @return formatted string of class name and object's ID\n*/",
        "org.apache.hadoop.ipc.Client$Call:callComplete()": "/**\n* Marks the call as complete and notifies waiting threads.\n*/",
        "org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)": "/**\n* Initializes a Call with RPC kind and parameters, assigning IDs and retry counts.\n* @param rpcKind type of RPC call\n* @param param parameters for the RPC call\n*/",
        "org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException)": "/**\n* Sets an IOException and marks the call as complete.\n* @param error the IOException to set\n*/",
        "org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable)": "/**\n* Sets the RPC response and marks the call as complete.\n* @param rpcResponse the response to be set\n*/"
    },
    "org.apache.hadoop.ipc.Client$Connection": {
        "org.apache.hadoop.ipc.Client$Connection:getRemoteAddress()": "/**\n* Retrieves the remote server's socket address.\n* @return InetSocketAddress of the remote server\n*/",
        "org.apache.hadoop.ipc.Client$Connection:addCall(org.apache.hadoop.ipc.Client$Call)": "/**\n* Adds a Call object if the connection is open.\n* @param call the Call object to be added\n* @return true if added, false if connection is closed\n*/",
        "org.apache.hadoop.ipc.Client$Connection:writeConnectionHeader(org.apache.hadoop.ipc.Client$IpcStreams)": "/**\n* Writes the connection header to the output stream without flushing.\n* @param streams IPC stream containing the output stream\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.ipc.Client$Connection:markClosed(java.io.IOException)": "/**\n* Marks the connection as closed and stores the exception.\n* @param e the IOException causing the closure\n*/",
        "org.apache.hadoop.ipc.Client$Connection:closeConnection()": "/**\n* Closes the current socket connection and resets the socket to null.\n*/",
        "org.apache.hadoop.ipc.Client$Connection:interruptConnectingThread()": "/**\n* Interrupts the connecting thread if it is currently active.\n*/",
        "org.apache.hadoop.ipc.Client$Connection:touch()": "/**\n* Updates last activity timestamp to the current time.\n*/",
        "org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)": "/**\n* Handles SASL connection failures with retries.\n* @param currRetries current retry count\n* @param maxRetries maximum allowed retries\n* @param ex the IOException encountered\n* @param rand random number generator for backoff\n* @param ugi user group information for privileged actions\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.ipc.Client$Connection:disposeSasl()": "/**\n* Disposes of SASL client resources if initialized.\n* @throws IOException if an error occurs during disposal\n*/",
        "org.apache.hadoop.ipc.Client$Connection:sendPing()": "/**\n* Sends a ping request if the interval since last activity has elapsed.\n* @throws IOException if an I/O error occurs during sending or flushing\n*/",
        "org.apache.hadoop.ipc.Client$Connection:waitForWork()": "/**** Waits for work or handles connection closure. \n* @return true if there are pending calls, false otherwise. \n*/",
        "org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)": "/**\n* Handles connection timeout by closing the connection and checking retry limits.\n* @param curRetries current retry count\n* @param maxRetries maximum allowed retries\n* @param ioe IOException to throw if retries exceeded\n*/",
        "org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)": "/**\n* Handles connection failures by retrying based on policy or closing the connection.\n* @param curRetries current retry attempt count\n* @param ioe the IOException encountered during connection\n*/",
        "org.apache.hadoop.ipc.Client$Connection:cleanupCalls()": "/**\n* Cleans up call entries and sets a local exception for each.\n* @param closeException the IOException to set for each Call\n*/",
        "org.apache.hadoop.ipc.Client$Connection:close()": "/**\n* Closes the connection if it's marked for closure and cleans up resources.\n* @throws IOException if an error occurs during cleanup\n*/",
        "org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)": "/**\n* Sends connection context and header to the output stream.\n* @param remoteId connection identifier\n* @param authMethod authentication method used\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call)": "/**\n* Sends an RPC request after serializing the call.\n* @param call contains RPC details and context\n* @throws InterruptedException if interrupted while waiting\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse()": "/**** Receives and processes RPC responses, handling success and error cases. */",
        "org.apache.hadoop.ipc.Client$Connection:updateAddress()": "/**\n* Updates the server address if changed and logs the update.\n* @return true if address was updated, false otherwise\n*/",
        "org.apache.hadoop.ipc.Client$Connection:run()": "/**\n* Manages RPC response processing and connection lifecycle.\n* @throws Throwable on unexpected errors during response handling\n*/",
        "org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation)": "/**\n* Establishes a connection using the provided Kerberos ticket.\n* @param ticket user credentials for Kerberos authentication\n* @throws IOException if connection setup fails\n*/",
        "org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams)": "/**\n* Sets up SASL connection using provided IPC streams.\n* @param streams IPC communication streams\n* @return AuthMethod negotiated authentication method\n*/",
        "org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean)": "/**\n* Sets fallback to simple authentication based on current auth settings.\n* @param fallbackToSimpleAuth indicates if fallback is enabled\n* @throws AccessControlException if fallback is not allowed\n*/",
        "org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)": "/**\n* Sets up IO streams for IPC connection with optional fallback to simple auth.\n* @param fallbackToSimpleAuth indicates if fallback authentication is enabled\n*/",
        "org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb()": "/**\n* Determines if authentication over Kerberos is required.\n* @return true if Kerberos authentication is needed, false otherwise\n* @throws IOException if user information retrieval fails\n*/"
    },
    "org.apache.hadoop.ipc.Server$Listener": {
        "org.apache.hadoop.ipc.Server$Listener:getSelector()": "/**\n* Retrieves the current Selector instance in a thread-safe manner.\n* @return the Selector object\n*/",
        "org.apache.hadoop.ipc.Server$Listener:closeCurrentConnection(java.nio.channels.SelectionKey,java.lang.Throwable)": "/**\n* Closes the current connection associated with the given SelectionKey.\n* @param key the SelectionKey for the connection\n* @param e optional Throwable for error handling\n*/",
        "org.apache.hadoop.ipc.Server$Listener:getAddress()": "/**\n* Retrieves the local socket address of the accept channel.\n* @return InetSocketAddress representing the local address\n*/",
        "org.apache.hadoop.ipc.Server$Listener:getReader()": "/**\n* Retrieves the next Reader in a circular manner.\n* @return the next Reader from the readers array\n*/",
        "org.apache.hadoop.ipc.Server$Listener:setIsAuxiliary()": "/**\n* Sets the auxiliary port status to true.\n*/",
        "org.apache.hadoop.ipc.Server$Listener:doStop()": "/**\n* Stops the selector and closes the accept channel, shutting down all readers.\n*/",
        "org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey)": "/**\n* Accepts new connections and registers them, handling failures and cleanup.\n* @param key the selection key for the server socket\n* @throws InterruptedException if interrupted while accepting connections\n* @throws IOException if an I/O error occurs\n* @throws OutOfMemoryError if memory is insufficient for new connections\n*/",
        "org.apache.hadoop.ipc.Server$Listener:run()": "/**\n* Manages server operations, handling connections and idle scans.\n*/",
        "org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey)": "/**\n* Reads data from the connection and manages its state based on the read count.\n* @param key the selection key associated with the connection\n*/"
    },
    "org.apache.hadoop.ipc.Server$Listener$Reader": {
        "org.apache.hadoop.ipc.Server$Listener$Reader:addConnection(org.apache.hadoop.ipc.Server$Connection)": "/**\n* Adds a new connection and wakes up the read selector.\n* @param conn the connection to be added\n* @throws InterruptedException if the thread is interrupted while waiting\n*/",
        "org.apache.hadoop.ipc.Server$Listener$Reader:shutdown()": "/**\n* Shuts down the current thread safely after asserting it's not running.\n*/",
        "org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop()": "/**\n* Processes incoming connections in a loop until stopped.\n* Handles reading and registration of connections.\n*/",
        "org.apache.hadoop.ipc.Server$Listener$Reader:run()": "/**\n* Runs the main loop for processing connections and handles cleanup on exit.\n*/"
    },
    "org.apache.hadoop.ipc.ClientCache": {
        "org.apache.hadoop.ipc.ClientCache:clearCache()": "/**\n* Clears the cache by stopping all clients and removing them from the cache.\n*/",
        "org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client)": "/**\n* Stops the client and removes it from cache if no references remain.\n* @param client the Client instance to stop\n*/",
        "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)": "/**\n* Retrieves or creates a cached Client instance.\n* @param conf configuration settings for the Client\n* @param factory socket factory for connections\n* @param valueClass class type for writable values\n* @return Client object from cache or newly created\n*/",
        "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a synchronized Client instance with default socket factory.\n* @param conf configuration settings for the Client\n* @return Client object from cache or newly created\n*/",
        "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Retrieves a cached Client instance with specified configuration and socket factory.\n* @param conf configuration settings for the Client\n* @param factory socket factory for connections\n* @return Client object from cache or newly created\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)": "/**\n* Retrieves the appropriate RPC invoker based on the RPC kind.\n* @param rpcKind type of RPC to determine the invoker\n* @return RpcInvoker instance for the specified RPC kind\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2()": "/**\n* Registers a callback for deferred responses.\n* @return ProtobufRpcEngineCallback2 instance for handling responses\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Initializes a Server instance with protocol and configuration settings.\n* @param protocolClass RPC protocol class\n* @param protocolImpl implementation of the protocol\n* @param conf configuration settings\n* @param bindAddress address to bind the server\n* @param port port number for the server\n* @param numHandlers number of handler threads\n* @param numReaders number of read threads\n* @param queueSizePerHandler size of the queue per handler\n* @param verbose enables verbose logging\n* @param secretManager security token manager\n* @param portRangeConfig config for port range\n* @param alignmentContext context for alignment\n*/"
    },
    "org.apache.hadoop.ipc.Server$RpcCall": {
        "org.apache.hadoop.ipc.Server$RpcCall:isOpen()": "/**\n* Checks if the connection channel is open.\n* @return true if the channel is open, false otherwise\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:setResponseFields(org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)": "/**\n* Sets the response fields with provided return value and parameters.\n* @param returnValue the value to be returned\n* @param responseParams parameters for the response\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:sendDeferedResponse()": "/**\n* Sends a deferred response and logs an error if sending fails.\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:setResponse(java.nio.ByteBuffer)": "/**\n* Sets the RPC response buffer.\n* @param response ByteBuffer containing the response data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)": "/**\n* Handles RPC response based on Throwable and status.\n* @param t Throwable causing the response, or null for success\n* @param status RpcStatusProto indicating response status\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress()": "/**\n* Returns the host's InetAddress from the connection.\n* @return InetAddress representing the host's address\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:getRemotePort()": "/**\n* Returns the remote port number from the connection.\n* @return the remote port as an integer\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable)": "/**** Sets up and sends a deferred response if the server is running. \n* @param response the response to be sent deferred\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:toString()": "/**\n* Returns a string representation of the call, including RPC request and connection info.\n* @return formatted string with super toString, rpcRequest, and connection details\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)": "/**\n* Populates response parameters based on the provided Throwable error.\n* @param t Throwable error to process\n* @param responseParams ResponseParams object to populate with error details\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:run()": "/**\n* Executes a remote procedure call and handles response processing.\n* @return null as the method is void\n*/",
        "org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable)": "/**\n* Sets a deferred error response based on a Throwable.\n* @param t Throwable error; creates IOException if null\n*/"
    },
    "org.apache.hadoop.ipc.RpcWritable$WritableWrapper": {
        "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:<init>(org.apache.hadoop.io.Writable)": "/**\n* Constructs a WritableWrapper with the specified Writable.\n* @param writable the Writable to wrap\n*/",
        "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer)": "/**\n* Writes data to the provided ResponseBuffer.\n* @param out the ResponseBuffer to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:readFrom(java.nio.ByteBuffer)": "/**\n* Reads data from ByteBuffer into a writable object.\n* @param bb ByteBuffer containing data to read\n* @return Writable object populated with data from ByteBuffer\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufWrapperLegacy": {
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:isUnshadedProtobufMessage(java.lang.Object)": "/**\n* Checks if the payload is an unshaded protobuf message.\n* @param payload object to check for protobuf compatibility\n* @return true if payload is a protobuf message, false otherwise\n*/",
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:readFrom(java.nio.ByteBuffer)": "/**\n* Deserializes a protobuf message from a ByteBuffer.\n* @param bb ByteBuffer containing the serialized message\n* @return Deserialized message of type T\n*/",
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object)": "/**\n* Constructs ProtobufWrapperLegacy with a protobuf message.\n* @param message the protobuf message object to wrap\n*/",
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer)": "/**\n* Writes serialized message to ResponseBuffer.\n* @param out the ResponseBuffer to write to\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.ipc.RpcConstants": {
        "org.apache.hadoop.ipc.RpcConstants:<init>()": "/**\n* Private constructor to prevent instantiation of the RpcConstants class.\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:getRequestHeader()": "/**\n* Retrieves the request header, initializing it if necessary.\n* @return RequestHeaderProto object or null if not initialized\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:writeTo(org.apache.hadoop.ipc.ResponseBuffer)": "/**\n* Writes request header and payload to the response buffer.\n* @param out the ResponseBuffer to write data to\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>()": "/**\n* Constructs a new RpcProtobufRequest instance.\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)": "/**\n* Initializes RpcProtobufRequest with header and payload.\n* @param header request metadata\n* @param payload message content\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString()": "/**\n* Returns a string representation of the method's declaring class and name.\n* @return formatted string of class and method names\n* @throws IllegalArgumentException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getAsyncReturnMessage()": "/**\n* Retrieves an asynchronous message.\n* @return AsyncGet containing Message or Exception\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache()": "/****\n* Clears the client cache by invoking the CLIENTS cache clearing method.\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine()": "/**\n* Registers the Protobuf RPC engine if not already registered.\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a Client instance using the provided configuration.\n* @param conf configuration settings for the Client\n* @return Client object from cache or newly created\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates a proxy for the specified protocol type.\n* @param protocol the class type of the protocol\n* @return ProtocolProxy instance for the given protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Creates a ProtocolProxy for ProtocolMetaInfoPB using provided connection and configuration.\n* @param connId connection identifier\n* @param conf configuration settings\n* @param factory socket factory for connections\n* @return ProtocolProxy instance\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates a proxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Retrieves a ProtocolProxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)": "/**\n* Retrieves a ProtocolProxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates and returns an RPC.Server instance with specified settings.\n* @param protocol RPC protocol class\n* @param protocolImpl implementation of the protocol\n* @param bindAddress address to bind the server\n* @param port port number for the server\n* @param numHandlers number of handler threads\n* @param numReaders number of read threads\n* @param queueSizePerHandler size of the queue per handler\n* @param verbose enables verbose logging\n* @param conf configuration settings\n* @param secretManager security token manager\n* @param portRangeConfig config for port range\n* @param alignmentContext context for alignment\n* @return initialized RPC.Server instance\n*/"
    },
    "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer": {
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getDefaultQueueWeights(int)": "/**\n* Generates an array of queue weights, doubling each value.\n* @param aNumQueues number of queues to create weights for\n* @return array of weights for the queues\n*/",
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:moveToNextQueue()": "/**\n* Advances to the next queue index and resets requests left for that queue.\n*/",
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getCurrentIndex()": "/**\n* Retrieves the current index from the queue.\n* @return currentQueueIndex as an integer\n*/",
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex()": "/**\n* Decrements requests left and advances queue index if zero requests remain.\n*/",
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex()": "/**\n* Retrieves and advances the current queue index.\n* @return currentQueueIndex as an integer\n*/",
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a WeightedRoundRobinMultiplexer with queue weights.\n* @param aNumQueues number of queues, must be > 0\n* @param ns namespace for configuration keys\n* @param conf configuration object for fetching weights\n*/"
    },
    "org.apache.hadoop.ipc.RPC$Builder": {
        "org.apache.hadoop.ipc.RPC$Builder:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the Builder with the provided configuration.\n* @param conf configuration settings for the Builder\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:setProtocol(java.lang.Class)": "/**\n* Sets the protocol class for the builder.\n* @param protocol class type to be set\n* @return the updated Builder instance\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:setInstance(java.lang.Object)": "/**\n* Sets the instance and returns the builder for chaining.\n* @param instance the object to be set\n* @return the Builder instance for further configuration\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:setBindAddress(java.lang.String)": "/**\n* Sets the bind address for the builder.\n* @param bindAddress the address to bind to\n* @return the current Builder instance\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:setPort(int)": "/**\n* Sets the port number for the builder.\n* @param port the port number to be set\n* @return the current Builder instance\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:setNumHandlers(int)": "/**\n* Sets the number of handlers for the builder.\n* @param numHandlers the number of handlers to set\n* @return the current Builder instance\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:setVerbose(boolean)": "/**\n* Sets the verbosity level for the builder.\n* @param verbose true for verbose output, false for normal\n* @return the current Builder instance\n*/",
        "org.apache.hadoop.ipc.RPC$Builder:build()": "/**\n* Builds and returns a Server instance, validating configuration and protocol settings.\n* @return Server instance\n* @throws IOException if an I/O error occurs\n* @throws HadoopIllegalArgumentException if required fields are not set\n*/"
    },
    "org.apache.hadoop.ipc.RpcWritable": {
        "org.apache.hadoop.ipc.RpcWritable:readFields(java.io.DataInput)": "/**\n* Reads fields from a DataInput stream; always throws UnsupportedOperationException.\n*/",
        "org.apache.hadoop.ipc.RpcWritable:write(java.io.DataOutput)": "/**\n* Throws UnsupportedOperationException for write functionality.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object)": "/**\n* Wraps an object into a suitable RpcWritable type.\n* @param o object to be wrapped\n* @return RpcWritable representation of the object\n*/"
    },
    "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler": {
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:<init>(java.lang.Class,java.lang.Object[])": "/**\n* Initializes CombinedProxyInvocationHandler with proxy interface and proxies array.\n* @param proxyInterface the interface type for the proxy\n* @param proxies array of proxy objects\n*/",
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes a method on underlying proxies, handling exceptions and logging errors.\n* @param proxy the proxy instance\n* @param method the method to invoke\n* @param args method parameters\n* @return result of method invocation\n* @throws Throwable if invocation fails\n*/",
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:toString()": "/**\n* Returns a string representation of the CombinedProxy instance.\n* @return formatted string with interface name and proxy details\n*/",
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId()": "/**\n* Retrieves the ConnectionId for the first proxy in the array.\n* @return ConnectionId associated with the first proxy\n*/",
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close()": "/**\n* Closes all Closeable proxies and throws MultipleIOException if any close fails.\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:<init>(org.apache.hadoop.ipc.RPC$Server,java.lang.String)": "/**\n* Initializes CallInfo with server and method name.\n* @param server RPC server instance\n* @param methodName name of the RPC method\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:getServer()": "/**\n* Retrieves the RPC server instance.\n* @return the RPC.Server object\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:getMethodName()": "/**\n* Retrieves the name of the method.\n* @return the name of the method as a String\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequestHeader(java.lang.reflect.Method)": "/**\n* Constructs an RPC request header from the given method.\n* @param method the method used to build the header\n* @return RequestHeaderProto object containing RPC details\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnProtoType(java.lang.reflect.Method)": "/**\n* Retrieves or creates a default instance of a method's return type.\n* @param method the method to inspect for return type\n* @return Message prototype of the return type\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getConnectionId()": "",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)": "/**\n* Constructs an RPC request with a header and payload.\n* @param method the RPC method\n* @param theRequest message content\n* @return Writable object representing the RPC request\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close()": "/**\n* Closes the client if not already closed, stopping it and releasing resources.\n* @throws IOException if an error occurs while stopping the client\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)": "/**\n* Retrieves a Message from a buffer based on the method's return type.\n* @param method the method to inspect for return type\n* @param buf the buffer to read the Message from\n* @return the retrieved Message\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes an RPC method with parameters and handles tracing and exceptions.\n* @param proxy the proxy instance\n* @param method the method to invoke\n* @param args method parameters (RpcController + Message)\n* @return Message response or null if in async mode\n* @throws ServiceException if parameter count is incorrect or RPC fails\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Initializes an Invoker with protocol, connection, and configuration settings.\n* @param protocol the protocol class type\n* @param connId connection identifier\n* @param conf configuration for the Client\n* @param factory socket factory for connections\n* @param alignmentContext context for alignment\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Initializes an Invoker with protocol, connection ID, and configuration.\n* @param protocol protocol class type\n* @param addr connection address\n* @param ticket user credentials\n* @param conf configuration settings\n* @param factory socket factory\n* @param rpcTimeout timeout for RPC calls\n* @param connectionRetryPolicy policy for connection retries\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @param alignmentContext context for alignment\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getAsyncReturnMessage()": "/**\n* Retrieves an asynchronous message return object.\n* @return AsyncGet<Message, Exception> containing the message or an exception\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a cached Client instance using the provided configuration.\n* @param conf configuration settings for the Client\n* @return Client object from cache or newly created\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates a proxy for the specified protocol.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)": "/**\n* Creates a ProtocolProxy for ProtocolMetaInfoPB using connection details.\n* @param connId connection identifier\n* @param conf configuration settings\n* @param factory socket factory\n* @return ProtocolProxy instance for the specified protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates a proxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Retrieves a proxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)": "/**\n* Retrieves a proxy for the specified protocol type.\n* @param protocol the protocol class type\n* @return ProtocolProxy instance for the protocol\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Creates and returns a configured RPC.Server instance.\n* @param protocol RPC protocol class, protocolImpl implementation, etc.\n* @return configured RPC.Server object\n*/"
    },
    "org.apache.hadoop.ipc.Client$Connection$PingInputStream": {
        "org.apache.hadoop.ipc.Client$Connection$PingInputStream:handleTimeout(java.net.SocketTimeoutException,int)": "/**\n* Handles socket timeout by either throwing the exception or sending a ping.\n* @param e the SocketTimeoutException to handle\n* @param waiting duration in milliseconds since the last activity\n*/",
        "org.apache.hadoop.ipc.Client$Connection$PingInputStream:read()": "/**\n* Reads data, handling socket timeouts by retrying until successful.\n* @return number of bytes read\n*/",
        "org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)": "/**\n* Reads data into a buffer, handling socket timeouts.\n* @param buf buffer to store read data\n* @param off offset in the buffer\n* @param len number of bytes to read\n* @return number of bytes read\n*/"
    },
    "org.apache.hadoop.ipc.WritableRpcEngine$Server": {
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:log(java.lang.String)": "/**\n* Logs a string value, truncating it to 55 characters if too long.\n* @param value the string to log\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)": "/**** Constructs a Server with specified protocol and configuration parameters. */",
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)": "/**\n* Constructs a deprecated Server instance with specified parameters.\n* @param protocolClass protocol class type\n* @param protocolImpl implementation of the protocol\n* @param conf configuration settings\n* @param bindAddress address to bind the server\n* @param port port number\n* @param numHandlers number of handler threads\n* @param numReaders number of reader threads\n* @param queueSizePerHandler queue size per handler\n* @param verbose enables verbose logging\n* @param secretManager secret manager for security\n* @param portRangeConfig optional port range configuration\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)": "/**\n* Constructs a Server instance with minimal parameters.\n* @param protocolClass protocol class type\n* @param protocolImpl implementation of the protocol\n* @param conf configuration settings\n* @param bindAddress address to bind the server\n* @param port port number\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)": "/**** Constructs a deprecated Server instance with specified parameters. \n* @param protocolImpl implementation of the protocol \n* @param conf configuration settings \n* @param bindAddress address to bind the server \n* @param port port number \n* @param numHandlers number of handler threads \n* @param numReaders number of reader threads \n* @param queueSizePerHandler queue size per handler \n* @param verbose enables verbose logging \n* @param secretManager secret manager for security \n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)": "/**\n* Constructs a deprecated Server instance with specified parameters.\n* @param instance protocol implementation object\n* @param conf configuration settings\n* @param bindAddress address to bind the server\n* @param port port number\n*/"
    },
    "org.apache.hadoop.ipc.metrics.RpcMetrics": {
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit()": "/**\n* Retrieves the current metrics time unit.\n* @return TimeUnit representing the metrics time unit\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected()": "/**\n* Retrieves the backoff value for disconnected clients.\n* @return long backoff duration in milliseconds\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls()": "/**\n* Retrieves the count of slow RPC calls.\n* @return long representing the number of slow RPC calls\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls()": "/**\n* Retrieves the number of RPC requeue calls.\n* @return long value representing RPC requeue calls count\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int)": "/**\n* Increments the sent bytes by a specified count.\n* @param count amount to add to the sent bytes\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int)": "/**\n* Increments the received byte count.\n* @param count number of bytes to add to the total received\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long)": "/**\n* Adds RPC enqueue time and updates quantiles if enabled.\n* @param enQTime time to be added to the enqueue duration\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long)": "/**\n* Adds RPC queue time and updates quantiles if enabled.\n* @param qTime time to be added to the RPC queue in milliseconds\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long)": "/**\n* Adds RPC lock wait time to the estimator and quantiles if enabled.\n* @param waitTime time in milliseconds to add\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long)": "/**\n* Adds RPC processing time and updates quantiles if enabled.\n* @param processingTime duration of RPC processing in milliseconds\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long)": "/**\n* Records RPC response time and updates quantiles if enabled.\n* @param responseTime time taken for the RPC response in milliseconds\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long)": "/**\n* Adds deferred RPC processing time and updates quantiles if enabled.\n* @param processingTime time to be added in milliseconds\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String)": "/**\n* Retrieves a MetricsTag by its name.\n* @param tagName the name of the MetricsTag\n* @return MetricsTag object or null if not found\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount()": "/**\n* Retrieves the count of processed samples.\n* @return total number of processed samples as a long\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount()": "/**\n* Retrieves the count of deferred RPC processing samples.\n* @return total number of samples as a long\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean()": "/**\n* Retrieves the mean processing time from the latest statistics.\n* @return mean processing time or 0.0 if no samples are present\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean()": "/**\n* Calculates the mean of deferred RPC processing time.\n* @return mean processing time or 0.0 if no samples exist\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler()": "/**\n* Returns the number of handlers currently in process.\n* @return count of in-process handlers from the server\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests()": "/**\n* Retrieves the total number of processed requests.\n* @return total number of requests as a long\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond()": "/**\n* Returns the total number of requests processed per second.\n* @return long representing total requests per second\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures()": "/**\n* Increments the count of authentication failures.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses()": "/**\n* Increments the count of authentication successes.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses()": "/**\n* Increments the count of successful authorization attempts.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures()": "/**\n* Increments the count of authorization failures.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff()": "/**\n* Increments the RPC client backoff value.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected()": "/**\n* Increments the backoff counter for disconnected RPC clients.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc()": "/**\n* Increments the count of slow RPC calls.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls()": "/**\n* Increments the count of requeue calls.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses()": "/**\n* Increments the count of successful RPC calls.\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown()": "/**\n* Shuts down the metrics source by unregistering it.\n* @param name the identifier of the metrics source to unregister\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev()": "/**\n* Retrieves the standard deviation of RPC processing times.\n* @return standard deviation of the last processing time statistics\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev()": "/**\n* Returns the standard deviation of deferred RPC processing time.\n* @return standard deviation value of processing times\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength()": "/**** Returns the length of the call queue. @return the number of elements in the call queue */",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections()": "/**\n* Retrieves the number of dropped connections from the server.\n* @return count of dropped connections as a long\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections()": "/**\n* Returns the number of open connections to the server.\n* @return count of current open connections as an integer\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser()": "/**\n* Retrieves the number of open connections per user as a JSON string.\n* @return JSON representation of open connections or null on error\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the TimeUnit for metrics from configuration.\n* @param conf configuration object\n* @return TimeUnit for metrics or default if invalid\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes RpcMetrics with server and configuration details.\n* @param server the RPC server instance, @param conf configuration settings\n*/",
        "org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)": "/**\n* Creates RpcMetrics and registers it with the MetricsSystem.\n* @param server the RPC server instance, @param conf configuration settings\n* @return registered RpcMetrics object\n*/"
    },
    "org.apache.hadoop.conf.Configuration$IntegerRanges": {
        "org.apache.hadoop.conf.Configuration$IntegerRanges:isEmpty()": "/**\n* Checks if the ranges collection is null or empty.\n* @return true if ranges is null or empty, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:<init>()": "/**\n* Constructs an instance of IntegerRanges.\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:convertToInt(java.lang.String,int)": "/**\n* Converts a trimmed string to an integer or returns default if empty.\n* @param value the string to convert\n* @param defaultValue the value to return if string is empty\n* @return the converted integer or defaultValue\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:isIncluded(int)": "/**\n* Checks if a value is within any defined range.\n* @param value the integer to check for inclusion\n* @return true if value is in any range, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:toString()": "/**\n* Converts the range list to a string representation.\n* @return formatted string of ranges in \"start-end\" format, separated by commas\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:getRangeStart()": "/**\n* Retrieves the start value of the first range or -1 if no ranges exist.\n* @return start value of the first range or -1 if ranges are empty\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:iterator()": "/**\n* Returns an iterator for range numbers.\n* @return Iterator for integers representing range values\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String)": "/**\n* Parses a string of integer ranges and initializes the IntegerRanges object.\n* @param newValue comma-separated string of ranges in the format 'start-end'\n*/"
    },
    "org.apache.hadoop.ipc.Server$Responder": {
        "org.apache.hadoop.ipc.Server$Responder:waitPending()": "/**\n* Waits until no pending tasks are left.\n* @throws InterruptedException if the wait is interrupted\n*/",
        "org.apache.hadoop.ipc.Server$Responder:doPurge(org.apache.hadoop.ipc.Server$RpcCall,long)": "/**\n* Purges outdated RPC calls from the response queue.\n* @param call the current RpcCall being processed\n* @param now the current time in nanoseconds\n*/",
        "org.apache.hadoop.ipc.Server$Responder:incPending()": "/**\n* Increments the pending count in a thread-safe manner.\n*/",
        "org.apache.hadoop.ipc.Server$Responder:decPending()": "/**\n* Decreases pending count and notifies waiting threads.\n*/",
        "org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)": "/**\n* Processes RPC response from the queue and handles writing to the channel.\n* @param responseQueue queue of RPC calls to process\n* @param inHandler indicates if the method is called within a handler\n* @return true if more data is pending, false if done\n*/",
        "org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey)": "/**\n* Handles asynchronous write operations for an RPC call.\n* @param key the selection key associated with the channel\n* @throws IOException if the channel is invalid or an error occurs\n*/",
        "org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall)": "/**\n* Handles RPC call response by wrapping and adding it to the response queue.\n* @param call the RPC call to respond to\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.ipc.Server$Responder:doRunLoop()": "/**\n* Manages the main loop for processing writable channels and purging old RPC calls.\n*/",
        "org.apache.hadoop.ipc.Server$Responder:run()": "/**\n* Executes the main server loop and handles cleanup on termination.\n*/"
    },
    "org.apache.hadoop.ipc.Server$FatalRpcServerException": {
        "org.apache.hadoop.ipc.Server$FatalRpcServerException:toString()": "/**\n* Returns the string representation of the cause of this throwable.\n* @return String representation of the cause\n*/",
        "org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)": "/**\n* Constructs a FatalRpcServerException with error code and IO exception.\n* @param errCode error code associated with the exception\n* @param ioe underlying IO exception\n*/",
        "org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)": "/**\n* Constructs a FatalRpcServerException with an error code and message.\n* @param errCode error code for the exception\n* @param message detailed exception message\n*/"
    },
    "org.apache.hadoop.tracing.SpanContext": {
        "org.apache.hadoop.tracing.SpanContext:<init>()": "/**\n* Constructs a new SpanContext instance.\n*/"
    },
    "org.apache.hadoop.tracing.TraceConfiguration": {
        "org.apache.hadoop.tracing.TraceConfiguration:<init>()": "/**\n* Default constructor for TraceConfiguration class.\n*/"
    },
    "org.apache.hadoop.util.MachineList$InetAddressFactory": {
        "org.apache.hadoop.util.MachineList$InetAddressFactory:getByName(java.lang.String)": "/**\n* Resolves the given host name to its corresponding IP address.\n* @param host the host name to resolve\n* @return InetAddress object representing the IP address\n* @throws UnknownHostException if the host cannot be resolved\n*/"
    },
    "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException": {
        "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException:<init>(java.lang.String)": "/**\n* Constructs a DiskOutOfSpaceException with a specified message.\n* @param msg detail message explaining the exception\n*/"
    },
    "org.apache.hadoop.util.ConfTest": {
        "org.apache.hadoop.util.ConfTest:<init>()": "/**\n* Constructor for ConfTest class, initializes the superclass.\n*/",
        "org.apache.hadoop.util.ConfTest:parseConf(java.io.InputStream)": "/**\n* Parses XML configuration from an InputStream.\n* @param in InputStream containing XML data\n* @return List of NodeInfo objects or null if invalid format\n*/",
        "org.apache.hadoop.util.ConfTest:terminate(int,java.lang.String)": "/**\n* Exits the application with a status code after logging a message.\n* @param status exit status code\n* @param msg message to log before termination\n*/",
        "org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream)": "/**\n* Validates XML configuration from InputStream.\n* @param in InputStream containing XML data\n* @return List of error messages or empty if valid\n*/",
        "org.apache.hadoop.util.ConfTest:listFiles(java.io.File)": "/**\n* Lists XML files in the specified directory.\n* @param dir directory to search for XML files\n* @return array of XML files or null if an error occurs\n*/",
        "org.apache.hadoop.util.ConfTest:main(java.lang.String[])": "/**\n* Main method to parse command-line options and validate configuration files.\n* @param args command-line arguments\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.ConfTest$1": {
        "org.apache.hadoop.util.ConfTest$1:<init>()": "/**\n* Default constructor for ConfTest class.\n*/"
    },
    "org.apache.hadoop.util.SysInfoLinux": {
        "org.apache.hadoop.util.SysInfoLinux:getCurrentTime()": "/**\n* Returns the current time in milliseconds since the epoch.\n* @return current time as a long value\n*/",
        "org.apache.hadoop.util.SysInfoLinux:safeParseLong(java.lang.String)": "/**\n* Parses a string to a long, returning 0 on failure.\n* @param strVal the string to parse\n* @return parsed long value or 0 if parsing fails\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readProcCpuInfoFile()": "/**\n* Reads CPU information from /proc/cpuinfo and updates processor and core counts.\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readProcNetInfoFile()": "/**\n* Reads network statistics from /proc/net/dev, updating byte counters.\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readDiskBlockInformation(java.lang.String,int)": "/**\n* Reads the disk sector size from a file.\n* @param diskName name of the disk\n* @param defSector default sector size if reading fails\n* @return sector size or defSector on error\n*/",
        "org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)": "/**\n* Initializes SysInfoLinux with file paths and jiffy length for system info tracking.\n* @param procfsMemFile memory file path\n* @param procfsCpuFile CPU file path\n* @param procfsStatFile statistics file path\n* @param procfsNetFile network file path\n* @param procfsDisksFile disks file path\n* @param jiffyLengthInMillis duration of a jiffy in milliseconds\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean)": "/**\n* Reads and parses memory info from /proc/memInfo file.\n* @param readAgain flag to control re-reading of the file\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getNumProcessors()": "/**\n* Retrieves the number of processors available.\n* @return int representing the number of processors\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getNumCores()": "/**\n* Retrieves the number of CPU cores.\n* @return int representing the number of cores\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getCpuFrequency()": "/**\n* Retrieves the CPU frequency after reading system CPU info.\n* @return CPU frequency in Hz\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readProcStatFile()": "/**\n* Reads CPU statistics from /proc/stat and updates elapsed jiffies.\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead()": "/**\n* Retrieves the total number of bytes read from the network.\n* @return total bytes read as a long value\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten()": "/**\n* Retrieves total network bytes written.\n* @return long representing total bytes written to the network\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile()": "/**\n* Reads disk information from /proc/diskstats and updates byte counts.\n*/",
        "org.apache.hadoop.util.SysInfoLinux:<init>()": "/**\n* Constructs SysInfoLinux with default file paths and jiffy length.\n*/",
        "org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile()": "/**\n* Reads memory information from /proc/memInfo file without re-reading.\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize()": "/**\n* Calculates available physical memory size in bytes.\n* @return available physical memory size or 0 if unavailable\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime()": "/**\n* Retrieves cumulative CPU time after reading process statistics.\n* @return cumulative CPU time in nanoseconds\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage()": "/**\n* Calculates CPU usage percentage based on CPU time and processor count.\n* @return CPU usage as a float, or 0 if unavailable\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed()": "/**\n* Calculates the used virtual CPU cores percentage.\n* @return usage as a float (0.0 to 1.0) or 0.0 if unavailable\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead()": "/**\n* Retrieves total bytes read from storage.\n* @return total bytes read since last update\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten()": "/**\n* Returns total bytes written to storage.\n* @return total bytes written across all disks\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize()": "/**\n* Calculates physical memory size in bytes after accounting for corrupt and huge pages.\n* @return physical memory size in bytes\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize()": "/**\n* Calculates available virtual memory size in bytes.\n* @return sum of available physical memory and free swap space\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize()": "/**\n* Calculates virtual memory size in bytes.\n* @return total virtual memory size including swap space\n*/",
        "org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[])": "/**\n* Displays system information including memory, CPU, and network stats.\n* @param args command-line arguments (not used)\n*/",
        "org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String)": "/**\n* Retrieves configuration value for a specified attribute.\n* @param attr attribute name for configuration\n* @return configuration value as long or -1 on error\n*/"
    },
    "org.apache.hadoop.util.CpuTimeTracker": {
        "org.apache.hadoop.util.CpuTimeTracker:<init>(long)": "/**\n* Initializes CpuTimeTracker with specified jiffy length in milliseconds.\n* @param jiffyLengthInMillis duration of a jiffy in milliseconds\n*/",
        "org.apache.hadoop.util.CpuTimeTracker:updateElapsedJiffies(java.math.BigInteger,long)": "/**\n* Updates cumulative CPU time based on elapsed jiffies and new timestamp.\n* @param elapsedJiffies time in jiffies to update\n* @param newTime new timestamp in milliseconds\n*/",
        "org.apache.hadoop.util.CpuTimeTracker:getCumulativeCpuTime()": "/**\n* Retrieves the cumulative CPU time in nanoseconds.\n* @return cumulative CPU time as a long value\n*/",
        "org.apache.hadoop.util.CpuTimeTracker:getCpuTrackerUsagePercent()": "/**\n* Calculates CPU usage percentage based on cumulative CPU time.\n* @return CPU usage as a float value\n*/",
        "org.apache.hadoop.util.CpuTimeTracker:toString()": "/**\n* Returns a string representation of the object's state and CPU metrics.\n* @return formatted string with sample and CPU time details\n*/"
    },
    "org.apache.hadoop.util.IdentityHashStore": {
        "org.apache.hadoop.util.IdentityHashStore:putInternal(java.lang.Object,java.lang.Object)": "/**\n* Inserts key-value pair into a buffer, handling collisions via linear probing.\n* @param k key to insert\n* @param v value associated with the key\n*/",
        "org.apache.hadoop.util.IdentityHashStore:getElementIndex(java.lang.Object)": "/**\n* Retrieves the index of the element in the buffer.\n* @param k element to find in the buffer\n* @return index of the element or -1 if not found\n*/",
        "org.apache.hadoop.util.IdentityHashStore:visitAll(org.apache.hadoop.util.IdentityHashStore$Visitor)": "/**\n* Visits all key-value pairs in the buffer using the provided visitor.\n* @param visitor a Visitor that processes each key-value pair\n*/",
        "org.apache.hadoop.util.IdentityHashStore:realloc(int)": "/**\n* Resizes the internal buffer and rehashes existing elements.\n* @param newCapacity new buffer size, must be positive\n*/",
        "org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object)": "/**\n* Retrieves value by key from the buffer.\n* @param k key to find in the buffer\n* @return corresponding value or null if not found\n*/",
        "org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object)": "/**\n* Removes and returns the value associated with the key.\n* @param k key to remove from the buffer\n* @return value associated with the key or null if not found\n*/",
        "org.apache.hadoop.util.IdentityHashStore:<init>(int)": "/**\n* Initializes IdentityHashStore with specified capacity.\n* @param capacity initial size, must be non-negative\n*/",
        "org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)": "/**\n* Inserts key-value pair, reallocating buffer if necessary.\n* @param k key to insert\n* @param v value associated with the key\n*/"
    },
    "org.apache.hadoop.util.LightWeightGSet$SetIterator": {
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:nextNonemptyEntry()": "/**\n* Retrieves the next non-empty entry in the entries array.\n* @return LinkedElement or null if no non-empty entry exists\n*/",
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext()": "/**\n* Ensures the next entry is set; checks for modification and retrieves next entry if needed.\n*/",
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext()": "/**\n* Checks if there is a next entry available.\n* @return true if next entry exists, false otherwise\n*/",
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:next()": "/**\n* Retrieves the next element, converting it to type E; throws if no elements remain.\n* @return Converted element of type E\n*/",
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:remove()": "/**\n* Removes the current element from the set if present.\n* @throws IllegalStateException if no current element exists\n*/"
    },
    "org.apache.hadoop.util.LightWeightGSet": {
        "org.apache.hadoop.util.LightWeightGSet:convert(org.apache.hadoop.util.LightWeightGSet$LinkedElement)": "/**\n* Converts a LinkedElement to a generic type E.\n* @param e LinkedElement to convert\n* @return Converted element of type E\n*/",
        "org.apache.hadoop.util.LightWeightGSet:<init>()": "/**\n* Constructs a new instance of LightWeightGSet.\n*/",
        "org.apache.hadoop.util.LightWeightGSet:size()": "/**\n* Returns the current size of the collection.\n* @return the number of elements in the collection\n*/",
        "org.apache.hadoop.util.LightWeightGSet:values()": "/**\n* Returns a collection of values, initializing if necessary.\n* @return Collection of values, never null\n*/",
        "org.apache.hadoop.util.LightWeightGSet:iterator()": "/**\n* Returns an iterator for the set elements.\n* @return Iterator for the set's elements\n*/",
        "org.apache.hadoop.util.LightWeightGSet:actualArrayLength(int)": "/**\n* Calculates the actual array length based on a recommended size.\n* @param recommended suggested array length\n* @return adjusted array length within defined limits\n*/",
        "org.apache.hadoop.util.LightWeightGSet:getIndex(java.lang.Object)": "/**\n* Computes the index for the given key using its hash code.\n* @param key the key to compute the index for\n* @return the computed index as an integer\n*/",
        "org.apache.hadoop.util.LightWeightGSet:toString()": "/**\n* Returns a string representation of the object with key attributes.\n* @return formatted string including size, hash mask, modification count, and entry length\n*/",
        "org.apache.hadoop.util.LightWeightGSet:printDetails(java.io.PrintStream)": "/**\n* Prints details of the object and its entries to the specified PrintStream.\n* @param out PrintStream to output the details\n*/",
        "org.apache.hadoop.util.LightWeightGSet:clear()": "/**\n* Clears the entries array and resets size to zero.\n* Increments modification count on clear operation.\n*/",
        "org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)": "/**\n* Removes an element by key at a specified index.\n* @param index position in the entries array\n* @param key element key to remove\n* @return Converted element or null if not found\n*/",
        "org.apache.hadoop.util.LightWeightGSet:<init>(int)": "/**\n* Initializes LightWeightGSet with adjusted array length.\n* @param recommended_length suggested initial size for the set\n*/",
        "org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object)": "/**\n* Retrieves element by key; returns null if not found.\n* @param key the key to search for\n* @return element of type E or null if not found\n*/",
        "org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object)": "/**\n* Adds an element to the list, replacing existing ones.\n* @param element the element to add\n* @return the previous element at the same index or null\n*/",
        "org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object)": "/**\n* Removes an element by key after validating it.\n* @param key the key of the element to remove\n* @return the removed element or null if not found\n*/",
        "org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)": "/**\n* Computes capacity based on max memory and percentage.\n* @param maxMemory total available memory in bytes\n* @param percentage fraction of max memory to use\n* @return calculated capacity as an integer\n*/",
        "org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object)": "/**\n* Checks if the map contains a key.\n* @param key the key to check for\n* @return true if the key exists, false otherwise\n*/",
        "org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)": "/**\n* Calculates capacity using max memory and given percentage.\n* @param percentage fraction of max memory to use\n* @param mapName identifier for the memory map\n* @return calculated capacity as an integer\n*/"
    },
    "org.apache.hadoop.util.MergeSort": {
        "org.apache.hadoop.util.MergeSort:swap(int[],int,int)": "/**\n* Swaps elements in an array at specified indices.\n* @param x array of integers\n* @param a index of first element\n* @param b index of second element\n*/",
        "org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)": "/**\n* Performs merge sort on an array.\n* @param src source array to be sorted\n* @param dest destination array for sorted elements\n* @param low starting index for sorting\n* @param high ending index for sorting\n*/",
        "org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator)": "/**\n* Initializes MergeSort with a custom comparator.\n* @param comparator defines the sorting order for IntWritable objects\n*/"
    },
    "org.apache.hadoop.util.XMLUtils": {
        "org.apache.hadoop.util.XMLUtils:newSecureDocumentBuilderFactory()": "/**\n* Creates a secure DocumentBuilderFactory for XML processing.\n* @return configured DocumentBuilderFactory instance\n* @throws ParserConfigurationException if configuration fails\n*/",
        "org.apache.hadoop.util.XMLUtils:newSecureSAXParserFactory()": "/**\n* Creates a secure SAXParserFactory instance with restricted features.\n* @return configured SAXParserFactory for secure XML processing\n*/",
        "org.apache.hadoop.util.XMLUtils:bestEffortSetAttribute(javax.xml.transform.TransformerFactory,java.util.concurrent.atomic.AtomicBoolean,java.lang.String,java.lang.Object)": "/**\n* Attempts to set an attribute on the TransformerFactory if the flag is true.\n* @param transformerFactory the factory to set the attribute on\n* @param flag controls whether the operation should proceed\n* @param name the name of the attribute to set\n* @param value the value to assign to the attribute\n*/",
        "org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory)": "/**\n* Sets optional secure attributes on the TransformerFactory.\n* @param transformerFactory the factory to configure attributes\n*/",
        "org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory()": "/**\n* Creates a secure TransformerFactory instance.\n* @return configured TransformerFactory with secure processing enabled\n*/",
        "org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory()": "/**\n* Creates a secure SAXTransformerFactory.\n* @return configured SAXTransformerFactory instance\n*/",
        "org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)": "/**\n* Transforms XML using a stylesheet and outputs to a Writer.\n* @param styleSheet input stylesheet as InputStream\n* @param xml input XML data as InputStream\n* @param out output destination as Writer\n*/"
    },
    "org.apache.hadoop.util.Waitable": {
        "org.apache.hadoop.util.Waitable:<init>(java.util.concurrent.locks.Condition)": "/**\n* Initializes Waitable with a specified Condition.\n* @param cond the Condition to be associated with this Waitable\n*/",
        "org.apache.hadoop.util.Waitable:await()": "/**\n* Waits for a value to become available.\n* @return the available value of type T\n* @throws InterruptedException if the wait is interrupted\n*/",
        "org.apache.hadoop.util.Waitable:provide(java.lang.Object)": "/**\n* Sets the value and signals all waiting threads.\n* @param val the value to be set\n*/"
    },
    "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix": {
        "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:valueOf(char)": "/**\n* Retrieves TraditionalBinaryPrefix by its symbol.\n* @param symbol character representing the prefix\n* @return corresponding TraditionalBinaryPrefix object\n*/",
        "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String)": "/**\n* Converts a string to a long, handling size prefixes.\n* @param s input string potentially containing a size prefix\n* @return long value represented by the string\n*/",
        "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)": "/**\n* Converts a long value to a formatted string with unit and decimal places.\n* @param n the long value to convert\n* @param unit the unit to append (optional)\n* @param decimalPlaces number of decimal places for formatting\n* @return formatted string representation of the value\n*/"
    },
    "org.apache.hadoop.util.HeapSort": {
        "org.apache.hadoop.util.HeapSort:<init>()": "/**\n* Constructs a new HeapSort instance.\n*/",
        "org.apache.hadoop.util.HeapSort:downHeap(org.apache.hadoop.util.IndexedSortable,int,int,int)": "/**\n* Performs down-heap operation to maintain heap property.\n* @param s sortable structure for comparison and swapping\n* @param b base index for the heap\n* @param i index of the element to down-heap\n* @param N size of the heap\n*/",
        "org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)": "/**\n* Sorts elements using heap sort algorithm.\n* @param s sortable structure to sort\n* @param p starting index\n* @param r ending index\n* @param rep progress indicator\n*/",
        "org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)": "/**\n* Sorts elements in the given sortable structure.\n* @param s sortable structure to sort\n* @param p starting index\n* @param r ending index\n*/"
    },
    "org.apache.hadoop.util.JvmPauseMonitor$GcTimes": {
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:<init>(java.lang.management.GarbageCollectorMXBean)": "/**\n* Initializes GcTimes with garbage collection count and time.\n* @param gcBean GarbageCollectorMXBean instance\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:<init>(long,long)": "/**\n* Constructs GcTimes with specified garbage collection count and time.\n* @param count number of garbage collections\n* @param time total time spent on garbage collection in milliseconds\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:toString()": "/**\n* Returns a string representation of garbage collection statistics.\n* @return formatted string with count and time in milliseconds\n*/",
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes)": "/**\n* Subtracts another GcTimes from this instance.\n* @param other GcTimes instance to subtract\n* @return new GcTimes with resulting gcCount and gcTimeMillis\n*/"
    },
    "org.apache.hadoop.util.CombinedIPList": {
        "org.apache.hadoop.util.CombinedIPList:isIn(java.lang.String)": "/**\n* Checks if the given IP address is in any network list.\n* @param ipAddress the IP address to check\n* @return true if found in any list, false otherwise\n*/",
        "org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)": "/**\n* Initializes CombinedIPList with fixed and optional variable blacklist files.\n* @param fixedBlackListFile path to the fixed blacklist file\n* @param variableBlackListFile path to the variable blacklist file or null\n* @param cacheExpiryInSeconds duration for caching the variable blacklist\n*/"
    },
    "org.apache.hadoop.util.DataChecksum$ChecksumNull": {
        "org.apache.hadoop.util.DataChecksum$ChecksumNull:<init>()": "/**\n* Constructs a ChecksumNull instance.\n*/"
    },
    "org.apache.hadoop.util.ExitUtil$HaltException": {
        "org.apache.hadoop.util.ExitUtil$HaltException:getExitCode()": "/**\n* Retrieves the exit code of the process.\n* @return exit code as an integer\n*/",
        "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.Throwable)": "/**\n* Constructs a HaltException with a status and cause.\n* @param status HTTP status code\n* @param cause underlying throwable cause\n*/",
        "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.String)": "/**\n* Constructs a HaltException with a status code and message.\n* @param status HTTP status code\n* @param msg error message for the exception\n*/",
        "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.String,java.lang.Throwable)": "/**\n* Constructs a HaltException with status, message, and cause.\n* @param status HTTP status code\n* @param message error message\n* @param cause underlying cause of the exception\n*/",
        "org.apache.hadoop.util.ExitUtil$HaltException:toString()": "/**\n* Returns a string representation of the object with status and message.\n* @return formatted string of status and message or superclass string if message is null\n*/"
    },
    "org.apache.hadoop.util.ConfigurationHelper": {
        "org.apache.hadoop.util.ConfigurationHelper:<init>()": "/**\n* Private constructor to prevent instantiation of ConfigurationHelper class.\n*/",
        "org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)": "",
        "org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)": "/**\n* Parses a string to create an EnumSet of specified enum type.\n* @param key identifier for the operation\n* @param valueString input string containing enum values\n* @param enumClass class of the enum type\n* @param ignoreUnknown flag to ignore unknown enum values\n* @return EnumSet of the parsed enum values\n*/"
    },
    "org.apache.hadoop.util.SysInfoWindows": {
        "org.apache.hadoop.util.SysInfoWindows:reset()": "/**\n* Resets all resource metrics to their default values.\n*/",
        "org.apache.hadoop.util.SysInfoWindows:now()": "/**\n* Retrieves the current time in milliseconds from a monotonic clock.\n* @return current time in milliseconds\n*/",
        "org.apache.hadoop.util.SysInfoWindows:<init>()": "/**\n* Initializes SysInfoWindows and resets resource metrics.\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell()": "/**\n* Retrieves system information by executing a shell command.\n* @return output string of system info or null if an error occurs\n*/",
        "org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded()": "/**\n* Refreshes system metrics if the refresh interval has passed.\n* Resets metrics and retrieves new system info.\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize()": "/**\n* Retrieves the current virtual memory size in bytes.\n* @return long representing the virtual memory size\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize()": "/**\n* Returns the current physical memory size in bytes.\n* @return long representing the physical memory size\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize()": "/**\n* Returns the available virtual memory size in bytes.\n* @return available virtual memory size\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize()": "/**\n* Returns the available physical memory size in bytes.\n* @return long representing available memory size\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getNumProcessors()": "/**\n* Retrieves the number of available processors after refreshing metrics.\n* @return number of processors in the system\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getCpuFrequency()": "/**\n* Retrieves current CPU frequency in kilohertz.\n* @return CPU frequency in kilohertz\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime()": "/**\n* Retrieves the total cumulative CPU time in milliseconds.\n* @return cumulative CPU time as a long value\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage()": "/**\n* Retrieves CPU usage percentage considering active processors.\n* @return CPU usage percentage or -1 if not available\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed()": "/**\n* Retrieves the used virtual CPU cores as a float.\n* @return float number of virtual CPU cores used (0.0 to 1.0) or -1 if unavailable\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead()": "/**\n* Returns the total network bytes read after refreshing metrics.\n* @return total bytes read from the network\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten()": "/**\n* Retrieves the total number of bytes written over the network.\n* @return total bytes written as a long value\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead()": "/**\n* Returns the total bytes read from storage.\n* @return long representing bytes read\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten()": "/**\n* Retrieves the total bytes written to storage.\n* @return total bytes written as a long value\n*/",
        "org.apache.hadoop.util.SysInfoWindows:getNumCores()": "/**\n* Returns the number of CPU cores available.\n* @return number of cores in the system\n*/"
    },
    "org.apache.hadoop.util.ShutdownHookManager$HookEntry": {
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)": "/**\n* Constructs a HookEntry with specified hook, priority, timeout, and time unit.\n* @param hook the Runnable to execute\n* @param priority the execution priority\n* @param timeout duration before timeout occurs\n* @param unit the time unit for the timeout\n*/",
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:hashCode()": "/**\n* Returns the hash code of the hook object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getHook()": "/**\n* Retrieves the current hook Runnable.\n* @return Runnable instance associated with the hook\n*/",
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getTimeout()": "/**\n* Retrieves the current timeout value.\n* @return the timeout duration in milliseconds\n*/",
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getTimeUnit()": "/**\n* Retrieves the time unit.\n* @return TimeUnit representing the current time unit\n*/",
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)": "/**\n* Constructs a HookEntry with specified hook and priority using default timeout settings.\n* @param hook the Runnable to execute\n* @param priority the execution priority\n*/"
    },
    "org.apache.hadoop.util.ComparableVersion$ListItem": {
        "org.apache.hadoop.util.ComparableVersion$ListItem:isNull()": "/**\n* Checks if the collection is empty.\n* @return true if the collection size is zero, otherwise false\n*/",
        "org.apache.hadoop.util.ComparableVersion$ListItem:normalize()": "/**\n* Removes trailing null items from the list.\n*/",
        "org.apache.hadoop.util.ComparableVersion$ListItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)": "/**\n* Compares this item with another for order.\n* @param item the item to be compared\n* @return negative if less than, positive if greater, or zero if equal\n*/",
        "org.apache.hadoop.util.ComparableVersion$ListItem:toString()": "/**\n* Returns a string representation of the object in a formatted list.\n* @return formatted string with item details enclosed in parentheses\n*/",
        "org.apache.hadoop.util.ComparableVersion$ListItem:getType()": ""
    },
    "org.apache.hadoop.util.ThreadUtil": {
        "org.apache.hadoop.util.ThreadUtil:joinUninterruptibly(java.lang.Thread)": "/**\n* Joins a thread without interruption, restoring interrupted status if interrupted.\n* @param toJoin the thread to join\n*/",
        "org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.ClassLoader,java.lang.String)": "/**\n* Retrieves an InputStream for a resource using the specified ClassLoader.\n* @param cl ClassLoader to load the resource\n* @param resourceName name of the resource to load\n* @return InputStream for the resource\n* @throws IOException if the class loader is null or resource is not found\n*/",
        "org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long)": "/**\n* Sleeps for at least the specified duration, ignoring interruptions.\n* @param millis minimum sleep time in milliseconds\n*/",
        "org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String)": "/**\n* Retrieves an InputStream for a resource by name using the current thread's ClassLoader.\n* @param resourceName name of the resource to load\n* @return InputStream for the resource\n* @throws IOException if class loader is null or resource is not found\n*/"
    },
    "org.apache.hadoop.util.InstrumentedLock": {
        "org.apache.hadoop.util.InstrumentedLock:newCondition()": "/**\n* Creates a new Condition instance for the lock.\n* @return a Condition object for thread signaling\n*/",
        "org.apache.hadoop.util.InstrumentedLock:getLock()": "/**\n* Retrieves the current lock instance.\n* @return Lock object representing the current lock\n*/",
        "org.apache.hadoop.util.InstrumentedLock:getTimer()": "/**\n* Retrieves the current Timer instance.\n* @return Timer object representing the clock\n*/",
        "org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)": "/**\n* Logs a warning when lock held time exceeds threshold.\n* @param lockHeldTime duration the lock was held\n* @param stats statistics on suppressed lock warnings\n*/",
        "org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)": "/**\n* Logs a warning for exceeded lock wait time.\n* @param lockWaitTime duration of the lock wait in milliseconds\n* @param stats snapshot of suppressed lock wait statistics\n*/",
        "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)": "/**\n* Constructs an InstrumentedLock with logging and timing parameters.\n* @param name lock identifier\n* @param logger logging utility\n* @param lock the actual lock object\n* @param minLoggingGapMs minimum log interval in milliseconds\n* @param lockWarningThresholdMs threshold for lock warnings in milliseconds\n* @param clock timer for monotonic time retrieval\n*/",
        "org.apache.hadoop.util.InstrumentedLock:startLockTiming()": "/**\n* Starts timing for lock acquisition.\n* Records the current time in milliseconds from a monotonic clock.\n*/",
        "org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)": "/**\n* Checks lock held time and logs warnings if thresholds are exceeded.\n* @param acquireTime time when the lock was acquired\n* @param releaseTime time when the lock was released\n* @param checkLockHeld flag to determine the type of log to generate\n*/",
        "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)": "/**** Constructs an InstrumentedLock with logging and timing parameters. \n* @param name lock identifier \n* @param logger logging utility \n* @param lock the actual lock object \n* @param minLoggingGapMs minimum log interval in milliseconds \n* @param lockWarningThresholdMs threshold for lock warnings in milliseconds \n*/",
        "org.apache.hadoop.util.InstrumentedLock:tryLock()": "/**\n* Attempts to acquire a lock and starts timing if successful.\n* @return true if lock acquired, false otherwise\n*/",
        "org.apache.hadoop.util.InstrumentedLock:lock()": "/**\n* Acquires the lock and logs timing information.\n* @param waitStart time when the lock attempt began\n*/",
        "org.apache.hadoop.util.InstrumentedLock:lockInterruptibly()": "/**\n* Acquires lock interruptibly and logs timing if interrupted.\n* @throws InterruptedException if the thread is interrupted while waiting\n*/",
        "org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)": "/**\n* Attempts to acquire a lock within a specified time.\n* @param time duration to wait for the lock\n* @param unit time unit of the wait duration\n* @return true if lock acquired, false otherwise\n*/",
        "org.apache.hadoop.util.InstrumentedLock:unlock()": "/**\n* Unlocks the resource and logs lock timing details.\n* @param lockAcquireTimestamp time when the lock was acquired\n*/",
        "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)": "/**\n* Initializes InstrumentedLock with a name, logger, and timing parameters.\n* @param name lock identifier\n* @param logger logging utility\n* @param minLoggingGapMs minimum log interval in milliseconds\n* @param lockWarningThresholdMs threshold for lock warnings in milliseconds\n*/"
    },
    "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot": {
        "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:getSuppressedCount()": "/**\n* Retrieves the current suppressed count value.\n* @return long representing the suppressed count\n*/",
        "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:getMaxSuppressedWait()": "/**\n* Retrieves the maximum suppressed wait time.\n* @return the maximum suppressed wait duration in milliseconds\n*/",
        "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:<init>(long,long)": "/**\n* Initializes a SuppressedSnapshot with count and wait time.\n* @param suppressedCount number of suppressed events\n* @param maxWait maximum wait time for suppression\n*/"
    },
    "org.apache.hadoop.util.InstrumentedLock$SuppressedStats": {
        "org.apache.hadoop.util.InstrumentedLock$SuppressedStats:incrementSuppressed(long)": "/**\n* Increments suppressed count and updates max wait time if exceeded.\n* @param wait time to compare against maxSuppressedWait\n*/",
        "org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot()": "/**\n* Creates and resets a SuppressedSnapshot with current counts and wait time.\n* @return SuppressedSnapshot object containing suppressed events data\n*/"
    },
    "org.apache.hadoop.util.QuickSort": {
        "org.apache.hadoop.util.QuickSort:<init>()": "/**\n* Constructs a QuickSort instance.\n*/",
        "org.apache.hadoop.util.QuickSort:fix(org.apache.hadoop.util.IndexedSortable,int,int)": "/**\n* Swaps elements in IndexedSortable if they are out of order.\n* @param s IndexedSortable instance to operate on\n* @param p first index to compare\n* @param r second index to compare\n*/",
        "org.apache.hadoop.util.QuickSort:getMaxDepth(int)": "/**\n* Calculates the maximum depth based on the input integer.\n* @param x positive integer for depth calculation\n* @return maximum depth as an integer\n*/",
        "org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)": "/**** Sorts elements in IndexedSortable using a recursive quicksort algorithm. \n* @param s IndexedSortable instance to sort \n* @param p starting index \n* @param r ending index \n* @param rep progress indicator \n* @param depth recursion depth limit \n*/",
        "org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)": "/**\n* Sorts elements in IndexedSortable from index p to r using quicksort.\n* @param s IndexedSortable instance to sort\n* @param p starting index\n* @param r ending index\n* @param rep progress indicator\n*/",
        "org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)": "/**\n* Sorts elements in IndexedSortable from index p to r.\n* @param s IndexedSortable instance to sort\n* @param p starting index\n* @param r ending index\n*/"
    },
    "org.apache.hadoop.util.LineReader": {
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,int)": "/**\n* Initializes LineReader with input stream and buffer size.\n* @param in InputStream to read data from\n* @param bufferSize size of the internal buffer\n*/",
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,byte[])": "/**\n* Initializes LineReader with input stream and record delimiter.\n* @param in input stream for reading data\n* @param recordDelimiterBytes byte array for record separation\n*/",
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,int,byte[])": "/**\n* Initializes LineReader with input stream, buffer size, and record delimiter bytes.\n* @param in input stream for reading data\n* @param bufferSize size of the buffer for reading\n* @param recordDelimiterBytes byte array for record delimiters\n*/",
        "org.apache.hadoop.util.LineReader:close()": "/**\n* Closes the input stream, releasing any associated resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.util.LineReader:fillBuffer(java.io.InputStream,byte[],boolean)": "/**\n* Reads bytes from InputStream into buffer.\n* @param in InputStream to read from\n* @param buffer byte array to fill\n* @param inDelimiter indicates delimiter presence\n* @return number of bytes read or -1 if end of stream\n*/",
        "org.apache.hadoop.util.LineReader:unsetNeedAdditionalRecordAfterSplit()": "/**\n* Resets the flag for additional record processing after a split.\n* Required for handling custom multi-byte line delimiters.\n*/",
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream)": "/**** Initializes LineReader with InputStream using default buffer size. \n* @param in InputStream to read data from \n*/",
        "org.apache.hadoop.util.LineReader:getIOStatistics()": "/**\n* Retrieves IOStatistics using the input source.\n* @return IOStatistics object or null if invalid source\n*/",
        "org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)": "/**\n* Reads a line from the input stream into str, limiting by maxLineLength and maxBytesToConsume.\n* @param str Text object to store the read line\n* @param maxLineLength maximum length of the line to read\n* @param maxBytesToConsume maximum bytes to read before stopping\n* @return number of bytes consumed\n*/",
        "org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)": "/**\n* Reads a custom line from InputStream, tracking ambiguous bytes.\n* @param str output Text object for the line\n* @param maxLineLength maximum length of the line\n* @param maxBytesToConsume maximum bytes to read\n* @return total bytes read before delimiter\n*/",
        "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)": "/**\n* Reads a line from the input, using custom or default logic.\n* @param str output Text object for the line\n* @param maxLineLength maximum length of the line\n* @param maxBytesToConsume maximum bytes to read\n* @return number of bytes read\n*/",
        "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)": "/**\n* Reads a line into the given Text object with a max length.\n* @param str output Text object for the line\n* @param maxLineLength maximum length of the line\n* @return number of bytes read\n*/",
        "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text)": "/**\n* Reads a line into the provided Text object.\n* @param str output Text object for the line\n* @return number of bytes read\n*/",
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs LineReader with InputStream and buffer size from Configuration.\n* @param in InputStream to read data from\n* @param conf Configuration object for retrieving buffer size\n*/",
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])": "/**\n* Constructs a LineReader with input stream and configuration settings.\n* @param in input stream for reading data\n* @param conf configuration containing buffer size\n* @param recordDelimiterBytes byte array for record delimiters\n*/"
    },
    "org.apache.hadoop.util.BlockingThreadPoolExecutorService": {
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:getNamedThreadFactory(java.lang.String)": "/**\n* Creates a named ThreadFactory with a specified prefix.\n* @param prefix thread name prefix\n* @return ThreadFactory for creating named threads\n*/",
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:getActiveCount()": "/**\n* Returns the count of active threads in the event processing executor.\n* @return number of active threads\n*/",
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String)": "/**\n* Creates a daemon ThreadFactory with a specified name prefix.\n* @param prefix thread name prefix\n* @return ThreadFactory for daemon threads\n*/",
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString()": "/**\n* Returns string representation of the executor's state with active thread count.\n* @return formatted string with executor details\n*/",
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)": "/**\n* Constructs BlockingThreadPoolExecutorService with specified permits and executor.\n* @param permitCount number of permits for semaphore\n* @param eventProcessingExecutor executor for processing events\n*/",
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)": "/**\n* Creates a BlockingThreadPoolExecutorService with specified task limits and thread settings.\n* @param activeTasks maximum number of active tasks\n* @param waitingTasks maximum number of waiting tasks\n* @param keepAliveTime time for which threads may remain idle\n* @param unit time unit for keepAliveTime\n* @param prefixName prefix for thread names\n* @return configured BlockingThreadPoolExecutorService instance\n*/"
    },
    "org.apache.hadoop.util.LightWeightResizableGSet": {
        "org.apache.hadoop.util.LightWeightResizableGSet:resize(int)": "/**\n* Resizes the hash table to a new capacity.\n* @param cap desired capacity for the hash table\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)": "/**\n* Initializes a resizable set with capacity and load factor.\n* @param initCapacity initial capacity, must be non-negative\n* @param loadFactor factor for resizing, must be in (0, 1]\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:size()": "/**\n* Returns the current size of the collection.\n* @return the number of elements in the collection\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer)": "/****\n* Provides an iterator over the collection of values.\n* @param consumer a Consumer to process the Iterator\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary()": "/**\n* Expands the hash table if size exceeds threshold and capacity allows.\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:<init>()": "/**\n* Constructs a LightWeightResizableGSet with default capacity and load factor.\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:<init>(int)": "/**\n* Constructs a LightWeightResizableGSet with specified initial capacity.\n* @param initCapacity initial capacity, must be non-negative\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object)": "/**\n* Retrieves element by key in a thread-safe manner.\n* @param key the key to search for\n* @return element of type E or null if not found\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object)": "/**\n* Adds an element to the collection and expands if necessary.\n* @param element the element to add\n* @return previous element at the same index or null\n*/",
        "org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object)": "/**\n* Removes an element by key in a synchronized manner.\n* @param key the key of the element to remove\n* @return the removed element or null if not found\n*/"
    },
    "org.apache.hadoop.util.ComparableVersion$IntegerItem": {
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:<init>()": "/**\n* Initializes IntegerItem with a default value of BIG_INTEGER_ZERO.\n*/",
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:<init>(java.lang.String)": "/**\n* Initializes IntegerItem with a value from the given string.\n* @param str string representation of a big integer\n*/",
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:isNull()": "/**\n* Checks if the value is equal to BIG_INTEGER_ZERO.\n* @return true if value is zero, false otherwise\n*/",
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)": "/**\n* Compares this Item with another Item for order.\n* @param item the Item to compare against\n* @return comparison result: negative, zero, or positive integer\n*/",
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:toString()": "/**\n* Returns string representation of the value.\n* @return string representation of the value object\n*/",
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:getType()": ""
    },
    "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting": {
        "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting:<init>(int)": "/**\n* Initializes a rate limiter with a specified capacity per second.\n* @param capacityPerSecond maximum requests allowed per second\n*/",
        "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting:acquire(int)": "/**\n* Acquires capacity and returns the delay duration.\n* @param requestedCapacity desired capacity to acquire\n* @return Duration of delay or INSTANTLY if no delay\n*/"
    },
    "org.apache.hadoop.util.ApplicationClassLoader": {
        "org.apache.hadoop.util.ApplicationClassLoader:isSystemClass(java.lang.String,java.util.List)": "/**\n* Checks if a class name belongs to the specified system classes.\n* @param name class name to check\n* @param systemClasses list of system class patterns\n* @return true if name matches a system class, false otherwise\n*/",
        "org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)": "/**\n* Constructs ApplicationClassLoader with specified URLs, parent, and system classes.\n* @param urls array of URLs for class loading\n* @param parent parent class loader\n* @param systemClasses list of system class names or default if null/empty\n*/",
        "org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String)": "/**\n* Retrieves a resource URL by name, checking system classes first.\n* @param name resource name to locate\n* @return URL of the resource or null if not found\n*/",
        "org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)": "/**\n* Loads a class by name, resolving it if specified.\n* @param name the name of the class to load\n* @param resolve whether to resolve the class after loading\n* @return the loaded Class object\n* @throws ClassNotFoundException if the class cannot be found\n*/",
        "org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String)": "/**\n* Loads a class by name without resolving it.\n* @param name the name of the class to load\n* @return the loaded Class object\n* @throws ClassNotFoundException if the class cannot be found\n*/",
        "org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String)": "/**\n* Constructs an array of URLs from the classpath string.\n* @param classpath colon-separated paths to JARs/directories\n* @return array of URLs representing JAR files and directories\n*/",
        "org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)": "/**** Constructs ApplicationClassLoader from classpath, parent, and system classes. \n* @param classpath colon-separated paths to JARs/directories \n* @param parent parent class loader \n* @param systemClasses list of system class names or default if null/empty \n*/"
    },
    "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator": {
        "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:hasNext()": "/**\n* Checks if there is a next element in the collection.\n* @return true if next exists, false if at the end\n*/",
        "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:next()": "/**\n* Returns the next element in the collection.\n* @return the next element or throws NoSuchElementException if at the end\n*/",
        "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:remove()": "/**\n* Removes the current element and prepares for the next operation.\n* @throws IllegalStateException if remove() is called consecutively\n*/"
    },
    "org.apache.hadoop.util.SemaphoredDelegatingExecutor$CallableWithPermitRelease": {
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor$CallableWithPermitRelease:call()": "/**\n* Executes a callable task and releases permits afterwards.\n* @return result of the callable task\n* @throws Exception if the task execution fails\n*/"
    },
    "org.apache.hadoop.util.StopWatch": {
        "org.apache.hadoop.util.StopWatch:reset()": "/**\n* Resets the stopwatch to initial state and returns the instance.\n* @return this StopWatch instance\n*/",
        "org.apache.hadoop.util.StopWatch:<init>(org.apache.hadoop.util.Timer)": "/**\n* Initializes a StopWatch with a specified Timer.\n* @param timer the Timer instance to be used by the StopWatch\n*/",
        "org.apache.hadoop.util.StopWatch:<init>()": "/**\n* Constructs a StopWatch using a new Timer instance.\n*/",
        "org.apache.hadoop.util.StopWatch:start()": "/**\n* Starts the stopwatch if not already running.\n* @return this StopWatch instance\n*/",
        "org.apache.hadoop.util.StopWatch:stop()": "/**\n* Stops the stopwatch and updates elapsed time.\n* @return this StopWatch instance for chaining\n*/",
        "org.apache.hadoop.util.StopWatch:now()": "/**\n* Returns the current elapsed time in nanoseconds.\n* @return elapsed time since start or currentElapsedNanos if not started\n*/",
        "org.apache.hadoop.util.StopWatch:close()": "/**\n* Closes the resource, stopping it if it was started.\n*/",
        "org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit)": "/**\n* Converts current time to specified TimeUnit.\n* @param timeUnit the unit to convert the time into\n* @return time in the specified TimeUnit\n*/",
        "org.apache.hadoop.util.StopWatch:toString()": "/**\n* Returns the string representation of the current elapsed time in nanoseconds.\n* @return elapsed time as a String\n*/"
    },
    "org.apache.hadoop.util.OperationDuration": {
        "org.apache.hadoop.util.OperationDuration:time()": "/**\n* Returns the current time in milliseconds since epoch.\n* @return current time in milliseconds\n*/",
        "org.apache.hadoop.util.OperationDuration:value()": "/**\n* Calculates the duration between start and finish times.\n* @return duration in milliseconds\n*/",
        "org.apache.hadoop.util.OperationDuration:humanTime(long)": "/**\n* Converts time in milliseconds to a human-readable format.\n* @param time duration in milliseconds\n* @return formatted time as \"minutes:seconds.milliseconds\"\n*/",
        "org.apache.hadoop.util.OperationDuration:<init>()": "/**** Initializes OperationDuration with current time as start and finish. */",
        "org.apache.hadoop.util.OperationDuration:finished()": "/**\n* Marks the finish time by capturing the current time in milliseconds.\n*/",
        "org.apache.hadoop.util.OperationDuration:asDuration()": "/**\n* Converts the value to a Duration object.\n* @return Duration representing the calculated time in milliseconds\n*/",
        "org.apache.hadoop.util.OperationDuration:getDurationString()": "/**\n* Returns duration as a formatted string.\n* @return formatted duration from milliseconds to \"minutes:seconds.milliseconds\"\n*/",
        "org.apache.hadoop.util.OperationDuration:toString()": "/**\n* Returns a string representation of the object.\n* @return formatted duration from milliseconds\n*/"
    },
    "org.apache.hadoop.util.LightWeightCache": {
        "org.apache.hadoop.util.LightWeightCache:updateRecommendedLength(int,int)": "/**\n* Updates recommended length based on size limit.\n* @param recommendedLength initial recommended length\n* @param sizeLimit maximum allowable size\n* @return adjusted length or original if size limit is invalid\n*/",
        "org.apache.hadoop.util.LightWeightCache:isExpired(org.apache.hadoop.util.LightWeightCache$Entry,long)": "/**\n* Checks if the given entry has expired based on the current time.\n* @param e the Entry to check for expiration\n* @param now the current timestamp to compare against\n* @return true if expired, false otherwise\n*/",
        "org.apache.hadoop.util.LightWeightCache:iterator()": "/**\n * Returns an iterator for the set elements with unsupported remove operation.\n * @return Iterator for the set's elements\n */",
        "org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)": "/**\n* Sets the expiration time for an entry.\n* @param e the Entry object to update\n* @param expirationPeriod duration in nanoseconds until expiration\n*/",
        "org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)": "/**\n* Initializes LightWeightCache with specified parameters.\n* @param recommendedLength initial size, @param sizeLimit max entries, \n* @param creationExpirationPeriod lifespan, @param accessExpirationPeriod lifespan,\n* @param timer scheduling tool\n*/",
        "org.apache.hadoop.util.LightWeightCache:get(java.lang.Object)": "/**\n* Retrieves an entry by key and updates its expiration time if accessed.\n* @param key the key to retrieve the entry\n* @return the entry of type E or null if not found\n*/",
        "org.apache.hadoop.util.LightWeightCache:evict()": "/**\n* Evicts and returns an element from the queue after validation.\n* @return the evicted element from the queue\n*/",
        "org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)": "/**** Initializes LightWeightCache with specified parameters. \n* @param recommendedLength initial size, @param sizeLimit max entries, \n* @param creationExpirationPeriod lifespan, @param accessExpirationPeriod lifespan \n*/",
        "org.apache.hadoop.util.LightWeightCache:evictExpiredEntries()": "/**\n* Evicts expired entries from the queue based on the current time.\n*/",
        "org.apache.hadoop.util.LightWeightCache:evictEntries()": "/**\n* Evicts entries from the queue until size limit is met.\n*/",
        "org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object)": "/**\n* Removes an element by key after evicting expired entries.\n* @param key the key of the element to remove\n* @return removed element or null if not found\n*/",
        "org.apache.hadoop.util.LightWeightCache:put(java.lang.Object)": "/**\n* Adds an entry to the queue, evicting expired ones and setting expiration time.\n* @param entry the Entry object to add\n* @return previous entry if exists, otherwise null\n*/"
    },
    "org.apache.hadoop.util.CrcUtil": {
        "org.apache.hadoop.util.CrcUtil:<init>()": "/**\n* Private constructor to prevent instantiation of the CrcUtil class.\n*/",
        "org.apache.hadoop.util.CrcUtil:galoisFieldMultiply(int,int,int)": "/**\n* Multiplies two elements in a Galois field.\n* @param p first element, @param q second element, @param m modulus for the field\n* @return result of p * q in the Galois field mod m\n*/",
        "org.apache.hadoop.util.CrcUtil:writeInt(byte[],int,int)": "/**\n* Writes a 4-byte integer to a byte array at a specified offset.\n* @param buf byte array to write to\n* @param offset position in the array to start writing\n* @param value integer value to write\n* @throws IOException if offset + 4 exceeds the array length\n*/",
        "org.apache.hadoop.util.CrcUtil:readInt(byte[],int)": "/**\n* Reads a 4-byte integer from the byte array starting at the given offset.\n* @param buf byte array containing data\n* @param offset starting position to read from\n* @return the integer value read\n* @throws IOException if offset is out of bounds\n*/",
        "org.apache.hadoop.util.CrcUtil:getMonomial(long,int)": "/**\n* Computes monomial for given byte length in a Galois field.\n* @param lengthBytes byte length, must be non-negative\n* @param mod modulus for Galois field operations\n* @return computed monomial value\n*/",
        "org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)": "/**\n* Composes two CRC values with a monomial using Galois field multiplication.\n* @param crcA first CRC value, @param crcB second CRC value, @param monomial multiplier, @param mod modulus\n* @return combined CRC value after applying the monomial\n*/",
        "org.apache.hadoop.util.CrcUtil:intToBytes(int)": "/**\n* Converts an integer to a 4-byte array.\n* @param value the integer to convert\n* @return byte array representing the integer\n*/",
        "org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[])": "/**\n* Converts a 4-byte array to a CRC string.\n* @param bytes byte array of length 4\n* @return formatted CRC string\n* @throws IOException if byte array length is not 4\n*/",
        "org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[])": "/**\n* Converts byte array to a CRC string representation.\n* @param bytes input byte array, must be divisible by 4\n* @return formatted string of CRC values\n*/",
        "org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)": "/**\n* Combines two CRC values using a monomial based on byte length.\n* @param crcA first CRC value, @param crcB second CRC value, \n* @param lengthB byte length for monomial, @param mod modulus\n* @return combined CRC value\n*/"
    },
    "org.apache.hadoop.util.RunJar": {
        "org.apache.hadoop.util.RunJar:ensureDirectory(java.io.File)": "/**\n* Ensures the specified directory exists, creating it if necessary.\n* @param dir the directory to ensure\n* @throws IOException if the directory cannot be created\n*/",
        "org.apache.hadoop.util.RunJar:skipUnjar()": "/**\n* Checks if unjarring is skipped based on environment variable.\n* @return true if skipping unjar, false otherwise\n*/",
        "org.apache.hadoop.util.RunJar:useClientClassLoader()": "/**\n* Checks if the client class loader should be used based on an environment variable.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.util.RunJar:getHadoopClasspath()": "/**\n* Retrieves the Hadoop classpath from the environment variables.\n* @return Hadoop classpath as a String, or null if not set\n*/",
        "org.apache.hadoop.util.RunJar:getSystemClasses()": "/**\n* Retrieves system classes from the environment variables.\n* @return String of system classes or null if not set\n*/",
        "org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)": "/**\n* Extracts files from a JAR input stream to a specified directory matching a regex pattern.\n* @param inputStream source JAR input stream\n* @param toDir destination directory for extracted files\n* @param unpackRegex pattern to filter which entries to extract\n* @throws IOException if an I/O error occurs during extraction\n*/",
        "org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)": "/**\n* Extracts files from a JAR to a specified directory matching a regex pattern.\n* @param jarFile the JAR file to unpack\n* @param toDir the directory to extract files into\n* @param unpackRegex pattern to filter files for extraction\n* @throws IOException if an I/O error occurs during extraction\n*/",
        "org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)": "/**\n* Extracts JAR contents to a directory while saving a copy of the input stream.\n* @param inputStream source JAR input stream\n* @param toDir destination directory for extracted files\n* @param name name of the output file\n* @param unpackRegex pattern to filter extracted entries\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)": "/**\n* Unpacks all files from a JAR to a specified directory.\n* @param jarFile the JAR file to unpack\n* @param toDir the directory to extract files into\n* @throws IOException if an I/O error occurs during extraction\n*/",
        "org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)": "/****\n* Creates a ClassLoader based on file and working directory.\n* @param file the main file for the class loader\n* @param workDir the working directory for classpath\n* @return ClassLoader instance\n*/",
        "org.apache.hadoop.util.RunJar:run(java.lang.String[])": "/**\n* Executes a JAR file with optional main class and arguments.\n* @param args command line arguments including jarFile and optional mainClass\n* @throws Throwable if an error occurs during execution\n*/",
        "org.apache.hadoop.util.RunJar:main(java.lang.String[])": "/**\n* Main method to execute a JAR file with command line arguments.\n* @param args command line arguments including jarFile and optional mainClass\n* @throws Throwable if an error occurs during execution\n*/"
    },
    "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor": {
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory)": "/**\n* Constructs a HadoopThreadPoolExecutor with specified parameters.\n* @param corePoolSize initial number of threads\n* @param maximumPoolSize maximum number of threads\n* @param keepAliveTime time for idle threads to wait\n* @param unit time unit for keepAliveTime\n* @param workQueue queue for holding tasks\n* @param threadFactory factory for creating new threads\n*/",
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue)": "/**\n* Initializes a HadoopThreadPoolExecutor with specified parameters.\n* @param corePoolSize initial number of threads\n* @param maximumPoolSize max number of threads allowed\n* @param keepAliveTime time for excess idle threads to wait\n* @param unit time unit for keepAliveTime\n* @param workQueue queue for holding tasks before execution\n*/",
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.RejectedExecutionHandler)": "/**\n* Constructs a HadoopThreadPoolExecutor with specified parameters.\n* @param corePoolSize initial number of threads\n* @param maximumPoolSize maximum number of threads\n* @param keepAliveTime time for idle threads to wait\n* @param unit time unit for keepAliveTime\n* @param workQueue queue for holding tasks\n* @param handler action for rejected tasks\n*/",
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory,java.util.concurrent.RejectedExecutionHandler)": "/**\n* Constructs a HadoopThreadPoolExecutor with specified parameters.\n* @param corePoolSize initial number of threads\n* @param maximumPoolSize max number of threads allowed\n* @param keepAliveTime time for idle threads to wait\n* @param unit time unit for keepAliveTime\n* @param workQueue queue for holding tasks\n* @param threadFactory factory for creating new threads\n* @param handler handler for rejected tasks\n*/",
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)": "/**\n* Logs debug information before executing a Runnable in a specific thread.\n* @param t the thread that will execute the runnable\n* @param r the Runnable to be executed\n*/",
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)": "/**\n* Handles post-execution actions for Runnable tasks.\n* @param r Runnable task executed\n* @param t Throwable that occurred, or null if none\n*/"
    },
    "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor": {
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int)": "/**\n* Constructs a HadoopScheduledThreadPoolExecutor with specified core pool size.\n* @param corePoolSize the number of threads to keep in the pool\n*/",
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.ThreadFactory)": "/**\n* Initializes a HadoopScheduledThreadPoolExecutor with specified core pool size and thread factory.\n* @param corePoolSize the number of threads to keep in the pool\n* @param threadFactory factory for creating new threads\n*/",
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.RejectedExecutionHandler)": "/**\n* Constructs a HadoopScheduledThreadPoolExecutor with specified core pool size and handler.\n* @param corePoolSize the number of threads to keep in the pool\n* @param handler the handler for rejected tasks\n*/",
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.ThreadFactory,java.util.concurrent.RejectedExecutionHandler)": "/**\n* Constructs a HadoopScheduledThreadPoolExecutor with specified parameters.\n* @param corePoolSize the number of threads to keep in the pool\n* @param threadFactory factory to create new threads\n* @param handler handler for rejected tasks\n*/",
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)": "/**\n* Logs debug information before executing a Runnable in a specific Thread.\n* @param t the Thread executing the Runnable\n* @param r the Runnable to be executed\n*/",
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)": "/**\n* Handles post-execution actions for Runnable, logging any thrown exceptions.\n* @param r Runnable task executed\n* @param t Throwable occurred during execution, or null if none\n*/"
    },
    "org.apache.hadoop.util.concurrent.ExecutorHelper": {
        "org.apache.hadoop.util.concurrent.ExecutorHelper:logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable)": "/**\n* Logs exceptions from Runnable execution in the current thread.\n* @param r Runnable task that was executed\n* @param t Throwable that occurred during execution, or null if none\n*/",
        "org.apache.hadoop.util.concurrent.ExecutorHelper:<init>()": "/**\n* Private constructor to prevent instantiation of ExecutorHelper class.\n*/"
    },
    "org.apache.hadoop.util.concurrent.AsyncGetFuture": {
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:<init>(org.apache.hadoop.util.concurrent.AsyncGet)": "/**\n* Initializes AsyncGetFuture with the provided AsyncGet instance.\n* @param asyncGet instance of AsyncGet for asynchronous operations\n*/",
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:callAsyncGet(long,java.util.concurrent.TimeUnit)": "/**\n* Initiates an asynchronous GET operation with a timeout.\n* @param timeout duration before timeout occurs\n* @param unit time unit for the timeout duration\n*/",
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:get()": "/**\n* Initiates an async GET and retrieves the result.\n* @return result of the asynchronous operation\n* @throws InterruptedException if interrupted while waiting\n* @throws ExecutionException if the computation threw an exception\n*/",
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)": "/**\n* Retrieves a value with a specified timeout.\n* @param timeout duration before timeout occurs\n* @param unit time unit for the timeout duration\n* @return value of type T\n*/",
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone()": "/**\n* Checks if the asynchronous operation is complete.\n* @return true if done, false otherwise\n*/"
    },
    "org.apache.hadoop.util.UTF8ByteArrayUtils": {
        "org.apache.hadoop.util.UTF8ByteArrayUtils:findByte(byte[],int,int,byte)": "/**\n* Finds the index of byte in the specified range.\n* @param utf byte array to search\n* @param start beginning index (inclusive)\n* @param end ending index (exclusive)\n* @param b byte to find\n* @return index of byte or -1 if not found\n*/",
        "org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)": "/**\n* Finds the index of the n-th occurrence of a byte in a byte array.\n* @param utf byte array to search\n* @param start beginning index (inclusive)\n* @param length range length to search\n* @param b byte to find\n* @param n occurrence index to find\n* @return index of the n-th byte or -1 if not found\n*/",
        "org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)": "/**\n* Finds the n-th occurrence of a byte in a byte array.\n* @param utf byte array to search\n* @param b byte to find\n* @param n occurrence index to find\n* @return index of the n-th byte or -1 if not found\n*/"
    },
    "org.apache.hadoop.util.RateLimitingFactory$NoRateLimiting": {
        "org.apache.hadoop.util.RateLimitingFactory$NoRateLimiting:acquire(int)": "/**\n* Acquires a specified capacity and returns an instant duration.\n* @param requestedCapacity the capacity to acquire\n* @return Duration representing an instant acquisition\n*/"
    },
    "org.apache.hadoop.util.CacheableIPList": {
        "org.apache.hadoop.util.CacheableIPList:updateCacheExpiryTime()": "/**\n* Updates cache expiry time based on cacheTimeout value.\n* If negative, sets expiry to -1 (no expiry).\n*/",
        "org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)": "/**\n* Initializes CacheableIPList with IP list and cache timeout.\n* @param ipList file-based IP list source\n* @param cacheTimeout duration for cache expiry\n*/",
        "org.apache.hadoop.util.CacheableIPList:reset()": "/**\n* Resets IP list and updates cache expiry time.\n* @return void\n*/",
        "org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String)": "/**\n* Checks if the IP address is allowed, resetting cache if expired.\n* @param ipAddress the IP address to verify\n* @return true if allowed, false otherwise\n*/"
    },
    "org.apache.hadoop.util.WeakReferenceMap": {
        "org.apache.hadoop.util.WeakReferenceMap:size()": "/**\n* Returns the number of entries in the map.\n* @return the size of the map as an integer\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:clear()": "/**\n* Clears all entries from the map.\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:lookup(java.lang.Object)": "/**\n* Retrieves a WeakReference associated with the given key.\n* @param key the key to look up in the map\n* @return WeakReference<V> or null if key is not found\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:resolve(java.lang.ref.WeakReference)": "/**\n* Resolves the value from a WeakReference.\n* @param r a WeakReference to the value\n* @return the referenced value or null if not present\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:noteLost(java.lang.Object)": "/**\n* Increments lost reference counter and notifies if a function is provided.\n* @param key the lost reference key\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:getReferenceLostCount()": "/**\n* Retrieves the count of lost references.\n* @return long representing the number of lost references\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:getEntriesCreatedCount()": "/**\n* Returns the count of entries created.\n* @return long representing the number of created entries\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)": "/**\n* Constructs a WeakReferenceMap with a factory and a reference lost callback.\n* @param factory function to create values for keys\n* @param referenceLost callback for when a reference is lost\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:toString()": "/**\n* Returns a string representation of the WeakReferenceMap.\n* @return formatted string with size and counts of references and entries\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)": "/**\n* Puts a value in the map with a key and resolves the WeakReference.\n* @param key the key to associate with the value\n* @param value the value to store\n* @return the previously associated value or null if absent\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object)": "/**\n* Removes a key-value pair and resolves the value.\n* @param key the key to remove from the map\n* @return resolved value or null if not present\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object)": "/**\n* Checks if the map contains a value for the specified key.\n* @param key the key to check for existence\n* @return true if the key is present, false otherwise\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object)": "/**\n* Creates and stores a value by key, ensuring strong reference during GC.\n* @param key the key to create the value for\n* @return the created value\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:prune()": "/**\n* Removes null references from the map and counts them.\n* @return number of removed entries\n*/",
        "org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object)": "/**\n* Retrieves a strong reference value by key, handling weak references and GC.\n* @param key the key to lookup\n* @return the strong value or a newly created one if not found\n*/"
    },
    "org.apache.hadoop.util.NativeCrc32": {
        "org.apache.hadoop.util.NativeCrc32:verifyChunkedSums(int,int,java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)": "/**\n* Verifies chunked sums for data integrity.\n* @param bytesPerSum size of each sum in bytes\n* @param checksumType type of checksum to compute\n* @param sums buffer for storing computed sums\n* @param data buffer containing data to verify\n* @param fileName name of the file being processed\n* @param basePos starting position in the file\n* @throws ChecksumException if verification fails\n*/",
        "org.apache.hadoop.util.NativeCrc32:verifyChunkedSumsByteArray(int,int,byte[],int,byte[],int,int,java.lang.String,long)": "/**\n* Verifies chunked sums in a byte array.\n* @param bytesPerSum number of bytes per sum\n* @param checksumType type of checksum to compute\n* @param sums array to hold computed sums\n* @param sumsOffset offset in sums array\n* @param data input data array\n* @param dataOffset offset in data array\n* @param dataLength length of data to process\n* @param fileName name of the file being processed\n* @param basePos base position for checksum calculation\n* @throws ChecksumException if checksum verification fails\n*/",
        "org.apache.hadoop.util.NativeCrc32:calculateChunkedSums(int,int,java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Computes chunked sums and stores them in the provided ByteBuffer.\n* @param bytesPerSum number of bytes per sum\n* @param checksumType type of checksum to compute\n* @param sums ByteBuffer to store computed sums\n* @param data ByteBuffer containing data for computation\n*/",
        "org.apache.hadoop.util.NativeCrc32:calculateChunkedSumsByteArray(int,int,byte[],int,byte[],int,int)": "/**\n* Computes chunked sums of a byte array.\n* @param bytesPerSum number of bytes per sum chunk\n* @param checksumType type of checksum to compute\n* @param sums output array for computed sums\n* @param sumsOffset offset in sums array\n* @param data input byte array for calculations\n* @param dataOffset offset in data array\n* @param dataLength length of data to process\n*/",
        "org.apache.hadoop.util.NativeCrc32:isAvailable()": "/**\n* Checks if the system is available for use.\n* @return true if native code is loaded and not on SPARC, false otherwise\n*/"
    },
    "org.apache.hadoop.util.VersionInfo": {
        "org.apache.hadoop.util.VersionInfo:_getVersion()": "/**\n* Retrieves the application version from properties.\n* @return version string or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:_getRevision()": "/**\n* Retrieves the revision property from info, defaulting to \"Unknown\".\n* @return revision string or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:_getBranch()": "/**\n* Retrieves the branch name from properties, defaulting to \"Unknown\".\n* @return branch name as a String\n*/",
        "org.apache.hadoop.util.VersionInfo:_getDate()": "/**\n* Retrieves the date property from info, defaults to \"Unknown\" if not found.\n* @return date as a String or \"Unknown\"\n*/",
        "org.apache.hadoop.util.VersionInfo:_getUser()": "/**\n* Retrieves the username from properties, defaults to \"Unknown\" if not found.\n* @return the username as a String\n*/",
        "org.apache.hadoop.util.VersionInfo:_getUrl()": "/**\n* Retrieves the URL from properties; defaults to \"Unknown\" if not found.\n* @return the URL string\n*/",
        "org.apache.hadoop.util.VersionInfo:_getSrcChecksum()": "/**\n* Retrieves the source checksum from properties.\n* @return source checksum as a String, or \"Unknown\" if not found\n*/",
        "org.apache.hadoop.util.VersionInfo:_getProtocVersion()": "/**\n* Retrieves the protoc version from properties.\n* @return protoc version as a String or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:_getCompilePlatform()": "/**\n* Retrieves the compile platform from properties.\n* @return compile platform name or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:getVersion()": "/**\n* Retrieves the application version.\n* @return version string or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:getRevision()": "/**\n* Retrieves the revision string from COMMON_VERSION_INFO.\n* @return revision string or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:getBranch()": "/**\n* Retrieves the branch name from COMMON_VERSION_INFO.\n* @return branch name as a String\n*/",
        "org.apache.hadoop.util.VersionInfo:getDate()": "/**\n* Returns the date from COMMON_VERSION_INFO.\n* @return date as a String or \"Unknown\" if not found\n*/",
        "org.apache.hadoop.util.VersionInfo:getUser()": "/**\n* Retrieves the username from COMMON_VERSION_INFO.\n* @return the username as a String, defaults to \"Unknown\" if not found\n*/",
        "org.apache.hadoop.util.VersionInfo:getUrl()": "/**\n* Retrieves the URL from COMMON_VERSION_INFO.\n* @return the URL string, defaults to \"Unknown\" if not found\n*/",
        "org.apache.hadoop.util.VersionInfo:_getBuildVersion()": "/**\n* Constructs build version string with version, revision, user, and checksum.\n* @return formatted build version string\n*/",
        "org.apache.hadoop.util.VersionInfo:getSrcChecksum()": "/**\n* Retrieves the source checksum from version info.\n* @return source checksum as a String, or \"Unknown\" if not found\n*/",
        "org.apache.hadoop.util.VersionInfo:getProtocVersion()": "/**\n* Retrieves the protoc version from COMMON_VERSION_INFO.\n* @return protoc version as a String or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:getCompilePlatform()": "/**\n* Retrieves the compile platform name from version info.\n* @return compile platform name or \"Unknown\" if not set\n*/",
        "org.apache.hadoop.util.VersionInfo:<init>(java.lang.String)": "/**** Initializes VersionInfo by loading properties from a version info file. \n* @param component name of the component to load version info for \n*/",
        "org.apache.hadoop.util.VersionInfo:getBuildVersion()": "/**\n* Retrieves the formatted build version string.\n* @return formatted build version from COMMON_VERSION_INFO\n*/",
        "org.apache.hadoop.util.VersionInfo:main(java.lang.String[])": "/**\n* Displays version info and build details in the console.\n* @param args command-line arguments (not used)\n*/"
    },
    "org.apache.hadoop.util.DiskValidatorFactory": {
        "org.apache.hadoop.util.DiskValidatorFactory:<init>()": "/**\n* Private constructor for DiskValidatorFactory to prevent instantiation.\n*/",
        "org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class)": "/**\n* Retrieves or creates a DiskValidator instance for the specified class.\n* @param clazz class type of DiskValidator to instantiate\n* @return DiskValidator instance associated with the class\n*/",
        "org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String)": "/**\n* Retrieves a DiskValidator instance based on the provided name.\n* @param diskValidator name of the DiskValidator class\n* @return DiskValidator instance\n* @throws DiskErrorException if the class is not found\n*/"
    },
    "org.apache.hadoop.util.ChunkedArrayList": {
        "org.apache.hadoop.util.ChunkedArrayList:iterator()": "/**\n* Returns an iterator over the concatenated chunks.\n* @return Iterator of type T for traversing elements\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:clear()": "/**\n* Clears all chunks and resets related attributes to initial state.\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:getNumChunks()": "/**\n* Returns the number of chunks in the collection.\n* @return count of chunks as an integer\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:getMaxChunkSize()": "/**\n* Calculates the maximum size of chunks in the list.\n* @return maximum chunk size as an integer\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:get(int)": "/**\n* Retrieves an element by index from a list of chunks.\n* @param idx index of the element to retrieve\n* @return element of type T at the specified index\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)": "/**\n* Initializes a ChunkedArrayList with specified chunk capacity and size limits.\n* @param initialChunkCapacity minimum capacity for each chunk\n* @param maxChunkSize maximum allowed size for a chunk\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:<init>()": "/**\n* Constructs a ChunkedArrayList with default capacity and size limits.\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:addChunk(int)": "/**\n* Initializes and adds a new chunk with specified capacity.\n* @param capacity the initial size of the new chunk\n*/",
        "org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object)": "/**\n* Adds an element to the list, expanding chunks as necessary.\n* @param e element to add; returns true if added successfully\n*/"
    },
    "org.apache.hadoop.util.PrintJarMainClass": {
        "org.apache.hadoop.util.PrintJarMainClass:main(java.lang.String[])": "/**\n* Prints the Main-Class from a JAR file specified in args.\n* @param args command-line arguments, first element is the JAR file path\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Initializes a WrappingRemoteIterator with a non-null RemoteIterator source.\n* @param source the RemoteIterator to wrap\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getSource()": "/**\n* Retrieves the source iterator.\n* @return RemoteIterator of type S\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:toString()": "/**\n* Returns string representation of the source object.\n* @return string representation of the source\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close()": "/**\n* Closes the source resource to prevent further use.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics()": "/**\n* Retrieves IOStatistics for the source object.\n* @return IOStatistics object or null if invalid source\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext()": "/**\n* Checks if the source iterator has more elements and closes on empty.\n* @return true if more elements exist, false otherwise\n* @throws IOException if an I/O error occurs during checking\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext()": "/**\n* Retrieves the next element from the source.\n* @return next element of type S\n* @throws IOException if an I/O error occurs\n* @throws NoSuchElementException if no more elements exist\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:fetch()": "/**\n* Retrieves the next valid element from the source based on a filter.\n* @return true if an element is found, false otherwise\n* @throws IOException if an error occurs while accessing the source\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:toString()": "/**\n* Returns a string representation of the FilteringRemoteIterator.\n* @return formatted string including the source of the iterator\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Constructs FilteringRemoteIterator with a source iterator and a filter function.\n* @param source the RemoteIterator to filter\n* @param filter function to determine if an item should be included\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext()": "/**\n* Checks if the next element is available.\n* @return true if next element exists, false otherwise\n* @throws IOException if an error occurs during fetch\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next()": "/**\n* Retrieves the next element if available.\n* @return the next element of type S\n* @throws IOException if an error occurs during fetch\n* @throws NoSuchElementException if no next element exists\n*/"
    },
    "org.apache.hadoop.util.functional.BiFunctionRaisingIOE": {
        "org.apache.hadoop.util.functional.BiFunctionRaisingIOE:unchecked(java.lang.Object,java.lang.Object)": "/**\n* Applies a function to two parameters, wrapping IOException in UncheckedIOException.\n* @param t first parameter\n* @param u second parameter\n* @return result of the applied function\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose": {
        "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object,boolean)": "/**\n* Initializes MaybeClose with a Closeable object if specified to close.\n* @param o object to potentially close\n* @param close flag indicating if the object should be closed\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:close()": "/**\n* Closes the resource, setting it to null to prevent further use.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object)": "/**\n* Initializes MaybeClose with a Closeable object and auto-close enabled.\n* @param o object to potentially close\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:<init>(long,long)": "/**\n* Initializes iterator with start value and excluded finish limit.\n* @param start initial value for iteration\n* @param excludedFinish value to exclude from iteration\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:hasNext()": "/**\n* Checks if there are more elements to iterate.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next()": "/**\n* Retrieves the next element in the iteration.\n* @return the next long value\n* @throws IOException if an I/O error occurs\n* @throws NoSuchElementException if no more elements exist\n*/"
    },
    "org.apache.hadoop.util.functional.CommonCallableSupplier": {
        "org.apache.hadoop.util.functional.CommonCallableSupplier:<init>(java.util.concurrent.Callable)": "/**\n* Initializes CommonCallableSupplier with a Callable instance.\n* @param call the Callable to be executed\n*/",
        "org.apache.hadoop.util.functional.CommonCallableSupplier:get()": "/**\n* Executes a callable and handles exceptions as unchecked IO exceptions.\n* @return result of type T from the callable\n*/",
        "org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)": "/**\n* Submits a Callable task for execution and returns a CompletableFuture.\n* @param executor the Executor to run the task\n* @param call the Callable task to execute\n* @return CompletableFuture for the result of the task\n*/",
        "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture)": "/**\n* Waits for CompletableFuture to complete, logging duration and handling exceptions.\n* @param future the CompletableFuture to wait for\n* @throws IOException if the future is cancelled or completes exceptionally\n*/",
        "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture)": "/**\n* Waits for CompletableFuture to complete, ignoring any exceptions.\n* @param future the CompletableFuture to wait for, may be null\n*/",
        "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List)": "/**\n* Waits for all given CompletableFutures to complete.\n* @param futures list of CompletableFutures to wait for\n* @throws IOException if any future is cancelled or completes exceptionally\n*/",
        "org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture)": "/**\n* Optionally waits for CompletableFuture to complete.\n* @param future the CompletableFuture to await, may be null\n* @throws IOException if the future is cancelled or completes exceptionally\n*/"
    },
    "org.apache.hadoop.util.functional.LazyAtomicReference": {
        "org.apache.hadoop.util.functional.LazyAtomicReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Initializes LazyAtomicReference with a callable constructor.\n* @param constructor callable that produces a value of type T\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:eval()": "/**\n* Evaluates and returns a value, creating it if absent.\n* @return the evaluated value of type T\n* @throws IOException if an I/O error occurs during evaluation\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:isSet()": "/**\n* Checks if the reference is set (not null).\n* @return true if reference is set, false otherwise\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:toString()": "/**\n* Returns a string representation of the LazyAtomicReference object.\n* @return formatted string with reference details\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:getConstructor()": "/**\n* Retrieves the constructor callable.\n* @return CallableRaisingIOE instance representing the constructor\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:getReference()": "/**\n* Retrieves the atomic reference.\n* @return AtomicReference<T> instance\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier)": "/**\n* Creates a LazyAtomicReference from a Supplier.\n* @param supplier provides values of type T\n* @return LazyAtomicReference containing the supplier's value\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:apply()": "/**\n* Applies the evaluation and returns its result.\n* @return evaluated value of type T\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.functional.LazyAtomicReference:get()": "/**\n* Retrieves a value by evaluating a callable, handling IOExceptions as unchecked.\n* @return evaluated value of type T\n* @throws UncheckedIOException if an I/O error occurs during evaluation\n*/"
    },
    "org.apache.hadoop.util.functional.LazyAutoCloseableReference": {
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:isClosed()": "/**\n* Checks if the resource is closed.\n* @return true if closed, false otherwise\n*/",
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:close()": "/**\n* Closes the resource if not already closed, nullifying the reference afterward.\n* @throws Exception if closing the resource fails\n*/",
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Initializes LazyAtomicReference with a callable constructor.\n* @param constructor callable that produces a value of type T\n*/",
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval()": "/**\n* Evaluates a value if the reference is open.\n* @return evaluated value of type T\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier)": "/**\n* Creates a LazyAutoCloseableReference from a Supplier.\n* @param supplier provides AutoCloseable instances\n* @return LazyAutoCloseableReference wrapping the supplier\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:hasNext()": "/**\n* Checks if there are more elements to iterate over.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:next()": "/**\n* Retrieves the next element from the source and applies a mapping function.\n* @return mapped element of type T\n* @throws IOException if an I/O error occurs during retrieval\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:toString()": "/**\n* Returns a string representation of the FunctionRemoteIterator object.\n* @return formatted string with source information\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Initializes MappingRemoteIterator with a source and a mapper function.\n* @param source iterator to wrap, @param mapper function to transform items\n*/"
    },
    "org.apache.hadoop.util.functional.CallableRaisingIOE": {
        "org.apache.hadoop.util.functional.CallableRaisingIOE:unchecked()": "/**\n* Executes apply() and wraps IOException in an UncheckedIOException.\n* @return result of apply() method\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:<init>(java.lang.Object)": "/**\n* Initializes SingletonIterator with a single element or null.\n* @param singleton the element to iterate or null to indicate processed\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:hasNext()": "/**\n* Checks if there are more elements to process.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:toString()": "/**\n* Returns a string representation of the SingletonIterator.\n* @return formatted string with singleton's value or empty if null\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next()": "/**\n* Returns the next element or throws an exception if none exists.\n* @return the next element of type T\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics()": "/**** Retrieves IOStatistics for the singleton object. \n* @return IOStatistics object or null if invalid \n*/"
    },
    "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter": {
        "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:<init>(java.util.concurrent.ExecutorService)": "/**\n* Initializes CloseableTaskPoolSubmitter with a given ExecutorService.\n* @param pool the ExecutorService for task execution\n*/",
        "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:close()": "/**\n* Closes the resource pool and releases associated resources.\n*/",
        "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:submit(java.lang.Runnable)": "/**\n* Submits a Runnable task for execution.\n* @param task the task to be executed\n* @return a Future representing the task's completion status\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:hasNext()": "/**\n* Checks if there is a next element available.\n* @return true if next element exists, false otherwise\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:next()": "/**\n* Retrieves the next element from the source.\n* @return the next element of type S\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)": "/**** \n* Initializes CloseRemoteIterator with a source iterator and a Closeable resource.\n* @param source the RemoteIterator to wrap\n* @param toClose the Closeable resource to manage\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close()": "/**\n* Closes the resource, preventing further use and logging the action.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.util.functional.ConsumerRaisingIOE": {
        "org.apache.hadoop.util.functional.ConsumerRaisingIOE:andThen(org.apache.hadoop.util.functional.ConsumerRaisingIOE)": "/**\n* Combines this consumer with another, chaining their execution.\n* @param next the next consumer to execute after this one\n* @return a new ConsumerRaisingIOE that executes both consumers\n*/"
    },
    "org.apache.hadoop.util.functional.TaskPool": {
        "org.apache.hadoop.util.functional.TaskPool:<init>()": "/**\n* Private constructor for TaskPool to prevent instantiation.\n*/",
        "org.apache.hadoop.util.functional.TaskPool:waitFor(java.util.Collection,int)": "/**\n* Waits for all futures to complete, checking periodically.\n* @param futures collection of Future tasks\n* @param sleepInterval time to wait between checks in milliseconds\n*/",
        "org.apache.hadoop.util.functional.TaskPool:castAndThrow(java.lang.Exception)": "/**\n* Casts and throws the given exception.\n* @param e the exception to be thrown\n* @throws E the type of exception being thrown\n*/",
        "org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Creates a Builder from a RemoteIterator of items.\n* @param items iterator of items to be processed\n* @return Builder instance for the given items\n*/",
        "org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection)": "/**\n* Throws the first exception from the collection, adding suppressed exceptions.\n* @param exceptions collection of exceptions to process\n* @throws E the type of exception being thrown\n*/",
        "org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable)": "/**\n* Creates a Builder from the provided Iterable items.\n* @param items source Iterable for item processing\n* @return Builder instance initialized with items\n*/",
        "org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[])": "/**\n* Creates a Builder from an array of items.\n* @param items array of items to process\n* @return Builder initialized with items as a list\n*/"
    },
    "org.apache.hadoop.util.functional.TaskPool$Builder": {
        "org.apache.hadoop.util.functional.TaskPool$Builder:<init>(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Initializes Builder with a non-null RemoteIterator of items.\n* @param items iterator of items to be processed\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(boolean)": "/**\n* Sets the suppression of exceptions in the builder.\n* @param suppress true to suppress exceptions, false otherwise\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:runSingleThreaded(org.apache.hadoop.util.functional.TaskPool$Task)": "/**\n* Executes a task on items, handling exceptions and potential rollbacks.\n* @param task the task to run on each item\n* @return true if all tasks succeeded, false otherwise\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable)": "/**\n* Initializes Builder with items from an Iterable.\n* @param items source Iterable for item processing\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions()": "/**\n* Suppresses exceptions in the builder.\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task)": "/**\n* Executes tasks in parallel, handling success and failure scenarios.\n* @param task the task to execute for each item\n* @return true if all tasks succeeded, false otherwise\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext()": "/**\n* Sets the IOStatisticsContext for the current thread if not null.\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext()": "/**\n* Resets the IOStatisticsContext for the current thread.\n*/",
        "org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task)": "/**\n* Executes a task on items, either in parallel or single-threaded.\n* @param task the task to execute\n* @return true if execution was successful, false if no items\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:<init>(java.util.Iterator)": "/**\n* Initializes a WrappedJavaIterator with a non-null source iterator.\n* @param source iterator to wrap and manage\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:hasNext()": "/**\n* Checks if there is a next element in the source.\n* @return true if next element exists, otherwise false\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:next()": "/**\n* Retrieves the next element from the source.\n* @return next element of type T\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:toString()": "/**\n* Returns a string representation of the FromIterator object.\n* @return formatted string including the source\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close()": "/**\n* Closes the resource to prevent further use.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics()": "/**\n* Retrieves IOStatistics from the source object.\n* @return IOStatistics object or null if invalid source\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:next()": "/**\n* Retrieves the next element from the source.\n* @return the next element of type S\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Constructs a HaltableRemoteIterator with a source iterator and a continuation condition.\n* @param source the RemoteIterator to wrap\n* @param continueWork a Callable that determines if iteration should continue\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext()": "/**\n* Checks if there are more elements to process.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext()": "/**\n* Checks if there are more elements to process.\n* @return true if more elements exist, false otherwise\n*/"
    },
    "org.apache.hadoop.util.functional.FunctionalIO": {
        "org.apache.hadoop.util.functional.FunctionalIO:<init>()": "/**\n* Private constructor to prevent instantiation of the FunctionalIO class.\n*/",
        "org.apache.hadoop.util.functional.FunctionalIO:extractIOExceptions(java.util.function.Supplier)": "/**\n* Executes a supplier and extracts IOExceptions from unchecked exceptions.\n* @param call supplier that produces a result\n* @return result of the supplier\n* @throws IOException if an IO exception occurs\n*/",
        "org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE)": "/**\n* Converts a FunctionRaisingIOE to a standard Function.\n* @param fun function that may throw IOException\n* @return Function that wraps the input function\n*/",
        "org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Executes a CallableRaisingIOE and handles IOExceptions as unchecked.\n* @param call callable that may throw an IOException\n* @return result of the callable execution\n*/",
        "org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE)": "/**\n* Converts a CallableRaisingIOE to a Supplier that throws UncheckedIOException.\n* @param call CallableRaisingIOE instance\n* @return Supplier<T> that executes call.unchecked() when invoked\n*/"
    },
    "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator": {
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:hasNext()": "/**\n* Checks if there is a next element in the source.\n* @return true if next element exists, false otherwise\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:next()": "/**\n* Retrieves the next element from the source.\n* @return next element of type T\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:toString()": "/**\n* Returns the string representation of the source object.\n* @return string representation of the source\n*/",
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator)": "/**\n* Constructs a TypeCastingRemoteIterator with a specified RemoteIterator source.\n* @param source the RemoteIterator to wrap\n*/"
    },
    "org.apache.hadoop.util.functional.Tuples$Tuple": {
        "org.apache.hadoop.util.functional.Tuples$Tuple:<init>(java.lang.Object,java.lang.Object)": "/**\n* Constructs a Tuple with a specified key and value.\n* @param key the key of the tuple\n* @param value the value of the tuple\n*/",
        "org.apache.hadoop.util.functional.Tuples$Tuple:setValue(java.lang.Object)": "/**\n* Throws an exception as setting value is not supported for immutable tuples.\n* @param value the value to set (ignored)\n*/",
        "org.apache.hadoop.util.functional.Tuples$Tuple:toString()": "/**\n* Returns a string representation of the key-value pair.\n* @return formatted string of the key and value\n*/",
        "org.apache.hadoop.util.functional.Tuples$Tuple:equals(java.lang.Object)": "/**\n* Compares this Tuple to another object for equality.\n* @param o object to compare with this Tuple\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.util.functional.Tuples$Tuple:hashCode()": "/**\n* Computes hash code based on key and value fields.\n* @return hash code as an integer\n*/"
    },
    "org.apache.hadoop.util.HttpExceptionUtils": {
        "org.apache.hadoop.util.HttpExceptionUtils:getOneLineMessage(java.lang.Throwable)": "/**\n* Extracts a single line message from a Throwable exception.\n* @param exception the Throwable to extract the message from\n* @return the first line of the exception message or null\n*/",
        "org.apache.hadoop.util.HttpExceptionUtils:throwException(java.lang.Throwable)": "/**\n* Throws a specified Throwable exception.\n* @param ex the exception to be thrown\n* @throws E the type of exception being thrown\n*/",
        "org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)": "/**\n* Sends a JSON error response for a servlet exception.\n* @param response HTTP response object\n* @param status HTTP status code\n* @param ex Throwable exception to process\n*/",
        "org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)": "/**\n* Creates a Jersey response for exceptions.\n* @param status HTTP status code\n* @param ex Throwable exception to process\n* @return Response object with error details\n*/",
        "org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable)": "/**\n* Throws a specified Throwable exception using HttpExceptionUtils.\n* @param ex the exception to be thrown\n*/",
        "org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)": "/**\n* Validates HTTP response status and throws an exception if it doesn't match expectedStatus.\n* @param conn HTTP connection to validate\n* @param expectedStatus expected HTTP status code\n* @throws IOException if the response code is invalid or an error occurs during processing\n*/"
    },
    "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics": {
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:sourceName(java.lang.String)": "/**\n* Constructs a source name string including the directory name.\n* @param dirName the name of the directory\n* @return formatted source name string\n*/",
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long)": "/**\n* Adds write latency to file quantiles if not null.\n* @param writeLatency duration of file write in milliseconds\n*/",
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long)": "/**\n* Adds read file latency to quantiles if available.\n* @param readLatency the latency value to be added\n*/",
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed()": "/**\n* Updates failure count and records the last failure timestamp.\n*/",
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>()": "/**\n* Initializes metrics for file read/write latencies using quantiles.\n*/",
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String)": "/**\n* Retrieves or creates metrics for a specified directory.\n* @param dirName the name of the directory\n* @return ReadWriteDiskValidatorMetrics for the directory\n*/"
    },
    "org.apache.hadoop.util.InvalidChecksumSizeException": {
        "org.apache.hadoop.util.InvalidChecksumSizeException:<init>(java.lang.String)": "/**\n* Constructs an InvalidChecksumSizeException with a specified message.\n* @param s error message detailing the exception\n*/"
    },
    "org.apache.hadoop.util.PureJavaCrc32C": {
        "org.apache.hadoop.util.PureJavaCrc32C:reset()": "/**\n* Resets the CRC value to its initial state.\n*/",
        "org.apache.hadoop.util.PureJavaCrc32C:<init>()": "/**\n* Initializes a new PureJavaCrc32C instance and resets the CRC value.\n*/"
    },
    "org.apache.hadoop.util.GSetByHashMap": {
        "org.apache.hadoop.util.GSetByHashMap:<init>(int,float)": "/**\n* Initializes a GSetByHashMap with specified capacity and load factor.\n* @param initialCapacity initial size of the hash map\n* @param loadFactor ratio for resizing the hash map\n*/",
        "org.apache.hadoop.util.GSetByHashMap:size()": "/**\n* Returns the number of elements in the collection.\n* @return the size of the collection as an integer\n*/",
        "org.apache.hadoop.util.GSetByHashMap:contains(java.lang.Object)": "/**\n* Checks if the map contains the specified key.\n* @param k the key to check for presence\n* @return true if the key exists, false otherwise\n*/",
        "org.apache.hadoop.util.GSetByHashMap:get(java.lang.Object)": "/**\n* Retrieves the value associated with the specified key.\n* @param k the key to look up\n* @return the value associated with the key or null if not found\n*/",
        "org.apache.hadoop.util.GSetByHashMap:put(java.lang.Object)": "/**\n* Adds an element to the collection; throws if element is null.\n* @param element the element to add\n* @return previous element associated with the key, or null\n*/",
        "org.apache.hadoop.util.GSetByHashMap:remove(java.lang.Object)": "/**\n* Removes the element associated with the specified key.\n* @param k the key of the element to remove\n* @return the removed element or null if not found\n*/",
        "org.apache.hadoop.util.GSetByHashMap:iterator()": "/**\n* Returns an iterator over the values in the collection.\n* @return Iterator for the collection's values\n*/",
        "org.apache.hadoop.util.GSetByHashMap:clear()": "/**\n* Clears all entries from the underlying collection.\n*/",
        "org.apache.hadoop.util.GSetByHashMap:values()": "/**\n* Returns a collection of values from the underlying map.\n* @return Collection of values stored in the map\n*/"
    },
    "org.apache.hadoop.util.IntrusiveCollection": {
        "org.apache.hadoop.util.IntrusiveCollection:removeElement(org.apache.hadoop.util.IntrusiveCollection$Element)": "/**\n* Removes the specified element and returns its next element.\n* @param elem the element to be removed\n* @return the next element after the removed one\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:iterator()": "/**\n* Returns an iterator over elements of type E.\n* @return Iterator for traversing elements\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:contains(java.lang.Object)": "/**\n* Checks if the specified element is in the list.\n* @param o object to check for presence in the list\n* @return true if the element is found, false otherwise\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:addFirst(org.apache.hadoop.util.IntrusiveCollection$Element)": "/**\n* Adds an element to the start of the collection.\n* @param elem the element to add\n* @return true if added successfully, false if null or already in list\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:addAll(java.util.Collection)": "/**\n* Adds all elements from the specified collection.\n* @param collection elements to add\n* @return true if any elements were added, false otherwise\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:add(java.lang.Object)": "/**\n* Adds an element to the collection if not null and not already present.\n* @param elem element to add\n* @return true if added, false if null or already in the list\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object)": "/**\n* Removes the specified element from the list.\n* @param o the object to be removed; returns false if not found or invalid\n* @return true if the element was successfully removed\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:toArray()": "/**\n* Converts collection elements to an array.\n* @return an array containing all elements of the collection\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection)": "/**\n* Retains only elements present in the specified collection.\n* @param collection the collection to retain elements from\n* @return true if the set was modified, false otherwise\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:clear()": "/**\n* Clears all elements from the collection.\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection)": "/**\n* Checks if all elements in the collection are present in this list.\n* @param collection elements to check for presence\n* @return true if all elements are found, false otherwise\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection)": "/**\n* Removes all elements in the specified collection.\n* @param collection elements to be removed; returns true if any were removed\n*/",
        "org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[])": "/**\n* Converts collection elements to an array.\n* @param array array to fill with collection elements\n* @return filled array or a new array if too small\n*/"
    },
    "org.apache.hadoop.util.HostsFileReader$HostDetails": {
        "org.apache.hadoop.util.HostsFileReader$HostDetails:<init>(java.lang.String,java.util.Set,java.lang.String,java.util.Map)": "/**\n* Initializes HostDetails with include/exclude file paths and sets.\n* @param includesFile path for included items\n* @param includes set of included item names\n* @param excludesFile path for excluded items\n* @param excludes map of excluded item names and their counts\n*/",
        "org.apache.hadoop.util.HostsFileReader$HostDetails:getExcludedHosts()": "/**\n* Retrieves a set of excluded host names.\n* @return Set of excluded host names from the excludes map\n*/",
        "org.apache.hadoop.util.HostsFileReader$HostDetails:getIncludedHosts()": "/**\n* Retrieves the set of included host names.\n* @return Set of included host names as strings\n*/",
        "org.apache.hadoop.util.HostsFileReader$HostDetails:getExcludedMap()": "/**\n* Retrieves the map of excluded items.\n* @return Map of excluded item names and their counts\n*/"
    },
    "org.apache.hadoop.util.DiskChecker": {
        "org.apache.hadoop.util.DiskChecker:mkdirsWithExistsCheck(java.io.File)": "/**\n* Creates directories if they don't exist, checking existence recursively.\n* @param dir directory to create\n* @return true if created or already exists, false on failure\n*/",
        "org.apache.hadoop.util.DiskChecker:getFileNameForDiskIoCheck(java.io.File,int)": "/**\n* Generates a file name for disk I/O checks based on iteration count.\n* @param dir directory for the file\n* @param iterationCount current iteration number\n* @return File object with the generated file name\n*/",
        "org.apache.hadoop.util.DiskChecker:replaceFileOutputStreamProvider(org.apache.hadoop.util.DiskChecker$FileIoProvider)": "/**\n* Replaces the current FileIoProvider with a new one.\n* @param newFosProvider the new FileIoProvider to set\n* @return the previous FileIoProvider\n*/",
        "org.apache.hadoop.util.DiskChecker:getFileOutputStreamProvider()": "/**\n* Retrieves the FileIoProvider instance for output stream operations.\n* @return FileIoProvider for file output streams\n*/",
        "org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File)": "/**\n* Checks disk I/O without native I/O; deletes file if writable.\n* @param file the file to check and delete\n* @throws IOException if I/O operation fails\n*/",
        "org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File)": "/**\n* Performs disk I/O checks on a directory, retrying on failure.\n* @param dir the directory to check for disk I/O issues\n* @throws DiskErrorException if all checks fail\n*/",
        "org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File)": "/**\n* Validates directory access permissions and throws exceptions if checks fail.\n* @param dir the directory to check access for\n* @throws DiskErrorException if the directory is invalid or inaccessible\n*/",
        "org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File)": "/**\n* Validates and creates a directory, checking access permissions.\n* @param dir the directory to validate and create\n* @throws DiskErrorException if the directory cannot be created\n*/",
        "org.apache.hadoop.util.DiskChecker:checkDir(java.io.File)": "/**\n* Validates and creates a directory.\n* @param dir the directory to validate and create\n* @throws DiskErrorException if the directory cannot be created\n*/",
        "org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File)": "/**\n* Validates directory and performs disk I/O checks.\n* @param dir the directory to validate and check\n* @throws DiskErrorException if directory issues occur\n*/",
        "org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directory if it doesn't exist and sets permissions if needed.\n* @param localFS the local file system\n* @param dir directory path to check/create\n* @param expected desired file permissions\n*/",
        "org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Validates and prepares a directory with specified permissions.\n* @param localFS the local file system\n* @param dir directory path to check/create\n* @param expected desired file permissions\n* @throws DiskErrorException if the directory is invalid or inaccessible\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Validates a directory with specified permissions.\n* @param localFS the local file system\n* @param dir directory path to check/create\n* @param expected desired file permissions\n* @throws DiskErrorException if the directory is invalid\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Validates directory and performs disk I/O checks.\n* @param localFS the local file system\n* @param dir directory path to check\n* @param expected desired file permissions\n* @throws DiskErrorException if the directory is invalid\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.Options$StringOption": {
        "org.apache.hadoop.util.Options$StringOption:<init>(java.lang.String)": "/**\n* Constructs a StringOption with the given value.\n* @param value the string value to be encapsulated\n*/"
    },
    "org.apache.hadoop.util.CrcComposer": {
        "org.apache.hadoop.util.CrcComposer:<init>(int,int,long,long)": "/**\n* Initializes CrcComposer with polynomial and hint values.\n* @param crcPolynomial CRC polynomial for computation\n* @param precomputedMonomialForHint precomputed hint for optimization\n* @param bytesPerCrcHint bytes per CRC for processing\n* @param stripeLength length of data stripe for CRC\n*/",
        "org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)": "/**\n* Creates a CrcComposer for striped CRC calculation.\n* @param type CRC type for polynomial selection\n* @param bytesPerCrcHint bytes per CRC for processing\n* @param stripeLength length of data stripe\n* @return initialized CrcComposer instance\n* @throws IOException if CRC type is unsupported\n*/",
        "org.apache.hadoop.util.CrcComposer:digest()": "/**\n* Computes and returns the current digest value as a byte array.\n* @return byte array representing the digest\n*/",
        "org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)": "/**\n* Creates a CrcComposer for CRC calculation.\n* @param type CRC type for polynomial selection\n* @param bytesPerCrcHint bytes per CRC for processing\n* @return initialized CrcComposer instance\n*/",
        "org.apache.hadoop.util.CrcComposer:update(int,long)": "/**\n* Updates the current composite CRC based on input values.\n* @param crcB CRC value to combine, @param bytesPerCrc bytes processed for CRC calculation\n*/",
        "org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)": "/**\n* Updates CRC from byte array in chunks.\n* @param crcBuffer byte array containing CRC data\n* @param offset starting position in the array\n* @param length number of bytes to process\n* @param bytesPerCrc bytes processed for CRC calculation\n* @throws IOException if length is not a multiple of CRC_SIZE_BYTES\n*/",
        "org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)": "/**\n* Reads checksums from input stream and updates CRC values.\n* @param checksumIn input stream for checksum data\n* @param numChecksumsToRead number of checksums to process\n* @param bytesPerCrc bytes processed for CRC calculation\n*/"
    },
    "org.apache.hadoop.util.ShutdownThreadsHelper": {
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread,long)": "/**\n* Attempts to shutdown a thread with a timeout.\n* @param thread the Thread to shutdown\n* @param timeoutInMilliSeconds max wait time for thread to stop\n* @return true if shutdown was initiated, false if interrupted\n*/",
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService,long)": "/**\n* Shuts down the given ExecutorService with a timeout.\n* @param service the ExecutorService to shut down\n* @param timeoutInMs maximum wait time in milliseconds\n* @return true if shutdown completed, false if forced shutdown occurred\n*/",
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread)": "/**\n* Initiates shutdown of a thread with a default timeout.\n* @param thread the Thread to shutdown\n* @return true if shutdown was initiated, false if interrupted\n*/",
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService)": "/**\n* Initiates shutdown of the ExecutorService with a default timeout.\n* @param service the ExecutorService to shut down\n* @return true if shutdown completed, false if forced shutdown occurred\n*/"
    },
    "org.apache.hadoop.util.curator.ZKCuratorManager": {
        "org.apache.hadoop.util.curator.ZKCuratorManager:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ZKCuratorManager with the given configuration.\n* @param config configuration settings for the manager\n* @throws IOException if configuration is invalid\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:close()": "/**\n* Closes the curator resource if it is not null.\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getACL(java.lang.String)": "/**\n* Retrieves the ACL for the specified path.\n* @param path the path to fetch ACL for\n* @return List of ACLs associated with the path\n* @throws Exception if an error occurs during retrieval\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getData(java.lang.String)": "/**\n* Retrieves data from the specified path.\n* @param path the path to fetch data from\n* @return byte array of data at the given path\n* @throws Exception if an error occurs during retrieval\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getData(java.lang.String,org.apache.zookeeper.data.Stat)": "/**\n* Retrieves data from the specified path and stores metadata in the provided Stat object.\n* @param path the data path to retrieve\n* @param stat the Stat object to store metadata\n* @return byte array of data from the path\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,byte[],int)": "/**\n* Sets data at the specified path with version control.\n* @param path the path to set data at\n* @param data the data to be stored\n* @param version the expected version for the update\n* @throws Exception if the operation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getChildren(java.lang.String)": "/**\n* Retrieves a list of child nodes for the specified path.\n* @param path the node path to fetch children from\n* @return List of child node names\n* @throws Exception if an error occurs while fetching children\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:exists(java.lang.String)": "/**\n* Checks if a path exists in the system.\n* @param path the path to check for existence\n* @return true if the path exists, false otherwise\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getNodePath(java.lang.String,java.lang.String)": "/**\n* Constructs a node path by appending nodeName to root.\n* @param root the base path\n* @param nodeName the name of the node to append\n* @return the combined node path as a String\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:createTransaction(java.util.List,java.lang.String)": "/**\n* Creates a SafeTransaction with specified ACLs and node path.\n* @param fencingACL list of access control lists for fencing\n* @param fencingNodePath path for the fencing node\n* @return a new SafeTransaction object\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String)": "/**\n* Retrieves string data from the specified path.\n* @param path the path to fetch data from\n* @return String representation of data or null if not found\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)": "/**\n* Retrieves string data from a specified path.\n* @param path the data path to retrieve\n* @param stat the Stat object for metadata\n* @return String data or null if not found\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)": "/**\n* Converts string data to bytes and sets it at the specified path with version control.\n* @param path the path to set data at\n* @param data the data to be stored\n* @param version the expected version for the update\n* @throws Exception if the operation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)": "/**\n* Creates a path if it doesn't exist and sets specified ACLs.\n* @param path the path to create\n* @param zkAcl list of ACLs for the created path\n* @return true if the path was created, false otherwise\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String)": "/**\n* Deletes a path if it exists.\n* @param path the path to delete\n* @return true if deleted, false if not found\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)": "/**\n* Safely creates a node if it doesn't exist.\n* @param path the node's path, data for the node, ACLs, and creation mode\n* @throws Exception if the transaction fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)": "/**\n* Safely deletes a node if it exists.\n* @param path the path of the node to delete\n* @param fencingACL access control lists for fencing\n* @param fencingNodePath path for the fencing node\n* @throws Exception if the delete operation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)": "/**\n* Safely sets data at a specified path with versioning and fencing.\n* @param path target data path, @param data byte array to set, @param version expected version\n* @param fencingACL access control lists for fencing, @param fencingNodePath path for fencing node\n* @throws Exception if the commit operation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String)": "/**\n* Creates a path with default ACLs.\n* @param path the path to create\n* @return true if the path was created, false otherwise\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)": "/**\n* Creates root directory recursively from the given path with specified ACLs.\n* @param path the directory path to create\n* @param zkAcl list of ACLs for the created directories\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String)": "/**\n* Initiates root directory creation with default ACLs.\n* @param path the directory path to create\n* @throws Exception if directory creation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration)": "/**\n* Validates SSL configuration parameters in the provided Configuration object.\n* @param config configuration containing SSL settings\n* @throws IOException if any SSL parameter is empty\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a list of ACLs from the provided configuration.\n* @param conf configuration containing ACL settings\n* @return List of ACL objects\n* @throws IOException if an I/O error occurs or ACL format is invalid\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves ZKAuthInfo list from the given configuration.\n* @param conf configuration object\n* @return List of ZKAuthInfo or empty if not found\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)": "/**\n* Starts the ZooKeeper client with authentication and SSL configuration.\n* @param authInfos list of authentication info, may be modified\n* @param sslEnabled flag to enable SSL\n* @throws IOException if configuration is invalid\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List)": "/**\n* Initiates the process with authentication info.\n* @param authInfos list of authentication details\n* @throws IOException if configuration is invalid\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager:start()": "/**\n* Initiates the process without authentication details.\n* @throws IOException if configuration is invalid\n*/"
    },
    "org.apache.hadoop.util.ZKUtil$ZKAuthInfo": {
        "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:getScheme()": "/**\n* Retrieves the current scheme value.\n* @return the scheme as a String\n*/",
        "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:getAuth()": "/**\n* Retrieves the authentication byte array.\n* @return byte array containing authentication data\n*/",
        "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:<init>(java.lang.String,byte[])": "/**\n* Constructs ZKAuthInfo with specified scheme and authentication data.\n* @param scheme authentication scheme\n* @param auth byte array containing authentication information\n*/"
    },
    "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction": {
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:create(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)": "/**\n* Creates a node at the specified path with data and ACLs.\n* @param path the node's path in the hierarchy\n* @param data the data to store in the node\n* @param acl the access control list for the node\n* @param mode the creation mode for the node\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:commit()": "/**\n* Commits a delete operation on the fencing node path.\n* @throws Exception if the commit operation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:delete(java.lang.String)": "/**\n* Deletes a node at the specified path.\n* @param path the path of the node to delete\n* @throws Exception if the delete operation fails\n*/",
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:setData(java.lang.String,byte[],int)": "/**\n* Sets data at the specified path with a given version.\n* @param path the target path in the data store\n* @param data the byte array to set at the path\n* @param version the expected version for the operation\n*/"
    },
    "org.apache.hadoop.util.ProgramDriver$ProgramDescription": {
        "org.apache.hadoop.util.ProgramDriver$ProgramDescription:<init>(java.lang.Class,java.lang.String)": "/**\n* Initializes ProgramDescription with main class and its description.\n* @param mainClass class containing the main method\n* @param description brief description of the program\n*/",
        "org.apache.hadoop.util.ProgramDriver$ProgramDescription:invoke(java.lang.String[])": "/**\n* Invokes the main method with provided arguments, handling exceptions.\n* @param args command-line arguments for the main method\n* @throws Throwable if an exception occurs during invocation\n*/",
        "org.apache.hadoop.util.ProgramDriver$ProgramDescription:getDescription()": "/**\n* Retrieves the description string.\n* @return the description value\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynConstructors$Builder": {
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:<init>(java.lang.Class)": "/**\n* Initializes the Builder with a specified base class.\n* @param baseClass the class to be used as the foundation for the Builder\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:<init>()": "/**\n* Initializes a new Builder instance with baseClass set to null.\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.Class,java.lang.Class[])": "/**\n* Attempts to find a constructor for targetClass with specified parameter types.\n* @param targetClass the class to find the constructor for\n* @param types the parameter types of the constructor\n* @return this Builder instance\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])": "/**\n* Attempts to retrieve a constructor for the specified class.\n* @param targetClass the class to inspect for a constructor\n* @param types parameter types of the constructor\n* @return the Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:buildChecked()": "/**\n* Builds and returns a constructor instance.\n* @return Ctor<C> constructor or throws NoSuchMethodException if not found\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:build()": "/**\n* Returns the constructor for the base class or throws an exception if not found.\n* @return Ctor<C> constructor instance\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])": "/**\n* Attempts to find an implementation constructor by class name and parameter types.\n* @param className the name of the class to find\n* @param types the parameter types of the constructor\n* @return this Builder instance\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[])": "/**\n* Retrieves a constructor for the base class with specified parameter types.\n* @param types parameter types of the constructor\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])": "/**** \n* Retrieves a Builder instance by class name and parameter types.\n* @param className name of the class to load\n* @param types parameter types for the constructor\n* @return Builder instance for method chaining\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods": {
        "org.apache.hadoop.util.dynamic.DynMethods:<init>()": "/**\n* Private constructor for DynMethods class, preventing instantiation.\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods:throwIfInstance(java.lang.Throwable,java.lang.Class)": "/**\n* Throws the throwable as an exception if it matches the specified class.\n* @param t throwable to check\n* @param excClass class of the exception to match\n* @throws E if t is an instance of excClass\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynConstructors": {
        "org.apache.hadoop.util.dynamic.DynConstructors:formatProblems(java.util.Map)": "/**\n* Formats a map of problems into a string summary.\n* @param problems map of problem descriptions and their associated exceptions\n* @return formatted string representation of the problems\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors:methodName(java.lang.Class,java.lang.Class[])": "/**\n* Constructs a string representation of a class and its parameter types.\n* @param targetClass the class to represent\n* @param types the parameter types of the class\n* @return formatted string of class name and parameter types\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod": {
        "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:<init>(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)": "/**\n* Initializes StaticMethod with the provided UnboundMethod instance.\n* @param method the UnboundMethod to be assigned\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[])": "/**\n* Invokes a method with specified arguments.\n* @param args arguments for the method\n* @return result of the method invocation\n* @throws Exception if invocation fails\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[])": "/**\n* Invokes a method with specified arguments.\n* @param args arguments for the invoked method\n* @return result of the method invocation\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods$Builder": {
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:<init>(java.lang.String)": "/**\n* Initializes a Builder with the specified method name.\n* @param methodName name of the method to build\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:orNoop()": "/**\n* Sets method to NOOP if not already set and returns the Builder instance.\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked()": "/**\n* Builds and returns an UnboundMethod; throws exception if method is not found.\n* @return UnboundMethod object\n* @throws NoSuchMethodException if the method is null\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:build()": "/**\n* Builds and returns an UnboundMethod if available.\n* @return UnboundMethod instance\n* @throws RuntimeException if method is not found\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])": "/**\n* Initializes constructor implementation for targetClass with specified argClasses.\n* @param targetClass the class to find the constructor for\n* @param argClasses parameter types of the constructor\n* @return this Builder instance\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])": "/**\n* Binds a method from the target class by name and argument types.\n* @param targetClass class containing the method\n* @param methodName name of the method to bind\n* @param argClasses parameter types of the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])": "/****\n* Retrieves a method by name from a target class and makes it accessible.\n* @param targetClass the class containing the method\n* @param methodName the name of the method to retrieve\n* @param argClasses optional parameter types for the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])": "/**\n* Initializes constructor implementation if not already found.\n* @param className name of the class for the constructor\n* @param argClasses parameter types for the constructor\n* @return this Builder instance\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])": "/**\n* Binds a method by class name and argument types.\n* @param className name of the target class\n* @param methodName name of the method to bind\n* @param argClasses parameter types of the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])": "/**\n* Binds a method from the target class for chaining.\n* @param targetClass the class containing the method\n* @param argClasses parameter types of the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])": "/**\n* Retrieves a method by name from a class and returns a Builder for chaining.\n* @param className the name of the target class\n* @param methodName the name of the method to retrieve\n* @param argClasses optional parameter types for the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])": "/**\n* Invokes hiddenImpl to access a method by name in the target class.\n* @param targetClass the class containing the method\n* @param argClasses optional parameter types for the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object)": "/**\n* Builds and binds a method to the specified receiver.\n* @param receiver the object to bind the method to\n* @return a BoundMethod instance\n* @throws NoSuchMethodException if the method is not found\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object)": "/**\n* Builds and binds a method to the specified receiver.\n* @param receiver the object to bind the method to\n* @return BoundMethod instance\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked()": "/**\n* Builds and returns a StaticMethod after checking validity.\n* @return StaticMethod instance\n* @throws NoSuchMethodException if method is invalid\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic()": "/**\n* Builds a StaticMethod from an UnboundMethod instance.\n* @return StaticMethod object derived from the build process\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])": "/**** \n* Binds a method using class name and argument types.\n* @param className name of the target class\n* @param argClasses parameter types of the method\n* @return Builder instance for method chaining\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])": "/**\n* Calls hiddenImpl to initialize method chaining for the specified class.\n* @param className the name of the target class\n* @param argClasses optional parameter types for the method\n* @return Builder instance for method chaining\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod": {
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:<init>(java.lang.reflect.Method,java.lang.String)": "/**\n* Initializes UnboundMethod with a Method and its name.\n* @param method Method object to bind\n* @param name   Name of the method\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:isNoop()": "/**\n* Checks if the current instance is the NOOP constant.\n* @return true if instance is NOOP, otherwise false\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:isStatic()": "/**\n* Checks if the method is static.\n* @return true if the method is static, false otherwise\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:toString()": "/**\n* Returns a string representation of the UnboundMethod.\n* @return formatted string with method name and generic method details\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])": "/**** Invokes a method on the target with specified arguments. \n* @param target the object to invoke the method on\n* @param args the arguments for the method\n* @return result of the method invocation\n* @throws Exception if invocation fails\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object)": "/**\n* Binds a method to a given receiver object.\n* @param receiver the object to bind the method to\n* @return a BoundMethod instance\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic()": "/**\n* Converts the method to a StaticMethod if it's static.\n* @return StaticMethod instance\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])": "/**\n* Invokes a method on a target with specified arguments.\n* @param target the object to invoke the method on\n* @param args the arguments for the method\n* @return result of the method invocation\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[])": "/**\n* Invokes a static method with given arguments.\n* @param args arguments for the method\n* @return result of the method invocation\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible": {
        "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible:<init>(java.lang.reflect.Method)": "/**\n* Sets the hidden method to be accessible.\n* @param hidden the Method object to be made accessible\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible:run()": "/**\n* Sets accessibility of a hidden field and returns null.\n* @return null as the method does not produce a value\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1": {
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:bind(java.lang.Object)": "/**\n* Binds a method to a specified receiver object.\n* @param receiver the object to bind the method to\n* @return BoundMethod instance representing the bound method\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:asStatic()": "/**\n* Converts the current method to a StaticMethod if it is static.\n* @return StaticMethod instance of the current method\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)": "/**\n* Initializes UnboundMethod with a Method and its name.\n* @param method Method object to bind\n* @param name   Name of the method\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible": {
        "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible:<init>(java.lang.reflect.Constructor)": "/**\n* Initializes MakeAccessible with a hidden constructor.\n* @param hidden the constructor to be made accessible\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible:run()": "/**\n* Sets accessibility of a hidden member and returns null.\n* @return always returns null\n*/"
    },
    "org.apache.hadoop.util.dynamic.DynConstructors$Ctor": {
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:bind(java.lang.Object)": "/**\n* Throws an exception when attempting to bind constructors.\n* @param receiver the object to bind, ignored in this method\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:toString()": "/**\n* Returns a string representation of the object with constructor and class details.\n* @return formatted string with constructor and class information\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[])": "/**\n* Creates a new instance using the constructor with provided arguments.\n* @param args constructor arguments\n* @throws Exception if instantiation fails or target exception occurs\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)": "/**\n* Initializes Ctor with a Constructor and its constructed class type.\n* @param constructor the Constructor to bind\n* @param constructed the class type being constructed\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[])": "/**\n* Creates a new instance using provided arguments.\n* @param args constructor arguments\n* @return new instance of type C\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])": "/**\n* Invokes a constructor with arguments, ensuring target is null.\n* @param target expected to be null\n* @param args constructor arguments\n* @return new instance of type R\n*/",
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])": "/**** Invokes a constructor with provided arguments. \n* @param target must be null; \n* @param args constructor arguments \n* @return new instance of type R \n*/"
    },
    "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod": {
        "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:<init>(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod,java.lang.Object)": "/**\n* Constructs a BoundMethod with specified UnboundMethod and receiver.\n* @param method the UnboundMethod to bind\n* @param receiver the object to invoke the method on\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[])": "/**\n* Invokes a method with specified arguments on a receiver.\n* @param args method arguments\n* @return result of the method invocation\n* @throws Exception if invocation fails\n*/",
        "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[])": "/****\n* Invokes a method with specified arguments on a receiver.\n* @param args method arguments\n* @return result of the method invocation\n*/"
    },
    "org.apache.hadoop.util.ClassUtil": {
        "org.apache.hadoop.util.ClassUtil:findContainingResource(java.lang.ClassLoader,java.lang.String,java.lang.String)": "/**\n* Finds the path of a resource linked to a specified class.\n* @param loader ClassLoader to search resources\n* @param clazz Fully qualified class name\n* @param resource Expected resource protocol\n* @return Resource path or null if not found\n*/",
        "org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class)": "/**\n* Retrieves the JAR path containing the specified class.\n* @param clazz Class to locate within its JAR\n* @return JAR path as a String or null if not found\n*/",
        "org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class)": "/**\n* Retrieves the file location of the specified class.\n* @param clazz the class to find the location for\n* @return the file path of the class or null if not found\n*/"
    },
    "org.apache.hadoop.util.DataChecksum$Java9Crc32CFactory": {
        "org.apache.hadoop.util.DataChecksum$Java9Crc32CFactory:createChecksum()": "/**\n* Creates a Checksum instance using CRC32C method.\n* @return Checksum object\n*/"
    },
    "org.apache.hadoop.util.DataChecksum$Type": {
        "org.apache.hadoop.util.DataChecksum$Type:valueOf(int)": "/**\n* Retrieves Type by its index.\n* @param id index of the desired Type\n* @return Type corresponding to the given index\n*/"
    },
    "org.apache.hadoop.util.RateLimitingFactory": {
        "org.apache.hadoop.util.RateLimitingFactory:<init>()": "/**\n* Private constructor for RateLimitingFactory to prevent instantiation.\n*/",
        "org.apache.hadoop.util.RateLimitingFactory:unlimitedRate()": "/**\n* Returns an unlimited rate limiting configuration.\n* @return RateLimiting instance representing no limits\n*/",
        "org.apache.hadoop.util.RateLimitingFactory:create(int)": "/**\n* Creates a RateLimiting instance based on capacity.\n* @param capacity maximum requests allowed; 0 for unlimited rate\n* @return RateLimiting instance\n*/"
    },
    "org.apache.hadoop.util.SignalLogger$Handler": {
        "org.apache.hadoop.util.SignalLogger$Handler:<init>(java.lang.String,org.slf4j.Logger)": "/**\n* Initializes a Handler with a signal name and logger.\n* @param name signal name to handle\n* @param log Logger instance for logging events\n*/",
        "org.apache.hadoop.util.SignalLogger$Handler:handle(sun.misc.Signal)": "/**\n* Logs received signal details and passes it to the previous handler.\n* @param signal the signal object containing signal number and name\n*/"
    },
    "org.apache.hadoop.util.ComparableVersion$StringItem": {
        "org.apache.hadoop.util.ComparableVersion$StringItem:<init>(java.lang.String,boolean)": "/**\n* Constructs a StringItem with value, replacing single chars if followed by a digit.\n* @param value initial string value\n* @param followedByDigit indicates if the value is followed by a digit\n*/",
        "org.apache.hadoop.util.ComparableVersion$StringItem:comparableQualifier(java.lang.String)": "/**\n* Returns index of qualifier or a formatted string if not found.\n* @param qualifier the string to find in _QUALIFIERS\n* @return index as string or formatted not found message\n*/",
        "org.apache.hadoop.util.ComparableVersion$StringItem:getType()": "",
        "org.apache.hadoop.util.ComparableVersion$StringItem:isNull()": "/**\n* Checks if the value matches the release version index.\n* @return true if value is null, false otherwise\n*/",
        "org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)": "/**\n* Compares this item with another based on type and value.\n* @param item the item to compare with\n* @return comparison result as int\n*/"
    },
    "org.apache.hadoop.util.ComparableVersion": {
        "org.apache.hadoop.util.ComparableVersion:equals(java.lang.Object)": "/**\n* Checks equality with another ComparableVersion object.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.util.ComparableVersion:hashCode()": "/**\n* Returns the hash code of the canonical object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion)": "/**\n* Compares this ComparableVersion with another.\n* @param o the ComparableVersion to compare against\n* @return comparison result of items\n*/",
        "org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)": "/**\n* Parses an item based on digit status.\n* @param isDigit indicates if the item is an integer\n* @param buf input string to create the item\n* @return IntegerItem or StringItem based on isDigit\n*/",
        "org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String)": "/**\n* Parses a version string into a structured list of items.\n* @param version the version string to parse\n*/",
        "org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String)": "/**\n* Constructs a ComparableVersion from a version string.\n* @param version the version string to parse\n*/"
    },
    "org.apache.hadoop.util.ShutdownHookManager$1": {
        "org.apache.hadoop.util.ShutdownHookManager$1:<init>()": "/**\n* Initializes the ShutdownHookManager for managing shutdown hooks.\n*/"
    },
    "org.apache.hadoop.util.SemaphoredDelegatingExecutor": {
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAll(java.util.Collection)": "/**\n* Executes a collection of tasks and returns their futures.\n* @param tasks collection of Callable tasks to execute\n* @return list of Future objects representing task results\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAll(java.util.Collection,long,java.util.concurrent.TimeUnit)": "/**\n* Executes a collection of tasks and returns their futures.\n* @param tasks collection of callable tasks to execute\n* @param timeout maximum time to wait for task completion\n* @param unit time unit for the timeout\n* @return list of futures representing the task results\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAny(java.util.Collection)": "/**\n* Invokes any task from a collection of callables.\n* @param tasks collection of callable tasks\n* @return result of the first successfully completed task\n* @throws InterruptedException if interrupted while waiting\n* @throws ExecutionException if a task fails during execution\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAny(java.util.Collection,long,java.util.concurrent.TimeUnit)": "/**\n* Executes a collection of tasks, returning the result of the first completed task.\n* @param tasks collection of Callable tasks to execute\n* @param timeout maximum time to wait for a task to complete\n* @param unit time unit of the timeout\n* @return result of the first completed task\n* @throws InterruptedException if interrupted while waiting\n* @throws ExecutionException if a task fails\n* @throws TimeoutException if timeout is reached\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getAvailablePermits()": "/**\n* Returns the number of available permits in the queue.\n* @return number of available permits as an integer\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getWaitingCount()": "/**\n* Retrieves the current count of waiting permits in the queue.\n* @return number of waiting permits\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getPermitCount()": "/**\n* Retrieves the current permit count.\n* @return the number of permits\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:delegate()": "/**\n* Returns the underlying ExecutorService instance.\n* @return ExecutorService that is delegated to\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)": "/**\n* Initializes SemaphoredDelegatingExecutor with permits and tracker factory.\n* @param executorDelegatee executor service to delegate tasks\n* @param permitCount number of permits for semaphore\n* @param fair indicates if semaphore should be fair\n* @param trackerFactory factory for tracking duration, stub if null\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString()": "/**\n* Returns a string representation of the executor's state.\n* @return formatted string with permit count, available, and waiting permits\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)": "/**** Initializes SemaphoredDelegatingExecutor with permits. \n* @param executorDelegatee the service to delegate tasks \n* @param permitCount number of permits for semaphore \n* @param fair indicates if semaphore should be fair \n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable)": "/**\n* Submits a task for execution with permit acquisition tracking.\n* @param task the callable task to execute\n* @return Future representing the task's completion status\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)": "/**\n* Submits a task for execution with a result and tracks duration.\n* @param task the task to execute\n* @param result the result to return upon completion\n* @return Future representing the task's completion status\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable)": "/**\n* Submits a task while tracking duration and managing permits.\n* @param task the Runnable task to be submitted\n* @return Future representing the task's completion status\n*/",
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable)": "/**\n* Executes a command with duration tracking and queue permit acquisition.\n* @param command the Runnable task to execute\n*/"
    },
    "org.apache.hadoop.util.SemaphoredDelegatingExecutor$RunnableWithPermitRelease": {
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor$RunnableWithPermitRelease:run()": "/**\n* Executes the delegatee's run method and releases queueing permits afterward.\n*/"
    },
    "org.apache.hadoop.util.HostsFileReader": {
        "org.apache.hadoop.util.HostsFileReader:readFileToSetWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Set)": "/**\n* Reads lines from a file and adds non-comment nodes to a set.\n* @param type description of the nodes being read\n* @param filename name of the file being processed\n* @param fileInputStream input stream of the file\n* @param set collection to store unique nodes\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:readFirstTagValue(org.w3c.dom.Element,java.lang.String)": "/**\n* Retrieves the text content of the first occurrence of a specified tag.\n* @param e XML element to search within\n* @param tag name of the tag to find\n* @return text content of the first tag or null if not found\n*/",
        "org.apache.hadoop.util.HostsFileReader:finishRefresh()": "/**\n* Completes the refresh process after lazy loading.\n* @throws IllegalStateException if lazyRefresh() wasn't called\n*/",
        "org.apache.hadoop.util.HostsFileReader:getHostDetails()": "/**\n* Retrieves the current host details.\n* @return HostDetails object representing the current host\n*/",
        "org.apache.hadoop.util.HostsFileReader:getLazyLoadedHostDetails()": "/**\n* Retrieves lazily loaded host details.\n* @return HostDetails object from lazy loading\n*/",
        "org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String)": "/**\n* Updates the includes file and sets new HostDetails.\n* @param includesFile path for included items\n*/",
        "org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String)": "/**\n* Updates the excludes file and logs the change.\n* @param excludesFile path for excluded items\n*/",
        "org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)": "/**\n* Updates file paths for included and excluded items.\n* @param includesFile path for included items\n* @param excludesFile path for excluded items\n*/",
        "org.apache.hadoop.util.HostsFileReader:getExcludedHosts()": "/**\n* Retrieves a set of excluded host names from current host details.\n* @return Set of excluded host names\n*/",
        "org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)": "/**\n* Reads file lines into a set of unique nodes.\n* @param type node type description\n* @param filename path to the file to read\n* @param set collection for unique nodes\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)": "/**\n* Parses XML file and populates a map with host names and their timeouts.\n* @param type description of host type\n* @param filename name of the XML file\n* @param fileInputStream input stream of the XML file\n* @param map map to store host names and timeouts\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:getHosts()": "/**\n* Retrieves the set of included host names.\n* @return Set of included host names as strings\n*/",
        "org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)": "/**\n* Retrieves host details and updates included and excluded host sets.\n* @param includes set to add included host names\n* @param excludes set to add excluded host names\n*/",
        "org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)": "/**\n* Updates provided sets with host details.\n* @param includeHosts set to add included hosts\n* @param excludeHosts map to populate with excluded hosts\n*/",
        "org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)": "/**\n* Reads a file and populates a map with host names and their timeouts.\n* @param type description of host type\n* @param filename name of the input file\n* @param inputStream input stream of the file\n* @param map map to store host names and timeouts\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)": "/**\n* Reads a file and populates a map with host names and timeouts.\n* @param type description of host type\n* @param filename name of the input file\n* @param map map to store host names and timeouts\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)": "/**\n* Refreshes host include/exclude lists from input streams.\n* @param inFileInputStream includes input stream\n* @param exFileInputStream excludes input stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)": "/**\n* Refreshes host include/exclude lists from specified files.\n* @param includesFile path for included items\n* @param excludesFile path for excluded items\n* @param lazy if true, updates lazy-loaded details\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)": "/**\n* Initializes HostsFileReader with include/exclude files and refreshes lists from streams.\n* @param includesFile path for included items\n* @param inFileInputStream includes input stream\n* @param excludesFile path for excluded items\n* @param exFileInputStream excludes input stream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)": "/**\n* Refreshes host include/exclude lists from specified files.\n* @param includesFile path for included items\n* @param excludesFile path for excluded items\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)": "/**\n* Refreshes host include/exclude lists lazily.\n* @param includesFile path for included items\n* @param excludesFile path for excluded items\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)": "/**\n* Initializes HostsFileReader with input and exclusion file paths.\n* @param inFile path for included items, exFile path for excluded items\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.HostsFileReader:refresh()": "/**\n* Refreshes host details using include/exclude file paths.\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.hash.JenkinsHash": {
        "org.apache.hadoop.util.hash.JenkinsHash:rot(long,int)": "/**\n* Rotates the bits of a long value left by a specified position.\n* @param val the value to rotate\n* @param pos the number of positions to rotate\n* @return the rotated long value\n*/",
        "org.apache.hadoop.util.hash.JenkinsHash:getInstance()": "/**\n* Retrieves the singleton instance of Hash.\n* @return the single Hash instance\n*/",
        "org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)": "/**\n* Computes a hash value from a byte array.\n* @param key input byte array\n* @param nbytes number of bytes to process\n* @param initval initial hash value\n* @return computed hash value as an integer\n*/",
        "org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[])": "/**\n* Computes and prints Jenkins hash from a file specified in args.\n* @param args command-line arguments, expects a single filename\n*/"
    },
    "org.apache.hadoop.util.hash.Hash": {
        "org.apache.hadoop.util.hash.Hash:parseHashType(java.lang.String)": "/**\n* Parses hash type from string name.\n* @param name hash type name (e.g., \"jenkins\" or \"murmur\")\n* @return corresponding hash type constant or INVALID_HASH\n*/",
        "org.apache.hadoop.util.hash.Hash:hash(byte[])": "/**\n* Computes hash value for the given byte array.\n* @param bytes input byte array\n* @return computed hash as an integer\n*/",
        "org.apache.hadoop.util.hash.Hash:hash(byte[],int)": "/**\n* Computes hash value for byte array with an initial value.\n* @param bytes input byte array\n* @param initval initial hash value\n* @return computed hash as an integer\n*/",
        "org.apache.hadoop.util.hash.Hash:getInstance(int)": "/**\n* Returns a Hash instance based on the specified type.\n* @param type identifier for the hash type\n* @return Hash instance or null for invalid type\n*/",
        "org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves hash type from configuration.\n* @param conf configuration object\n* @return hash type constant\n*/",
        "org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a Hash instance based on configuration.\n* @param conf configuration object\n* @return Hash instance or null for invalid type\n*/"
    },
    "org.apache.hadoop.util.hash.MurmurHash": {
        "org.apache.hadoop.util.hash.MurmurHash:getInstance()": "/**\n* Retrieves the singleton instance of Hash.\n* @return the single instance of Hash\n*/",
        "org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int,int)": "/**\n* Computes a hash value from byte array data.\n* @param data byte array to hash\n* @param offset starting index in data\n* @param length number of bytes to hash\n* @param seed initial hash seed\n* @return computed hash value as an integer\n*/",
        "org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)": "/**\n* Computes a hash value for the given byte array.\n* @param data byte array to hash\n* @param length number of bytes to hash\n* @param seed initial hash seed\n* @return computed hash value as an integer\n*/"
    },
    "org.apache.hadoop.util.bloom.Filter": {
        "org.apache.hadoop.util.bloom.Filter:<init>()": "/**\n* Default constructor for the Filter class.\n*/",
        "org.apache.hadoop.util.bloom.Filter:write(java.io.DataOutput)": "/**\n* Writes object data to output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.bloom.Filter:add(java.util.List)": "/**\n* Adds a list of keys to the collection.\n* @param keys list of keys to be added; must not be null\n*/",
        "org.apache.hadoop.util.bloom.Filter:add(java.util.Collection)": "/**\n* Adds a collection of keys; throws exception if collection is null.\n* @param keys collection of keys to add\n*/",
        "org.apache.hadoop.util.bloom.Filter:add(org.apache.hadoop.util.bloom.Key[])": "/**\n* Adds an array of keys to the collection.\n* @param keys array of keys to be added; must not be null\n*/",
        "org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)": "/**\n* Initializes a Filter with specified vector size and hash parameters.\n* @param vectorSize size of the filter vector\n* @param nbHash number of hash functions to use\n* @param hashType type identifier for the hash function\n*/",
        "org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and initializes hash function parameters.\n* @param in DataInput stream to read from\n* @throws IOException if version is unsupported or read fails\n*/"
    },
    "org.apache.hadoop.util.bloom.CountingBloomFilter": {
        "org.apache.hadoop.util.bloom.CountingBloomFilter:buckets2words(int)": "/**\n* Calculates the number of words needed for the given vector size.\n* @param vectorSize the size of the vector\n* @return number of words required\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:not()": "/**\n* Throws UnsupportedOperationException for unsupported 'not' operation.\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:xor(org.apache.hadoop.util.bloom.Filter)": "/**\n* Throws UnsupportedOperationException for xor() method.\n* @param filter the filter to apply (not used)\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:toString()": "/**\n* Converts the object to a string representation of its bucket values.\n* @return String of bucket values separated by spaces\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:<init>()": "/**\n* Initializes a new instance of the CountingBloomFilter class.\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs bitwise AND operation on two CountingBloomFilters.\n* @param filter the CountingBloomFilter to AND with this filter\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs bitwise OR operation on CountingBloomFilter if valid.\n* @param filter the CountingBloomFilter to combine with this filter\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput)": "/**\n* Writes object data to output stream and serializes bucket values.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key)": "/**\n* Adds a key after validating and computing its hash values.\n* @param key the Key object to add\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)": "/**\n* Tests membership of a key in the collection.\n* @param key the Key object to test for membership\n* @return true if the key is present, false otherwise\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key)": "/**\n* Estimates count using hashing; returns minimum bucket value or 0 if none found.\n* @param key the Key object for hash computation\n* @return estimated count or 0 if no valid count exists\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key)": "/**\n* Deletes the specified key from the collection.\n* @param key the Key object to delete\n* @throws NullPointerException if key is null\n* @throws IllegalArgumentException if key is not a member\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)": "/**\n* Initializes a CountingBloomFilter with specified parameters.\n* @param vectorSize size of the filter vector\n* @param nbHash number of hash functions to use\n* @param hashType type identifier for the hash function\n*/",
        "org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and initializes bucket array.\n* @param in DataInput stream to read from\n* @throws IOException if read fails\n*/"
    },
    "org.apache.hadoop.util.bloom.HashFunction": {
        "org.apache.hadoop.util.bloom.HashFunction:clear()": "/**\n* Clears all data from the current context or collection.\n*/",
        "org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key)": "/**\n* Generates an array of hash values from a given key.\n* @param k the Key object to hash\n* @return array of computed hash values\n*/",
        "org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)": "/**\n* Constructs a HashFunction with specified parameters.\n* @param maxValue upper limit for hash values\n* @param nbHash number of hash functions to use\n* @param hashType type identifier for the hash function\n*/"
    },
    "org.apache.hadoop.util.bloom.BloomFilter": {
        "org.apache.hadoop.util.bloom.BloomFilter:and(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs logical AND operation with another BloomFilter.\n* @param filter the BloomFilter to AND with; must match size and hash count\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:not()": "/**\n* Flips all bits in the specified range of the bit vector.\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:or(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs logical OR operation with another BloomFilter.\n* @param filter BloomFilter to combine; must match size and hash count\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:xor(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs XOR operation on the current BloomFilter with another.\n* @param filter the BloomFilter to XOR with; must match size and hash count\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:toString()": "/**\n* Returns a string representation of the bits object.\n* @return string representation of bits\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:getNBytes()": "/**\n* Calculates the number of bytes needed for the vector size.\n* @return number of bytes as an integer\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:getVectorSize()": "/**\n* Retrieves the size of the vector.\n* @return the current size of the vector as an integer\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:<init>()": "/**\n* Initializes a new instance of the BloomFilter class.\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput)": "/**\n* Writes bit vector data to output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key)": "/**\n* Adds a key by generating and storing its hash values.\n* @param key the Key object to add, must not be null\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)": "/**\n* Tests membership of a key in a set.\n* @param key the Key object to test\n* @return true if key is present, false otherwise\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)": "/**\n* Constructs a BloomFilter with specified size and hash parameters.\n* @param vectorSize size of the filter vector\n* @param nbHash number of hash functions\n* @param hashType type identifier for the hash function\n*/",
        "org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and initializes BitSet based on vector size.\n* @param in DataInput stream to read from\n* @throws IOException if read fails\n*/"
    },
    "org.apache.hadoop.util.bloom.DynamicBloomFilter": {
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:getActiveStandardBF()": "/**\n* Retrieves the active standard Bloom filter if record limit not reached.\n* @return BloomFilter or null if limit exceeded\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:toString()": "/**\n* Returns a string representation of the matrix.\n* @return formatted string of matrix rows\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>()": "/**\n* Initializes a new instance of the DynamicBloomFilter class.\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs logical AND operation with another DynamicBloomFilter.\n* @param filter the BloomFilter to AND with; must match size and hash count\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:not()": "/**\n* Flips all bits in each row of the matrix.\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter)": "/**\n* Combines the current BloomFilter with another using logical OR.\n* @param filter BloomFilter to combine; must match size and hash count\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter)": "/**\n* Performs XOR operation on the current BloomFilter with another.\n* @param filter the BloomFilter to XOR with; must match size and hash count\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput)": "/**\n* Writes object data to output stream.\n* @param out DataOutput stream to write to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)": "/**\n* Tests if a key is present in the membership matrix.\n* @param key the Key object to test\n* @return true if key is present, false otherwise\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)": "/**\n* Initializes DynamicBloomFilter with size, hash functions, and record count.\n* @param vectorSize size of the filter vector\n* @param nbHash number of hash functions\n* @param hashType type identifier for the hash function\n* @param nr initial number of records\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow()": "/**\n* Adds a new row to the Bloom filter matrix.\n* Creates a new BloomFilter instance and updates the matrix.\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and initializes matrix of BloomFilters.\n* @param in DataInput stream to read from\n* @throws IOException if read fails\n*/",
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key)": "/**\n* Adds a key to the Bloom filter; creates a new row if limit is reached.\n* @param key the Key object to add, must not be null\n*/"
    },
    "org.apache.hadoop.util.bloom.RetouchedBloomFilter": {
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:createVector()": "/**\n* Initializes vectors for storing keys and ratios.\n* Allocates synchronized lists and zeroes the ratio array.\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:randomRemove()": "/**\n* Generates a random integer between 0 and nbHash-1.\n* @return a random integer\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List)": "/**\n* Calculates total weight from a list of keys.\n* @param keyList list of keys to compute weight from\n* @return total weight as a double value\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>()": "/**\n* Initializes a new instance of the RetouchedBloomFilter class.\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput)": "/**\n* Writes object data to DataOutput stream, including lists and ratios.\n* @param out the DataOutput stream to write data to\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key)": "/**\n* Adds a key after hashing and clearing previous data.\n* @param key the Key object to add; must not be null\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key)": "/**\n* Adds a false positive key to the fpVector.\n* @param key the Key object to be added as a false positive\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])": "/**\n* Removes a key from the specified hash table structure.\n* @param k the Key to remove\n* @param vector the array of Key lists to modify\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[])": "/**\n* Finds index of minimum weight key from hash array.\n* @param h array of hash indices\n* @return index of key with minimum weight\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[])": "/**\n* Finds index of maximum weight from hash keys.\n* @param h array of hash keys\n* @return index of the key with maximum weight\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio()": "/**\n* Computes ratio of weights for key and false positive vectors.\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection)": "/**\n* Adds false positive keys from a collection.\n* @param coll collection of Key objects to add as false positives\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List)": "/**\n* Adds false positive keys from the provided list.\n* @param keys list of Key objects to be added as false positives\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[])": "/**\n* Adds multiple false positive keys to the fpVector.\n* @param keys array of Key objects to be added as false positives\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int)": "/**\n* Clears the bit and associated keys at the specified index.\n* @param index position to clear in key and false positive vectors\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[])": "/**\n* Finds index of minimum ratio in given hash array.\n* @param h array of indices to evaluate\n* @return index of minimum ratio or Integer.MAX_VALUE if none found\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)": "/**\n* Performs selective clearing based on the provided scheme and key.\n* @param k the Key object to clear\n* @param scheme the clearing strategy to use\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)": "/**\n* Constructs a RetouchedBloomFilter with specified vector and hash parameters.\n* @param vectorSize size of the filter vector\n* @param nbHash number of hash functions\n* @param hashType type identifier for the hash function\n*/",
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput)": "/**\n* Reads fields from DataInput to populate vectors and ratios.\n* @param in DataInput stream to read from\n* @throws IOException if an I/O error occurs during reading\n*/"
    },
    "org.apache.hadoop.util.Sets": {
        "org.apache.hadoop.util.Sets:intersection(java.util.Set,java.util.Set)": "/**\n* Computes the intersection of two sets.\n* @param set1 first set\n* @param set2 second set\n* @return unmodifiable set containing common elements\n*/",
        "org.apache.hadoop.util.Sets:<init>()": "/**\n* Constructs an empty Sets instance.\n*/",
        "org.apache.hadoop.util.Sets:newHashSet()": "/**\n* Creates and returns a new empty HashSet.\n* @return a new instance of HashSet\n*/",
        "org.apache.hadoop.util.Sets:newTreeSet()": "/**\n* Creates a new empty TreeSet.\n* @return a new instance of TreeSet<E> \n*/",
        "org.apache.hadoop.util.Sets:cast(java.lang.Iterable)": "/**\n* Casts an Iterable to a Collection.\n* @param iterable the Iterable to be cast\n* @return the casted Collection\n*/",
        "org.apache.hadoop.util.Sets:addAll(java.util.Collection,java.util.Iterator)": "/**\n* Adds all elements from the iterator to the collection.\n* @param addTo collection to add elements to\n* @param iterator source of elements to add\n* @return true if collection was modified, false otherwise\n*/",
        "org.apache.hadoop.util.Sets:capacity(int)": "/**\n* Calculates the capacity based on expected size for efficient storage.\n* @param expectedSize the anticipated number of elements\n* @return adjusted capacity value\n*/",
        "org.apache.hadoop.util.Sets:union(java.util.Set,java.util.Set)": "/**\n* Returns the union of two sets.\n* @param set1 first input set\n* @param set2 second input set\n* @return unmodifiable set containing elements from both sets\n*/",
        "org.apache.hadoop.util.Sets:difference(java.util.Set,java.util.Set)": "/**\n* Computes the difference between two sets.\n* @param set1 first set\n* @param set2 second set\n* @return a set of elements in set1 not in set2\n*/",
        "org.apache.hadoop.util.Sets:differenceInTreeSets(java.util.Set,java.util.Set)": "/**\n* Computes the difference between two sets.\n* @param set1 first input set\n* @param set2 second input set\n* @return unmodifiable set of elements in set1 not in set2\n*/",
        "org.apache.hadoop.util.Sets:symmetricDifference(java.util.Set,java.util.Set)": "/**\n* Computes the symmetric difference of two sets.\n* @param set1 first input set\n* @param set2 second input set\n* @return unmodifiable set of elements in either set but not both\n*/",
        "org.apache.hadoop.util.Sets:newConcurrentHashSet()": "/**\n* Creates a new thread-safe Set using ConcurrentHashMap.\n* @return a concurrent Set instance\n*/",
        "org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)": "/**\n* Adds elements to a TreeSet from an Iterable or Collection.\n* @param addTo TreeSet to add elements to\n* @param elementsToAdd source of elements, can be Iterable or Collection\n* @return true if TreeSet was modified\n*/",
        "org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator)": "/**\n* Creates a HashSet from an iterator of elements.\n* @param elements iterator of elements to add\n* @return a new HashSet containing the elements\n*/",
        "org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int)": "/**\n* Creates a HashSet with a specified initial capacity.\n* @param expectedSize anticipated number of elements\n* @return a new HashSet with adjusted capacity\n*/",
        "org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable)": "/**\n* Creates a TreeSet from the provided elements.\n* @param elements source of elements to add\n* @return a new TreeSet containing the added elements\n*/",
        "org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable)": "/**\n* Creates a HashSet from an Iterable of elements.\n* @param elements Iterable source for HashSet\n* @return HashSet containing the elements\n*/",
        "org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[])": "/**\n* Creates a HashSet from provided elements.\n* @param elements variable number of elements to add\n* @return a HashSet containing the specified elements\n*/"
    },
    "org.apache.hadoop.util.LimitInputStream": {
        "org.apache.hadoop.util.LimitInputStream:available()": "/**\n* Returns the number of bytes available for reading.\n* @return number of available bytes, limited by 'left'\n*/",
        "org.apache.hadoop.util.LimitInputStream:mark(int)": "/**\n* Marks the current position in the input stream with a specified read limit.\n* @param readLimit maximum number of bytes to read before the mark becomes invalid\n*/",
        "org.apache.hadoop.util.LimitInputStream:read()": "/**\n* Reads a byte from the input stream, decrementing the remaining count.\n* @return byte value or -1 if end of stream is reached\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.util.LimitInputStream:read(byte[],int,int)": "/**\n* Reads bytes into an array from the input stream.\n* @param b byte array to store read data\n* @param off offset in the array to start storing data\n* @param len maximum number of bytes to read\n* @return number of bytes read, or -1 if end of stream is reached\n*/",
        "org.apache.hadoop.util.LimitInputStream:reset()": "/**\n* Resets the input stream to the last marked position.\n* @throws IOException if mark is not supported or not set\n*/",
        "org.apache.hadoop.util.LimitInputStream:skip(long)": "/**\n* Skips n bytes in the input stream and updates remaining bytes.\n* @param n number of bytes to skip\n* @return actual bytes skipped\n*/",
        "org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)": "/**\n* Initializes LimitInputStream with input stream and byte limit.\n* @param in input stream to read from\n* @param limit maximum bytes to read, must be non-negative\n*/"
    },
    "org.apache.hadoop.util.AutoCloseableLock": {
        "org.apache.hadoop.util.AutoCloseableLock:<init>(java.util.concurrent.locks.Lock)": "/**\n* Initializes AutoCloseableLock with a specified Lock.\n* @param lock the Lock to be managed by this instance\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:acquire()": "/**\n* Acquires the lock and returns an AutoCloseableLock instance.\n* @return AutoCloseableLock for managing lock release\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:release()": "/**\n* Releases the acquired lock.\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:tryLock()": "/**\n* Attempts to acquire the lock.\n* @return true if the lock was acquired, false otherwise\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:isLocked()": "/**\n* Checks if the lock is currently held.\n* @return true if locked, false if not; throws exception for unsupported locks\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:newCondition()": "/**\n* Creates a new Condition instance for the lock.\n* @return a new Condition object associated with the lock\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:<init>()": "/**** \n* Constructs AutoCloseableLock with a default ReentrantLock.\n*/",
        "org.apache.hadoop.util.AutoCloseableLock:close()": "/**\n* Closes the resource and releases the acquired lock.\n*/"
    },
    "org.apache.hadoop.util.FindClass": {
        "org.apache.hadoop.util.FindClass:err(java.lang.String,java.lang.Object[])": "/**\n* Formats and prints an error message to standard error output.\n* @param s format string for the error message\n* @param args arguments for the format string\n*/",
        "org.apache.hadoop.util.FindClass:out(java.lang.String,java.lang.Object[])": "/**\n* Formats and prints a message to standard output.\n* @param s format string with placeholders\n* @param args values to replace placeholders in the format string\n*/",
        "org.apache.hadoop.util.FindClass:getResource(java.lang.String)": "/**\n* Retrieves a resource URL by its name.\n* @param name the name of the resource to fetch\n* @return URL of the resource or null if not found\n*/",
        "org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])": "/**\n* Logs an error message and prints the stack trace of a Throwable.\n* @param e the Throwable to print the stack trace for\n* @param text format string for the error message\n* @param args arguments for the format string\n*/",
        "org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)": "/**\n* Logs an error message with code and text.\n* @param errorcode numeric error identifier\n* @param text descriptive error message\n*/",
        "org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)": "/**\n* Logs class loading information including name and URL.\n* @param name class name\n* @param clazz class object\n*/",
        "org.apache.hadoop.util.FindClass:loadResource(java.lang.String)": "/**\n* Loads a resource by name and logs its URL or an error.\n* @param name the name of the resource to load\n* @return SUCCESS or E_NOT_FOUND if resource is missing\n*/",
        "org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FindClass with the given Hadoop Configuration.\n* @param conf the Configuration for this FindClass instance\n*/",
        "org.apache.hadoop.util.FindClass:getClass(java.lang.String)": "/**\n* Retrieves Class object by name.\n* @param name fully qualified class name\n* @return Class object\n* @throws ClassNotFoundException if class is not found\n*/",
        "org.apache.hadoop.util.FindClass:dumpResource(java.lang.String)": "/**\n* Loads and prints a resource by name.\n* @param name the name of the resource to load\n* @return status code indicating success or error\n*/",
        "org.apache.hadoop.util.FindClass:usage(java.lang.String[])": "/**\n* Displays usage instructions and error codes for operations.\n* @param args command-line arguments\n* @return error code indicating usage message displayed\n*/",
        "org.apache.hadoop.util.FindClass:loadClass(java.lang.String)": "/**\n* Loads a class by name, returns success or error codes.\n* @param name fully qualified class name\n* @return status code indicating success or error\n*/",
        "org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String)": "/**\n* Creates an instance of a class by name.\n* @param name fully qualified class name\n* @return success or error code based on outcome\n*/",
        "org.apache.hadoop.util.FindClass:<init>()": "/**\n* Constructs a FindClass instance using a default Configuration.\n*/",
        "org.apache.hadoop.util.FindClass:run(java.lang.String[])": "/**** Executes an action based on command-line arguments. \n* @param args command-line arguments with action and resource name \n* @return status code indicating success or error \n*/",
        "org.apache.hadoop.util.FindClass:main(java.lang.String[])": "/**\n* Executes FindClass tool with given arguments and handles exceptions.\n* @param args command-line arguments for the tool\n*/"
    },
    "org.apache.hadoop.util.FastNumberFormat": {
        "org.apache.hadoop.util.FastNumberFormat:format(java.lang.StringBuilder,long,int)": "/**\n* Formats a long value into a StringBuilder with leading zeros.\n* @param sb StringBuilder to append formatted value\n* @param value long number to format\n* @param minimumDigits minimum number of digits to display\n* @return updated StringBuilder with formatted value\n*/"
    },
    "org.apache.hadoop.util.FileBasedIPList": {
        "org.apache.hadoop.util.FileBasedIPList:readLines(java.lang.String)": "/**\n* Reads lines from a specified file.\n* @param fileName the name of the file to read\n* @return an array of lines or null if the file doesn't exist\n*/",
        "org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String)": "/**\n* Checks if the given IP address is in the allowed address list.\n* @param ipAddress the IP address to check\n* @return true if included, false if null or not found\n*/",
        "org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String)": "/**\n* Initializes FileBasedIPList from a file's lines.\n* @param fileName name of the file to read IP addresses from\n*/",
        "org.apache.hadoop.util.FileBasedIPList:reload()": "/**\n* Reloads IP list from the specified file.\n* @return a new FileBasedIPList instance\n*/"
    },
    "org.apache.hadoop.util.GcTimeMonitor$TsAndData": {
        "org.apache.hadoop.util.GcTimeMonitor$TsAndData:setValues(long,long)": "/**\n* Sets timestamp and garbage collection pause values.\n* @param inTs timestamp value to set\n* @param inGcPause garbage collection pause duration\n*/"
    },
    "org.apache.hadoop.util.PureJavaCrc32": {
        "org.apache.hadoop.util.PureJavaCrc32:reset()": "/**\n* Resets the CRC value to its initial state (0xffffffff).\n*/",
        "org.apache.hadoop.util.PureJavaCrc32:<init>()": "/**\n* Initializes a new PureJavaCrc32 instance and resets the CRC value.\n*/"
    },
    "org.apache.hadoop.util.PriorityQueue": {
        "org.apache.hadoop.util.PriorityQueue:upHeap()": "/**\n* Restores heap property by moving the last node up the heap.\n*/",
        "org.apache.hadoop.util.PriorityQueue:top()": "/**\n* Retrieves the top element of the heap.\n* @return top element or null if heap is empty\n*/",
        "org.apache.hadoop.util.PriorityQueue:downHeap()": "/**\n* Maintains heap property by down-heapifying from the root.\n*/",
        "org.apache.hadoop.util.PriorityQueue:clear()": "/**\n* Clears the heap and resets its size to zero.\n*/",
        "org.apache.hadoop.util.PriorityQueue:initialize(int)": "/**\n* Initializes a heap with a specified maximum size.\n* @param maxSize the maximum number of elements in the heap\n*/",
        "org.apache.hadoop.util.PriorityQueue:size()": "/**\n* Returns the current size of the collection.\n* @return the number of elements in the collection\n*/",
        "org.apache.hadoop.util.PriorityQueue:put(java.lang.Object)": "/**\n* Inserts an element into the heap and restores heap property.\n* @param element the element to be added to the heap\n*/",
        "org.apache.hadoop.util.PriorityQueue:pop()": "/**\n* Removes and returns the top element from the heap.\n* @return top element or null if heap is empty\n*/",
        "org.apache.hadoop.util.PriorityQueue:adjustTop()": "/**\n* Adjusts the heap by maintaining the heap property from the top.\n*/",
        "org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object)": "/**\n* Inserts an element into the heap or replaces the top if larger.\n* @param element the element to insert\n* @return true if inserted, false otherwise\n*/"
    },
    "org.apache.hadoop.util.AsyncDiskService": {
        "org.apache.hadoop.util.AsyncDiskService:<init>(java.lang.String[])": "/**\n* Initializes AsyncDiskService with thread pools for each specified volume.\n* @param volumes array of volume identifiers\n*/",
        "org.apache.hadoop.util.AsyncDiskService:execute(java.lang.String,java.lang.Runnable)": "/**\n* Executes a task on the specified root's thread pool.\n* @param root identifier for the thread pool\n* @param task Runnable task to be executed\n*/",
        "org.apache.hadoop.util.AsyncDiskService:shutdown()": "/**\n* Shuts down all AsyncDiskService threads gracefully.\n*/",
        "org.apache.hadoop.util.AsyncDiskService:shutdownNow()": "/**\n* Immediately shuts down all AsyncDiskService threads.\n* @return list of Runnable tasks that were not executed\n*/",
        "org.apache.hadoop.util.AsyncDiskService:awaitTermination(long)": "/**\n* Awaits termination of thread pool executors for a specified duration.\n* @param milliseconds max wait time in milliseconds\n* @return true if all executors terminated, false if timed out\n*/"
    },
    "org.apache.hadoop.util.SequentialNumber": {
        "org.apache.hadoop.util.SequentialNumber:<init>(long)": "/**\n* Initializes SequentialNumber with a starting value.\n* @param initialValue the initial number for the sequence\n*/",
        "org.apache.hadoop.util.SequentialNumber:getCurrentValue()": "/**\n* Retrieves the current value.\n* @return the current long value\n*/",
        "org.apache.hadoop.util.SequentialNumber:setCurrentValue(long)": "/**\n* Sets the current value to the specified long value.\n* @param value the new current value\n*/",
        "org.apache.hadoop.util.SequentialNumber:setIfGreater(long)": "/**\n* Sets currentValue to value if value is greater.\n* @param value the value to compare and potentially set\n* @return true if set successfully, false if not\n*/",
        "org.apache.hadoop.util.SequentialNumber:nextValue()": "/**\n* Increments and returns the next value.\n* @return the incremented long value\n*/",
        "org.apache.hadoop.util.SequentialNumber:equals(java.lang.Object)": "/**\n* Compares this object to another for equality.\n* @param that object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.util.SequentialNumber:hashCode()": "/**\n* Computes the hash code based on the current value.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.util.SequentialNumber:skipTo(long)": "/**\n* Skips to a new value if it's greater than or equal to the current value.\n* @param newValue the target value to skip to\n* @throws IllegalStateException if newValue is less than the current value\n*/"
    },
    "org.apache.hadoop.util.ProgramDriver": {
        "org.apache.hadoop.util.ProgramDriver:<init>()": "/**\n* Initializes a ProgramDriver with an empty TreeMap of program descriptions.\n*/",
        "org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)": "/**\n* Adds a program description with a name, main class, and description.\n* @param name program name\n* @param mainClass class containing the main method\n* @param description brief description of the program\n*/",
        "org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map)": "/**\n* Prints valid program names and their descriptions.\n* @param programs map of program names to their descriptions\n*/",
        "org.apache.hadoop.util.ProgramDriver:run(java.lang.String[])": "/**\n* Runs a program based on provided arguments.\n* @param args command-line arguments including program name\n* @return 0 on success, -1 if program name is invalid\n*/",
        "org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[])": "/**\n* Executes a program with given arguments; exits on invalid program name.\n* @param argv command-line arguments for the program\n*/"
    },
    "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider": {
        "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider:get(java.io.File)": "/**\n* Returns a FileOutputStream for the specified file.\n* @param f the file to write to\n* @return FileOutputStream for the given file\n* @throws FileNotFoundException if the file does not exist\n*/",
        "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider:write(java.io.FileOutputStream,byte[])": "/**\n* Writes byte array data to the specified FileOutputStream.\n* @param fos output stream to write data to\n* @param data byte array containing data to write\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB)": "/**\n* Initializes the translator with the given RPC proxy.\n* @param rpcProxy proxy for user mappings protocol\n*/",
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy connection.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String)": "/**\n* Retrieves groups associated with a specified user.\n* @param user the username to fetch groups for\n* @return array of group names for the user\n* @throws IOException if an IPC call fails\n*/",
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)": "/**\n* Checks if a method is supported for the given RPC protocol.\n* @param methodName name of the method to check\n* @return true if supported, false otherwise\n*/"
    },
    "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.tools.GetUserMappingsProtocol)": "/**\n* Initializes the translator with the given user mappings protocol implementation.\n* @param impl the user mappings protocol implementation\n*/",
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB:getGroupsForUser(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto)": "/**\n* Retrieves groups associated with a user.\n* @param controller RPC controller for request management\n* @param request contains user identifier\n* @return GetGroupsForUserResponseProto with user groups\n*/"
    },
    "org.apache.hadoop.tools.CommandShell": {
        "org.apache.hadoop.tools.CommandShell:printShellUsage()": "/**\n* Prints usage information for the subcommand or the command.\n*/",
        "org.apache.hadoop.tools.CommandShell:printException(java.lang.Exception)": "/**\n* Prints the stack trace of the given exception to the error stream.\n* @param ex the exception to be printed\n*/",
        "org.apache.hadoop.tools.CommandShell:getErr()": "/**\n* Returns the PrintStream for error output.\n* @return PrintStream for error messages\n*/",
        "org.apache.hadoop.tools.CommandShell:getOut()": "/**\n* Returns the current PrintStream output.\n* @return PrintStream object for output operations\n*/",
        "org.apache.hadoop.tools.CommandShell:setErr(java.io.PrintStream)": "/**\n* Sets the error output stream.\n* @param p PrintStream to be used for error output\n*/",
        "org.apache.hadoop.tools.CommandShell:setOut(java.io.PrintStream)": "/**\n* Sets the output stream for the class.\n* @param p the PrintStream to be set as output\n*/",
        "org.apache.hadoop.tools.CommandShell:setSubCommand(org.apache.hadoop.tools.CommandShell$SubCommand)": "/**\n* Sets the subcommand for processing.\n* @param cmd the SubCommand to be set\n*/",
        "org.apache.hadoop.tools.CommandShell:run(java.lang.String[])": "/**\n* Executes a command with arguments and handles exceptions.\n* @param args command-line arguments\n* @return exit code indicating success or failure\n*/"
    },
    "org.apache.hadoop.tools.CommandShell$SubCommand": {
        "org.apache.hadoop.tools.CommandShell$SubCommand:validate()": "/**\n* Validates the current object state.\n* @return true if valid, false otherwise\n*/"
    },
    "org.apache.hadoop.tools.TableListing$Column": {
        "org.apache.hadoop.tools.TableListing$Column:addRow(java.lang.String)": "/**\n* Adds a row with the specified value, updating maxWidth if necessary.\n* @param val the string value to add; defaults to empty if null\n*/",
        "org.apache.hadoop.tools.TableListing$Column:setWrapWidth(int)": "/**\n* Sets the wrap width and adjusts maxWidth accordingly.\n* @param width new wrap width value\n*/",
        "org.apache.hadoop.tools.TableListing$Column:getMaxWidth()": "/**\n* Retrieves the maximum width value.\n* @return the maximum width as an integer\n*/",
        "org.apache.hadoop.tools.TableListing$Column:getRow(int)": "/**\n* Retrieves and formats a row from the dataset.\n* @param idx index of the row to retrieve\n* @return formatted string array of the row's lines\n*/",
        "org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)": "/**\n* Initializes a Column with title, justification, and wrap settings.\n* @param title column title; used as the first row\n* @param justification alignment of the column content\n* @param wrap determines if content should wrap\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$7": {
        "org.apache.hadoop.conf.StorageUnit$7:toKBs(double)": "/**\n* Converts a value in bytes to kilobytes.\n* @param value the value in bytes\n* @return the equivalent value in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:toMBs(double)": "/**\n* Converts a value in bytes to megabytes.\n* @param value size in bytes\n* @return equivalent size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value size in bytes\n* @return equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:toTBs(double)": "/**\n* Converts a value to terabits.\n* @param value input value in bits\n* @return equivalent value in terabits\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value the value in bytes\n* @return equivalent value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:toEBs(double)": "/**\n* Converts a given value to exbibytes.\n* @param value the value to convert\n* @return the equivalent value in exbibytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:toBytes(double)": "/**\n* Converts a value to its equivalent in bytes.\n* @param value the value to convert\n* @return the converted value in bytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:fromBytes(double)": "/**\n* Converts a byte value to a double representation.\n* @param value byte value to convert\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the entity\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:getShortName()": "/**\n* Retrieves the short name representation of an object.\n* @return short name as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:getSuffixChar()": "/**\n* Retrieves the suffix character as a String.\n* @return suffix character as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$7:getDefault(double)": "/**\n* Retrieves the default value based on the input.\n* @param value the input value to determine the default\n* @return the default double value\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$3": {
        "org.apache.hadoop.conf.StorageUnit$3:toBytes(double)": "/**\n* Converts a given value to bytes.\n* @param value the value to convert\n* @return the equivalent byte representation\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:toKBs(double)": "/**\n* Converts a value in bytes to kilobytes.\n* @param value the value in bytes\n* @return the equivalent value in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:toMBs(double)": "/**\n* Converts a value in bytes to megabytes.\n* @param value the size in bytes\n* @return the size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value size in bytes\n* @return equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value the value to convert in bytes\n* @return converted value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:toEBs(double)": "/**\n* Converts a value to exabytes.\n* @param value amount to convert in bytes\n* @return equivalent value in exabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:toTBs(double)": "/**\n* Converts a value to terabits.\n* @param value input value in bits\n* @return equivalent value in terabits\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:fromBytes(double)": "/**\n* Converts a byte representation to a double value.\n* @param value byte representation of a double\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the object\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:getShortName()": "/**\n* Retrieves the short name representation of an object.\n* @return String representing the short name\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:getSuffixChar()": "/**\n* Retrieves the suffix character as a string.\n* @return suffix character as a string\n*/",
        "org.apache.hadoop.conf.StorageUnit$3:getDefault(double)": "/**\n* Retrieves the default value based on the input.\n* @param value input value to determine the default\n* @return calculated default value\n*/"
    },
    "org.apache.hadoop.conf.Configuration$DeprecationContext": {
        "org.apache.hadoop.conf.Configuration$DeprecationContext:getDeprecatedKeyMap()": "/**\n* Retrieves the map of deprecated keys and their information.\n* @return Map of deprecated key names to their info objects\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationContext:getReverseDeprecatedKeyMap()": "/**\n* Retrieves the map of deprecated keys and their reversed values.\n* @return Map with deprecated keys as keys and their reversed values as values\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])": "/**\n* Initializes DeprecationContext from another and a list of deltas.\n* @param other existing context to copy from\n* @param deltas array of changes to apply\n*/"
    },
    "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo": {
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getAndSetAccessed()": "/**\n* Sets accessed status to true and returns previous value.\n* @return previous accessed status\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:<init>(java.lang.String[],java.lang.String)": "/**\n* Constructs DeprecatedKeyInfo with new keys and a custom message.\n* @param newKeys array of new keys\n* @param customMessage message related to deprecation\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:clearAccessed()": "/**\n* Resets the accessed state to false.\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String,java.lang.String)": "/**\n* Generates a deprecation warning message based on a key and source.\n* @param key the deprecation key\n* @param source optional source context\n* @return formatted warning message string\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String)": "/**\n* Retrieves a deprecation warning message using a key.\n* @param key the deprecation key\n* @return formatted warning message string\n*/"
    },
    "org.apache.hadoop.conf.Configuration$Resource": {
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String,boolean)": "/**\n* Constructs a Resource with specified parameters.\n* @param resource the resource object\n* @param name the name of the resource\n* @param restrictParser flag to restrict parser usage\n*/",
        "org.apache.hadoop.conf.Configuration$Resource:isParserRestricted()": "/**\n* Checks if the parser is restricted.\n* @return true if restricted, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration$Resource:getResource()": "/**\n* Retrieves the resource object.\n* @return the resource object\n*/",
        "org.apache.hadoop.conf.Configuration$Resource:getName()": "/**\n* Returns the name of the object.\n* @return the name as a String\n*/",
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)": "/**\n* Constructs a Resource with resource object and its string representation.\n* @param resource the resource object\n* @param useRestrictedParser flag to restrict parser usage\n*/",
        "org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object)": "/**\n* Determines if parser restrictions are enabled based on user information.\n* @param resource the resource to check\n* @return true if restrictions apply, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)": "/**** Constructs a Resource with specified object and name, using parser restriction flag. */",
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object)": "/**\n* Constructs a Resource from an object using its string representation.\n* @param resource the object to create a Resource from\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration:unitFor(java.util.concurrent.TimeUnit)": "/**\n* Retrieves ParsedTimeDuration for the specified TimeUnit.\n* @param unit the TimeUnit to find the corresponding ParsedTimeDuration\n* @return ParsedTimeDuration or null if not found\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration:unitFor(java.lang.String)": "/**\n* Returns ParsedTimeDuration matching the suffix of the input string.\n* @param s input string to check for duration suffix\n* @return ParsedTimeDuration or null if no match found\n*/"
    },
    "org.apache.hadoop.conf.StorageSize": {
        "org.apache.hadoop.conf.StorageSize:getValue()": "/**\n* Retrieves the current value.\n* @return the current value as a double\n*/",
        "org.apache.hadoop.conf.StorageSize:getUnit()": "/**\n* Retrieves the StorageUnit instance.\n* @return StorageUnit object associated with this instance\n*/",
        "org.apache.hadoop.conf.StorageSize:<init>(org.apache.hadoop.conf.StorageUnit,double)": "/**\n* Constructs a StorageSize with specified unit and value.\n* @param unit the storage unit type\n* @param value the size in the given unit\n*/",
        "org.apache.hadoop.conf.StorageSize:checkState(boolean,java.lang.String)": "/**\n* Validates state and throws an exception if false.\n* @param state condition to check\n* @param errorString message for exception if state is false\n*/",
        "org.apache.hadoop.conf.StorageSize:parse(java.lang.String)": "/**\n* Parses a size string into a StorageSize object.\n* @param value size string in the format <number><unit>\n* @return StorageSize corresponding to the parsed unit and value\n*/"
    },
    "org.apache.hadoop.conf.Configuration$Parser": {
        "org.apache.hadoop.conf.Configuration$Parser:parse()": "/**\n* Parses items from the input stream and returns a list of parsed items.\n* @return List of ParsedItem objects\n* @throws IOException if an I/O error occurs\n* @throws XMLStreamException if a parsing error occurs\n*/",
        "org.apache.hadoop.conf.Configuration$Parser:handleStartProperty()": "/**\n* Initializes configuration properties from XML attributes.\n*/",
        "org.apache.hadoop.conf.Configuration$Parser:handleInclude()": "/**\n* Handles xi:include directive, fetching and parsing included resources.\n* @throws XMLStreamException if XML parsing fails\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.conf.Configuration$Parser:handleEndProperty()": "/**\n* Handles end of property by processing tags and updating results.\n* @param confName configuration name, confValue value, confSourceArray sources\n*/",
        "org.apache.hadoop.conf.Configuration$Parser:handleStartElement()": "/**\n* Handles XML start elements, directing processing based on the element's name.\n* @throws XMLStreamException if XML parsing fails\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.conf.Configuration$Parser:handleEndElement()": "/**\n* Processes XML end elements and updates configuration properties.\n* @throws IOException if fallback is not allowed during include processing\n*/",
        "org.apache.hadoop.conf.Configuration$Parser:parseNext()": "/**\n* Parses the next XML element and processes it accordingly.\n* @throws IOException if an I/O error occurs\n* @throws XMLStreamException if XML parsing fails\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$6": {
        "org.apache.hadoop.conf.StorageUnit$6:toBytes(double)": "/**\n* Converts a given value to bytes.\n* @param value the value to convert\n* @return the equivalent byte representation\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:toMBs(double)": "/**\n* Converts a value in bytes to megabytes.\n* @param value the size in bytes\n* @return the size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value size in bytes\n* @return equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:toTBs(double)": "/**\n* Converts a value to terabits.\n* @param value input value in bits\n* @return equivalent value in terabits\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value the value in bytes to convert\n* @return equivalent value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:toEBs(double)": "/**\n* Converts a value to exabytes.\n* @param value the value in bytes to convert\n* @return the equivalent value in exabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:toKBs(double)": "/**\n* Converts a value to kilobytes.\n* @param value size in bytes\n* @return equivalent size in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:fromBytes(double)": "/**\n* Converts a byte representation to a double value.\n* @param value byte representation of a double\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the entity\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:getShortName()": "/**\n* Retrieves the short name representation of an object.\n* @return short name as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:getSuffixChar()": "/**\n* Retrieves the suffix character as a string.\n* @return suffix character string\n*/",
        "org.apache.hadoop.conf.StorageUnit$6:getDefault(double)": "/**\n* Retrieves the default value based on the provided input.\n* @param value input value to determine the default\n* @return computed default value as a double\n*/"
    },
    "org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange": {
        "org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Initializes a PropertyChange with property name and its old and new values.\n* @param prop property name\n* @param newVal new value of the property\n* @param oldVal old value of the property\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$2": {
        "org.apache.hadoop.conf.StorageUnit$2:toBytes(double)": "/**\n* Converts a value to its byte representation.\n* @param value the value to convert\n* @return the byte equivalent of the value\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:toKBs(double)": "/**\n* Converts a value in bytes to kilobytes.\n* @param value size in bytes\n* @return equivalent size in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:toMBs(double)": "/**\n* Converts a value in bytes to megabytes.\n* @param value size in bytes\n* @return equivalent size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value size in bytes\n* @return equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:toTBs(double)": "/**\n* Converts a value to terabits.\n* @param value the value in bits to convert\n* @return the equivalent value in terabits\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:toEBs(double)": "/**\n* Converts a value to exabytes.\n* @param value size in bytes\n* @return equivalent size in exabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value the value in bytes to convert\n* @return equivalent value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:fromBytes(double)": "/**\n* Converts a byte value to a double representation.\n* @param value byte value to convert\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the object\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:getShortName()": "/**\n* Returns the short name representation of the object.\n* @return short name as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:getSuffixChar()": "/**\n* Retrieves the suffix character as a string.\n* @return suffix character string\n*/",
        "org.apache.hadoop.conf.StorageUnit$2:getDefault(double)": "/**\n* Retrieves the default value based on the input.\n* @param value input value for default calculation\n* @return default value as a double\n*/"
    },
    "org.apache.hadoop.conf.ConfigRedactor": {
        "org.apache.hadoop.conf.ConfigRedactor:configIsSensitive(java.lang.String)": "/**\n* Checks if the given key matches any sensitive configuration patterns.\n* @param key configuration key to check\n* @return true if key is sensitive, false otherwise\n*/",
        "org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)": "/**\n* Redacts sensitive values based on the configuration key.\n* @param key configuration key to check\n* @param value input value to return or redact\n* @return redacted text or original value\n*/",
        "org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)": "/**\n* Redacts XML value if the key is sensitive.\n* @param key configuration key to check\n* @param value original XML value\n* @return REDACTED_XML or original value\n*/",
        "org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ConfigRedactor with compiled regex patterns from configuration.\n* @param conf configuration object containing sensitive keys\n*/"
    },
    "org.apache.hadoop.conf.ReconfigurationServlet": {
        "org.apache.hadoop.conf.ReconfigurationServlet:init()": "/**\n* Initializes the servlet, calling the superclass init method.\n* @throws ServletException if an initialization error occurs\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:getReconfigurable(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves a Reconfigurable object from the servlet context.\n* @param req HTTP request containing the servlet path\n* @return Reconfigurable object or null if not found\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:printHeader(java.io.PrintWriter,java.lang.String)": "/**\n* Prints HTML header with title and main heading for the reconfiguration utility.\n* @param out PrintWriter for outputting HTML content\n* @param nodeName name used in title and heading, escaped for HTML\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:printFooter(java.io.PrintWriter)": "/**\n* Prints the closing HTML tags for the document.\n* @param out PrintWriter to send output to the client\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:getParams(javax.servlet.http.HttpServletRequest)": "/**\n* Retrieves parameter names from the HTTP request.\n* @param req the HttpServletRequest object\n* @return Enumeration of parameter names as Strings\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)": "/**\n* Applies configuration changes from HTTP request and outputs changes to PrintWriter.\n* @param out PrintWriter for outputting change messages\n* @param reconf Reconfigurable object for applying changes\n* @param req HttpServletRequest containing parameters to update\n* @throws ReconfigurationException if reconfiguration fails\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles POST requests for reconfiguration, applying changes and managing errors.\n* @param req HTTP request with parameters for reconfiguration\n* @param resp HTTP response for sending output to the client\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)": "/**\n* Prints configuration changes in an HTML form.\n* @param out PrintWriter to output HTML content\n* @param reconf Reconfigurable object to retrieve configurations\n*/",
        "org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests, outputs HTML with configuration details.\n* @param req HTTP request object\n* @param resp HTTP response object\n*/"
    },
    "org.apache.hadoop.conf.ReconfigurationTaskStatus": {
        "org.apache.hadoop.conf.ReconfigurationTaskStatus:<init>(long,long,java.util.Map)": "/**\n* Constructs a ReconfigurationTaskStatus with specified time and status changes.\n* @param startTime task start time in milliseconds\n* @param endTime task end time in milliseconds\n* @param status map of property changes with optional values\n*/"
    },
    "org.apache.hadoop.conf.ConfServlet": {
        "org.apache.hadoop.conf.ConfServlet:getConfFromContext()": "/**\n* Retrieves the Configuration object from the servlet context.\n* @return Configuration object, never null\n*/",
        "org.apache.hadoop.conf.ConfServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)": "/**\n* Parses the Accept header from the request.\n* @param request the HTTP request object\n* @return JSON format if accepted, otherwise XML format\n*/",
        "org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)": "/**** Writes configuration to output in specified format (JSON/XML). \n* @param conf configuration object \n* @param out Writer for output \n* @param format desired output format \n* @param propertyName optional property to include \n* @throws IOException if an I/O error occurs \n* @throws IllegalArgumentException if format is invalid \n* @throws BadFormatException for unsupported formats \n*/",
        "org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)": "/**\n* Writes configuration to output in specified format.\n* @param conf configuration object\n* @param out Writer for output\n* @param format desired output format\n* @throws IOException if an I/O error occurs\n* @throws BadFormatException for unsupported formats\n*/",
        "org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests, checks access, and writes response in specified format.\n* @param request HTTP request object\n* @param response HTTP response object\n*/"
    },
    "org.apache.hadoop.conf.ConfServlet$BadFormatException": {
        "org.apache.hadoop.conf.ConfServlet$BadFormatException:<init>(java.lang.String)": "/**\n* Constructs a BadFormatException with the specified detail message.\n* @param msg the detail message\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit": {
        "org.apache.hadoop.conf.StorageUnit:divide(double,double)": "/**\n* Divides two double values with specified precision.\n* @param value the dividend\n* @param divisor the divisor\n* @return the result of division rounded to predefined precision\n*/",
        "org.apache.hadoop.conf.StorageUnit:multiply(double,double)": "/**\n* Multiplies two doubles with specified precision.\n* @param first first double to multiply\n* @param second second double to multiply\n* @return product of first and second rounded to precision\n*/",
        "org.apache.hadoop.conf.StorageUnit:toString()": "/**\n* Returns the string representation of the object using its long name.\n* @return String representation of the object\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$4": {
        "org.apache.hadoop.conf.StorageUnit$4:toBytes(double)": "/**\n* Converts a value to its byte representation.\n* @param value the value to convert\n* @return the byte representation of the value\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:toKBs(double)": "/**\n* Converts a value to kilobytes.\n* @param value input value in bytes\n* @return equivalent value in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:toMBs(double)": "/**\n* Converts a value in bytes to megabytes.\n* @param value size in bytes\n* @return equivalent size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:toTBs(double)": "/**\n* Converts a value to terabits.\n* @param value the input value in bits\n* @return equivalent value in terabits\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value input value in bytes\n* @return equivalent value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:toEBs(double)": "/**\n* Converts a value to exabytes.\n* @param value the value in bytes to convert\n* @return the equivalent value in exabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value size in bytes\n* @return equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:fromBytes(double)": "/**\n* Converts a byte representation to a double value.\n* @param value byte value to convert\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the object\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:getShortName()": "/**\n* Retrieves the short name as a String.\n* @return short name representation\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:getSuffixChar()": "/**\n* Retrieves the suffix character as a String.\n* @return suffix character as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$4:getDefault(double)": "/**\n* Retrieves the default value based on the provided input.\n* @param value input value for default calculation\n* @return calculated default value\n*/"
    },
    "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread": {
        "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:<init>(org.apache.hadoop.conf.ReconfigurableBase)": "/**\n* Initializes ReconfigurationThread with a ReconfigurableBase instance.\n* @param base the ReconfigurableBase to associate with this thread\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run()": "/**\n* Executes reconfiguration task, logging changes and updating properties.\n*/"
    },
    "org.apache.hadoop.conf.ReconfigurableBase": {
        "org.apache.hadoop.conf.ReconfigurableBase:shutdownReconfigurationTask()": "/**\n* Stops the reconfiguration task and waits for its completion.\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:isPropertyReconfigurable(java.lang.String)": "/**\n* Checks if a given property can be reconfigured.\n* @param property name of the property to check\n* @return true if reconfigurable, false otherwise\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus()": "/**\n* Retrieves the current reconfiguration task status.\n* @return ReconfigurationTaskStatus object with task timing and status info\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask()": "/**\n* Starts a reconfiguration task if not already running; throws IOException on errors.\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil)": "/**\n* Sets the reconfiguration utility, ensuring it's non-null.\n* @param ru the ReconfigurationUtil instance to set\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:<init>()": "/**\n* Constructs a ReconfigurableBase object with a default Configuration.\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a ReconfigurableBase with specified or default Configuration.\n* @param conf the Configuration object, null defaults to a new instance\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)": "/**\n* Reconfigures a property if reconfigurable; throws exception otherwise.\n* @param property the property name to change\n* @param newVal the new value for the property\n*/",
        "org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves changed properties between two configurations.\n* @param newConf the new configuration to compare\n* @param oldConf the old configuration for comparison\n* @return collection of PropertyChange objects\n*/"
    },
    "org.apache.hadoop.conf.Configuration$DeprecationDelta": {
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:getKey()": "/**\n* Retrieves the value of the key.\n* @return the key as a String\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:getNewKeys()": "/**\n* Retrieves an array of new keys.\n* @return array of new keys\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:getCustomMessage()": "/**\n* Retrieves the custom message.\n* @return the custom message string\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)": "/**\n* Constructs a DeprecationDelta with key, newKeys, and customMessage.\n* @param key unique identifier for deprecation\n* @param newKeys array of new identifiers\n* @param customMessage optional message for deprecation\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Constructs a DeprecationDelta with key, single newKey, and customMessage.\n* @param key unique identifier for deprecation\n* @param newKey new identifier for deprecation\n* @param customMessage optional message for deprecation\n*/",
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)": "/**** Constructs a DeprecationDelta with a key and a single new identifier. \n* @param key unique identifier for deprecation \n* @param newKey new identifier to replace the deprecated one \n*/"
    },
    "org.apache.hadoop.conf.ReconfigurationException": {
        "org.apache.hadoop.conf.ReconfigurationException:constructMessage(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Constructs a message indicating a property change.\n* @param property name of the property\n* @param newVal new value of the property\n* @param oldVal old value of the property\n* @return formatted message string\n*/",
        "org.apache.hadoop.conf.ReconfigurationException:<init>()": "/**\n* Constructs a ReconfigurationException with a default message.\n*/",
        "org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)": "/**\n* Constructs a ReconfigurationException with property details and a cause.\n* @param property property name causing the exception\n* @param newVal new value of the property\n* @param oldVal old value of the property\n* @param cause underlying cause of the exception\n*/",
        "org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Constructs a ReconfigurationException with property change details.\n* @param property name of the property\n* @param newVal new value of the property\n* @param oldVal old value of the property\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedItem": {
        "org.apache.hadoop.conf.Configuration$ParsedItem:<init>(java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])": "/**\n* Constructs a ParsedItem with specified attributes.\n* @param name item name, key, and value for the item\n* @param isFinal indicates if the item is final\n* @param sources array of source identifiers\n*/"
    },
    "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator": {
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:<init>(java.util.List)": "/**\n* Initializes iterator for a list of Range objects.\n* @param ranges list of Range objects to iterate over\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:hasNext()": "/**\n* Checks if there are more elements to iterate.\n* @return true if more elements exist, false otherwise\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:next()": "/**\n* Returns the next integer in the range or from internal source.\n* @return next integer or null if no more elements exist\n*/",
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:remove()": "/**\n* Throws UnsupportedOperationException to indicate removal is not supported.\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$5": {
        "org.apache.hadoop.conf.StorageUnit$5:toBytes(double)": "/**\n* Converts a value to its byte representation.\n* @param value the numeric value to convert\n* @return the equivalent byte representation of the value\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:toKBs(double)": "/**\n* Converts a value to kilobytes.\n* @param value data size in bytes\n* @return equivalent size in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value size in bytes\n* @return equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:toTBs(double)": "/**\n* Converts a value to terabits.\n* @param value input value in bits\n* @return equivalent value in terabits\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value input value in bytes\n* @return equivalent value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:toEBs(double)": "/**\n* Converts a value to exabytes.\n* @param value input value in bytes\n* @return equivalent value in exabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:fromBytes(double)": "/**\n* Converts a byte representation to a double value.\n* @param value byte representation of the double\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:toMBs(double)": "/**\n* Converts a value in bytes to megabytes.\n* @param value size in bytes\n* @return equivalent size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the object\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:getShortName()": "/**\n* Retrieves the short name representation of an object.\n* @return short name as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:getSuffixChar()": "/**\n* Retrieves the suffix character as a string.\n* @return suffix character string\n*/",
        "org.apache.hadoop.conf.StorageUnit$5:getDefault(double)": "/**\n* Retrieves the default value based on the provided input.\n* @param value input value to determine the default\n* @return computed default value as a double\n*/"
    },
    "org.apache.hadoop.conf.StorageUnit$1": {
        "org.apache.hadoop.conf.StorageUnit$1:toBytes(double)": "/**\n* Converts a value to its byte representation.\n* @param value the value to convert\n* @return the byte representation of the value\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:toKBs(double)": "/**\n* Converts a value to kilobytes.\n* @param value size in bytes\n* @return equivalent size in kilobytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:toMBs(double)": "/**\n* Converts a value to megabytes.\n* @param value size in bytes\n* @return equivalent size in megabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:toGBs(double)": "/**\n* Converts a value in bytes to gigabytes.\n* @param value the size in bytes\n* @return the equivalent size in gigabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:toTBs(double)": "/**\n* Converts a value to terabytes.\n* @param value size in bytes to convert\n* @return equivalent size in terabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:toPBs(double)": "/**\n* Converts a value to petabytes.\n* @param value the value in bytes\n* @return equivalent value in petabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:toEBs(double)": "/**\n* Converts a value to exabytes.\n* @param value the value in bytes to convert\n* @return the equivalent value in exabytes\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:fromBytes(double)": "/**\n* Converts a byte value to a double representation.\n* @param value byte value to convert\n* @return converted double value\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:getLongName()": "/**\n* Retrieves the long name as a String.\n* @return the long name of the object\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:getShortName()": "/**\n* Retrieves the short name representation of an object.\n* @return String representing the short name\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:getSuffixChar()": "/**\n* Retrieves the suffix character as a string.\n* @return suffix character as a String\n*/",
        "org.apache.hadoop.conf.StorageUnit$1:getDefault(double)": "/**\n* Retrieves the default value based on the provided input.\n* @param value input value to determine the default\n* @return default value as a double\n*/"
    },
    "org.apache.hadoop.ha.StreamPumper": {
        "org.apache.hadoop.ha.StreamPumper:pump()": "/**\n* Reads lines from a stream and logs them based on the stream type.\n* @throws IOException if an I/O error occurs during reading\n*/",
        "org.apache.hadoop.ha.StreamPumper:<init>(org.slf4j.Logger,java.lang.String,java.io.InputStream,org.apache.hadoop.ha.StreamPumper$StreamType)": "/**\n* Initializes a StreamPumper to read from an InputStream and log output.\n* @param log Logger for output messages\n* @param logPrefix Prefix for log messages\n* @param stream InputStream to be pumped\n* @param type Type of stream being processed\n*/",
        "org.apache.hadoop.ha.StreamPumper:join()": "/**\n* Waits for the thread to die after it has started.\n* @throws InterruptedException if the current thread is interrupted\n*/",
        "org.apache.hadoop.ha.StreamPumper:start()": "/**\n* Starts the thread if not already started.\n* @assert ensures thread has not been started before\n*/"
    },
    "org.apache.hadoop.ha.ActiveStandbyElector": {
        "org.apache.hadoop.ha.ActiveStandbyElector:terminateConnection()": "/**\n* Terminates the ZooKeeper connection and resets related fields.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:isNodeExists(org.apache.zookeeper.KeeperException$Code)": "/**\n* Checks if the given code represents an existing node.\n* @param code the Code to check\n* @return true if code is NODEEXISTS, false otherwise\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:isNodeDoesNotExist(org.apache.zookeeper.KeeperException$Code)": "/**\n* Checks if the given code represents a non-existent node.\n* @param code the Code to evaluate\n* @return true if code is NONODE, false otherwise\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:isSuccess(org.apache.zookeeper.KeeperException$Code)": "/**\n* Checks if the given code represents a successful operation.\n* @param code the Code to evaluate\n* @return true if code is OK, otherwise false\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:becomeStandby()": "/**\n* Transitions the system to standby state if not already in standby.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code)": "/**\n* Determines if a retry is needed based on the error code.\n* @param code the error code to evaluate\n* @return true if retry is warranted, otherwise false\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:createLockNodeAsync()": "/**\n* Asynchronously creates an ephemeral lock node in ZooKeeper.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:isSessionExpired(org.apache.zookeeper.KeeperException$Code)": "/**\n* Checks if the given code indicates a session expiration.\n* @param code the Code to evaluate\n* @return true if session is expired, false otherwise\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:enterNeutralMode()": "/**\n* Enters neutral mode if not already in that state, logging the action.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:monitorLockNodeAsync()": "/**\n* Initiates asynchronous monitoring of a lock node in ZooKeeper.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:initiateZookeeper(org.apache.zookeeper.client.ZKClientConfig)": "/**\n* Initializes a ZooKeeper instance with the provided configuration.\n* @param zkClientConfig configuration for the ZooKeeper client\n* @return ZooKeeper instance\n* @throws IOException if an I/O error occurs during initialization\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:sleepFor(int)": "/**\n* Pauses execution for a specified duration in milliseconds.\n* @param sleepMs duration to sleep in milliseconds\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:preventSessionReestablishmentForTests()": "/**\n* Locks the session reestablishment for testing purposes.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:allowSessionReestablishmentForTests()": "/**\n* Unlocks the session reestablish lock for testing purposes.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:getZKSessionIdForTests()": "/**\n* Retrieves the ZooKeeper session ID for testing.\n* @return session ID or -1 if zkClient is null\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code,org.apache.zookeeper.KeeperException$Code)": "/**\n* Determines if a retry is needed based on error codes.\n* @param code current error code\n* @param retryIfCode code to check for retry condition\n* @return true if retry is needed, false otherwise\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:getHAZookeeperConnectionState()": "/**\n* Retrieves the name of the current Zookeeper connection state.\n* @return String representation of the Zookeeper connection state\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists()": "/**\n* Checks if the parent z-node exists in ZooKeeper.\n* @return true if exists, false otherwise; throws IOException on errors\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:reset()": "/**\n* Resets the state to INIT and terminates the connection.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus()": "/****\n* Monitors the active status of a leader in an election process.\n* Asserts election participation and logs debug info.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)": "/**\n* Executes ZKAction with retries on KeeperException.\n* @param action the ZKAction to perform\n* @param retryCode code to check for retry condition\n* @return result of the ZKAction\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object)": "/**\n* Checks if the client context is stale.\n* @param ctx client context object\n* @return true if stale, false otherwise\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String)": "/**\n* Logs a fatal error and resets the application state.\n* @param errorMessage the error message to log\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String)": "/**\n* Sets ACLs for the given path with retries on failure.\n* @param path the Zookeeper path for which to set ACLs\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction)": "/**\n* Executes ZKAction with retries on KeeperException.\n* @param action the ZKAction to perform\n* @return result of the ZKAction\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper()": "/**\n* Creates a ZooKeeper instance with optional SSL configuration.\n* @return ZooKeeper instance\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:toString()": "/**\n* Returns a string representation of the object with identity hash and appData.\n* @return formatted string with identity hash and appData in hex or null\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode()": "/**\n* Clears the parent ZNode in ZooKeeper if not in election.\n* @throws IOException if deletion fails\n* @throws InterruptedException if interrupted during operation\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive()": "/**\n* Checks for old active nodes to fence and returns their status.\n* @return Stat object of the old node or null if none exists\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)": "/**\n* Creates a node with retries.\n* @param path the node's path, data the node's data, acl access control list, mode creation mode\n* @return node path or throws exceptions on failure\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)": "/**\n* Retrieves data from ZK with retries.\n* @param path ZK node path, @param watch flag to watch for changes, @param stat metadata\n* @return byte array of data from the ZK node\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)": "/**\n* Sets data at the specified path with retries.\n* @param path the ZNode path\n* @param data the data to set\n* @param version the expected version\n* @return Stat object containing the new ZNode status\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)": "/**\n* Deletes a znode with retries.\n* @param path znode path to delete\n* @param version expected version number for deletion\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper()": "/**\n* Connects to ZooKeeper and registers an event watcher.\n* @return ZooKeeper instance after successful connection\n* @throws IOException if connection fails\n* @throws KeeperException if ZooKeeper encounters an error\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat)": "/**\n* Writes or updates a breadcrumb node in ZooKeeper.\n* @param oldBreadcrumbStat status of the previous breadcrumb node\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode()": "/**\n* Attempts to delete the active node's breadcrumb in ZooKeeper.\n* Ensures data integrity before deletion.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:createConnection()": "/**\n* Creates and initializes a ZooKeeper connection, closing existing ones if necessary.\n* @throws IOException if closing or connecting fails\n* @throws KeeperException if ZooKeeper encounters an error\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:becomeActive()": "/**\n* Activates the current instance if not already active.\n* @return true if activation succeeds, false otherwise\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean)": "/**\n* Exits the election process, optionally removing the breadcrumb node.\n* @param needFence indicates if a fence is required before quitting\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode()": "/**\n* Ensures the existence of parent ZNodes for a given working directory.\n* @throws IOException on connection or creation failure\n* @throws InterruptedException if interrupted during execution\n* @throws KeeperException if Zookeeper encounters an error\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:getActiveData()": "/**\n* Retrieves active data from ZooKeeper, handling node absence exceptions.\n* @return byte array of active data\n* @throws ActiveNotFoundException if the active data node does not exist\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession()": "/**\n* Attempts to re-establish a ZooKeeper session with retries.\n* @return true if successful, false if max retries exceeded\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)": "/**\n* Initializes ActiveStandbyElector with ZooKeeper parameters and establishes a connection.\n* @param zookeeperHostPorts ZooKeeper server addresses\n* @param zookeeperSessionTimeout session timeout duration\n* @param parentZnodeName parent znode for coordination\n* @param acl access control list for ZooKeeper\n* @param authInfo authentication information for ZooKeeper\n* @param app callback for application events\n* @param maxRetryNum maximum retries for session establishment\n* @param failFast flag for immediate connection attempt\n* @param truststoreKeystore truststore and keystore for secure connections\n* @throws IOException if connection fails\n* @throws HadoopIllegalArgumentException if parameters are invalid\n* @throws KeeperException if ZooKeeper encounters an error\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal()": "/**** Joins the election process, ensuring app data is available and session is valid. */",
        "org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)": "/**\n* Constructs ActiveStandbyElector with ZooKeeper parameters.\n* @param zookeeperHostPorts ZooKeeper server addresses\n* @param zookeeperSessionTimeout session timeout duration\n* @param parentZnodeName parent znode for coordination\n* @param acl access control list for ZooKeeper\n* @param authInfo authentication info for ZooKeeper\n* @param app callback for application events\n* @param maxRetryNum maximum retries for session establishment\n* @param truststoreKeystore truststore and keystore for secure connections\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[])": "/**\n* Joins the election process with provided app data.\n* @param data byte array containing application data\n* @throws HadoopIllegalArgumentException if data is null\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int)": "/**\n* Re-establishes ZK session and joins election if app data is available.\n* @param sleepTime duration to sleep before joining election\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive()": "/**\n* Re-attempts to join election after failure to become active.\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)": "/**\n* Processes ZooKeeper watch events to manage connection states and election participation.\n* @param zk ZooKeeper instance for connection management\n* @param event the watched event to process\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)": "/**** \n* Processes the result of a znode creation attempt.\n* @param rc result code from the operation\n* @param path the znode path\n* @param ctx client context object\n* @param name name of the operation\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)": "/**\n* Processes the result from a znode operation and manages election state.\n* @param rc result code from the operation\n* @param path the path of the znode\n* @param ctx client context object\n* @param stat metadata about the znode\n*/"
    },
    "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ha.ZKFCProtocol)": "/**\n* Constructs ZKFCProtocolServerSideTranslatorPB with a specified ZKFCProtocol server.\n* @param server the ZKFCProtocol instance to be used\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:cedeActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto)": "/**\n* Cedes active status based on request parameters.\n* @param controller RPC controller for handling requests\n* @param request contains milliseconds to cede\n* @return default CedeActiveResponseProto instance\n* @throws ServiceException if an I/O error occurs\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:gracefulFailover(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto)": "/**\n* Initiates a graceful failover process.\n* @param controller RPC controller for request handling\n* @param request failover request details\n* @return default response indicating success\n* @throws ServiceException if an error occurs during failover\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)": "/**\n* Gets the protocol version for a given protocol and client version.\n* @param protocol the protocol name\n* @param clientVersion the client's version\n* @return the protocol version as a long\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)": "/**\n* Retrieves protocol signature for given protocol and client version.\n* @param protocol requested protocol name\n* @param clientVersion client's protocol version\n* @param clientMethodsHash client's methods hash code\n* @return ProtocolSignature object for the requested protocol\n*/"
    },
    "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto)": "/**\n* Converts HAServiceStateProto to HAServiceState.\n* @param state HAServiceStateProto to convert\n* @return corresponding HAServiceState value\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getUnderlyingProxyObject()": "/**\n* Returns the underlying RPC proxy object.\n* @return the RPC proxy object\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Converts StateChangeRequestInfo to HAStateChangeRequestInfoProto.\n* @param reqInfo source request information\n* @return HAStateChangeRequestInfoProto object\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy connection.\n* @param rpcProxy the proxy object to close\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth()": "/**\n* Monitors system health via an IPC call.\n* @throws IOException if the IPC call fails\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus()": "/**\n* Retrieves the service status and its readiness state.\n* @return HAServiceStatus object indicating service status\n* @throws IOException if IPC call fails\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Transitions the system to active state using provided request information.\n* @param reqInfo source request information\n* @throws IOException if an IPC call fails\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Transitions to standby state using provided request info.\n* @param reqInfo source request information\n* @throws IOException if RPC call fails\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Transitions to observer state using request information.\n* @param reqInfo source request information\n* @throws IOException if IPC call fails\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)": "/**\n* Initializes HAServiceProtocolClient with RPC settings.\n* @param addr server address, @param conf configuration, \n* @param socketFactory socket factory, @param timeout RPC timeout\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes HAServiceProtocol client with address and configuration.\n* @param addr server address for RPC connection\n* @param conf configuration settings for the protocol\n* @throws IOException if RPC setup fails\n*/"
    },
    "org.apache.hadoop.ha.HAServiceStatus": {
        "org.apache.hadoop.ha.HAServiceStatus:<init>(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)": "/**\n* Initializes HAServiceStatus with the given state.\n* @param state the state of the HA service\n*/",
        "org.apache.hadoop.ha.HAServiceStatus:setReadyToBecomeActive()": "/**\n* Sets the service as ready to become active and clears any not ready reason.\n* @return HAServiceStatus instance for method chaining\n*/",
        "org.apache.hadoop.ha.HAServiceStatus:setNotReadyToBecomeActive(java.lang.String)": "/**\n* Marks the service as not ready to become active with a reason.\n* @param reason explanation for not being ready\n* @return HAServiceStatus instance for method chaining\n*/",
        "org.apache.hadoop.ha.HAServiceStatus:getState()": "/**\n* Retrieves the current state of the HA service.\n* @return HAServiceState representing the service's state\n*/",
        "org.apache.hadoop.ha.HAServiceStatus:isReadyToBecomeActive()": "/**\n* Checks if the entity is ready to become active.\n* @return true if ready, false otherwise\n*/",
        "org.apache.hadoop.ha.HAServiceStatus:getNotReadyReason()": "/**\n* Retrieves the reason for not being ready.\n* @return String representing the not ready reason\n*/"
    },
    "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo": {
        "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo:getSource()": "/**\n* Retrieves the current request source.\n* @return RequestSource object representing the source\n*/",
        "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo:<init>(org.apache.hadoop.ha.HAServiceProtocol$RequestSource)": "/**\n* Initializes StateChangeRequestInfo with the specified request source.\n* @param source the origin of the request\n*/"
    },
    "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB": {
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ha.HAServiceProtocol)": "/**\n* Initializes the HAServiceProtocolServerSideTranslatorPB with a server instance.\n* @param server HAServiceProtocol instance for communication\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:monitorHealth(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto)": "/**\n* Monitors server health and returns the response.\n* @param controller RPC controller for request handling\n* @param request health monitoring request\n* @return MonitorHealthResponseProto response object\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)": "/**\n* Retrieves the protocol version for the specified protocol.\n* @param protocol string identifier for the protocol\n* @param clientVersion client's version for compatibility checks\n* @return protocol version as a long\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto)": "/**\n* Converts HAStateChangeRequestInfoProto to StateChangeRequestInfo.\n* @param proto source protocol object\n* @return StateChangeRequestInfo instance with request source\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)": "/**\n* Retrieves service status and readiness from the HA server.\n* @param controller RPC controller for managing calls\n* @param request service status request details\n* @return GetServiceStatusResponseProto containing service state and readiness info\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)": "/**\n* Transitions server to active state and returns response.\n* @param controller RPC controller for handling requests\n* @param request transition request containing necessary info\n* @return TransitionToActiveResponseProto response object\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)": "/**\n* Transitions the server to standby mode.\n* @param controller RPC controller for request management\n* @param request contains transition request information\n* @return TransitionToStandbyResponseProto response object\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)": "/**\n* Transitions server to observer state.\n* @param controller RPC controller for request handling\n* @param request transition request containing necessary info\n* @return TransitionToObserverResponseProto response object\n*/",
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)": "/**\n* Validates protocol and retrieves its signature.\n* @param protocol expected protocol name\n* @param clientVersion client's protocol version\n* @param clientMethodsHash client's methods hash code\n* @return ProtocolSignature for the specified protocol\n*/"
    },
    "org.apache.hadoop.ha.ZKFCRpcServer": {
        "org.apache.hadoop.ha.ZKFCRpcServer:start()": "/**\n* Starts the server.\n*/",
        "org.apache.hadoop.ha.ZKFCRpcServer:getAddress()": "/**\n* Retrieves the server's listener address.\n* @return InetSocketAddress of the server\n*/",
        "org.apache.hadoop.ha.ZKFCRpcServer:stopAndJoin()": "/**\n* Stops the server and waits for it to terminate.\n* @throws InterruptedException if the current thread is interrupted while waiting\n*/",
        "org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)": "/**\n* Initializes ZKFCRpcServer with configuration and address, setting up RPC service.\n* @param conf configuration settings\n* @param bindAddr address to bind the server\n* @param zkfc failover controller instance\n* @param policy authorization security policy\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int)": "/**\n* Cedes active control for a specified duration.\n* @param millisToCede duration in milliseconds to cede control\n* @throws IOException if an I/O error occurs\n* @throws AccessControlException if access is denied\n*/",
        "org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover()": "/**\n* Initiates a graceful failover after checking RPC admin access.\n* @throws IOException if an I/O error occurs\n* @throws AccessControlException if access is denied\n*/"
    },
    "org.apache.hadoop.ha.BadFencingConfigurationException": {
        "org.apache.hadoop.ha.BadFencingConfigurationException:<init>(java.lang.String)": "/**\n* Constructs a BadFencingConfigurationException with a message.\n* @param msg detailed error message\n*/",
        "org.apache.hadoop.ha.BadFencingConfigurationException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a BadFencingConfigurationException with a message and cause.\n* @param msg detailed error message\n* @param cause underlying throwable cause\n*/"
    },
    "org.apache.hadoop.ha.HAAdmin": {
        "org.apache.hadoop.ha.HAAdmin:getTargetIds(java.lang.String)": "/**\n* Returns a collection containing the specified target node ID.\n* @param targetNodeToActivate the ID of the target node\n* @return a singleton collection with the target node ID\n*/",
        "org.apache.hadoop.ha.HAAdmin:getUsageString()": "/**\n* Returns the usage string for the HAAdmin command.\n* @return usage instructions as a String\n*/",
        "org.apache.hadoop.ha.HAAdmin:addTransitionToActiveCliOpts(org.apache.commons.cli.Options)": "/**\n* Adds a \"force active\" option to the specified transition options.\n* @param transitionToActiveCliOpts options to modify\n*/",
        "org.apache.hadoop.ha.HAAdmin:confirmForceManual()": "/**\n* Prompts user for confirmation before using a dangerous flag.\n* @return true if confirmed, false otherwise\n*/",
        "org.apache.hadoop.ha.HAAdmin:createReqInfo()": "/**\n* Creates StateChangeRequestInfo using the specified request source.\n* @return initialized StateChangeRequestInfo object\n*/",
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)": "/**\n* Prints command usage information to the specified PrintStream.\n* @param pStr output stream for usage info, @param cmd command name, @param helpEntries map of usage info\n*/",
        "org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Checks if manual HA state management is allowed based on auto failover status.\n* @param target the HA service target\n* @return true if manual management is allowed, false otherwise\n*/",
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)": "/**\n* Prints command usage to the provided PrintStream.\n* @param pStr output stream for usage information\n* @param helpEntries map of command names to their usage info\n*/",
        "org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes HAAdmin with the provided Hadoop configuration.\n* @param conf the Configuration to set for HAAdmin\n*/",
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)": "/**\n* Prints command usage information to the specified output stream.\n* @param pStr output stream for usage info, @param cmd command name\n*/",
        "org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)": "/**\n* Parses command line options and handles parsing errors.\n* @param cmdName command name, @param opts options to parse, \n* @param argv arguments array, @param helpEntries usage info map\n* @return CommandLine object or null if parsing fails\n*/",
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream)": "/**\n* Prints command usage to the specified output stream.\n* @param pStr output stream for usage information\n*/",
        "org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)": "/**\n* Validates command parameters and prints usage if invalid.\n* @param argv command line arguments\n* @param helpEntries map of command names to usage info\n* @return true if parameters are valid, false otherwise\n*/",
        "org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)": "/**\n* Displays command usage based on input arguments.\n* @param argv command line arguments, @param helpEntries map of command usage info\n* @return 0 on success, -1 for errors\n*/",
        "org.apache.hadoop.ha.HAAdmin:<init>()": "/**\n* Initializes a new instance of HAAdmin.\n*/",
        "org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])": "/**\n* Parses command line options and returns a CommandLine object.\n* @param cmdName command name, @param opts options to parse, @param argv arguments array\n* @return CommandLine object or null if parsing fails\n*/",
        "org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[])": "/**\n* Validates command parameters using predefined usage info.\n* @param argv command line arguments\n* @return true if parameters are valid, false otherwise\n*/",
        "org.apache.hadoop.ha.HAAdmin:help(java.lang.String[])": "/**\n* Displays command usage based on input arguments.\n* @param argv command line arguments\n* @return result of help method call\n*/",
        "org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and updates RPC timeout based on provided settings.\n* @param conf the Configuration to set\n*/",
        "org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Performs graceful failover to a specified ZKFC node.\n* @param toNode target HA service node\n* @return 0 on success, -1 on failure\n*/",
        "org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)": "/**\n* Checks if other target nodes are active.\n* @param targetNodeToActivate node ID to check\n* @param forceActive forces activation if true\n* @return true if another node is active, false otherwise\n*/",
        "org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine)": "/**\n* Transitions service to standby if valid target and arguments are provided.\n* @param cmd command line arguments\n* @return 0 on success, -1 on failure\n*/",
        "org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine)": "/**\n* Checks health of a service based on command line arguments.\n* @param cmd command line arguments\n* @return 0 if successful, -1 if failed or incorrect arguments\n*/",
        "org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine)": "/**\n* Retrieves the state of a service based on command line arguments.\n* @param cmd command line input\n* @return 0 on success, -1 if argument count is incorrect\n*/",
        "org.apache.hadoop.ha.HAAdmin:getAllServiceState()": "/**\n* Fetches and prints the state of all services.\n* @return 0 on success, -1 if no service IDs are found\n*/",
        "org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine)": "/**** Transitions service to active state based on command line input. \n* @param cmd command line arguments \n* @return 0 on success, -1 on failure \n*/",
        "org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[])": "/**\n* Executes command based on input arguments.\n* @param argv command line arguments\n* @return status code indicating success or failure\n*/",
        "org.apache.hadoop.ha.HAAdmin:run(java.lang.String[])": "/**\n* Executes a command and handles exceptions.\n* @param argv command line arguments\n* @return status code indicating success or failure\n*/"
    },
    "org.apache.hadoop.ha.HAServiceTarget": {
        "org.apache.hadoop.ha.HAServiceTarget:isAutoFailoverEnabled()": "/**\n* Checks if auto failover is enabled.\n* @return false indicating auto failover is disabled\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getTransitionTargetHAStatus()": "/**\n* Retrieves the current target HA service state.\n* @return HAServiceState representing the target status\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorAddress()": "/**\n* Retrieves the health monitor's socket address.\n* @return InetSocketAddress or null if not set\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:addFencingParameters(java.util.Map)": "/**\n* Adds fencing parameters to the provided map.\n* @param ret map to store fencing parameters\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getFencingParameters()": "/**\n* Retrieves fencing parameters in a map.\n* @return Map containing fencing parameters\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)": "/**\n* Creates a ZKFC proxy with a modified connection timeout.\n* @param conf configuration settings\n* @param timeoutMs connection timeout in milliseconds\n* @return ZKFCProtocol instance\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)": "/**\n* Creates a proxy for HAServiceProtocol with specified configuration and retries.\n* @param conf configuration settings, @param timeoutMs RPC timeout, @param retries max retries, \n* @param addr server address\n* @return HAServiceProtocol proxy instance\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)": "/**\n* Retrieves HAServiceProtocol proxy for health monitor.\n* @param conf configuration settings, @param timeoutMs RPC timeout, @param retries max retries\n* @return HAServiceProtocol proxy instance\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)": "/**\n* Retrieves HAServiceProtocol proxy with reduced retries for quick connection failure.\n* @param conf configuration settings\n* @param timeoutMs RPC timeout in milliseconds\n* @param addr server address\n* @return HAServiceProtocol proxy instance\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)": "/**\n* Retrieves HAServiceProtocol proxy for health monitor.\n* @param conf configuration settings, @param timeoutMs RPC timeout\n* @return HAServiceProtocol proxy instance\n*/",
        "org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)": "/**\n* Retrieves HAServiceProtocol proxy using configuration and timeout settings.\n* @param conf configuration settings\n* @param timeoutMs RPC timeout in milliseconds\n* @return HAServiceProtocol proxy instance\n*/"
    },
    "org.apache.hadoop.ha.HAAdmin$UsageInfo": {
        "org.apache.hadoop.ha.HAAdmin$UsageInfo:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs UsageInfo with command arguments and help description.\n* @param args command line arguments\n* @param help help text for the command\n*/"
    },
    "org.apache.hadoop.ha.SshFenceByTcpPort": {
        "org.apache.hadoop.ha.SshFenceByTcpPort:cleanup(com.jcraft.jsch.ChannelExec)": "/**\n* Cleans up and disconnects the given SSH channel.\n* @param exec the ChannelExec to disconnect\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)": "/**\n* Executes a command on a remote session and logs the output.\n* @param session the SSH session to execute the command\n* @param cmd the command to run on the remote server\n* @return exit status of the executed command\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)": "/**\n* Attempts to kill a process on a specified port via SSH.\n* @param session SSH session for command execution\n* @param serviceAddr address of the service to fence\n* @return true if the process was killed or not running, false otherwise\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String)": "/**\n* Validates and processes input arguments for fencing configuration.\n* @param argStr input string with user and port info\n* @throws BadFencingConfigurationException if input string is invalid\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles()": "/**\n* Retrieves a collection of key files from configuration.\n* @return Collection of key file names or empty if not found\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout()": "/**\n* Retrieves SSH connection timeout value.\n* @return timeout in seconds, using default if not set\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)": "/**\n* Creates an SSH session with specified host and arguments.\n* @param host the hostname for the SSH connection\n* @param args contains user and SSH port details\n* @return Session object for the established connection\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)": "/**\n* Attempts to fence a service by establishing an SSH session.\n* @param target service target to be fenced\n* @param argsStr string containing user and port info\n* @return true if fencing succeeded, false otherwise\n*/"
    },
    "org.apache.hadoop.ha.HealthCheckFailedException": {
        "org.apache.hadoop.ha.HealthCheckFailedException:<init>(java.lang.String)": "/**\n* Constructs a HealthCheckFailedException with a specified error message.\n* @param message detailed error message for the exception\n*/",
        "org.apache.hadoop.ha.HealthCheckFailedException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a HealthCheckFailedException with a message and cause.\n* @param message error message\n* @param cause underlying Throwable cause\n*/"
    },
    "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter": {
        "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:isEnabled(int)": "/**\n* Checks if logging is enabled for a specified level.\n* @param level log level constant from Logger\n* @return true if logging is enabled, false otherwise\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String)": "/**\n* Logs a message at the specified log level.\n* @param level the log level (DEBUG, INFO, WARN, ERROR, FATAL)\n* @param message the message to log\n*/"
    },
    "org.apache.hadoop.ha.PowerShellFencer": {
        "org.apache.hadoop.ha.PowerShellFencer:checkArgs(java.lang.String)": "/**\n* Logs the PowerShell fencer parameter.\n* @param argStr the argument string to log\n* @throws BadFencingConfigurationException if configuration is invalid\n*/",
        "org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)": "",
        "org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)": "/**\n* Attempts to kill a remote process using PowerShell.\n* @param target target service for fencing\n* @param argsStr process name to kill\n* @return true if successful, false otherwise\n*/"
    },
    "org.apache.hadoop.ha.ZKFailoverController": {
        "org.apache.hadoop.ha.ZKFailoverController:setLastHealthState(org.apache.hadoop.ha.HealthMonitor$State)": "/**\n* Updates the last health state and logs the change.\n* @param newState the new health state to set\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Initializes ZKFailoverController with configuration and local target.\n* @param conf configuration settings\n* @param localTarget local HA service target\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:mainLoop()": "/**\n* Waits for fatal error signal and throws exception if error occurs.\n* @throws InterruptedException if interrupted while waiting\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:printUsage()": "/**\n* Prints the usage information to the standard error output.\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:fatalError(java.lang.String)": "/**\n* Logs a fatal error message and updates the fatalError state.\n* @param err the error message to log\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:recordActiveAttempt(org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord)": "/**\n* Records the latest active attempt and notifies waiting threads.\n* @param record the active attempt record to be stored\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:waitForActiveAttempt(int,long)": "/**\n* Waits for an active attempt to become available or timeout.\n* @param timeoutMillis maximum wait time in milliseconds\n* @param onlyAfterNanoTime minimum time after which to check for an active attempt\n* @return ActiveAttemptRecord if available, otherwise null\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:getLastHealthState()": "/**\n* Retrieves the last recorded health state.\n* @return lastHealthState object representing the health state\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:scheduleRecheck(long)": "/**\n* Schedules a recheck of electability after a specified delay.\n* @param whenNanos delay in nanoseconds before rechecking\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:createReqInfo()": "/**\n* Creates StateChangeRequestInfo for ZKFC request source.\n* @return initialized StateChangeRequestInfo object\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:startRPC()": "/**\n* Initiates the RPC server.\n* @throws IOException if the server fails to start\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String)": "/**\n* Handles bad argument by printing usage and throwing an exception.\n* @param arg the invalid argument causing the exception\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover()": "/**\n* Validates if the service can undergo failover.\n* @throws ServiceFailedException if service is unhealthy or in observer state\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:getCurrentActive()": "/**** Retrieves current active HAServiceTarget from ZooKeeper. \n* @return HAServiceTarget or null if not found \n* @throws IOException for ZooKeeper issues \n*/",
        "org.apache.hadoop.ha.ZKFailoverController:recheckElectability()": "/**\n* Rechecks node electability and manages election participation based on health state.\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)": "/**\n* Verifies and handles changes in service state, managing election processes accordingly.\n* @param changedState the new service state to verify against the current state\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:getParentZnode()": "/**\n* Retrieves the parent znode path, ensuring it ends with a slash.\n* @return formatted znode path with scope appended\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:confirmFormat()": "/**\n* Prompts user to confirm clearing failover info from existing parent znode.\n* @return true if confirmed, false otherwise\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)": "/**\n* Formats ZooKeeper by clearing parent z-node if conditions are met.\n* @param force forces the operation if true; @param interactive prompts for confirmation.\n* @return error code based on operation success or failure.\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:initHM()": "/**\n* Initializes HealthMonitor with configuration and target, adding callbacks and starting it.\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:initRPC()": "/**\n* Initializes the RPC server for ZKFC with the binding address.\n* @throws IOException if an I/O error occurs during initialization\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:initZK()": "/**\n* Initializes ZooKeeper connection and configuration settings.\n* @throws HadoopIllegalArgumentException if config is invalid\n* @throws IOException if an I/O error occurs\n* @throws KeeperException if ZooKeeper operation fails\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])": "/**** Executes the failover controller logic, initializing ZooKeeper and handling commands. \n* @param args command-line arguments for formatting or configuration\n* @return error code based on execution outcome\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:cedeActive(int)": "/**\n* Cedes active control for a specified duration.\n* @param millisToCede duration in milliseconds to cede control\n* @throws AccessControlException if access is denied\n* @throws ServiceFailedException if the service fails\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou()": "/**\n* Performs a graceful failover using the logged-in user context.\n* @throws ServiceFailedException if the service fails\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[])": "/***************************************\n* Executes the main logic if auto failover is enabled.\n* @param args command-line arguments\n* @return status code from doRun method\n***************************************/",
        "org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)": "/**\n* Requests remote service to cede its active state and returns its ZKFC proxy.\n* @param remote target HA service\n* @param timeout duration for ceding in milliseconds\n* @return ZKFCProtocol instance of the remote service\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover()": "/**\n* Manages graceful failover process for HA services.\n* @throws ServiceFailedException if failover conditions are not met\n* @throws IOException for communication issues\n* @throws InterruptedException if interrupted during operation\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:becomeActive()": "/**** Attempts to transition service to active state and handles exceptions. Throws ServiceFailedException if failed. */",
        "org.apache.hadoop.ha.ZKFailoverController:becomeStandby()": "/**\n* Transitions the local target to standby state after ZK election.\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int)": "/**\n* Cedes active role, managing transitions and potential fencing.\n* @param millisToCede duration to delay before rejoining\n* @throws AccessControlException if access is denied\n* @throws ServiceFailedException if service operation fails\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Fences a service target, attempting graceful transition first.\n* @param target the HAServiceTarget to be fenced\n*/",
        "org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[])": "/**\n* Fences an old active service based on provided data.\n* @param data byte array representing the service target\n*/"
    },
    "org.apache.hadoop.ha.FailoverFailedException": {
        "org.apache.hadoop.ha.FailoverFailedException:<init>(java.lang.String)": "/**\n* Constructs a FailoverFailedException with the specified message.\n* @param message detailed error message\n*/",
        "org.apache.hadoop.ha.FailoverFailedException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a FailoverFailedException with a message and cause.\n* @param message error message\n* @param cause underlying Throwable cause\n*/"
    },
    "org.apache.hadoop.ha.HealthMonitor": {
        "org.apache.hadoop.ha.HealthMonitor:shutdown()": "/**\n* Stops the HealthMonitor thread and interrupts the daemon.\n*/",
        "org.apache.hadoop.ha.HealthMonitor:join()": "/**\n* Waits for the daemon thread to die.\n* @throws InterruptedException if the current thread is interrupted while waiting\n*/",
        "org.apache.hadoop.ha.HealthMonitor:addCallback(org.apache.hadoop.ha.HealthMonitor$Callback)": "/**\n* Adds a callback to the list of callbacks.\n* @param cb the callback to be added\n*/",
        "org.apache.hadoop.ha.HealthMonitor:addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback)": "/**\n* Adds a ServiceStateCallback to the list of callbacks.\n* @param cb the callback to be added\n*/",
        "org.apache.hadoop.ha.HealthMonitor:start()": "/**\n* Starts the daemon process.\n*/",
        "org.apache.hadoop.ha.HealthMonitor:enterState(org.apache.hadoop.ha.HealthMonitor$State)": "/**\n* Updates the current state and notifies callbacks of the change.\n* @param newState the new state to enter\n*/",
        "org.apache.hadoop.ha.HealthMonitor:setLastServiceStatus(org.apache.hadoop.ha.HAServiceStatus)": "/**\n* Updates last service status and notifies registered callbacks.\n* @param status new service status to set\n*/",
        "org.apache.hadoop.ha.HealthMonitor:isAlive()": "/**\n* Checks if the daemon thread is alive.\n* @return true if the daemon is running, false otherwise\n*/",
        "org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable)": "/**\n* Checks if the given throwable is a HealthCheckFailedException.\n* @param t throwable to evaluate\n* @return true if it is a HealthCheckFailedException, false otherwise\n*/",
        "org.apache.hadoop.ha.HealthMonitor:doHealthChecks()": "/**\n* Performs health checks on a service, updating status and handling exceptions.\n* @throws InterruptedException if the thread is interrupted during sleep\n*/",
        "org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Initializes HealthMonitor with configuration and target for monitoring.\n* @param conf configuration settings\n* @param target target service to monitor\n*/",
        "org.apache.hadoop.ha.HealthMonitor:createProxy()": "/**\n* Creates a HAServiceProtocol proxy for health monitoring.\n* @return HAServiceProtocol proxy instance\n*/",
        "org.apache.hadoop.ha.HealthMonitor:tryConnect()": "/**\n* Attempts to establish a connection and handles failures.\n* @return void; sets proxy or enters error state on failure\n*/",
        "org.apache.hadoop.ha.HealthMonitor:loopUntilConnected()": "/**\n* Loops until a connection is established, retrying at intervals.\n* @throws InterruptedException if the thread is interrupted while sleeping\n*/"
    },
    "org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord": {
        "org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord:<init>(boolean,java.lang.String)": "/**\n* Initializes an ActiveAttemptRecord with success status and message.\n* @param succeeded indicates if the attempt was successful\n* @param status provides the status message of the attempt\n*/"
    },
    "org.apache.hadoop.ha.ServiceFailedException": {
        "org.apache.hadoop.ha.ServiceFailedException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a ServiceFailedException with a message and cause.\n* @param message error description\n* @param cause underlying throwable cause of the exception\n*/",
        "org.apache.hadoop.ha.ServiceFailedException:<init>(java.lang.String)": "/**\n* Constructs a ServiceFailedException with a specified error message.\n* @param message error description for the exception\n*/"
    },
    "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks": {
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeActive()": "/**\n* Activates the failover controller.\n* @throws ServiceFailedException if activation fails\n*/",
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeStandby()": "/**\n* Transitions the controller to standby mode.\n*/",
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:notifyFatalError(java.lang.String)": "/**\n* Notifies of a fatal error with a given message.\n* @param errorMessage description of the fatal error\n*/",
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:fenceOldActive(byte[])": "/**\n* Delegates fencing of old active nodes with provided data.\n* @param data byte array containing fencing information\n*/",
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:toString()": "/**\n* Returns a string representation of the elector callbacks.\n* @return formatted string including localTarget information\n*/",
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:enterNeutralMode()": "/**\n* Enters the neutral mode for the system, disabling all active controls.\n*/"
    },
    "org.apache.hadoop.ha.NodeFencer": {
        "org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Attempts to fence a service from one target to another.\n* @param fromSvc source service target\n* @param toSvc target service to fence first\n* @return true if fencing succeeds, false otherwise\n*/",
        "org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Fences the specified service target from a null target.\n* @param fromSvc source service to be fenced\n* @return true if fencing succeeds, false otherwise\n*/",
        "org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)": "/**\n* Creates a FenceMethodWithArg from a class name and configuration.\n* @param conf configuration settings\n* @param clazzName fully qualified class name of the method\n* @param arg argument for the method\n* @return FenceMethodWithArg instance\n*/",
        "org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Parses method line to create FenceMethodWithArg from configuration.\n* @param conf configuration settings\n* @param line input string to parse\n* @return FenceMethodWithArg instance\n* @throws BadFencingConfigurationException if parsing fails\n*/",
        "org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Parses method specifications into a list of FenceMethodWithArg.\n* @param conf configuration settings\n* @param spec input string with method specifications\n* @return List of FenceMethodWithArg objects\n*/",
        "org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Initializes NodeFencer with parsed fencing methods.\n* @param conf configuration settings\n* @param spec input string with method specifications\n*/",
        "org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Creates a NodeFencer from configuration settings.\n* @param conf configuration settings\n* @param confKey key for retrieving method specifications\n* @return NodeFencer instance or null if not found\n*/"
    },
    "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef": {
        "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:waitForZKConnectionEvent(int)": "/**\n* Waits for ZooKeeper connection event with a timeout.\n* @param connectionTimeoutMs timeout in milliseconds\n* @throws KeeperException if connection is lost\n* @throws IOException if interrupted during connection\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper)": "/**\n* Sets the ZooKeeper reference if not already set.\n* @param zk ZooKeeper instance to be set\n*/",
        "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent)": "/****\n* Processes a ZooKeeper watch event and handles potential errors.\n* @param event the watched event to process\n*/"
    },
    "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg": {
        "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg:<init>(org.apache.hadoop.ha.FenceMethod,java.lang.String)": "/**\n* Initializes FenceMethodWithArg with a method and an argument.\n* @param method the FenceMethod to be set\n* @param arg the argument associated with the method\n*/",
        "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg:toString()": "/**\n* Returns a string representation of the method and its argument.\n* @return formatted string with method class and argument\n*/"
    },
    "org.apache.hadoop.ha.ShellCommandFencer": {
        "org.apache.hadoop.ha.ShellCommandFencer:parseArgs(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState,java.lang.String)": "/**\n* Parses command arguments based on HA service state.\n* @param state current HA service state\n* @param cmd command string to parse\n* @return relevant command argument based on state\n*/",
        "org.apache.hadoop.ha.ShellCommandFencer:tryGetPid(java.lang.Process)": "/**\n* Retrieves the PID of a UNIXProcess.\n* @param p the Process instance\n* @return the PID as a String or null if not applicable\n*/",
        "org.apache.hadoop.ha.ShellCommandFencer:abbreviate(java.lang.String,int)": "/**\n* Abbreviates a command string to a specified length.\n* @param cmd the command string to abbreviate\n* @param len the maximum length of the abbreviated string\n* @return abbreviated string or original if no abbreviation is needed\n*/",
        "org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String)": "/**\n* Validates input arguments for the 'shell' fencing method.\n* @param args command-line arguments to check\n* @throws BadFencingConfigurationException if args is null or empty\n*/",
        "org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)": "/**\n* Adds target info as environment variables based on HA service state.\n* @param target HA service target with state information\n* @param environment map to store environment variables\n*/",
        "org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map)": "/**\n* Sets environment variables from configuration pairs.\n* @param env map to store environment variable key-value pairs\n*/",
        "org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)": "/**\n* Executes a fencing command and logs its output.\n* @param target HA service target for the command\n* @param args command arguments to parse\n* @return true if command executed successfully, false otherwise\n*/"
    },
    "org.apache.hadoop.io.ByteBufferPool": {
        "org.apache.hadoop.io.ByteBufferPool:release()": "/**\n* Releases resources held by the implementing class.\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:suffix()": "/**\n* Returns the suffix as a String for the implementing class.\n* @return the suffix string\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:unit()": "/**\n* Returns the time unit associated with this instance.\n* @return TimeUnit representing the unit of time\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:suffix()": "/**\n* Abstract method to retrieve the suffix as a string.\n* @return the suffix string\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:unit()": "/**\n* Returns the time unit associated with the implementation.\n* @return TimeUnit representing the specific time unit\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:suffix()": "/**\n* Returns the suffix as a String.\n* @return the suffix of the implementing class\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:unit()": "/**\n* Returns the time unit associated with this instance.\n* @return TimeUnit representing the unit of time\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:suffix()": "/**\n* Returns the string suffix for the implementing class.\n* @return the suffix as a String\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:unit()": "/**\n* Returns the time unit associated with the implementation.\n* @return TimeUnit representing the unit of time\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:suffix()": "/**\n* Returns a string representing a suffix.\n* @return the suffix as a String\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:unit()": "/**\n* Retrieves the time unit associated with the implementation.\n* @return TimeUnit representing the unit of time\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:suffix()": "/**\n* Returns the string suffix specific to the implementing class.\n* @return suffix as a String\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:unit()": "/**\n* Returns the time unit associated with the implementation.\n* @return TimeUnit instance representing the unit of time\n*/"
    },
    "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7": {
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:suffix()": "/**\n* Returns the string suffix for the implementing class.\n* @return the suffix as a String\n*/",
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:unit()": "/**\n* Returns the time unit associated with the implementation.\n* @return TimeUnit representing the unit of time\n*/"
    },
    "org.apache.hadoop.fs.http.HttpFileSystem": {
        "org.apache.hadoop.fs.http.HttpFileSystem:getScheme()": "/**\n* Returns the scheme used for URLs.\n* @return String representing the URL scheme, always \"http\"\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Creates a file at the specified path with given permissions.\n* @param path file path to create\n* @param fsPermission permissions for the file\n* @param b flag for overwrite\n* @param i buffer size\n* @param i1 replication factor\n* @param l block size\n* @param progressable progress callback\n* @throws IOException if operation is not supported\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param path file path to append data\n* @param i unused parameter\n* @param progressable callback for progress updates\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory but always throws UnsupportedOperationException.\n* @param path current file or directory path\n* @param path1 new file or directory path\n* @return always false due to exception\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes the specified path; always throws UnsupportedOperationException.\n* @param path the path to delete\n* @param b flag indicating deletion options\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for the given path.\n* @param path the directory path to list statuses for\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories at the specified path with given permissions.\n* @param path directory path to create\n* @param fsPermission permissions for the new directory\n* @return false as the implementation is not complete\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory()": "/**\n* Returns the predefined working directory path.\n* @return Path object representing the working directory\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory to the specified path.\n* @param path the new working directory path\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:getUri()": "/**\n* Returns the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a stream to a file at the given path with specified buffer size.\n* @param path file location\n* @param bufferSize size of the buffer\n* @return FSDataInputStream for reading the file\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Returns a FileStatus object for the specified path.\n* @param path the file path to get status for\n* @return a new FileStatus object with default attributes\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the object with a URI and configuration.\n* @param name URI for initialization\n* @param conf configuration object\n*/",
        "org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the path supports a specific capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if capability is supported, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.http.HttpsFileSystem": {
        "org.apache.hadoop.fs.http.HttpsFileSystem:getScheme()": "/**\n* Returns the scheme used for URLs.\n* @return String representing the URL scheme, always \"https\"\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)": "/**\n* Throws UnsupportedOperationException for file creation.\n* @param path file path to create\n* @param fsPermission permissions for the file\n* @param b flag for overwrite\n* @param i buffer size\n* @param i1 replication factor\n* @param l block size\n* @param progressable progress callback\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)": "/**\n* Appends data to a file at the specified path.\n* @param path file path to append data\n* @param i unused parameter\n* @param progressable callback for progress updates\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Renames a file or directory, always throws UnsupportedOperationException.\n* @param path current path, ignored due to exception\n* @param path1 new path, ignored due to exception\n* @return always false due to exception\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)": "/**\n* Deletes the specified path.\n* @param path the path to delete\n* @param b flag indicating deletion options\n* @return always throws UnsupportedOperationException\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path)": "/**\n* Lists file statuses for the given path, but operation is unsupported.\n* @param path the directory path to list statuses for\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Creates directories specified by the path with given permissions.\n* @param path directory path to create\n* @param fsPermission permissions for the new directory\n* @return false as the implementation is not complete\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory()": "/**\n* Returns the predefined working directory path.\n* @return Path object representing the working directory\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)": "/**\n* Sets the working directory to the specified path.\n* @param path the new working directory path\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:getUri()": "/**\n* Retrieves the URI associated with this object.\n* @return URI instance representing the object's URI\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)": "/**\n* Opens a stream to a file at the given path with specified buffer size.\n* @param path file location\n* @param bufferSize size of the buffer\n* @return FSDataInputStream for reading the file\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path)": "/**\n* Returns a FileStatus object for the specified path.\n* @param path the file path to check status\n* @return FileStatus object with default attributes\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes the object with a URI and configuration.\n* @param name URI for initialization\n* @param conf configuration object\n*/",
        "org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Checks if the path supports a specific capability.\n* @param path the Path to check\n* @param capability the capability string to verify\n* @return true if capability is supported, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.FSDataOutputStreamBuilder": {
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getBlockSize()": "/**\n* Retrieves the size of the block.\n* @return the size of the block as a long value\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getBufferSize()": "/**\n* Retrieves the current buffer size.\n* @return the size of the buffer as an integer\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getChecksumOpt()": "/**\n* Retrieves the current checksum option.\n* @return ChecksumOpt object representing the checksum configuration\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFlags()": "/**\n* Retrieves the current set of creation flags.\n* @return EnumSet of CreateFlag representing the flags\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getProgress()": "/**\n* Retrieves the current progress object.\n* @return Progressable instance representing current progress\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getReplication()": "/**\n* Retrieves the current replication value.\n* @return short representing the replication factor\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:isRecursive()": "/**\n* Checks if the current operation is recursive.\n* @return true if recursive, false otherwise\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder()": "/**\n* Returns the current instance of FileSystemDataOutputStreamBuilder.\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int)": "/**\n* Sets the buffer size and returns the builder instance.\n* @param bufSize the size of the buffer to set\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short)": "/**\n* Sets the replication factor and returns the builder instance.\n* @param replica the number of replicas to set\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long)": "/**\n* Sets the block size and returns the builder instance.\n* @param blkSize the size of the block to set\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive()": "/**\n* Enables recursive mode and returns the builder instance.\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:create()": "/**\n* Creates a new instance with the CREATE flag set.\n* @return the builder instance for further configuration\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean)": "/**\n* Sets the overwrite flag and returns the builder instance.\n* @param overwrite true to enable, false to disable overwrite\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:append()": "/**\n* Adds APPEND flag and returns the builder instance.\n* @return the updated builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS()": "/**\n* Retrieves the FileSystem instance after validating it's non-null.\n* @return FileSystem instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission)": "/**\n* Sets the file system permission and returns the builder instance.\n* @param perm the permission to set\n* @return the builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable)": "/**\n* Sets progress handler and returns the builder instance.\n* @param prog the Progressable object to set\n* @return the current builder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt)": "/**\n* Sets checksum options and returns the builder instance.\n* @param chksumOpt options for checksum calculation\n* @return FileSystemDataOutputStreamBuilder instance\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission()": "/**\n* Retrieves file permissions, initializing if not set.\n* @return FsPermission object with current permissions\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)": "/**\n* Initializes FSDataOutputStreamBuilder with file context and path.\n* @param fc FileContext for file operations\n* @param p Path to the file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Initializes FSDataOutputStreamBuilder with file system and path.\n* @param fileSystem the file system to use\n* @param p the path for the output stream\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockManager": {
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:getBlockData()": "/**\n* Retrieves the current block data.\n* @return BlockData object representing the block information\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData)": "/**\n* Initializes BlockManager with provided block data.\n* @param blockData data for the block, must not be null\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)": "/**\n* Releases the provided BufferData; no action taken as new buffer is always allocated.\n* @param data the BufferData to release, must not be null\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int)": "/**\n* Validates blockNumber and requests prefetch (not supported).\n* @param blockNumber index of the block to prefetch\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int)": "/**\n* Retrieves BufferData for a specified block number.\n* @param blockNumber non-negative block identifier\n* @return BufferData object containing block information\n*/"
    },
    "org.apache.hadoop.io.MapFile$Writer": {
        "org.apache.hadoop.io.MapFile$Writer:getIndexInterval()": "/**\n* Retrieves the value of indexInterval.\n* @return the current index interval value\n*/",
        "org.apache.hadoop.io.MapFile$Writer:setIndexInterval(int)": "/**\n* Sets the index interval value.\n* @param interval the new index interval to be set\n*/",
        "org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator)": "/**\n* Creates a ComparatorOption using the provided WritableComparator.\n* @param value the WritableComparator to be used\n* @return a new ComparatorOption instance\n*/",
        "org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class)": "/**\n* Creates an Option for the specified key class type.\n* @param value the class type extending WritableComparable\n* @return KeyClassOption instance for the given class type\n*/",
        "org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)": "/**\n* Creates a compression option for SequenceFile.\n* @param type compression type\n* @param codec compression codec\n* @return CompressionOption instance\n*/",
        "org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable)": "/**\n* Validates key order and updates lastKey with the provided key.\n* @param key the WritableComparable key to check\n* @throws IOException if keys are out of order\n*/",
        "org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable)": "/**\n* Wraps a Progressable in a SequenceFile.Writer.Option.\n* @param value the Progressable object to wrap\n* @return SequenceFile.Writer.Option instance\n*/",
        "org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class)": "/**\n* Creates an Option for SequenceFile.Writer with specified value class type.\n* @param value the class type to be set\n* @return ValueClassOption instance\n*/",
        "org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Wraps compression option creation for SequenceFile.\n* @param type compression type to be used\n* @return CompressionOption instance\n*/",
        "org.apache.hadoop.io.MapFile$Writer:close()": "/**\n* Closes data and index resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)": "/**\n* Appends key-value pair to data and updates index if position changes.\n* @param key the key to append\n* @param val the value to append\n* @throws IOException on I/O errors\n*/",
        "org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)": "/**\n* Sets the index interval in the configuration.\n* @param conf configuration object to update\n* @param interval value for the index interval\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])": "/**\n* Initializes a SequenceFile Writer with specified options and directory.\n* @param conf configuration settings\n* @param dirName directory for file output\n* @param opts optional writer settings\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)": "/**\n* Constructs a deprecated Writer for SequenceFile with specified configuration and classes.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName output directory name\n* @param keyClass class type for the key\n* @param valClass class type for the value\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated Writer for SequenceFile with specified settings.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName output directory path\n* @param keyClass key class type\n* @param valClass value class type\n* @param compress compression type\n* @param progress optional progress indicator\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated SequenceFile Writer with specified configurations and options.\n* @param conf configuration settings\n* @param fs file system for writing\n* @param dirName output directory\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param compress compression type\n* @param codec compression codec\n* @param progress Progressable for tracking progress\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Constructs a deprecated Writer for SequenceFile with specified configurations.\n* @param conf configuration settings\n* @param fs file system to use\n* @param dirName output directory\n* @param keyClass key class type\n* @param valClass value class type\n* @param compress compression type\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)": "/**\n* Constructs a deprecated Writer for SequenceFile with specified configuration and options.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName directory for file output\n* @param comparator comparator for ordering\n* @param valClass class type for values\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Constructs a deprecated SequenceFile Writer with specified settings.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName output directory name\n* @param comparator comparator for sorting\n* @param valClass class type for values\n* @param compress compression type\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated SequenceFile.Writer with specified options.\n* @param conf configuration settings\n* @param fs file system reference\n* @param dirName output directory name\n* @param comparator comparator for sorting\n* @param valClass class type for values\n* @param compress compression type\n* @param progress progressable for tracking\n*/",
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated SequenceFile.Writer with specified configurations and options.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName output directory\n* @param comparator comparator for sorting\n* @param valClass class type of values\n* @param compress compression type\n* @param codec compression codec\n* @param progress progressable object\n* @throws IOException if file system operations fail\n*/"
    },
    "org.apache.hadoop.io.compress.SplitCompressionInputStream": {
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:getAdjustedEnd()": "/**\n* Retrieves the adjusted end value.\n* @return long representing the adjusted end\n*/",
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:getAdjustedStart()": "/**\n* Retrieves the adjusted start value.\n* @return the adjusted start as a long\n*/",
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:setEnd(long)": "/**\n* Sets the end value for the current object.\n* @param end the end value to be set\n*/",
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:setStart(long)": "/**\n* Sets the start time value.\n* @param start the start time in milliseconds\n*/",
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)": "/**\n* Initializes SplitCompressionInputStream with input range.\n* @param in InputStream for compressed data, @param start start position, @param end end position\n*/"
    },
    "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream": {
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:resetState()": "/**\n* Resets the state of the object.\n* @throws IOException if an I/O error occurs during the reset process\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream)": "/**\n* Constructs a BZip2CompressionOutputStream.\n* @param out OutputStream for writing compressed data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader()": "/**\n* Writes stream header if output stream is initialized.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset()": "/**\n* Resets the stream if needed, writing the header and reinitializing the output.\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish()": "/**\n* Completes the stream process, resetting if necessary before finalizing output.\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close()": "/**\n* Closes the stream and releases resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int)": "/**\n* Writes a character to output, resetting stream if needed.\n* @param b character to write\n* @throws IOException if output is closed or an I/O error occurs\n*/",
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)": "/**\n* Writes bytes to output, resetting stream if needed.\n* @param b byte array to write from\n* @param off starting offset in b\n* @param len number of bytes to write\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)": "/**\n* Performs no action as output buffers are already reset.\n* @param decodingState state of the decoding process\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)": "/**\n* No operation for decoding; output buffers are already reset.\n* @param decodingState current state of the decoding process\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs a DummyRawDecoder with specified erasure coding options.\n* @param coderOptions configuration for erasure coding\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)": "/**\n* No operation for encoding; output buffers are already reset.\n* @param encodingState state of the byte array encoding\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)": "/**\n* No operation for encoding as output buffers are already reset.\n* @param encodingState state of the encoding process\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs a DummyRawEncoder with specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2": {
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:isSupported()": "/**\n* Checks if the current implementation is supported.\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec()": "/**\n* Retrieves the CompressionCodec instance.\n* @return CompressionCodec object for compression, or throws IOException on error\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)": "/**\n* Creates a compression output stream.\n* @param downStream the stream to compress data into\n* @param compressor the compressor to use\n* @param downStreamBufferSize size of the buffer for downStream\n* @return OutputStream for compressed data\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)": "/**\n* Creates a decompression stream from the given input stream.\n* @param downStream input stream for compressed data\n* @param decompressor instance for decompression\n* @param downStreamBufferSize buffer size for the stream\n* @return InputStream for reading decompressed data\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep": {
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:calculateSleepTime(int)": "/**\n* Calculates sleep time based on the number of retries.\n* @param retries number of retry attempts\n* @return fixed sleep time in milliseconds\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)": "/**\n* Constructs a retry mechanism with specified limits and sleep duration.\n* @param maxRetries maximum number of retry attempts\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep": {
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:calculateSleepTime(int)": "/**\n* Calculates sleep time based on the number of retries.\n* @param retries number of retry attempts\n* @return calculated sleep time in milliseconds\n*/",
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)": "/**\n* Constructs a retry mechanism with specified limits and sleep parameters.\n* @param maxRetries maximum number of retry attempts\n* @param sleepTime time to wait between retries\n* @param timeUnit unit of time for sleep duration\n*/"
    },
    "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback": {
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:postStop()": "/**\n* Called when the actor is stopped, allowing for cleanup operations.\n*/",
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:preStart()": "/**\n* Prepares the system before starting the main process.\n*/",
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:preStop()": "/**\n* Prepares the object for stopping, performing necessary cleanup actions.\n*/",
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:postStart()": ""
    },
    "org.apache.hadoop.metrics2.lib.MutableCounter": {
        "org.apache.hadoop.metrics2.lib.MutableCounter:info()": "/**\n* Retrieves the MetricsInfo instance.\n* @return MetricsInfo object containing metrics data\n*/",
        "org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo)": "/**\n* Constructs a MutableCounter with specified metrics info.\n* @param info metrics information, must not be null\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableGauge": {
        "org.apache.hadoop.metrics2.lib.MutableGauge:info()": "/**\n* Retrieves the MetricsInfo instance.\n* @return MetricsInfo object containing metrics data\n*/",
        "org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo)": "/**\n* Initializes MutableGauge with provided MetricsInfo.\n* @param info metrics information, must not be null\n*/"
    },
    "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode": {
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:getFs()": "/**\n* Retrieves the ChRootedFileSystem instance.\n* @return ChRootedFileSystem object\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object)": "/**\n* Compares this NodeBase object with another for equality.\n* @param o object to compare with this NodeBase\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode()": "/**\n* Returns the object's hash code.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)": "/**\n* Constructs an NflyNode with host and rack names, and a filesystem.\n* @param hostName name of the host\n* @param rackName name of the rack\n* @param fs ChRootedFileSystem instance for the node\n*/",
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)": "/**** Constructs an NflyNode with host, rack, and filesystem configuration. \n* @param hostName name of the host \n* @param rackName name of the rack \n* @param uri filesystem URI \n* @param conf configuration settings \n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable": {
        "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:setRunRenewalLoop(boolean)": "/**\n* Sets the state of the renewal loop.\n* @param runRenewalLoop true to enable, false to disable the loop\n*/",
        "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run()": "/**** Continually attempts to renew TGT until the thread is terminated. */"
    },
    "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider": {
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getAlgorithm()": "/**\n* Returns the algorithm used in this context.\n* @return String representing the algorithm name\n*/",
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getKeyStoreType()": "/**\n* Returns the type of the keystore.\n* @return a string representing the keystore type\n*/",
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getSchemeName()": "/**\n* Returns the scheme name constant.\n* @return the scheme name as a String\n*/",
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes BouncyCastleFipsKeyStoreProvider with URI and configuration.\n* @param uri keystore URI\n* @param conf configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.security.alias.JavaKeyStoreProvider": {
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getAlgorithm()": "/**\n* Returns the algorithm used by this class.\n* @return constant algorithm string\n*/",
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getKeyStoreType()": "/**\n* Returns the type of the keystore.\n* @return String representing the keystore type\n*/",
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getSchemeName()": "/**\n* Returns the scheme name constant.\n* @return String representing the scheme name\n*/",
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes JavaKeyStoreProvider with URI and configuration.\n* @param uri keystore URI\n* @param conf configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider": {
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getAlgorithm()": "/**\n* Returns the algorithm used in the process.\n* @return the algorithm as a String\n*/",
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getKeyStoreType()": "/**\n* Returns the type of the key store.\n* @return String representing the key store type\n*/",
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getSchemeName()": "/**\n* Returns the name of the scheme.\n* @return a string representing the scheme name\n*/",
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes LocalBouncyCastleFipsKeyStoreProvider with URI and configuration.\n* @param uri the URI of the keystore\n* @param conf the configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider": {
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getAlgorithm()": "/**\n* Returns the algorithm used for processing.\n* @return String representation of the algorithm\n*/",
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getKeyStoreType()": "/**\n* Returns the key store type.\n* @return a string representing the key store type\n*/",
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getSchemeName()": "/**\n* Returns the scheme name.\n* @return constant SCHEME_NAME as a String\n*/",
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes LocalJavaKeyStoreProvider with URI and configuration.\n* @param uri the URI of the keystore\n* @param conf the configuration settings\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.security.authorize.PolicyProvider$1": {
        "org.apache.hadoop.security.authorize.PolicyProvider$1:getServices()": "/**\n* Retrieves an array of available services.\n* @return array of Service objects\n*/"
    },
    "org.apache.hadoop.security.ssl.SSLHostnameVerifier$5": {
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String[],java.lang.String[],java.lang.String[])": "/**\n* Validates SSL hostnames against provided hosts and subject alternative names.\n* @param hosts array of hostnames to check\n* @param cns array of common names to validate\n* @param subjectAlts array of subject alternative names\n* @throws SSLException if validation fails\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:getKind()": "/**\n* Retrieves the kind of the object.\n* @return Text representing the kind\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)": "/**\n* Constructs a DelegationTokenIdentifier with specified owner and renewer.\n* @param kind type of the token\n* @param owner owner of the token\n* @param renewer entity renewing the token\n* @param realUser actual user associated with the token\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text)": "/**\n* Constructs a DelegationTokenIdentifier with specified kind.\n* @param kind type of the delegation token\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)": "/**\n* Checks if the specified token is managed.\n* @param token the token to check\n* @return true if managed, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text)": "/**\n* Checks if the given Text kind equals TOKEN_KIND.\n* @param kind the Text object to compare\n* @return true if equal to TOKEN_KIND, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a KeyProvider from a Token or configuration.\n* @param token authentication token\n* @param conf configuration settings\n* @return KeyProvider instance or null if URI is not set\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)": "/**\n* Renews a delegation token using a provided key provider.\n* @param token the token to renew\n* @param conf configuration settings\n* @return renewed token's expiration time in milliseconds\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)": "/**\n* Cancels a delegation token using the provided token and configuration.\n* @param token the delegation token to cancel\n* @param conf configuration settings for key provider\n* @throws IOException if cancellation fails or key provider is invalid\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell$CreateCommand": {
        "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:getUsage()": "/**\n* Returns usage information as a formatted string.\n* @return String containing usage and description details\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate()": "/**\n* Validates key provider and checks for password requirements.\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute()": "/**\n* Executes key creation and handles potential exceptions during the process.\n* @throws IOException if an I/O error occurs\n* @throws NoSuchAlgorithmException if the algorithm is not available\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyShell$ListCommand": {
        "org.apache.hadoop.crypto.key.KeyShell$ListCommand:getUsage()": "/**\n* Returns usage information and description.\n* @return formatted string of usage and description\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute()": "/**\n* Executes key listing for the provider, optionally including metadata.\n* @throws IOException if an I/O error occurs during execution\n*/",
        "org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate()": "/**\n* Validates key provider availability and retrieves metadata boolean.\n* @return true if provider exists, false otherwise\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell$CheckCommand": {
        "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:getUsage()": "/**\n* Returns usage information including description.\n* @return formatted usage string\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate()": "/**\n* Validates the alias and credential provider; returns false if invalid.\n* @return true if validation succeeds, false otherwise\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute()": "/**\n* Executes credential verification for a given alias.\n* @throws IOException if console access fails or credential retrieval encounters an error\n* @throws NoSuchAlgorithmException if a required algorithm is not available\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialShell$CreateCommand": {
        "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:getUsage()": "/**\n* Returns usage information including description.\n* @return String containing usage and description details\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate()": "/**\n* Validates the alias and checks password requirements.\n* @return true if valid, false otherwise\n*/",
        "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute()": "/**\n* Executes credential creation or displays help.\n* @throws IOException if an I/O error occurs\n* @throws NoSuchAlgorithmException if the algorithm is not found\n*/"
    },
    "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher": {
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:isContextReset()": "/**\n* Checks if the context has been reset.\n* @return true if context is reset, false otherwise\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])": "/**\n* Initializes the cipher with the provided key and IV.\n* @param key encryption key\n* @param iv initialization vector\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**** Processes input buffer and writes output; handles encryption context reset. \n* @param inBuffer source buffer for input data \n* @param outBuffer target buffer for encrypted output \n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)": "/**\n* Initializes OpensslCtrCipher with mode and cipher suite.\n* @param mode operation mode (encrypt/decrypt)\n* @param suite cipher suite for configuration\n* @param engineId optional engine identifier\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Encrypts data from input buffer to output buffer.\n* @param inBuffer source buffer for input data\n* @param outBuffer target buffer for encrypted output\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)": "/**\n* Decrypts data from input buffer to output buffer.\n* @param inBuffer source buffer for encrypted data\n* @param outBuffer target buffer for decrypted output\n*/",
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)": "/**\n* Initializes OpensslCtrCipher with mode and cipher suite.\n* @param mode operation mode (e.g., encrypt or decrypt)\n* @param suite cipher suite for configuration\n* @throws GeneralSecurityException for security-related issues\n*/"
    },
    "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension": {
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:cancelDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Cancels the specified delegation token.\n* @param token the delegation token to cancel\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getCanonicalServiceName()": "/**\n* Returns the canonical service name, currently returns null.\n* @return canonical service name as String or null\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getDelegationToken(java.lang.String)": "/**\n* Retrieves a delegation token for the specified renewer.\n* @param renewer the entity requesting the token\n* @return a Token object or null if not available\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:renewDelegationToken(org.apache.hadoop.security.token.Token)": "/**\n* Renews the given delegation token.\n* @param token the delegation token to renew\n* @return the renewed token's expiration time in milliseconds\n*/",
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:selectDelegationToken(org.apache.hadoop.security.Credentials)": "/**\n* Selects a delegation token based on provided credentials.\n* @param creds user credentials for token selection\n* @return Token object or null if no token is found\n*/"
    },
    "org.apache.hadoop.fs.GlobFilter$1": {
        "org.apache.hadoop.fs.GlobFilter$1:accept(org.apache.hadoop.fs.Path)": "/**\n* Checks if the path matches the pattern and passes the user filter.\n* @param path the Path object to evaluate\n* @return true if both conditions are met, false otherwise\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker": {
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:failed()": "/**\n* Marks the operation as failed.\n*/",
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close()": "/**\n* Closes the operation, updating statistics based on success or failure.\n*/",
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)": "/**\n* Initializes StatisticDurationTracker and increments counter if count is positive.\n* @param iostats statistics store for IO operations\n* @param key unique identifier for the statistic\n* @param count value to increment the counter\n*/",
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)": "/**\n* Initializes StatisticDurationTracker with IOStatisticsStore and key.\n* @param iostats statistics store for IO operations\n* @param key unique identifier for the statistic\n*/",
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString()": "/**\n* Returns a string representation of the duration, including failure info if applicable.\n* @return formatted duration string\n*/"
    },
    "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB": {
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:getUnderlyingProxyObject()": "/**\n* Returns the underlying RPC proxy object.\n* @return the RPC proxy object\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close()": "/**\n* Closes the RPC proxy connection.\n* @param rpcProxy the proxy object to close\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int)": "/**\n* Sends a request to cede active status for a specified duration.\n* @param millisToCede duration in milliseconds to cede active status\n* @throws IOException if an IPC call fails\n* @throws AccessControlException if access is denied\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover()": "/**\n* Initiates a graceful failover process via RPC.\n* @throws IOException if the IPC call fails\n* @throws AccessControlException if access is denied\n*/",
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)": "/**\n* Initializes ZKFCProtocolClientSideTranslatorPB with address and configuration settings.\n* @param addr server address for RPC\n* @param conf configuration settings\n* @param socketFactory factory for socket creation\n* @param timeout timeout for RPC calls\n*/"
    },
    "org.apache.hadoop.io.compress.Lz4Codec": {
        "org.apache.hadoop.io.compress.Lz4Codec:getConf()": "/**\n* Retrieves the current configuration.\n* @return Configuration object representing the current settings\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets the configuration object.\n* @param conf Configuration to be set\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:getCompressorType()": "/**\n* Returns the compressor class type.\n* @return Class of the compressor, specifically Lz4Compressor\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:getDecompressorType()": "/**\n* Returns the class type of the decompressor.\n* @return Class of the Lz4Decompressor\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:getDefaultExtension()": "/**\n* Returns the default file extension for the codec.\n* @return String representing the codec's default extension\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream for the given OutputStream.\n* @param out the OutputStream to write compressed data\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream from the provided InputStream.\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a CompressionOutputStream for LZ4 compression.\n* @param out output stream for compressed data\n* @param compressor compressor to use\n* @return CompressionOutputStream for writing compressed data\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream for decompression.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance\n* @return CompressionInputStream for reading decompressed data\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:createDecompressor()": "/**\n* Creates an Lz4Decompressor with a buffer size from configuration.\n* @return Decompressor instance for LZ4 decompression\n*/",
        "org.apache.hadoop.io.compress.Lz4Codec:createCompressor()": "/**\n* Creates an Lz4Compressor with configured buffer size and compression type.\n* @return Lz4Compressor instance\n*/"
    },
    "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor": {
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:decompress(byte[],int,int)": "/**\n* Decompresses a byte array segment.\n* @param b byte array to decompress\n* @param off offset to start decompression\n* @param len length of data to decompress\n* @return number of bytes decompressed\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:end()": "/**\n* Ends the current process or operation without returning a value.\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:finished()": "/**\n* Indicates if the process is finished.\n* @return false, indicating the process is not complete\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:getRemaining()": "/**\n* Returns the number of remaining items.\n* @return int representing remaining items, always 0\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsDictionary()": "/**\n* Indicates if a dictionary is required.\n* @return false as a dictionary is not needed\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsInput()": "/**\n* Indicates if user input is required.\n* @return false, as input is not needed\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:reset()": "/**\n* Resets the state of the object to its initial configuration.\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:setDictionary(byte[],int,int)": "/**\n* Sets the dictionary using a byte array from a specified offset and length.\n* @param b byte array containing dictionary data\n* @param off starting offset in the byte array\n* @param len number of bytes to use from the array\n*/",
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:setInput(byte[],int,int)": "/**\n* Sets input data from a byte array.\n* @param b byte array containing input data\n* @param off offset to start reading from\n* @param len number of bytes to read\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory": {
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCodecName()": "/**\n* Retrieves the name of the codec.\n* @return String representing the codec name\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCoderName()": "/**\n* Retrieves the name of the coder.\n* @return the coder's name as a String\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder using specified erasure coding options.\n* @param coderOptions configuration for erasure coding\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder using specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n* @return a new DummyRawEncoder instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory": {
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCodecName()": "/**\n* Returns the name of the codec used for erasure coding.\n* @return String representing the codec name\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCoderName()": "/**\n* Returns the name of the coder.\n* @return String representing the coder's name\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder using specified coding options.\n* @param coderOptions configuration for erasure coding\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder using the specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n* @return a new RawErasureEncoder instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory": {
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCodecName()": "/**\n* Returns the name of the codec used for erasure coding.\n* @return codec name as a String\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCoderName()": "/**\n* Retrieves the name of the coder.\n* @return the coder's name as a String\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder using specified coding options.\n* @param coderOptions configuration for erasure coding\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder using specified coding options.\n* @param coderOptions settings for erasure coding\n* @return RawErasureEncoder instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCodecName()": "/**\n* Retrieves the name of the codec.\n* @return String representing the codec name\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCoderName()": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder using specified coder options.\n* @param coderOptions configuration options for the erasure coder\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder with specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n* @return a new RawErasureEncoder instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCodecName()": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCoderName()": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder using specified ErasureCoderOptions.\n* @param coderOptions configuration options for the erasure coder\n* @return a new RSRawDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates an erasure encoder using specified coder options.\n* @param coderOptions configuration for the erasure coder\n* @return RawErasureEncoder instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory": {
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCodecName()": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCoderName()": "",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureDecoder using specified coding options.\n* @param coderOptions configuration for erasure coding\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Creates a RawErasureEncoder using specified coding options.\n* @param coderOptions settings for the erasure coder\n* @return a new XORRawEncoder instance\n*/"
    },
    "org.apache.hadoop.io.retry.RetryPolicies$RetryForever": {
        "org.apache.hadoop.io.retry.RetryPolicies$RetryForever:shouldRetry(java.lang.Exception,int,int,boolean)": ""
    },
    "org.apache.hadoop.ipc.DefaultCostProvider": {
        "org.apache.hadoop.ipc.DefaultCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)": "",
        "org.apache.hadoop.ipc.DefaultCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)": ""
    },
    "org.apache.hadoop.ipc.WritableRpcEngine$Invoker": {
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:getConnectionId()": "",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close()": "/**\n* Closes the client if not already closed, stopping it and releasing resources.\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes a method on a proxy, tracing execution time and RPC call details.\n* @param proxy the proxy instance\n* @param method the method to invoke\n* @param args arguments for the method\n* @return result of the invoked method\n* @throws Throwable if an error occurs during invocation\n*/",
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Initializes an Invoker with connection settings and authentication context.\n* @param protocol RPC protocol class\n* @param address server address for the connection\n* @param ticket user credentials for authentication\n* @param conf configuration settings for the client\n* @param factory socket factory for connections\n* @param rpcTimeout timeout for RPC calls\n* @param fallbackToSimpleAuth flag for simple auth fallback\n* @param alignmentContext context for alignment operations\n*/"
    },
    "org.apache.hadoop.metrics2.sink.StatsDSink": {
        "org.apache.hadoop.metrics2.sink.StatsDSink:flush()": "",
        "org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**\n* Initializes configuration for StatsD with server details and optional hostname.\n* @param conf configuration settings for StatsD\n*/",
        "org.apache.hadoop.metrics2.sink.StatsDSink:close()": "/**\n* Closes the statsd connection.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String)": "/**** Writes a metric line to StatsD, logging errors and throwing MetricsException on failure. \n* @param line metric data to send \n*/",
        "org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Processes and sends metrics data from a MetricsRecord object.\n* @param record contains metrics information to be sent\n*/"
    },
    "org.apache.hadoop.fs.FileRange": {
        "org.apache.hadoop.fs.FileRange:createFileRange(long,int)": "",
        "org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)": ""
    },
    "org.apache.hadoop.net.SocketInputWrapper": {
        "org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)": "",
        "org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel()": "/**\n* Retrieves a readable byte channel after validating channel existence.\n* @return ReadableByteChannel associated with the socket\n*/",
        "org.apache.hadoop.net.SocketInputWrapper:setTimeout(long)": "/**\n* Sets the timeout for the socket input stream.\n* @param timeoutMs timeout duration in milliseconds\n* @throws SocketException if an error occurs while setting the timeout\n*/"
    },
    "org.apache.hadoop.security.alias.KeyStoreProvider": {
        "org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists()": "",
        "org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions()": "/**\n* Saves original file permissions for potential restoration.\n* @throws IOException if an I/O error occurs while accessing the file system\n*/",
        "org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String)": "/**\n* Creates FsPermission from a mode string.\n* @param perms string representation of permissions\n*/",
        "org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a KeyStoreProvider with specified URI and configuration.\n* @param uri the URI of the keystore\n* @param conf the configuration settings\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile()": "/**\n* Retrieves an InputStream for a file at a specified path.\n* @return InputStream for the file\n* @throws IOException if the file cannot be opened\n*/",
        "org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore()": "/**\n* Creates and returns an output stream for the keystore.\n* @return OutputStream for writing to the keystore file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI)": "/**\n* Initializes the file system with the given URI.\n* @param uri the URI for the file system to initialize\n*/"
    },
    "org.apache.hadoop.fs.viewfs.ViewFsFileStatus": {
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime()": "",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication()": "/**\n* Returns the block replication factor.\n* @return short representing the replication factor\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize()": "/**\n* Returns the size of the block from the file system.\n* @return block size as a long value\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime()": "",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission()": "",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner()": "",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup()": "/**\n* Returns the group associated with the instance.\n* @return the group as a String\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory()": "/**\n* Returns true if the current object is a directory.\n* @return true if it's a directory, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink()": "",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen()": "",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile()": "/**\n* Checks if the current object is a file.\n* @return true if it is a file, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink()": "/**\n* Retrieves the symbolic link path.\n* @return Path of the symbolic link\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object)": "/**\n* Compares this FileStatus object with another for equality.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode()": "/**\n* Returns the hash code of the current object.\n* @return hash code as an integer\n*/",
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)": "/**\n* Initializes ViewFsFileStatus with FileStatus and a new Path.\n* @param fs FileStatus object representing file details\n* @param newPath Path object representing the modified file location\n*/"
    },
    "org.apache.hadoop.fs.FsUrlConnection": {
        "org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)": "",
        "org.apache.hadoop.fs.FsUrlConnection:connect()": "/**\n* Connects to a file system using a URI.\n* @throws IOException if connection or file opening fails\n*/",
        "org.apache.hadoop.fs.FsUrlConnection:getInputStream()": "/**\n* Retrieves an InputStream; connects if not already established.\n* @return InputStream object\n* @throws IOException if connection fails\n*/"
    },
    "org.apache.hadoop.fs.FSInputStream": {
        "org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)": "/**\n* Validates arguments for reading from a buffer.\n* @param position starting position in the buffer\n* @param buffer byte array to read from\n* @param offset starting index in the buffer\n* @param length number of bytes to read\n* @throws EOFException if position is negative\n*/",
        "org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)": "/**\n* Reads bytes from a specified position into a buffer.\n* @param position start position in the source\n* @param buffer destination byte array\n* @param offset start index in the buffer\n* @param length number of bytes to read\n* @return number of bytes read or -1 on EOF\n*/",
        "org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)": "/**\n* Reads specified bytes into a buffer from a given position.\n* @param position start position in the source\n* @param buffer destination byte array\n* @param offset start index in the buffer\n* @param length number of bytes to read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])": "/**\n* Reads entire buffer from the specified position.\n* @param position start position in the source\n* @param buffer destination byte array\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FSInputStream:toString()": "/**\n* Returns a string representation of the object, including IOStatistics if applicable.\n* @return formatted string representation\n*/"
    },
    "org.apache.hadoop.fs.impl.PathCapabilitiesSupport": {
        "org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)": "/**\n* Validates path and capability parameters.\n* @param path the Path object to validate\n* @param capability the capability string to validate\n* @return lowercase capability string\n*/"
    },
    "org.apache.hadoop.fs.PathAccessDeniedException": {
        "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String)": "",
        "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathAccessDeniedException with specified details.\n* @param path the file path related to the error\n* @param error the error message\n* @param cause the underlying cause of the exception\n*/",
        "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathAccessDeniedException with specified path and cause.\n* @param path the file path related to the error\n* @param cause the underlying cause of the exception\n*/"
    },
    "org.apache.hadoop.fs.PathPermissionException": {
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String)": "/**\n* Constructs a PathPermissionException for a specified file path.\n* @param path the file path causing the exception\n*/",
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a PathPermissionException with a specified path and error message.\n* @param path the file path causing the exception\n* @param error the error message to be displayed\n*/",
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)": "",
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathPermissionException with specified path and cause.\n* @param path the file path related to the error\n* @param cause the underlying cause of the exception\n*/"
    },
    "org.apache.hadoop.fs.PathNotFoundException": {
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String)": "",
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a PathNotFoundException with a specified path and error message.\n* @param path the file path causing the exception\n* @param error the error message to be displayed\n*/",
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathNotFoundException with specified path, error message, and cause.\n* @param path the file path related to the error\n* @param error the error message\n* @param cause the underlying cause of the exception\n*/",
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a PathNotFoundException with specified path and cause.\n* @param path the file path related to the error\n* @param cause the underlying cause of the exception\n*/"
    },
    "org.apache.hadoop.fs.PathExistsException": {
        "org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String)": "/**\n* Constructs a PathExistsException with a specified file path.\n* @param path the file path that caused the exception\n*/",
        "org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a PathExistsException with a specified path and error message.\n* @param path the file path causing the exception\n* @param error the error message to be displayed\n*/"
    },
    "org.apache.hadoop.fs.ClosedIOException": {
        "org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a ClosedIOException with a specified path and message.\n* @param path the file path causing the exception\n* @param message the error message to be displayed\n*/"
    },
    "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand": {
        "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData)": "",
        "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes path data to manage ACLs based on specified options.\n* @param item PathData containing file system path and ACL information\n* @throws IOException if an I/O error occurs during ACL operations\n*/",
        "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList)": "/****\n* Processes command-line options for ACL modifications.\n* @param args list of command-line arguments\n* @throws IOException if an ACL specification is missing or invalid\n*/"
    },
    "org.apache.hadoop.fs.shell.FsUsage$Du": {
        "org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item; recurses if it's a directory and not in summary mode.\n* @param item the PathData object to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean)": "/**\n* Sets the human-readable flag.\n* @param humanReadable true for human-readable format, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)": "/**\n* Sets the usages table for the current object.\n* @param usagesTable the TableBuilder instance to set\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable()": "/**\n* Retrieves the usages table builder instance.\n* @return TableBuilder object for usages\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and updates internal flags.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList)": "/**** Processes command line arguments and initializes the usages table based on conditions. */",
        "org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes file path data and updates usages table with size and space information.\n* @param item PathData containing file system and path details\n*/"
    },
    "org.apache.hadoop.fs.permission.ChmodParser": {
        "org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus)": "/**\n* Applies new permissions to a file.\n* @param file FileStatus object representing the file\n* @return short representation of the updated permissions\n*/",
        "org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String)": "/**\n* Constructs a ChmodParser with a permission mode string.\n* @param modeStr permission mode string\n* @throws IllegalArgumentException if modeStr is invalid\n*/"
    },
    "org.apache.hadoop.fs.impl.FileSystemMultipartUploader": {
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List)": "/**\n* Calculates total length of files from given path handles.\n* @param partHandles list of file paths\n* @return total length as a long\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path)": "/**\n* Initiates a file upload and returns a CompletableFuture of UploadHandle.\n* @param filePath path of the file to upload\n* @return CompletableFuture containing the upload handle\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)": "/**\n* Uploads a part of a file for a given upload handle.\n* @param uploadId unique identifier for the upload\n* @param partNumber part sequence number\n* @param filePath path to the file\n* @param inputStream input stream of the file part\n* @param lengthInBytes size of the file part in bytes\n* @return CompletableFuture containing PartHandle object\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)": "/**\n* Completes an upload and returns a CompletableFuture of PathHandle.\n* @param uploadId identifier for the upload\n* @param filePath path of the file to complete\n* @param handleMap mapping of part indices to PartHandle\n* @return CompletableFuture containing the PathHandle result\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path)": "/**** Retrieves a PathHandle for a given file path. \n* @param filePath the path of the file \n* @return PathHandle object for the file \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)": "/**\n* Aborts an upload by deleting the associated file.\n* @param uploadId identifier for the upload\n* @param filePath path of the file to check and delete\n* @return CompletableFuture indicating completion\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)": "/**\n* Uploads a part of a file and returns its PartHandle.\n* @param filePath path to the file being uploaded\n* @param inputStream input stream of the file part\n* @param partNumber part number of the upload\n* @param uploadId unique identifier for the upload\n* @param lengthInBytes length of the part in bytes\n* @return PartHandle for the uploaded part\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)": "/**\n* Constructs a FileSystemMultipartUploader with specified builder and filesystem.\n* @param builder configuration for uploader; @param fs filesystem for uploads\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path)": "/**\n* Creates a collector Path using a UUID and the parent of the given filePath.\n* @param filePath the original Path to derive the collector Path from\n* @return new Path object for the collector\n*/",
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)": "/**\n* Completes a multipart upload by merging parts into a file.\n* @param multipartUploadId identifier for the upload\n* @param filePath destination file path\n* @param handleMap mapping of part indexes to PartHandles\n* @return PathHandle for the completed file\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableRates": {
        "org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry)": "/**** Initializes MutableRates with a non-null MetricsRegistry. \n* @param registry metrics registry to track rates \n*/",
        "org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures metrics snapshot into the builder.\n* @param rb MetricsRecordBuilder for storing the snapshot\n* @param all whether to include all metrics\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class)": "/**\n* Initializes protocol by caching it and creating rate metrics for its methods.\n* @param protocol the protocol class to initialize\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)": "/**** Adds elapsed time to a metric by name. \n* @param name metric name, @param elapsed time to add \n*/"
    },
    "org.apache.hadoop.util.GcTimeMonitor": {
        "org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)": "/**\n* Initializes GcTimeMonitor with specified parameters for GC time tracking.\n* @param observationWindowMs duration for observation in milliseconds\n* @param sleepIntervalMs interval between checks in milliseconds\n* @param maxGcTimePercentage maximum allowed GC time percentage\n* @param alertHandler handler for GC time alerts\n*/",
        "org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval()": "/**\n* Calculates GC time percentage within the observation interval.\n* Updates current data with GC metrics and timestamps. \n*/",
        "org.apache.hadoop.util.GcTimeMonitor:getLatestGcData()": "/**\n* Retrieves the latest GcData instance.\n* @return cloned GcData object from current data\n*/",
        "org.apache.hadoop.util.GcTimeMonitor:run()": "/**\n* Monitors GC time and triggers alerts if thresholds are exceeded.\n* @param sleepIntervalMs duration to pause between checks\n*/"
    },
    "org.apache.hadoop.fs.statistics.DurationTrackerFactory": {
        "org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)": "/**\n* Tracks duration using a specified key and count.\n* @param key identifier for the duration tracking\n* @param count number of occurrences to track\n* @return DurationTracker instance for tracking durations\n*/",
        "org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String)": "/**\n* Tracks duration using a specified key with a default count of 1.\n* @param key identifier for the duration tracking\n* @return DurationTracker instance for tracking durations\n*/"
    },
    "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder": {
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance()": "/**** Retrieves the active DynamicIOStatistics instance. \n* @return DynamicIOStatistics instance; throws IllegalStateException if not built \n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build()": "/**\n* Builds and retrieves DynamicIOStatistics, then nullifies the instance.\n* @return DynamicIOStatistics instance; throws IllegalStateException if not built\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)": "/**\n* Adds a long function counter to the builder.\n* @param key unique identifier for the function\n* @param eval function to convert String to long\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)": "/**\n* Configures a long function gauge for the builder.\n* @param key unique identifier for the gauge function\n* @param eval function to evaluate the gauge as long\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)": "/**\n* Adds a long function minimum to the builder.\n* @param key unique identifier for the function\n* @param eval function to be associated with the key\n* @return the current DynamicIOStatisticsBuilder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)": "/**\n* Adds a maximum function for long evaluation.\n* @param key unique identifier for the function\n* @param eval function to evaluate long values\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)": "/**\n* Adds a mean statistic function to the builder.\n* @param key unique identifier for the function\n* @param eval function to evaluate the mean statistic\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)": "/**\n* Adds an AtomicLong counter to the builder.\n* @param key unique identifier for the counter\n* @param source AtomicLong providing the counter value\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)": "/**\n* Adds an atomic integer counter to the builder.\n* @param key unique identifier for the counter\n* @param source atomic integer source for counter value\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)": "/**\n* Adds a mutable counter to the builder.\n* @param key unique identifier for the counter\n* @param source MutableCounterLong providing the counter value\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)": "/**\n* Configures an atomic long gauge for the statistics builder.\n* @param key unique identifier for the gauge\n* @param source AtomicLong to retrieve the gauge value\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)": "/**\n* Configures an AtomicInteger gauge for the builder.\n* @param key unique identifier for the gauge\n* @param source AtomicInteger source for gauge value\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)": "/**\n* Adds an AtomicLong minimum function to the builder.\n* @param key unique identifier for the function\n* @param source AtomicLong source for minimum value\n* @return current DynamicIOStatisticsBuilder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)": "/**\n* Adds an AtomicInteger minimum to the builder.\n* @param key unique identifier for the function\n* @param source AtomicInteger providing the minimum value\n* @return current DynamicIOStatisticsBuilder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)": "/**\n* Sets a maximum function using an AtomicLong source.\n* @param key unique identifier for the function\n* @param source AtomicLong to evaluate maximum value\n* @return this builder instance\n*/",
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)": "/**\n* Sets maximum AtomicInteger value for the given key.\n* @param key unique identifier for the function\n* @param source AtomicInteger source for value retrieval\n* @return this builder instance\n*/"
    },
    "org.apache.hadoop.ha.HealthMonitor$MonitorDaemon": {
        "org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run()": "/**** Runs a loop performing health checks until interrupted. */"
    },
    "org.apache.hadoop.ipc.metrics.RetryCacheMetrics": {
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit()": "/**\n* Retrieves the number of cache hits.\n* @return long value representing cache hit count\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared()": "/**\n* Retrieves the number of cache clears.\n* @return long value representing cache cleared count\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated()": "/**\n* Retrieves the updated cache timestamp.\n* @return long value representing the cache update time\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit()": "/**\n* Increments the cache hit count.\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared()": "/**\n* Increments the cache cleared count.\n* @return void\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated()": "/**\n* Increments the cache updated value.\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache)": "/**\n* Initializes RetryCacheMetrics with a retry cache's name.\n* @param retryCache the RetryCache to derive the name from\n*/",
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache)": "/**\n* Creates and registers RetryCacheMetrics from a given RetryCache.\n* @param cache the RetryCache to derive metrics from\n* @return registered RetryCacheMetrics instance\n*/"
    },
    "org.apache.hadoop.fs.statistics.IOStatisticsContext": {
        "org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled()": "/**\n* Checks if thread-level I/O statistics are enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext()": "/**\n* Retrieves the current IOStatisticsContext, ensuring it's not null.\n* @return IOStatisticsContext for the current thread\n*/",
        "org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext)": "/**\n* Sets the IOStatisticsContext for the current thread.\n* @param statisticsContext context to set or null to remove\n*/"
    },
    "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum": {
        "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)": "",
        "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>()": "/**\n* Constructs MD5MD5CRC32GzipFileChecksum with default values.\n*/"
    },
    "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum": {
        "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)": "/**\n* Constructs a checksum object for MD5 and CRC32 using specified parameters.\n* @param bytesPerCRC bytes processed per CRC calculation\n* @param crcPerBlock number of CRCs per block\n* @param md5 MD5 hash for the file\n*/",
        "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>()": "/**\n* Constructs a MD5MD5CRC32CastagnoliFileChecksum object with default parameters.\n*/"
    },
    "org.apache.hadoop.fs.GlobFilter": {
        "org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path)": "/**\n* Checks if the path matches the pattern and passes the user filter.\n* @param path the Path object to evaluate\n* @return true if both conditions are met, false otherwise\n*/",
        "org.apache.hadoop.fs.GlobFilter:hasPattern()": "/**\n* Checks if the pattern has a wildcard.\n* @return true if wildcard is enabled, false otherwise\n*/",
        "org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)": "/**\n* Initializes user filter and glob pattern from file pattern.\n* @param filePattern pattern for file matching\n* @param filter filter applied to paths\n* @throws IOException if the file pattern is invalid\n*/",
        "org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String)": "/**\n* Constructs a GlobFilter with the specified file pattern.\n* @param filePattern pattern for file matching\n* @throws IOException if the file pattern is invalid\n*/",
        "org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)": "/**\n* Constructs a GlobFilter with specified pattern and path filter.\n* @param filePattern pattern for file matching\n* @param filter filter applied to paths\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.fs.sftp.SFTPFileSystem$2": {
        "org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close()": ""
    },
    "org.apache.hadoop.fs.DU": {
        "org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)": "",
        "org.apache.hadoop.fs.DU:refresh()": "",
        "org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)": "/**\n* Constructs DU object from builder parameters.\n* @param builder CachingGetSpaceUsed.Builder instance to extract configuration values\n*/",
        "org.apache.hadoop.fs.DU:main(java.lang.String[])": "/**\n* Main method to initialize and display disk usage information.\n* @param args command-line arguments for setting the file path\n*/"
    },
    "org.apache.hadoop.fs.CreateFlag": {
        "org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet)": "/**\n* Validates CreateFlag options; throws exception for invalid states.\n* @param flag set of CreateFlag options\n*/",
        "org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)": "/**\n* Validates file creation options based on path existence and flags.\n* @param path file path to validate\n* @param pathExists indicates if the path already exists\n* @param flag set of CreateFlag options for file creation\n* @throws IOException if validation fails\n*/",
        "org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet)": "/**\n* Validates CreateFlag for append operation.\n* @param flag set of CreateFlag options to validate\n*/"
    },
    "org.apache.hadoop.fs.XAttrSetFlag": {
        "org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)": "/**\n* Validates XAttr existence and required flags.\n* @param xAttrName name of the XAttr\n* @param xAttrExists indicates if the XAttr exists\n* @param flag set of flags for validation\n* @throws IOException if validation fails\n*/"
    },
    "org.apache.hadoop.fs.InvalidPathException": {
        "org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String)": "/**\n* Constructs an InvalidPathException with a specified path.\n* @param path the invalid path name causing the exception\n*/",
        "org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs an InvalidPathException with a path and optional reason.\n* @param path the invalid path\n* @param reason the reason for the invalidity, may be null\n*/"
    },
    "org.apache.hadoop.log.LogLevel$CLI": {
        "org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)": "/**\n* Parses arguments for the -getlevel command.\n* @param args command-line arguments\n* @param index starting index for parsing\n* @return next index after processing\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)": "/**\n* Parses set level command arguments and validates them.\n* @param args command-line arguments\n* @param index starting index in args\n* @return next index after processing\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)": "/**\n* Parses protocol arguments and validates them.\n* @param args command line arguments\n* @param index starting index for protocol\n* @return next index after protocol\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[])": "/**\n* Parses command-line arguments and validates operations and protocol.\n* @param args command-line arguments\n* @throws HadoopIllegalArgumentException if validation fails\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes CLI with the provided configuration.\n* @param conf configuration object for CLI setup\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL)": "/**\n* Establishes a URL connection with optional SSL configuration.\n* @param url the URL to connect to\n* @return URLConnection object\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String)": "/**\n* Processes the given URL string and reads lines from the connection.\n* @param urlString the URL to connect and read from\n* @throws Exception if an error occurs during processing\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:doGetLevel()": "/**\n* Constructs a log level URL and processes it.\n* @throws Exception if an error occurs during processing\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:doSetLevel()": "/**\n* Sets the log level by processing a constructed URL.\n* @throws Exception if an error occurs during processing\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest()": "/**\n* Sends a log level request based on the operation type.\n* @throws HadoopIllegalArgumentException if operation is invalid\n* @throws Exception for other processing errors\n*/",
        "org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[])": "/**\n* Runs the main process after parsing arguments and sending log level request.\n* @param args command-line arguments\n* @return 0 on success, -1 on argument parsing error\n*/"
    },
    "org.apache.hadoop.util.ZKUtil$BadAclFormatException": {
        "org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String)": "/****\n* Constructs a BadAclFormatException with a specified message.\n* @param message detail message for the exception\n*/"
    },
    "org.apache.hadoop.util.ZKUtil$BadAuthFormatException": {
        "org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String)": "/**\n* Constructs a BadAuthFormatException with a specified message.\n* @param message detail message for the exception\n*/"
    },
    "org.apache.hadoop.fs.shell.TouchCommands$Touch": {
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData)": "/**\n* Updates file access and modification times based on provided PathData.\n* @param item PathData containing file path and filesystem\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from the provided argument list.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData)": "/**\n* Creates a file if it doesn't exist and updates its timestamp if specified.\n* @param item PathData containing file path and filesystem\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a file path by touching it to update its timestamp.\n* @param item PathData containing file path and filesystem\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a nonexistent path; throws if parent doesn't exist, else touches the path.\n* @param item PathData representing the file path\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.ftp.FTPFileSystem$1": {
        "org.apache.hadoop.fs.ftp.FTPFileSystem$1:close()": "/**\n* Closes the output stream, releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$DirListingIterator": {
        "org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext()": "/**\n* Checks if there are more entries to iterate over.\n* @return true if more entries exist, false otherwise\n*/",
        "org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore()": "/**\n* Fetches more directory entries using a pagination token.\n* @throws IOException if an I/O error occurs during fetching\n*/",
        "org.apache.hadoop.fs.FileSystem$DirListingIterator:next()": "/**\n* Returns the next entry in the iterator.\n* @return next entry of type T\n* @throws IOException if an I/O error occurs\n* @throws NoSuchElementException if no more items exist\n*/"
    },
    "org.apache.hadoop.util.Timer": {
        "org.apache.hadoop.util.Timer:now()": "/**\n* Returns the current time in milliseconds since epoch.\n* @return current time in milliseconds\n*/",
        "org.apache.hadoop.util.Timer:monotonicNow()": "/**\n* Retrieves current time in milliseconds from a monotonic clock.\n* @return time in milliseconds\n*/",
        "org.apache.hadoop.util.Timer:monotonicNowNanos()": "/**\n* Retrieves the current system time in nanoseconds.\n* @return current time in nanoseconds since the epoch\n*/"
    },
    "org.apache.hadoop.fs.FsShellPermissions": {
        "org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the factory.\n* @param factory CommandFactory to associate commands with classes\n*/"
    },
    "org.apache.hadoop.fs.shell.Test": {
        "org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the command factory.\n* @param factory CommandFactory instance to register commands\n*/",
        "org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and validates test flag input.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)": "/**\n* Tests access permissions for a file path.\n* @param item the path data to check permissions for\n* @param action the action mode to verify permissions\n* @return true if access is granted, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item based on a flag to check various conditions.\n* @param item the PathData object to process\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.SnapshotCommands": {
        "org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with specified names in the CommandFactory.\n* @param factory the CommandFactory instance to register commands with\n*/"
    },
    "org.apache.hadoop.fs.shell.Head": {
        "org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the factory.\n* @param factory CommandFactory instance to register classes with\n*/",
        "org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from the provided arguments.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData)": "/**\n* Dumps file content to output stream up to a specified offset.\n* @param item PathData object representing the file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item; throws exception if it's a directory and dumps its content.\n* @param item PathData object representing the file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String)": "/**\n* Expands argument into a list of PathData objects.\n* @param arg the input argument as a file path\n* @return list of PathData containing the expanded argument\n*/"
    },
    "org.apache.hadoop.fs.shell.Tail": {
        "org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**** Registers command classes with the factory. \n* @param factory CommandFactory instance to register commands \n*/",
        "org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options for follow behavior and sleep interval.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)": "/**\n* Dumps bytes from a file starting at a given offset.\n* @param item file metadata; @param offset starting position in the file\n* @return new position in the file after reading\n*/",
        "org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a file path; throws exception if directory and dumps bytes from offset.\n* @param item file metadata\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String)": "/**\n* Expands a single argument into a list of PathData.\n* @param arg the file path as a string\n* @return List of PathData containing the expanded argument\n*/"
    },
    "org.apache.hadoop.fs.shell.XAttrCommands": {
        "org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the command factory.\n* @param factory CommandFactory instance to register commands\n*/"
    },
    "org.apache.hadoop.fs.shell.Delete": {
        "org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**** Registers command classes with their corresponding names in the CommandFactory. \n* @param factory CommandFactory instance to register classes with \n*/"
    },
    "org.apache.hadoop.fs.shell.TouchCommands": {
        "org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with their associated names.\n* @param factory CommandFactory instance to register commands\n*/"
    },
    "org.apache.hadoop.fs.shell.Mkdir": {
        "org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the command factory.\n* @param factory the CommandFactory to register classes with\n*/",
        "org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from args and sets createParents flag.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item; throws exceptions for non-directory or existing directories.\n* @param item the PathData object to process\n* @throws IOException if the path is not a directory or already exists\n*/",
        "org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a nonexistent path, creating it if parents exist.\n* @param item PathData containing the path to process\n* @throws IOException if the parent path is missing or creation fails\n*/"
    },
    "org.apache.hadoop.fs.shell.Concat": {
        "org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the command factory.\n* @param factory the CommandFactory to register commands with\n*/",
        "org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList)": "/**\n* Processes input paths, validates them, and concatenates source files to a target file.\n* @param args list of PathData containing target and source file paths\n* @throws IOException if validation fails or file operations are unsuccessful\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommands": {
        "org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with their corresponding names in the factory.\n* @param factory CommandFactory instance to associate classes with names\n*/"
    },
    "org.apache.hadoop.fs.shell.MoveCommands": {
        "org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**** Registers command classes with their corresponding names in the command factory. \n* @param factory the CommandFactory instance to register commands with\n*/"
    },
    "org.apache.hadoop.fs.shell.Stat": {
        "org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the CommandFactory.\n* @param factory CommandFactory instance to register commands\n*/",
        "org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes PathData and formats output based on specified format characters.\n* @param item PathData object containing file status information\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and sets recursive flag.\n* @param args list of command-line arguments\n*/"
    },
    "org.apache.hadoop.fs.shell.Display": {
        "org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the CommandFactory.\n* @param factory the CommandFactory to register commands with\n*/"
    },
    "org.apache.hadoop.fs.shell.AclCommands": {
        "org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the CommandFactory.\n* @param factory the CommandFactory to register commands with\n*/"
    },
    "org.apache.hadoop.fs.shell.Truncate": {
        "org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the factory.\n* @param factory CommandFactory instance for adding command classes\n*/",
        "org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and validates the length parameter.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item, truncating it if valid; throws exceptions on errors.\n* @param item the PathData object to process\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Truncate:waitForRecovery()": "/**\n* Waits for recovery by checking and refreshing file statuses in waitList.\n* @throws IOException if an I/O error occurs during status refresh\n*/",
        "org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList)": "/**\n* Processes command arguments and waits for recovery if enabled.\n* @param args list of PathData arguments\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.fs.shell.SetReplication": {
        "org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)": "/**\n* Registers command classes with the factory.\n* @param factory CommandFactory to register commands with\n*/",
        "org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and validates replication count.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item, setting replication if not a symlink or erasure coded.\n* @param item the PathData object to process\n* @throws IOException if an I/O error occurs or symlink is unsupported\n*/",
        "org.apache.hadoop.fs.shell.SetReplication:waitForReplication()": "/**\n* Waits for file replication to complete, printing status updates.\n* @throws IOException if an I/O error occurs during status refresh\n*/",
        "org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList)": "/**\n* Processes command-line arguments and waits for replication if enabled.\n* @param args list of PathData arguments\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End": {
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)": "/**\n* Constructs an End operation from the given Operation object.\n* @param op the Operation to initialize from\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder)": "/**\n* Appends 'E' to StringBuilder and calls superclass summary method.\n* @param sb StringBuilder to append summary to\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo()": "/**\n* Overrides superclass method to prepend debug info with \"***\".\n* @return formatted debug string with added prefix\n*/",
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration()": "/**\n* Calculates the duration between current and operation timestamps in seconds.\n* @return duration in seconds as a double\n*/"
    },
    "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand": {
        "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and validates arguments.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs or validation fails\n*/",
        "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes file path attributes; sets or removes extended attributes.\n* @param item PathData containing file path and filesystem\n* @throws IOException if setting/removing attributes fails\n*/"
    },
    "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal": {
        "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options, throwing an exception for unknown options.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**\n* Moves path data, throwing exception if target exists as a directory.\n* @param src source path data\n* @param target target path data\n* @throws IOException if an error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Deletes the specified path; throws PathIOException on failure.\n* @param src PathData containing path information\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Name$Iname": {
        "org.apache.hadoop.fs.shell.find.Name$Iname:<init>()": "/**\n* Initializes Iname by calling superclass constructor with a new Name object.\n*/"
    },
    "org.apache.hadoop.fs.shell.find.Print$Print0": {
        "org.apache.hadoop.fs.shell.find.Print$Print0:<init>()": "/**\n* Constructs a Print0 instance with a null character print expression.\n*/"
    },
    "org.apache.hadoop.fs.shell.FsUsage$Df": {
        "org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean)": "/**\n* Sets the human-readable flag.\n* @param humanReadable true for human-readable format, false otherwise\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)": "/**\n* Sets the usages table for the current object.\n* @param usagesTable the TableBuilder instance to set\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable()": "/**\n* Returns the usages table builder instance.\n* @return TableBuilder object for usages\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)": "/**\n* Adds a row to the usages table with filesystem status and URI details.\n* @param uri URI of the filesystem\n* @param fsStatus filesystem status containing capacity data\n* @param mountedOnPath path where the filesystem is mounted\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from the provided arguments.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList)": "/**\n* Processes filesystem arguments and prints usage table if not empty.\n* @param args list of PathData arguments\n*/",
        "org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item, updating filesystem usage or hiding columns based on type.\n* @param item PathData object containing filesystem and path information\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException": {
        "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)": "/**\n* Constructs an exception for incorrect number of arguments.\n* @param expected expected number of arguments\n* @param actual actual number of arguments received\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage()": "/**\n* Constructs an error message for insufficient arguments.\n* @return formatted error message string\n*/"
    },
    "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException": {
        "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)": "/**\n* Constructs an exception for too many arguments.\n* @param expected number of expected arguments\n* @param actual number of actual arguments received\n*/",
        "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage()": "/**\n* Constructs an error message for too many arguments.\n* @return formatted error message string\n*/"
    },
    "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand": {
        "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList)": "/**** Processes command-line options and validates arguments for execution. \n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs or validation fails\n*/",
        "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])": "/**\n* Prints the name and encoded value of an attribute.\n* @param name attribute name\n* @param value byte array to encode and print\n*/",
        "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes file attributes for a given PathData item.\n* @param item PathData containing file system path and attributes\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommands$Cp": {
        "org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List)": "/**\n* Parses args to set preserve option; stops at '--'.\n* @param args list of argument strings\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from args.\n* @param args list of command-line arguments\n*/"
    },
    "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread": {
        "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run()": "/**\n* Continuously refreshes disk information while the process is running.\n*/",
        "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)": "/**\n* Initializes RefreshThread with spaceUsed and runImmediately flag.\n* @param spaceUsed object to get cached space usage\n* @param runImmediately flag to start refresh immediately\n*/"
    },
    "org.apache.hadoop.fs.DFCachingGetSpaceUsed": {
        "org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh()": "/**\n* Updates the used disk space from the data source.\n*/",
        "org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)": "/**\n* Initializes DFCachingGetSpaceUsed with a DF object from builder parameters.\n* @param builder configuration builder for DF initialization\n*/"
    },
    "org.apache.hadoop.fs.ByteBufferUtil": {
        "org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream)": "/**\n* Checks if the InputStream supports ByteBuffer reading.\n* @param stream the InputStream to check\n* @return true if ByteBufferReadable, false otherwise\n*/",
        "org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)": "/**\n* Reads data into a ByteBuffer from an InputStream using a ByteBufferPool.\n* @param stream the InputStream to read from\n* @param bufferPool the pool for managing ByteBuffer allocation\n* @param maxLength the maximum number of bytes to read\n* @return ByteBuffer containing read data or null on failure\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory": {
        "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs an ArrayBlockFactory with buffer directory and configuration.\n* @param keyToBufferDir directory for buffer files\n* @param conf configuration settings for the factory\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)": "/**\n* Creates a DataBlock with specified index, limit, and upload statistics.\n* @param index block index, @param limit buffer size limit, @param statistics upload stats\n* @return DataBlock instance\n*/"
    },
    "org.apache.hadoop.service.launcher.ServiceLaunchException": {
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)": "/**\n* Constructs a ServiceLaunchException with an exit code and cause.\n* @param exitCode exit status code\n* @param cause the throwable cause of the exception\n*/",
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)": "/**\n* Constructs a ServiceLaunchException with an exit code and message.\n* @param exitCode exit status code\n* @param message detailed exception message\n*/",
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])": "/**\n* Constructs a ServiceLaunchException with exit code and formatted message.\n* @param exitCode exit status code\n* @param format message format with optional arguments\n* @param args optional arguments for the message format\n*/",
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])": "/**\n* Constructs a ServiceLaunchException with formatted message and cause.\n* @param exitCode exit status code\n* @param cause underlying throwable cause\n* @param format message format string\n* @param args arguments for the format string\n*/"
    },
    "org.apache.hadoop.security.KDiag$KerberosDiagsFailure": {
        "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)": "/**\n* Constructs a KerberosDiagsFailure with a category and message.\n* @param category error category\n* @param message error message details\n*/",
        "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])": "/**\n* Constructs a KerberosDiagsFailure with formatted message.\n* @param category error category, @param message error message template, @param args message args\n*/",
        "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])": "/**\n* Constructs a KerberosDiagsFailure with error details.\n* @param category error category, @param throwable cause of the failure, \n* @param message error message template, @param args message args\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable)": "/**\n* Initializes ProgressableOption with a given Progressable value.\n* @param value the Progressable object to set\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader$LengthOption": {
        "org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long)": "/**\n* Initializes LengthOption with a specified long value.\n* @param value the long value to set for this LengthOption\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader$StartOption": {
        "org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long)": "/**\n* Constructs a StartOption with a specified long value.\n* @param value the long value for the StartOption\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long)": "/**\n* Initializes BlockSizeOption with a specified long value.\n* @param value the long value to set for this BlockSizeOption\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int)": "/**\n* Initializes SyncIntervalOption with a value, defaulting if negative.\n* @param val sync interval value\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int)": "/**\n* Constructs a ReplicationOption with the specified integer value.\n* @param value the integer value to be assigned\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption": {
        "org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int)": "/**\n* Constructs a BufferSizeOption with the specified integer value.\n* @param value the integer value for buffer size\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int)": "/**\n* Initializes BufferSizeOption with a specified integer value.\n* @param value the integer value for buffer size\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator": {
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)": "/**\n* Compares two IntWritable objects based on their key data.\n* @param I first IntWritable for comparison\n* @param J second IntWritable for comparison\n* @return comparison result as an integer\n*/"
    },
    "org.apache.hadoop.io.SetFile": {
        "org.apache.hadoop.io.SetFile:<init>()": "/**\n* Protected constructor for SetFile, prevents public instantiation.\n*/"
    },
    "org.apache.hadoop.io.ArrayFile": {
        "org.apache.hadoop.io.ArrayFile:<init>()": "/**\n* Protected constructor for ArrayFile, prevents public instantiation.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class)": "/**\n* Constructs ValueClassOption with specified class type.\n* @param value the class type to be set\n*/"
    },
    "org.apache.hadoop.io.MapFile$Writer$KeyClassOption": {
        "org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class)": "/**\n* Initializes KeyClassOption with the specified class type.\n* @param value the class type to be set\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class)": "/**\n* Initializes KeyClassOption with the specified class type.\n* @param value the class type to be set\n*/"
    },
    "org.apache.hadoop.io.MapFile$Reader": {
        "org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable)": "/**\n* Performs binary search for a key in a sorted array.\n* @param key the key to search for\n* @return index of key or negative insertion point if not found\n*/",
        "org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator)": "/**\n* Creates a ComparatorOption using the provided WritableComparator.\n* @param value the WritableComparator to be used\n* @return ComparatorOption instance\n*/",
        "org.apache.hadoop.io.MapFile$Reader:reset()": "/**\n* Resets the stream to the initial position.\n* @throws IOException if an I/O error occurs during seeking\n*/",
        "org.apache.hadoop.io.MapFile$Reader:close()": "/**\n* Closes index and data resources if not already closed.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.MapFile$Reader:getKeyClass()": "/**\n* Retrieves the key class from the data source.\n* @return Class<?> representing the key class\n*/",
        "org.apache.hadoop.io.MapFile$Reader:getValueClass()": "/**\n* Retrieves the value class from the data object.\n* @return Class<?> of the value or throws RuntimeException on failure\n*/",
        "org.apache.hadoop.io.MapFile$Reader:readIndex()": "/**\n* Reads the index into memory, populating keys and positions arrays.\n* @throws IOException if an I/O error occurs during reading or closing\n*/",
        "org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)": "/**\n* Advances to the next entry, populating key and value.\n* @param key WritableComparable for the next key; @param val Writable for the current value\n* @return true if more entries exist, false otherwise\n*/",
        "org.apache.hadoop.io.MapFile$Reader:midKey()": "/**\n* Retrieves the median key from the keys array.\n* @return WritableComparable median key or null if no keys exist\n*/",
        "org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable)": "/**\n* Processes final key by reading data until EOF, restoring original stream position.\n* @param key WritableComparable to populate with the final key\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)": "/**\n* Seeks to the position of a key, optionally before it.\n* @param key the key to seek\n* @param before true to seek before the key, false otherwise\n* @return comparison result with the next key\n*/",
        "org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable)": "/**\n* Seeks to the position of a key without seeking before it.\n* @param key the key to seek\n* @return comparison result with the next key\n*/",
        "org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)": "/**\n* Retrieves the closest key based on a given key.\n* @param key the key to find closest match for\n* @param val Writable object to store the current value\n* @param before true to seek before the key, false otherwise\n* @return closest WritableComparable key or null if not found\n*/",
        "org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable)": "/**\n* Seeks to the position of a key and checks if it's found.\n* @param key the key to seek\n* @return true if the key is found, false otherwise\n*/",
        "org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)": "/**\n* Retrieves the closest key to the specified key without seeking before it.\n* @param key the key to find closest match for\n* @param val Writable object to store the current value\n* @return closest WritableComparable key or null if not found\n*/",
        "org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)": "/**\n* Retrieves the current value for the given key.\n* @param key the key to seek\n* @param val Writable object to store the current value\n* @return Writable object or null if the key is not found\n*/",
        "org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])": "/**\n* Creates a SequenceFile.Reader for the specified data file.\n* @param dataFile path to the data file\n* @param conf configuration settings\n* @param options additional reader options\n* @return initialized SequenceFile.Reader\n*/",
        "org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])": "/**\n* Opens data and index files with specified configurations and options.\n* @param dir directory containing files\n* @param comparator optional comparator for key sorting\n* @param conf configuration settings\n* @param options additional reader options\n* @throws IOException if file operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])": "/**\n* Initializes a SequenceFile reader with specified directory and options.\n* @param dir directory containing files\n* @param conf configuration settings\n* @param opts additional reader options\n* @throws IOException if file operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a deprecated SequenceFile Reader from a filesystem and directory name.\n* @param fs filesystem instance\n* @param dirName directory name for the SequenceFile\n* @param conf configuration settings\n* @throws IOException if file operations fail\n*/",
        "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a deprecated SequenceFile reader with directory and comparator.\n* @param fs file system instance\n* @param dirName directory name for the files\n* @param comparator comparator for sorting\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal": {
        "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>()": "/**\n* Initializes Internal object for read operations.\n*/",
        "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object)": "/**\n* Constructs Internal object for writes.\n* @param value non-null object to initialize the superclass\n*/"
    },
    "org.apache.hadoop.io.SetFile$Writer": {
        "org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable)": "/**\n* Appends a key with a null writable value.\n* @param key the key to append\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Constructs a SequenceFile Writer with specified configuration and output directory.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName output directory path\n* @param comparator custom WritableComparator\n* @param compress compression type for the file\n* @throws IOException if file system operations fail\n*/",
        "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Initializes a SequenceFile Writer with configuration and output directory.\n* @param conf configuration settings\n* @param fs file system instance\n* @param dirName output directory path\n* @param keyClass class type for keys\n* @param compress compression type for the file\n*/",
        "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)": "/**** Constructs a Writer for SequenceFile with specified file system and key class. \n* @param fs file system instance \n* @param dirName output directory name \n* @param keyClass class type for the key \n* @throws IOException if file system operations fail \n*/"
    },
    "org.apache.hadoop.io.SetFile$Reader": {
        "org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable)": "/**\n* Advances to the next entry using the given key.\n* @param key the key to advance to the next entry\n* @return true if an entry is found, false otherwise\n*/",
        "org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable)": "/**\n* Seeks to the position of a key in the data.\n* @param key the key to seek\n* @return true if the key is found, false otherwise\n*/",
        "org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable)": "/**\n* Retrieves a key if found; otherwise returns null.\n* @param key the key to retrieve\n* @return the key or null if not found\n*/",
        "org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Reader for SequenceFile with specified filesystem and comparator.\n* @param fs FileSystem instance\n* @param dirName directory name for SequenceFile\n* @param comparator WritableComparator for data sorting\n* @param conf configuration settings\n* @throws IOException if file operations fail\n*/",
        "org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Reader for SequenceFile with specified filesystem and directory.\n* @param fs filesystem instance\n* @param dirName directory name for the SequenceFile\n* @param conf configuration settings\n* @throws IOException if file operations fail\n*/"
    },
    "org.apache.hadoop.io.Text$Comparator": {
        "org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays starting from specified offsets.\n* @param b1 first byte array, s1 offset, l1 length\n* @param b2 second byte array, s2 offset, l2 length\n* @return comparison result of the byte arrays\n*/",
        "org.apache.hadoop.io.Text$Comparator:<init>()": "/**\n* Initializes a Comparator for Text class.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption": {
        "org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream)": "/**\n* Constructs InputStreamOption with a given FSDataInputStream.\n* @param value the FSDataInputStream to be wrapped\n*/"
    },
    "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator": {
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)": "/**\n* Advises on file access patterns to the kernel if supported.\n* @param identifier unique operation identifier\n* @param fd file descriptor to advise on\n* @param offset starting byte offset for advice\n* @param len length in bytes for advice\n* @param flags advice flags for the operation\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize()": "/**\n* Retrieves the operating system's page size in bytes.\n* @return page size, defaults to 4096 if retrieval fails\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock()": "/**\n* Verifies if memory locking is possible.\n* @return true if native code is available, false otherwise\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit()": "/**\n* Retrieves the memory lock limit from native code.\n* @return long memory lock limit or 0 if unavailable\n*/",
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)": "/**\n* Locks a direct ByteBuffer in memory using its identifier.\n* @param identifier unique identifier for the lock operation\n* @param buffer the ByteBuffer to lock\n* @param len length of the memory to lock\n* @throws IOException if buffer is non-direct or native code is not loaded\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder": {
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release()": "/**\n* Releases resources held by the encoder if it is initialized.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes RSErasureEncoder with specified ErasureCoderOptions.\n* @param options configuration settings for the encoder\n*/",
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder()": "/**\n* Creates or retrieves a RawErasureEncoder based on the codec if not already created.\n* @return RawErasureEncoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares an ErasureCodingStep using input/output blocks and a raw encoder.\n* @param blockGroup the ECBlockGroup to process\n* @return ErasureEncodingStep configured for encoding\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder": {
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release()": "/**\n* Releases resources held by encoders if they are not null.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes HHXORErasureEncoder with specified configuration options.\n* @param options encoder configuration settings\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder()": "/**\n* Creates or retrieves an existing RawErasureEncoder instance.\n* @return RawErasureEncoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder()": "/**\n* Creates or retrieves the XOR RawErasureEncoder instance.\n* @return RawErasureEncoder instance, creates if not already initialized\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares encoding step using input and output blocks from the block group.\n* @param blockGroup the EC block group for encoding\n* @return ErasureCodingStep for the specified block group\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder": {
        "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Retrieves erased ECBlock objects from the given block group.\n* @param blockGroup the ECBlockGroup to check for erased blocks\n* @return array of erased ECBlock objects\n*/",
        "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs XORErasureDecoder with specified ErasureCoderOptions.\n* @param options configuration for data and parity units\n*/",
        "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares an ErasureCodingStep for decoding with specified ECBlockGroup.\n* @param blockGroup the ECBlockGroup to decode\n* @return ErasureCodingStep for decoding process\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder": {
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release()": "/**\n* Releases resources held by rsRawDecoder if not null.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs an RSErasureDecoder with specified erasure coding options.\n* @param options configuration for data and parity units\n*/",
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder()": "/**** Checks and creates a RawErasureDecoder if not already initialized. \n* @return RawErasureDecoder instance \n*/",
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares decoding step using input/output blocks and a raw decoder.\n* @param blockGroup the ECBlockGroup for processing\n* @return ErasureCodingStep for decoding\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder": {
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release()": "/**\n* Releases resources held by rsRawDecoder and xorRawEncoder if they are not null.\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs HHXORErasureDecoder with specified ErasureCoderOptions.\n* @param options configuration for data and parity units\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder()": "/**\n* Creates or retrieves the XOR RawErasureEncoder instance.\n* @return RawErasureEncoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder()": "/**\n* Checks and creates RawErasureDecoder if not initialized.\n* @return RawErasureDecoder instance\n*/",
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares decoding step with input/output blocks and encoders/decoders.\n* @param blockGroup group of EC blocks for decoding\n* @return configured ErasureCodingStep instance\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs XORRawDecoder with specified erasure coding options.\n* @param coderOptions configuration for erasure coding\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)": "/**\n* Decodes input buffers and updates output buffer based on decoding state.\n* @param decodingState holds input buffers and decoding information\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)": "/**\n* Decodes input data into output, skipping erased indexes.\n* @param decodingState state containing inputs, outputs, and offsets\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes XORRawEncoder with specified coding options.\n* @param coderOptions configuration settings for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)": "/**\n* Encodes input buffers into an output buffer using XOR operation.\n* @param encodingState contains input buffers and output buffer details\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)": "/**\n* Encodes data by resetting buffers and applying XOR on input arrays.\n* @param encodingState state containing inputs and output buffers\n*/"
    },
    "org.apache.hadoop.io.compress.GzipCodec": {
        "org.apache.hadoop.io.compress.GzipCodec:getCompressorType()": "/**\n* Returns the compressor class based on native Zlib availability.\n* @return Compressor class type for Zlib or built-in Gzip\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:getDecompressorType()": "/**\n* Returns the decompressor class based on native Zlib availability.\n* @return Class of the selected Decompressor implementation\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createCompressor()": "/**\n* Creates a compressor based on native Zlib availability.\n* @return Compressor instance for Gzip compression\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor()": "/**\n* Creates a DirectDecompressor if native Zlib is available; otherwise returns null.\n* @return DirectDecompressor instance or null if native Zlib is not loaded\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createDecompressor()": "/**\n* Creates a Decompressor based on native Zlib support.\n* @return Decompressor instance for Gzip compression\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream for the given OutputStream.\n* @param out the OutputStream to write compressed data\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream)": "/**\n* Creates a CompressionInputStream from the given InputStream.\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)": "/**\n* Creates a CompressionOutputStream with specified compressor or defaults to another method.\n* @param out the output stream for compressed data\n* @param compressor the compressor to use\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)": "/**\n* Creates a CompressionInputStream with optional decompressor.\n* @param in InputStream for compressed data\n* @param decompressor Decompressor instance, creates if null\n* @return CompressionInputStream for decompressed data\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor": {
        "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup()": "/**\n* Cleans up resources, deleting files if input preservation is not enabled.\n* @throws IOException if an I/O error occurs during cleanup\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object)": "/**\n* Checks equality with another object.\n* @param o object to compare with\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$StreamOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream)": "/**\n* Constructs StreamOption with a specified FSDataOutputStream.\n* @param stream the FSDataOutputStream to be encapsulated\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean)": "/**\n* Constructs an AppendIfExistsOption with a boolean value.\n* @param value the boolean value to be stored\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption": {
        "org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>()": "/**\n* Constructs an OnlyHeaderOption instance, initializing with true.\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Reader$FileOption": {
        "org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path)": "/**\n* Constructs a FileOption with the specified Path value.\n* @param value the Path to be associated with this FileOption\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Writer$FileOption": {
        "org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path)": "/**** Constructs a FileOption with the specified Path. \n* @param path the Path to be associated with this FileOption \n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall": {
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)": "/**\n* Initializes an asynchronous call with method details and handler.\n* @param method the method to be called\n* @param args arguments for the method\n* @param isRpc indicates if the call is an RPC\n* @param callId unique identifier for the call\n* @param retryInvocationHandler handles retry logic\n* @param asyncCallHandler handler for asynchronous call processing\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo()": "/**\n* Processes wait time and retry information.\n* @return CallReturn indicating wait or retry status\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone()": "/**\n* Checks if the asynchronous call is completed.\n* @return true if done, false otherwise\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke()": "/**\n* Invokes an asynchronous call or checks the status of a pending one.\n* @return CallReturn indicating the state of the async operation\n*/"
    },
    "org.apache.hadoop.ipc.Server$MetricsUpdateRunner": {
        "org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run()": "/**\n* Updates request metrics based on elapsed time since last execution.\n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler$1": {
        "org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone()": "/**\n* Checks if the operation is completed.\n* @return true if completed, false otherwise\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Writer": {
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm()": "/**\n* Retrieves the default compression algorithm.\n* @return Algorithm object representing the default compression method\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Writer with output stream and configuration.\n* @param fout output stream for writing data\n* @param compressionName name of the compression algorithm\n* @param conf configuration settings\n* @throws IOException if output file is not at zero offset\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)": "/**\n* Prepares a MetaBlock for writing with a given name and compression algorithm.\n* @param name unique name for the MetaBlock\n* @param compressAlgo algorithm used for compression\n* @return BlockAppender for the new MetaBlock\n* @throws IOException if I/O error occurs\n* @throws MetaBlockAlreadyExists if a MetaBlock with the same name exists\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock()": "/**\n* Prepares a data block for writing.\n* @return BlockAppender for managing the data block\n* @throws IOException if block preparation fails\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:close()": "/**\n* Closes the resource, finalizing the output and writing metadata.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)": "/***************\n* Prepares a MetaBlock with a given name and compression algorithm.\n* @param name unique name for the MetaBlock\n* @param compressionName name of the compression algorithm\n* @return BlockAppender for the new MetaBlock\n* @throws IOException if I/O error occurs\n* @throws MetaBlockAlreadyExists if a MetaBlock with the same name exists\n***************/",
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String)": "/**\n* Prepares a MetaBlock with a unique name using the default compression algorithm.\n* @param name unique name for the MetaBlock\n* @return BlockAppender for the new MetaBlock\n* @throws IOException if I/O error occurs\n* @throws MetaBlockAlreadyExists if a MetaBlock with the same name exists\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister": {
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)": "/**\n* Registers a block region with specified raw data and position.\n* @param raw unique identifier for raw data\n* @param begin starting position of the block\n* @param end ending position of the block\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader": {
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState)": "/**\n* Constructs BlockReader with RBlockState input.\n* @param rbs RBlockState object containing input stream\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize()": "/**\n* Returns the raw size from the current block region.\n* @return raw size as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize()": "/**\n* Returns the compressed size of the current block region.\n* @return size in bytes of the compressed data\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos()": "/**\n* Retrieves the starting position offset from the current block region.\n* @return the starting position as a long\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName()": "/**\n* Retrieves the compression algorithm name.\n* @return String representing the compression name\n*/",
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close()": "/**\n* Closes the resource and marks it as closed.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocket$DomainChannel": {
        "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer)": "/**** Reads data into a ByteBuffer and manages reference counting. \n* @param dst the ByteBuffer to read data into \n* @return number of bytes read \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen()": "/**\n* Checks if the DomainSocket is open.\n* @return true if open, false if closed\n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close()": "/**\n* Closes the DomainSocket, releasing all resources.\n* @throws IOException if an I/O error occurs during closure\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream": {
        "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int)": "/**\n* Writes a byte value to the socket and manages reference counting.\n* @param val the byte value to write\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)": "/**\n* Writes a byte array to a domain socket.\n* @param b byte array to write, @param off start offset, @param len number of bytes to write\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close()": "/**\n* Closes the DomainSocket, releasing all resources.\n* @throws IOException if an I/O error occurs during closure\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream": {
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read()": "/**\n* Reads a byte from the socket.\n* @return byte value read or -1 if end of stream is reached\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)": "/**\n* Reads bytes into an array from the socket.\n* @param b byte array to store read data\n* @param off offset in the array to start storing data\n* @param len maximum number of bytes to read\n* @return number of bytes read\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available()": "/**\n* Returns the number of available bytes; manages reference count.\n* @return number of available bytes in the socket\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close()": "/**\n* Closes the DomainSocket, releasing all resources.\n* @throws IOException if an I/O error occurs during closure\n*/"
    },
    "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool": {
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long)": "/**\n* Trims idle selectors based on last activity time.\n* @param now current time in milliseconds since epoch\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel)": "/**\n* Retrieves SelectorInfo for a given channel, creating it if necessary.\n* @param channel the selectable channel to get SelectorInfo for\n* @return SelectorInfo associated with the channel\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo)": "/**\n* Releases selector info by updating last activity time and queuing it.\n* @param info SelectorInfo to be released\n*/",
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)": "/**\n* Selects a channel for operations with a timeout.\n* @param channel the selectable channel to monitor\n* @param ops operations to perform on the channel\n* @param timeout maximum wait time in milliseconds\n* @return number of keys selected or 0 if timeout occurs\n*/"
    },
    "org.apache.hadoop.http.ProfilerDisabledServlet": {
        "org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests, responding with an error message if the profiler servlet is disabled.\n* @param req the HttpServletRequest object\n* @param resp the HttpServletResponse to modify\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsConfigException": {
        "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String)": "/**\n* Constructs a MetricsConfigException with a specified error message.\n* @param message detailed error description\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs a MetricsConfigException with a message and a cause.\n* @param message error message\n* @param cause underlying throwable cause\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable)": "/**\n* Constructs a MetricsConfigException with the specified cause.\n* @param cause the underlying reason for the exception\n*/"
    },
    "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory": {
        "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class)": "/**** Retrieves a singleton instance of a specified metrics factory class. \n* @param cls the class type of the metrics factory\n* @return instance of the metrics factory or throws MetricsException if unknown type \n*/",
        "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory()": "/**\n* Retrieves the singleton instance of MutableMetricsFactory.\n* @return MutableMetricsFactory instance\n*/"
    },
    "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30": {
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)": "/**\n* Appends tags from a MetricsRecord to a StringBuilder with specific exclusions.\n* @param record the MetricsRecord containing tags\n* @param sb the StringBuilder to append formatted tags\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)": "/**\n* Emits a metric with specified parameters to Ganglia.\n* @param groupName the group name for the metric\n* @param name the name of the metric\n* @param type the type of the metric\n* @param value the value of the metric\n* @param gConf configuration for Ganglia\n* @param gSlope slope type for the metric\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)": "/**\n* Calculates the slope based on configuration or metric.\n* @param gConf configuration settings\n* @param slopeFromMetric derived slope if config is absent\n* @return GangliaSlope object representing the calculated slope\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Publishes metrics to Ganglia, handling dense and sparse updates.\n* @param record the MetricsRecord containing metrics to publish\n*/",
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**\n* Initializes configuration and processes tags from the given settings.\n* @param conf configuration settings for GangliaSink\n*/"
    },
    "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31": {
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)": "/**\n* Emits a metric with metadata and value to Ganglia hosts.\n* @param groupName the group name for the metric\n* @param name the metric name\n* @param type the metric type\n* @param value the metric value\n* @param gConf Ganglia configuration settings\n* @param gSlope slope configuration for the metric\n*/"
    },
    "org.apache.hadoop.metrics2.sink.GraphiteSink": {
        "org.apache.hadoop.metrics2.sink.GraphiteSink:close()": "/**\n* Closes the graphite writer and socket, handling IOExceptions.\n* @throws IOException if an error occurs while closing resources\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink:flush()": "/**\n* Flushes metrics to Graphite, handling errors and closing connection if needed.\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration)": "/**\n* Initializes Graphite connection and metrics prefix from configuration.\n* @param conf configuration containing server details and metrics prefix\n*/",
        "org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)": "/**\n* Sends metrics data to Graphite, formatting it for display.\n* @param record contains metrics information to send\n*/"
    },
    "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder": {
        "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)": "/**\n* Adds metrics records for a given source name.\n* @param name source name for the metrics\n* @param records iterable of metrics records to add\n* @return true if addition is successful, false otherwise\n*/",
        "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get()": "/**\n* Creates and returns a new MetricsBuffer instance.\n* @return a new MetricsBuffer object initialized with this context\n*/"
    },
    "org.apache.hadoop.metrics2.lib.UniqueNames": {
        "org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String)": "/**\n* Generates a unique name by appending a counter to the base name.\n* @param name the base name to make unique\n* @return a unique name based on the input\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$UgiMetrics": {
        "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long)": "/**\n* Adds latency to groups and updates quantiles if present.\n* @param latency the long value representing latency to be added\n*/",
        "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create()": "/**\n* Creates and registers a new UgiMetrics instance.\n* @return registered UgiMetrics object\n*/",
        "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach()": "/**\n* Reattaches metrics by creating a new UgiMetrics instance.\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile": {
        "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double)": "/**\n* Constructs an InversePercentile object with scaled quantile values.\n* @param inversePercentile value to scale for quantile calculation\n*/"
    },
    "org.apache.hadoop.metrics2.lib.Interns": {
        "org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)": "/**** Retrieves or adds metrics information based on name and description. \n* @param name metric name \n* @param description metric description \n* @return MetricsInfo object from cache \n*/",
        "org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)": "/**\n* Adds a tag to the metrics cache.\n* @param info metrics information object\n* @param value tag value to be added\n* @return MetricsTag object from cache\n*/",
        "org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)": "/**** Creates and adds a metrics tag. \n* @param name metric name \n* @param description metric description \n* @param value tag value \n* @return MetricsTag object from cache \n*/"
    },
    "org.apache.hadoop.metrics2.lib.MethodMetric$1": {
        "org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures a snapshot of metrics.\n* @param builder MetricsRecordBuilder to store the snapshot\n* @param all boolean indicating if all metrics should be captured\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MethodMetric$2": {
        "org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)": "/**\n* Captures a snapshot of metrics.\n* @param builder MetricsRecordBuilder for recording metrics\n* @param all indicates whether to record all metrics\n*/"
    },
    "org.apache.hadoop.util.Daemon$DaemonFactory": {
        "org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable)": "/**\n* Creates a new daemon thread with the specified Runnable.\n* @param runnable the Runnable to execute in the thread\n* @return a new Daemon thread\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager": {
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)": "/**\n* Sets the external delegation token secret manager.\n* @param secretManager new secret manager instance\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy()": "/**\n* Stops token remover thread if managedSecretManager is active.\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)": "/**\n* Creates a delegation token for the specified user and service.\n* @param ugi UserGroupInformation for the user\n* @param renewer User to renew the token, defaults to short username if null\n* @param service Service associated with the token\n* @return Token object for the delegation\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init()": "/**\n* Initializes the secret manager by starting its threads if managedSecretManager is true.\n* @throws RuntimeException if starting threads fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)": "/**\n* Creates a delegation token for the specified user.\n* @param ugi UserGroupInformation for the user\n* @param renewer User to renew the token\n* @return Token for the delegation\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)": "/**\n* Decodes a token into a DelegationTokenIdentifier.\n* @param token the token to decode\n* @param tokenKind the type of the delegation token\n* @return DelegationTokenIdentifier object\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token)": "/**\n* Verifies a token and retrieves UserGroupInformation.\n* @param token the token to validate\n* @return UserGroupInformation associated with the token\n* @throws IOException if token verification fails\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)": "/**** Renews a delegation token and returns its new expiration time. \n* @param token the token to renew \n* @param renewer the entity requesting the renewal \n* @return new expiration time in milliseconds \n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)": "/**\n* Cancels a token using the provided canceller or verified username.\n* @param token the token to cancel\n* @param canceler the user requesting the cancellation\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)": "/**\n* Initializes DelegationTokenManager based on configuration for token management.\n* @param conf configuration settings\n* @param tokenKind type of token being managed\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation": {
        "org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get()": "/**\n* Retrieves UserGroupInformation from the current context.\n* @return UserGroupInformation object for the caller's context\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler": {
        "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>()": "/**\n* Constructs a KerberosDelegationTokenAuthenticationHandler using a KerberosAuthenticationHandler.\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler": {
        "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>()": "/**\n* Constructs a PseudoDelegationTokenAuthenticationHandler with a PseudoAuthenticationHandler.\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator": {
        "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>()": "/**\n* Constructs PseudoDelegationTokenAuthenticator using a PseudoAuthenticator for username retrieval.\n*/"
    },
    "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator": {
        "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>()": "/**\n* Constructs a KerberosDelegationTokenAuthenticator with a fallback authenticator.\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>()": "/**\n* Initializes TokenSelector with a specified token kind.\n*/"
    },
    "org.apache.hadoop.security.token.DtUtilShell": {
        "org.apache.hadoop.security.token.DtUtilShell:getCommandUsage()": "/**\n* Constructs command usage string for various operations.\n* @return formatted usage details for commands\n*/",
        "org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[])": "/**\n* Processes login parameters and attempts Kerberos login if specified.\n* @param args input arguments for processing\n* @return filtered arguments excluding principal and keytab\n*/",
        "org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[])": "/**\n* Initializes command processing from input arguments.\n* @param args command-line arguments\n* @return 0 if successful, 1 if validation fails\n*/",
        "org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[])": "/**\n* Executes the main application logic using ToolRunner.\n* @param args command-line arguments for the tool\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])": "/**\n* Constructs a KMSKeyVersion instance with specified key details.\n* @param keyName identifier for the key\n* @param versionName version of the key\n* @param material byte array containing key material\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)": "/**\n* Constructs a KMSMetadata object with encryption details.\n* @param cipher encryption algorithm name\n* @param bitLength key size in bits\n* @param description metadata description\n* @param attributes optional key-value pairs\n* @param created creation timestamp\n* @param versions number of versions available\n*/"
    },
    "org.apache.hadoop.ipc.ObserverRetryOnActiveException": {
        "org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String)": "/**\n* Constructs an ObserverRetryOnActiveException with a detailed message.\n* @param msg detailed message for the exception\n*/"
    },
    "org.apache.hadoop.ipc.WeightedTimeCostProvider": {
        "org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)": "/**\n* Calculates total cost based on processing details and weights.\n* @param details timing details for cost calculation\n* @return computed total cost as a long value\n*/",
        "org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes weight configuration based on the provided namespace and settings.\n* @param namespace identifier for weight configuration\n* @param conf configuration object for retrieving weight values\n*/"
    },
    "org.apache.hadoop.ipc.UnexpectedServerException": {
        "org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String)": "/**\n* Constructs an UnexpectedServerException with a specified error message.\n* @param message descriptive error message for the exception\n*/",
        "org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an UnexpectedServerException with a message and cause.\n* @param message error description\n* @param cause underlying throwable cause\n*/"
    },
    "org.apache.hadoop.ipc.RpcClientException": {
        "org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String)": "/**\n* Constructs an RpcClientException with a specified error message.\n* @param message descriptive error message for the exception\n*/",
        "org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)": "/**\n* Constructs an RpcClientException with a message and cause.\n* @param message error description\n* @param cause underlying throwable cause\n*/"
    },
    "org.apache.hadoop.ipc.ResponseBuffer": {
        "org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int)": "/**\n* Adjusts the buffer capacity by setting a new size.\n* @param capacity new buffer size excluding framing bytes\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer()": "/**\n* Retrieves the FramedBuffer and sets its size.\n* @return FramedBuffer instance with updated size\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:capacity()": "/**\n* Retrieves the available capacity of the framed buffer.\n* @return the capacity of the buffer after subtracting framing bytes\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int)": "/**\n* Ensures the output buffer has at least the specified capacity.\n* @param capacity minimum required buffer capacity\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:reset()": "/**\n* Resets written count and framed buffer state.\n* @return this ResponseBuffer instance for chaining\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream)": "/**\n* Writes the framed buffer data to the specified output stream.\n* @param out the output stream to write data to\n* @throws IOException if an I/O error occurs during writing\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:toByteArray()": "/**\n* Converts the framed buffer to a byte array.\n* @return byte array representation of the framed buffer\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:<init>(int)": "/**\n* Constructs ResponseBuffer with specified capacity.\n* @param capacity buffer capacity excluding framing bytes\n*/",
        "org.apache.hadoop.ipc.ResponseBuffer:<init>()": "/**\n* Initializes a ResponseBuffer with a default capacity of 1024 bytes.\n*/"
    },
    "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload": {
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object)": "/**\n* Compares this CacheEntry to another object for equality.\n* @param obj the object to compare\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)": "/**\n* Constructs a CacheEntryWithPayload with a given payload.\n* @param clientId byte array representing the client ID\n* @param callId identifier for the call\n* @param payload the payload object associated with the entry\n* @param expirationTime time until the entry expires\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode()": "/**\n* Returns the hash code for the object.\n* @return computed hash code as an int\n*/",
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)": "/**\n* Constructs a CacheEntryWithPayload with a payload.\n* @param clientId byte array representing the client ID\n* @param callId identifier for the call\n* @param payload associated data with the entry\n* @param expirationTime time until the entry expires\n* @param success indicates if the operation was successful\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>()": "/**\n* Initializes ProtobufRpcEngineCallbackImpl with server, call, method name, and setup time.\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable)": "/**\n* Handles errors by updating metrics and deferring error for later processing.\n* @param t the Throwable error to be processed\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message)": "/**** Sets the response message and updates processing metrics. \n* @param message the Message to be set as response \n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>()": "/**\n* Initializes ProtobufRpcEngineCallbackImpl with server, call, method name, and setup time.\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable)": "/**\n* Handles error by updating metrics and deferring the error for later processing.\n* @param t the Throwable error to be processed\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message)": "/**\n* Sets the response message and updates processing metrics.\n* @param message the Message to be processed\n*/"
    },
    "org.apache.hadoop.tracing.NullTraceScope": {
        "org.apache.hadoop.tracing.NullTraceScope:<init>()": "/**\n* Constructs a NullTraceScope instance with no associated Span.\n*/"
    },
    "org.apache.hadoop.util.Shell$1": {
        "org.apache.hadoop.util.Shell$1:run()": "/**\n* Executes a command if the interval has elapsed.\n* @throws IOException if an I/O error occurs during command execution\n*/"
    },
    "org.apache.hadoop.util.LightWeightGSet$Values": {
        "org.apache.hadoop.util.LightWeightGSet$Values:iterator()": "/**\n* Returns an iterator for the set elements.\n* @return Iterator for the set's elements\n*/",
        "org.apache.hadoop.util.LightWeightGSet$Values:clear()": "/**\n* Clears the set by delegating to the parent clear method.\n*/",
        "org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object)": "/**\n* Checks if the set contains the specified element.\n* @param o the element to check for\n* @return true if the element exists, false otherwise\n*/"
    },
    "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask": {
        "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run()": "/**\n* Monitors and terminates the process if not completed.\n* @param none\n* @return void\n*/",
        "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell)": "/**\n* Initializes ShellTimeoutTimerTask with a Shell instance.\n* @param shell the Shell object to be managed by the task\n*/"
    },
    "org.apache.hadoop.util.SignalLogger": {
        "org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger)": "/**\n* Registers signal handlers and logs the registration process.\n* @param log Logger instance for logging events\n*/"
    },
    "org.apache.hadoop.ha.FailoverController": {
        "org.apache.hadoop.ha.FailoverController:createReqInfo()": "/**\n* Creates StateChangeRequestInfo using the specified request source.\n* @return StateChangeRequestInfo instance\n*/",
        "org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves graceful fence timeout from configuration.\n* @param conf configuration object\n* @return timeout value as integer\n*/",
        "org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves RPC timeout for new active configuration.\n* @param conf configuration object\n* @return timeout value in milliseconds\n*/",
        "org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)": "/**\n* Initializes FailoverController with configuration and request source.\n* @param conf configuration settings\n* @param source source of the request\n*/",
        "org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)": "/**\n* Validates conditions for service failover; throws FailoverFailedException on failure.\n* @param from source service target\n* @param target destination service target\n* @param forceActive if true, allows failover to a non-ready service\n*/",
        "org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget)": "/**\n* Attempts to gracefully transition service to standby.\n* @param svc service target to be transitioned\n* @return true if successful, false otherwise\n*/",
        "org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)": "/**\n* Manages service failover between two targets.\n* @param fromSvc source service to failover from\n* @param toSvc destination service to failover to\n* @param forceFence if true, forces fencing of the source service\n* @param forceActive if true, allows activation of a non-ready service\n* @throws FailoverFailedException if failover fails\n*/"
    },
    "org.apache.hadoop.ha.SshFenceByTcpPort$Args": {
        "org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String)": "/**\n* Parses and returns the configured port from a string.\n* @param portStr string representation of the port\n* @return parsed port number\n* @throws BadFencingConfigurationException if the portStr is invalid\n*/",
        "org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String)": "/**\n* Initializes Args with user and optional SSH port from input string.\n* @param arg input string containing user and port information\n* @throws BadFencingConfigurationException if input string is invalid\n*/"
    },
    "org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks": {
        "org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State)": "/**\n* Updates health state and rechecks electability.\n* @param newState the new health state to enter\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner": {
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run()": "/**\n* Continuously processes statistics data references until interrupted.\n* Cleans up each reference using cleanUp() method.\n*/"
    },
    "org.apache.hadoop.fs.PathIsDirectoryException": {
        "org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String)": "/**\n* Constructs a PathIsDirectoryException for a specified path.\n* @param path the directory path causing the exception\n*/"
    },
    "org.apache.hadoop.fs.PathIsNotDirectoryException": {
        "org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String)": "/**\n* Constructs a PathIsNotDirectoryException with a specified path.\n* @param path the file path causing the exception\n*/"
    },
    "org.apache.hadoop.fs.PathOperationException": {
        "org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String)": "/**\n* Constructs a PathOperationException for unsupported operations.\n* @param path the file path related to the exception\n*/"
    },
    "org.apache.hadoop.fs.PathIsNotEmptyDirectoryException": {
        "org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String)": "/**\n* Constructs a PathIsNotEmptyDirectoryException with a specified path.\n* @param path the directory path causing the exception\n*/"
    },
    "org.apache.hadoop.fs.impl.WrappedIOException": {
        "org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException)": "/**\n* Constructs WrappedIOException with a non-null cause.\n* @param cause the original IOException to wrap\n*/"
    },
    "org.apache.hadoop.util.GcTimeMonitor$Builder": {
        "org.apache.hadoop.util.GcTimeMonitor$Builder:build()": "/**\n* Builds and returns a GcTimeMonitor instance.\n* @return GcTimeMonitor configured with specified parameters\n*/"
    },
    "org.apache.hadoop.security.token.Token$PrivateToken": {
        "org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)": "/**\n* Constructs a PrivateToken from a publicToken and a new service.\n* @param publicToken the public token to clone\n* @param newService the new associated service\n*/",
        "org.apache.hadoop.security.token.Token$PrivateToken:hashCode()": "/**\n* Computes the hash code for this object.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text)": "/**\n* Checks if this Text is a private clone of the given public service.\n* @param thePublicService Text to compare against\n* @return true if equal, false otherwise\n*/",
        "org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object)": "/**\n* Compares this PrivateToken object with another for equality.\n* @param o object to compare; @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue": {
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall)": "/**\n* Adds an AsyncCall to the queue and starts processing if not already running.\n* @param call the AsyncCall to be added\n*/",
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls()": "/**\n* Processes async calls and returns minimum wait time.\n* @return minimum wait time or MAX_WAIT_PERIOD if none\n*/"
    },
    "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand": {
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)": "/**\n* Prints ACL entry details with effective permissions.\n* @param aclStatus current ACL status\n* @param fsPerm file system permissions\n* @param entry ACL entry to print\n*/",
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and validates the argument list.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)": "/**** Prints ACL entries based on their type. \n* @param aclStatus current ACL status \n* @param fsPerm file system permissions \n* @param entries list of ACL entries to print \n*/",
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes file path data and prints its owner, group, permissions, and ACL entries.\n* @param item PathData object containing file information\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream": {
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos()": "/**\n* Retrieves the current cached position from fsOut.\n* @return current position as a long\n*/",
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)": "/**** \n* Constructs a CryptoFSDataOutputStream with encryption support.\n* @param out output stream for data\n* @param codec codec for encryption\n* @param bufferSize size of the buffer\n* @param key encryption key\n* @param iv initialization vector\n* @param closeOutputStream flag to close output stream\n*/",
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])": "/**\n* Constructs a CryptoFSDataOutputStream for encrypted data output.\n* @param out output stream for data\n* @param codec codec for encryption\n* @param bufferSize size of the buffer\n* @param key encryption key\n* @param iv initialization vector\n*/",
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])": "/**** Constructs a CryptoFSDataOutputStream with encryption.\\n* @param out FSDataOutputStream to wrap\\n* @param codec CryptoCodec for encryption\\n* @param key encryption key\\n* @param iv initialization vector\\n*/"
    },
    "org.apache.hadoop.io.ObjectWritable$NullInstance": {
        "org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput)": "/**\n* Writes the class name to the output stream.\n* @param out DataOutput stream to write the class name\n*/",
        "org.apache.hadoop.io.ObjectWritable$NullInstance:<init>()": "/**\n* Constructs a NullInstance by initializing with a null configuration.\n*/",
        "org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a NullInstance with the specified class and configuration.\n* @param declaredClass the class to be associated with this instance\n* @param conf the Configuration used for initialization\n*/",
        "org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput)": "/**\n* Reads fields from input stream and resolves class name to Class object.\n* @param in input data stream\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.BinaryComparable": {
        "org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable)": "/**\n* Compares this object with another BinaryComparable.\n* @param other the BinaryComparable to compare with\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)": "/**\n* Compares byte array with another using specified offset and length.\n* @param other byte array to compare against, off offset, len length\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.BinaryComparable:hashCode()": "/**\n* Computes the hash code using byte array and length.\n* @return computed hash code as an integer\n*/",
        "org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object)": "/**\n* Checks equality of this BinaryComparable with another object.\n* @param other object to compare\n* @return true if equal, false otherwise\n*/"
    },
    "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec": {
        "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Constructs an RSErasureCodec with configuration and options.\n* @param conf configuration settings\n* @param options erasure codec options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder()": "/**\n* Creates an ErasureEncoder using configured options.\n* @return ErasureEncoder instance for encoding data\n*/",
        "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder()": "/**\n* Creates an ErasureDecoder using configured options.\n* @return ErasureDecoder instance for data recovery\n*/"
    },
    "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec": {
        "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Constructs HHXORErasureCodec with specified configuration and options.\n* @param conf configuration settings\n* @param options erasure codec options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder()": "/**\n* Creates an ErasureEncoder instance using configured options.\n* @return ErasureEncoder object\n*/",
        "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder()": "/**\n* Creates an ErasureDecoder instance using configured options.\n* @return ErasureDecoder for data recovery\n*/"
    },
    "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec": {
        "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Constructs a DummyErasureCodec with specified configuration and options.\n* @param conf configuration settings\n* @param options erasure codec options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder()": "/**\n* Creates and returns a new DummyErasureEncoder instance.\n* @return ErasureEncoder configured with coder options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder()": "/**\n* Creates an ErasureDecoder instance.\n* @return ErasureDecoder configured with specified options\n*/"
    },
    "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec": {
        "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)": "/**\n* Constructs XORErasureCodec with configuration and options.\n* @param conf configuration settings\n* @param options erasure codec options\n*/",
        "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder()": "/**\n* Creates an ErasureEncoder instance using specified coder options.\n* @return ErasureEncoder object for encoding\n*/",
        "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder()": "/**\n* Creates an ErasureDecoder instance using XORErasureDecoder.\n* @return ErasureDecoder configured for data recovery\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.DecodingState": {
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])": "/**\n* Validates input parameters for decoder's operation.\n* @param inputs array of input elements\n* @param erasedIndexes indices of erased inputs\n* @param outputs array for output elements\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.EncodingState": {
        "org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])": "/**\n* Validates input and output array lengths against encoder specifications.\n* @param inputs array of input elements\n* @param outputs array of output elements\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder": {
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares an ErasureCodingStep for decoding from the given block group.\n* @param blockGroup the ECBlockGroup to decode\n* @return initialized ErasureDecodingStep for processing\n*/",
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs a DummyErasureDecoder with specified ErasureCoderOptions.\n* @param options configuration for data and parity units\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder": {
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/**\n* Prepares an ErasureCodingStep for the given block group.\n* @param blockGroup the ECBlockGroup to encode\n* @return ErasureEncodingStep instance with input/output blocks and encoder\n*/",
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs DummyErasureEncoder with specified ErasureCoderOptions.\n* @param options configuration settings for the encoder\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes RSRawDecoder with specified options and prepares encoding matrix.\n* @param coderOptions configuration options for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[])": "/**\n* Generates a decode matrix from erased data unit indexes.\n* @param erasedIndexes indexes of erased data units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[])": "/**\n* Processes erased data units and initializes decoding matrices and Galois Field tables.\n* @param erasedIndexes indexes of erased data units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])": "/**\n* Prepares decoding by updating cached indexes based on inputs and erased indexes.\n* @param inputs array of data elements to validate\n* @param erasedIndexes array of indexes for erased data units\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)": "/**\n* Decodes data using the provided decoding state.\n* @param decodingState contains inputs, outputs, and erased indexes for decoding\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)": "/**\n* Decodes data using provided state and resets output buffers.\n* @param decodingState contains input data and output configurations\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes RSRawEncoder with options and prepares encoding matrices.\n* @param coderOptions configuration for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)": "/**\n* Encodes data using the provided encoding state.\n* @param encodingState contains input/output buffers and encoding length\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)": "/**\n* Encodes data using provided encoding state and resets output buffers.\n* @param encodingState state containing input/output data and offsets\n*/"
    },
    "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor": {
        "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>()": "/**\n* Constructs a GzipZlibCompressor with default compression settings and buffer size.\n*/",
        "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs GzipZlibCompressor with settings from configuration.\n* @param conf configuration object for compression settings\n*/"
    },
    "org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor": {
        "org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>()": "/****\n* Constructs a GzipZlibDecompressor with GZIP/ZLIB settings and buffer size.\n*/"
    },
    "org.apache.hadoop.io.serializer.DeserializerComparator": {
        "org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer)": "/**\n* Initializes DeserializerComparator with a specified Deserializer.\n* @param deserializer the Deserializer instance to be used\n* @throws IOException if an error occurs while opening the buffer\n*/",
        "org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)": "/**\n* Compares two byte arrays after resetting the buffer and deserializing keys.\n* @param b1 first byte array, s1 start index, l1 length\n* @param b2 second byte array, s2 start index, l2 length\n* @return comparison result of deserialized keys\n*/"
    },
    "org.apache.hadoop.util.InstrumentedWriteLock": {
        "org.apache.hadoop.util.InstrumentedWriteLock:unlock()": "/**\n* Unlocks the write lock and reports if it was the last holder.\n*/",
        "org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming()": "/**\n* Records timestamp if write lock is held for the first time.\n*/",
        "org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)": "/**** Constructs an InstrumentedWriteLock with logging and timing parameters. \n* @param name lock identifier \n* @param logger logging utility \n* @param readWriteLock the actual read-write lock \n* @param minLoggingGapMs minimum log interval in milliseconds \n* @param lockWarningThresholdMs threshold for lock warnings in milliseconds \n* @param clock timer for monotonic time retrieval \n*/",
        "org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)": "/**\n* Initializes InstrumentedWriteLock with logging and timing parameters.\n* @param name lock identifier, logger for logging, readWriteLock for lock management\n*/"
    },
    "org.apache.hadoop.util.InstrumentedReadLock": {
        "org.apache.hadoop.util.InstrumentedReadLock:unlock()": "/**\n* Unlocks the read lock and reports if it was the last holder.\n*/",
        "org.apache.hadoop.util.InstrumentedReadLock:startLockTiming()": "/**\n* Starts timing for read lock if it's the first hold.\n* Uses monotonic clock for accurate timestamping.\n*/",
        "org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)": "/**\n* Constructs an InstrumentedReadLock with logging and timing parameters.\n* @param name lock identifier, logger logging utility, readWriteLock the lock object\n*/",
        "org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)": "/**\n* Constructs an InstrumentedReadLock with logging and timing parameters.\n* @param name lock identifier, logger logging utility, readWriteLock the lock object\n*/"
    },
    "org.apache.hadoop.io.retry.RetryProxy": {
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Creates a dynamic proxy for the specified interface with retry policies.\n* @param iface interface class to proxy\n* @param proxyProvider provider for failover proxy\n* @param methodNameToPolicyMap mapping of methods to retry policies\n* @param defaultPolicy default retry policy\n* @return proxy instance implementing the specified interface\n*/",
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Creates a proxy instance for the specified interface with retry handling.\n* @param iface the interface class for the proxy\n* @param proxyProvider provider for failover proxy details\n* @param retryPolicy policy for retrying failed calls\n* @return a proxy instance implementing the specified interface\n*/",
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)": "/**\n* Creates a proxy for the specified interface with retry policies.\n* @param iface interface class to proxy\n* @param implementation instance to delegate calls to\n* @param methodNameToPolicyMap mapping of methods to retry policies\n* @return proxy instance implementing the specified interface\n*/",
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Creates a proxy instance with retry handling.\n* @param iface interface class type\n* @param implementation implementation instance\n* @param retryPolicy policy for retrying failed calls\n* @return proxy instance implementing the specified interface\n*/"
    },
    "org.apache.hadoop.io.retry.LossyRetryInvocationHandler": {
        "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes a method with retry logic; drops response if retry count is low.\n* @param method the method to invoke\n* @param args the arguments for the method call\n* @return result of the method invocation\n*/",
        "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)": "/**\n* Constructs a LossyRetryInvocationHandler with drop count, proxy, and retry policies.\n* @param numToDrop number of invocations to drop\n* @param proxyProvider the FailoverProxyProvider for proxy info\n* @param retryPolicy the default retry policy\n*/",
        "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])": "/**\n* Invokes a method with reset retry count.\n* @param proxy the proxy object for invocation\n* @param method the method to invoke\n* @param args arguments for the method\n* @return result of the method invocation\n*/"
    },
    "org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister": {
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)": "/**\n* Registers a block region with metadata in the index.\n* @param raw raw data identifier, @param begin start offset, @param end end offset\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner": {
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd()": "/**\n* Checks if the current location is at or beyond the end location.\n* @return true if at end, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey()": "/**\n* Validates and reads key-value lengths from input stream.\n* @throws IOException if reading fails or end of file is reached\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd()": "/**\n* Parks the cursor at the end location and closes the block reader if open.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry()": "/**\n* Creates a new Entry after validating input key.\n* @return new Entry instance\n* @throws IOException if key validation fails\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Compares current key with another RawComparable.\n* @param other the RawComparable to compare against\n* @return comparison result as integer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long)": "/**\n* Advances the block by n records, closing the input stream if open.\n* @param n number of records to advance\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd()": "/**\n* Moves cursor to the end and closes the block reader if open.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close()": "/**\n* Closes the method by parking the cursor at the end.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)": "/**\n* Advances record index in block, checking key comparison.\n* @param key key to compare against\n* @param greater flag to determine comparison behavior\n* @return true if key matches, false if greater\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int)": "/**** Initializes a block reader and resets the current location. \n* @param blockIndex index of the block to initialize \n* @throws IOException if an I/O error occurs \n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum()": "/**\n* Retrieves the total record number at the current location.\n* @return total record number as a long value\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Initializes Scanner with reader and location bounds.\n* @param reader input source, @param begin start location, @param end end location\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)": "/**\n* Seeks to a specified location, validating bounds and updating the cursor position.\n* @param l the target Location to seek to\n* @throws IOException if an I/O error occurs during operations\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance()": "/**\n* Advances to the next record, returning false if at the end.\n* @return true if advanced, false if at end\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)": "/**\n* Initializes Scanner with reader and location bounds.\n* @param reader input source, @param offBegin start offset, @param offEnd end offset\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)": "/**\n* Seeks to a specified key location within bounds.\n* @param key the key to seek to\n* @param beyond if true, seeks beyond the key\n* @return true if successful, false if at end\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind()": "/**\n* Resets the cursor position to the beginning location.\n* @throws IOException if an I/O error occurs during seeking\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)": "/**\n* Initializes Scanner with reader and key bounds.\n* @param reader input source, @param beginKey start key, @param endKey end key\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)": "/**\n* Seeks to a specified key within a byte array.\n* @param key byte array containing the key data\n* @param keyOffset starting position of the key in the array\n* @param keyLen length of the key to seek\n* @return true if successful, false if at end\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)": "/**\n* Seeks to the lower bound of a specified key in a byte array.\n* @param key byte array containing the key data\n* @param keyOffset starting position of the key in the array\n* @param keyLen length of the key to consider\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)": "/**\n* Seeks to the upper bound of a specified key.\n* @param key byte array of the key\n* @param keyOffset starting position in key\n* @param keyLen number of bytes in key\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[])": "/**\n* Seeks to a specified key within a byte array.\n* @param key byte array containing the key data\n* @return true if successful, false if at end\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[])": "/**\n* Finds the lower bound for a given key in a byte array.\n* @param key byte array containing the key data\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[])": "/**\n* Invokes upperBound with full key range.\n* @param key byte array of the key\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$BlockCompressWriter": {
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer)": "/**\n* Writes compressed data to output after resetting buffers.\n* @param uncompressedDataBuffer input data buffer to compress\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync()": "/**\n* Synchronizes output and resets buffers if buffered records exist.\n* @throws IOException on output errors\n*/",
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close()": "/**\n* Closes the output, synchronizing if necessary.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)": "/**\n* Appends a key-value pair after validation and serialization.\n* @param key the key to append\n* @param val the value to append\n* @throws IOException if key/value classes are incorrect or during I/O operations\n*/",
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)": "/**** Appends raw key/value data to buffers and synchronizes if block size exceeds limit. */",
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])": "/**\n* Initializes BlockCompressWriter with configuration and options.\n* @param conf configuration settings\n* @param options optional parameters for customization\n*/"
    },
    "org.apache.hadoop.io.BloomMapFile": {
        "org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer)": "/**\n* Creates a byte array for a Bloom filter key from the given buffer.\n* @param buf DataOutputBuffer containing the data\n* @return byte array representing the Bloom key\n*/",
        "org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)": "/**\n* Deletes specified files and directory from the filesystem.\n* @param fs the FileSystem to operate on\n* @param name the directory name to delete files from\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.security.token.TokenIdentifier": {
        "org.apache.hadoop.security.token.TokenIdentifier:getBytes()": "/**\n* Writes data to a buffer and returns its byte array copy.\n* @return byte array of written data\n*/",
        "org.apache.hadoop.security.token.TokenIdentifier:getTrackingId()": "/**\n* Retrieves or generates the tracking ID using MD5 hash.\n* @return tracking ID as a String\n*/"
    },
    "org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer": {
        "org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData()": "/**\n* Moves data from output buffer to input buffer.\n*/"
    },
    "org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler": {
        "org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket)": "/**\n* Handles domain socket input; returns true if closed, false otherwise.\n* @param sock the DomainSocket to read from\n* @return boolean indicating socket closure status\n*/"
    },
    "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton": {
        "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown()": "/**\n* Shuts down the metrics system and unregisters the JVM metrics source.\n*/",
        "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)": "/**\n* Initializes and returns JvmMetrics, creating it if not already initialized.\n* @param processName name of the process\n* @param sessionId unique session identifier\n* @return JvmMetrics instance\n*/"
    },
    "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics": {
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown()": "/**\n* Shuts down the metrics source by unregistering it.\n* @param name the name of the metrics source to unregister\n*/",
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)": "/**\n* Records processing time for a specific RPC call.\n* @param rpcCallName identifier for the RPC call\n* @param processingTime duration to add to statistics\n*/",
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)": "/**\n* Adds deferred processing time for a given sample name.\n* @param name sample identifier\n* @param processingTime time duration to add\n*/",
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)": "/**\n* Adds overall processing time for a specific RPC call.\n* @param rpcCallName identifier for the RPC call\n* @param overallProcessingTime duration to add to statistics\n*/",
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int)": "/**\n* Initializes RpcDetailedMetrics for a specific port.\n* @param port the RPC port number for metrics tracking\n*/",
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int)": "/**\n* Creates RpcDetailedMetrics and registers it in the MetricsSystem.\n* @param port the RPC port number for metrics tracking\n* @return registered RpcDetailedMetrics instance\n*/",
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class)": "/**\n* Initializes various rate metrics for the specified protocol.\n* @param protocol the class representing the protocol to initialize rates for\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles": {
        "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)": "/**\n* Sets quantile metrics based on input names and description.\n* @param ucName upper confidence name, uvName unique value name,\n* @param desc description, lvName lower value name, df DecimalFormat for formatting\n*/",
        "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)": "/**\n* Constructs MutableInverseQuantiles with specified metrics.\n* @param name metric name, description metric description, sampleName name for samples,\n* @param valueName name for values, intervalSecs time in seconds for scheduling\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat": {
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double)": "/**\n* Synchronously adds a sample value to statistics.\n* @param x sample value to add\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate)": "/**\n* Updates metric with current stats if samples exist and resets statistics.\n* @param metric MutableRate object to update with sample data\n*/"
    },
    "org.apache.hadoop.security.authorize.ImpersonationProvider": {
        "org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)": "/**\n* Authorizes user access based on remote address.\n* @param user user information for authorization\n* @param remoteAddress user's remote IP address\n* @throws AuthorizationException if authorization fails\n*/"
    },
    "org.apache.hadoop.ipc.UserIdentityProvider": {
        "org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable)": "/**\n* Creates user identity string from Schedulable object.\n* @param obj Schedulable instance to extract user information\n* @return short username or null if user info is unavailable\n*/"
    },
    "org.apache.hadoop.tools.GetGroupsBase": {
        "org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)": "/**\n* Constructs GetGroupsBase with configuration and output stream.\n* @param conf the Configuration object\n* @param out the PrintStream for output\n*/",
        "org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes GetGroupsBase with configuration and default output stream.\n* @param conf the Configuration object\n*/",
        "org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol()": "/**\n* Retrieves GetUserMappingsProtocol instance for user group mapping.\n* @return GetUserMappingsProtocol object\n* @throws IOException if proxy retrieval fails\n*/",
        "org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[])": "/**\n* Prints user groups for specified usernames or current user if none provided.\n* @param args array of usernames; defaults to current user if empty\n* @return 0 on successful execution\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])": "/**\n* Constructs a KMSEncryptedKeyVersion with specified parameters.\n* @param keyName name of the encryption key\n* @param keyVersionName version name of the encryption key\n* @param iv initialization vector for encryption\n* @param encryptedVersionName associated encrypted version name\n* @param keyMaterial encryption key material\n*/"
    },
    "org.apache.hadoop.ha.HAServiceProtocolHelper": {
        "org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Monitors health of a service; throws IOException on failure.\n* @param svc service to monitor\n* @param reqInfo request information for state change\n*/",
        "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Transitions service to active state.\n* @param svc service protocol to transition\n* @param reqInfo request information for state change\n* @throws IOException if transition fails\n*/",
        "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Transitions the service to standby state.\n* @param svc service protocol to transition\n* @param reqInfo request information for state change\n* @throws IOException if a remote exception occurs\n*/",
        "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)": "/**\n* Transitions service to observer state.\n* @param svc service protocol to transition\n* @param reqInfo state change request information\n* @throws IOException if a remote exception occurs\n*/"
    },
    "org.apache.hadoop.ipc.RpcNoSuchProtocolException": {
        "org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String)": "/**\n* Constructs an RpcNoSuchProtocolException with a specified error message.\n* @param message descriptive error message for the exception\n*/"
    },
    "org.apache.hadoop.ipc.RpcNoSuchMethodException": {
        "org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String)": "/**\n* Constructs an RpcNoSuchMethodException with a specified error message.\n* @param message descriptive error message for the exception\n*/"
    },
    "org.apache.hadoop.ipc.RPC$VersionMismatch": {
        "org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)": "/**\n* Constructs a VersionMismatch exception for protocol version errors.\n* @param interfaceName name of the interface causing the mismatch\n* @param clientVersion version from the client\n* @param serverVersion version from the server\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine$Server": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse()": "/**\n* Registers a callback for deferred responses.\n* @return ProtobufRpcEngineCallback instance\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)": "/**\n* Processes an RPC call and returns the result as RpcWritable.\n* @param server RPC server instance\n* @param connectionProtocolName name of the connection protocol\n* @param request input buffer containing the request\n* @param methodName name of the RPC method to invoke\n* @param protocolImpl implementation of the protocol\n* @return RpcWritable result of the RPC call or null if deferred\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)": "/**\n* Constructs a Server with specified protocol and configuration.\n* @param protocolClass RPC protocol class, protocolImpl implementation, conf settings, etc.\n*/"
    },
    "org.apache.hadoop.fs.HardLink$HardLinkCGWin": {
        "org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File)": "/**\n* Constructs a command array for link count using the specified file.\n* @param file the file for which to count links\n* @return command array with file path appended\n*/"
    },
    "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics": {
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable()": "/**\n* Checks if I/O statistics are available.\n* @return true if I/O statistics method is available, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable()": "/**\n* Checks if I/O statistics context method is available.\n* @return true if the method is available, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable)": "/**\n* Retrieves I/O statistics counters from the specified source.\n* @param source the data source for statistics\n* @return a map of statistics counters\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable)": "/**\n* Invokes iostatistics gauges method with the provided source.\n* @param source data source for statistics\n* @return Map of gauge statistics with their values\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable)": "/**\n* Retrieves minimum I/O statistics from a source.\n* @param source the source of I/O data\n* @return a map of statistics with keys as strings and values as longs\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable)": "/**\n* Retrieves maximum I/O statistics for a given source.\n* @param source the input from which statistics are derived\n* @return a map of statistics with their maximum values\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable)": "/**\n* Retrieves I/O statistics means from a source.\n* @param source data source for statistics\n* @return map of statistics entries with Long values\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object)": "/**\n* Checks if the given object is a valid I/O statistics source.\n* @param object the object to check\n* @return true if it is a valid source, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object)": "/**\n* Checks if I/O statistics are available and invokes the method if so.\n* @param object the argument for the I/O statistics method\n* @return true if I/O statistics are available and invoked successfully, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable)": "/**\n* Checks if I/O statistics snapshot is available for the given object.\n* @param object the object to check for I/O statistics snapshot\n* @return true if available, otherwise false\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled()": "/**\n* Checks if I/O statistics context is enabled.\n* @return true if enabled, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString()": "/**\n* Returns a string representation of DynamicWrappedStatistics with I/O statistics availability.\n* @return formatted string with I/O statistics availability status\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable()": "/**\n* Checks if I/O statistics are available.\n* @throws Exception if the method is unavailable\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable()": "/**\n* Checks if IO statistics context method is available.\n* @throws IllegalStateException if method is unavailable\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String)": "/**\n* Initializes DynamicWrappedStatistics with methods from the specified class.\n* @param classname fully qualified name of the class to wrap\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)": "/**\n* Aggregates I/O statistics snapshot.\n* @param snapshot data to aggregate\n* @param statistics optional additional stats\n* @return true if aggregation is successful\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create()": "/**\n* Creates an I/O statistics snapshot.\n* @return Serializable snapshot data\n* @throws UnsupportedOperationException if I/O statistics are unavailable\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object)": "/**\n* Creates an I/O statistics snapshot from the given source.\n* @param source input for snapshot creation\n* @return Serializable snapshot object\n* @throws UnsupportedOperationException if operation is not supported\n* @throws ClassCastException if source is of an invalid type\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable)": "/**\n* Converts I/O statistics snapshot to JSON string.\n* @param snapshot the I/O statistics snapshot\n* @return JSON representation of the snapshot\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String)": "/**\n* Creates a Serializable I/O statistics snapshot from a JSON string.\n* @param json input JSON string representation of I/O statistics\n* @return Serializable snapshot object\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Loads I/O statistics snapshot from the specified filesystem path.\n* @param fs the filesystem to load statistics from\n* @param path the path to the statistics file\n* @return Serializable snapshot of I/O statistics\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object)": "/**\n* Retrieves I/O statistics snapshot from the specified source.\n* @param source the object source for statistics retrieval\n* @throws UnsupportedOperationException if I/O statistics are unavailable\n* @return Serializable snapshot of I/O statistics\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)": "/**\n* Saves I/O statistics snapshot to a specified path.\n* @param snapshot the data to save; may be null\n* @param fs the file system to use\n* @param path the target path for saving\n* @param overwrite whether to overwrite existing data\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object)": "/**\n* Converts I/O statistics to a pretty string format.\n* @param statistics the I/O statistics object\n* @return formatted string representation of the statistics\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent()": "/**\n* Retrieves current IO statistics context.\n* @return result of the IO statistics context method invocation\n* @throws UnsupportedOperationException if the operation is not supported\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object)": "/**\n* Sets the thread's IO statistics context.\n* @param statisticsContext context to set for IO statistics\n* @throws UnsupportedOperationException if operation is not supported\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset()": "/**\n* Resets the IO statistics context if available.\n* @throws UnsupportedOperationException if the operation is not supported\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot()": "/**\n* Retrieves IO statistics context snapshot.\n* @return Serializable snapshot of the IO statistics context\n* @throws UnsupportedOperationException if operation is not supported\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object)": "/**\n* Aggregates IO statistics context data from the given source object.\n* @param source data source for aggregation\n* @return true if aggregation was successful, false otherwise\n*/",
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>()": "/**\n* Constructs DynamicWrappedStatistics with default class name.\n*/"
    },
    "org.apache.hadoop.fs.ftp.FtpConfigKeys": {
        "org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults()": "/**\n* Retrieves default file system server settings.\n* @return FsServerDefaults object with configuration parameters\n*/"
    },
    "org.apache.hadoop.fs.local.LocalConfigKeys": {
        "org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults()": "/**\n* Retrieves default filesystem server configuration settings.\n* @return FsServerDefaults object with server configuration values\n*/"
    },
    "org.apache.hadoop.fs.WindowsGetSpaceUsed": {
        "org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh()": "/**\n* Updates the used space by fetching folder size.\n* @return void\n*/",
        "org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)": "/**\n* Constructs WindowsGetSpaceUsed with caching parameters from builder.\n* @param builder configuration for caching parameters\n*/"
    },
    "org.apache.hadoop.fs.shell.Display$Checksum": {
        "org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and sets display block size.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item, checks if it's a directory, and prints its checksum and block size.\n* @param item PathData object containing path information\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.fs.shell.Delete$Expunge": {
        "org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and initializes filesystem settings.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList)": "/**\n* Processes file system arguments and manages Trash expunging or checkpointing.\n* @param args list of PathData arguments\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.Delete$Rmdir": {
        "org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from the provided arguments.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Validates and processes the given path; throws exceptions for invalid states.\n* @param item PathData object containing path information\n* @throws IOException if path is not a directory or cannot be deleted\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommands$Get": {
        "org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and sets configuration parameters.\n* @param args list of command-line arguments\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommands$Put": {
        "org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and configures settings based on provided arguments.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList)": "/**\n* Processes PathData arguments, handling special case for stdin.\n* @param args list of PathData arguments\n* @throws IOException if path conditions are not met\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String)": "/**\n* Expands argument into a list of PathData.\n* @param arg the input argument representing a file path\n* @return List of PathData objects created from the argument\n*/"
    },
    "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile": {
        "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and validates arguments.\n* @param args list of command-line arguments\n* @throws IOException if destination argument is missing\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList)": "/**\n* Processes input arguments to read data and write to a specified destination.\n* @param args list of PathData objects representing input files\n* @throws IOException if an I/O error occurs during processing\n*/",
        "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String)": "/**\n* Expands argument into PathData list; handles stdin and URI parsing.\n* @param arg input argument string\n* @return List of PathData objects\n*/"
    },
    "org.apache.hadoop.fs.shell.Display$Cat": {
        "org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and verifies checksum based on 'ignoreCrc' flag.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream)": "/**\n* Copies bytes from InputStream to standard output and closes the stream.\n* @param in InputStream source to read bytes from\n*/",
        "org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData)": "/**\n* Retrieves an InputStream for sequential reading from the given PathData item.\n* @param item the PathData object to read from\n* @return InputStream for sequential file access\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item; throws if it's a directory, else prints its content.\n* @param item the PathData object to process\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.TouchCommands$Touchz": {
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options from the provided argument list.\n* @param args list of command-line arguments\n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData)": "/**** Touches a file by creating it if it doesn't exist. \n* @param item contains file system and path data \n* @throws IOException if an I/O error occurs during creation or closing \n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a file path; throws exceptions for directories or non-zero-length files.\n* @param item the path data to process\n* @throws IOException if an I/O error occurs or if conditions are not met\n*/",
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a nonexistent path; throws exception if parent doesn't exist.\n* @param item PathData object representing the file system path\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.MoveCommands$Rename": {
        "org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and configures command format.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)": "/**\n* Validates and renames source path to target path in the same filesystem.\n* @param src source PathData object\n* @param target target PathData object\n* @throws IOException if filesystem mismatch or rename fails\n*/"
    },
    "org.apache.hadoop.fs.FsShellPermissions$Chmod": {
        "org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData)": "/**\n* Processes a PathData item to update file permissions if they differ.\n* @param item PathData containing file status and path information\n* @throws IOException if permission change fails\n*/",
        "org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList)": "/**\n* Processes command-line options and initializes ChmodParser.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream": {
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength()": "/**\n* Retrieves the length of the file, calculating if not previously cached.\n* @return length of the file as a long value\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long)": "/**\n* Skips bytes in the file, adjusting if beyond file length.\n* @param n number of bytes to skip\n* @return new position after skipping\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long)": "/**\n* Seeks to the specified position in the stream, ensuring it's within file length.\n* @param pos desired position to seek to in the stream\n* @throws EOFException if seeking beyond end of file\n*/",
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)": "/**\n* Initializes FSDataBoundedInputStream with file system, path, and input stream.\n* @param fs file system to interact with\n* @param file path of the file to read\n* @param in input stream for reading data\n*/"
    },
    "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder": {
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Initializes RSLegacyRawEncoder with coding options and computes generating polynomial.\n* @param coderOptions configuration settings for the erasure coder\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)": "/**\n* Encodes data by resetting buffers and combining inputs/outputs.\n* @param encodingState state containing input/output buffers and lengths\n*/",
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)": "/**\n* Encodes data using specified state and resets output buffers.\n* @param encodingState contains input/output data and offsets\n*/"
    },
    "org.apache.hadoop.io.compress.CompressionCodec$Util": {
        "org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)": "/**\n* Creates a CompressionOutputStream with a pooled Compressor.\n* @param codec the CompressionCodec to use\n* @param conf configuration for the compressor\n* @param out the OutputStream to write to\n* @return CompressionOutputStream or null if creation fails\n*/",
        "org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)": "/**\n* Creates a CompressionInputStream with a codec and manages decompressor lifecycle.\n* @param codec the compression codec to use\n* @param conf configuration settings\n* @param in input stream to decompress\n* @return CompressionInputStream or null if creation fails\n*/"
    },
    "org.apache.hadoop.security.alias.UserProvider": {
        "org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String)": "/**\n* Retrieves a CredentialEntry by alias.\n* @param alias unique identifier for the credential\n* @return CredentialEntry or null if not found\n*/",
        "org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])": "/**\n* Creates a new CredentialEntry if it doesn't already exist.\n* @param name unique credential identifier\n* @param credential secret key as a char array\n* @return new CredentialEntry object\n* @throws IOException if credential already exists\n*/",
        "org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String)": "/**\n* Deletes a credential entry by name.\n* @param name the name of the credential to delete\n* @throws IOException if the credential does not exist\n*/",
        "org.apache.hadoop.security.alias.UserProvider:getAliases()": "/**\n* Retrieves a list of aliases from secret keys.\n* @return List of alias strings derived from secret keys\n*/",
        "org.apache.hadoop.security.alias.UserProvider:flush()": "/**\n* Flushes and adds credentials to the user securely.\n* @param credentials Credentials to be added to the user\n*/",
        "org.apache.hadoop.security.alias.UserProvider:<init>()": "/**\n* Initializes UserProvider with current user and their credentials.\n* @throws IOException if user retrieval fails\n*/"
    },
    "org.apache.hadoop.crypto.key.UserProvider": {
        "org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String)": "/**\n* Retrieves KeyVersion by version name.\n* @param versionName name of the key version\n* @return KeyVersion object or null if secret key not found\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String)": "/**\n* Retrieves Metadata by name, caching results for efficiency.\n* @param name identifier for the Metadata\n* @return Metadata object or null if not found\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)": "/**\n* Creates a new key with specified name and material.\n* @param name key identifier, @param material key byte array\n* @return KeyVersion object representing the created key\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String)": "/**\n* Deletes a key and its versions; throws IOException if key not found.\n* @param name the key identifier to delete\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])": "/**\n* Rolls a new key version with specified material.\n* @param name key identifier\n* @param material byte array containing key material\n* @return KeyVersion instance of the new version\n* @throws IOException if key not found or wrong length\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String)": "/**\n* Retrieves all KeyVersions for a given Metadata name.\n* @param name identifier for the Metadata\n* @return list of KeyVersion objects\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:getKeys()": "/**\n* Retrieves secret keys without '@' character.\n* @return List of filtered secret keys as strings\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:flush()": "/**\n* Synchronizes and flushes credentials to the user.\n* @param credentials Credentials to be added\n*/",
        "org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes UserProvider with configuration and retrieves current user credentials.\n* @param conf Configuration object for user management\n* @throws IOException if user initialization fails\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister": {
        "org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close()": "/**\n* Closes the key output stream and verifies key length and order.\n* @throws IOException if an I/O error occurs or key conditions are violated\n*/"
    },
    "org.apache.hadoop.ipc.Server$Handler": {
        "org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call)": "/**\n* Requeues a call and updates metrics; handles RpcServerException if thrown.\n* @param call the Call object to be requeued\n* @throws IOException if an I/O error occurs\n* @throws InterruptedException if the operation is interrupted\n*/",
        "org.apache.hadoop.ipc.Server$Handler:run()": "/**** Runs the server, processing calls from the queue and updating metrics accordingly. */"
    },
    "org.apache.hadoop.metrics2.lib.MutableRate": {
        "org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)": "/**\n* Constructs a MutableRate with specified name, description, and extended flag.\n* @param name metric name\n* @param description metric description\n* @param extended flag for extended metrics\n*/"
    },
    "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker": {
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)": "/**\n* Retrieves protocol implementation for a given name and client version.\n* @param server RPC server instance\n* @param protoName name of the protocol\n* @param clientVersion requested client version\n* @return ProtoClassProtoImpl object\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)": "/**\n* Executes a RPC call and handles response or errors.\n* @param server RPC server instance, methodName the RPC method name, \n* @param request the request buffer, protocolImpl the protocol implementation\n* @return wrapped result of the RPC call\n*/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)": "/********************************************************************************\n* Executes an RPC call based on protocol implementation.\n* @param server RPC server instance, connectionProtocolName the protocol name,\n* @param request input buffer, receiveTime timestamp, methodName the RPC method,\n* @param declaringClassProtoName protocol class name, clientVersion requested version\n* @return Writable result of the RPC call\n********************************************************************************/",
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)": "/**\n* Handles RPC calls, returning protobuf responses or exceptions in Writable format.\n* @param server the RPC server instance\n* @param connectionProtocolName the protocol name for the connection\n* @param writableRequest the request payload\n* @param receiveTime timestamp of the request reception\n* @return Writable response containing the result or error information\n*/"
    },
    "org.apache.hadoop.ipc.Client$Connection$RpcRequestSender": {
        "org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run()": "/**\n* Processes RPC requests until connection closure.\n* @throws InterruptedException if the thread is interrupted\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.util.SysInfo": {
        "org.apache.hadoop.util.SysInfo:newInstance()": "/**\n* Creates a new SysInfo instance based on the operating system.\n* @return SysInfo object for the current OS type\n*/"
    },
    "org.apache.hadoop.util.Classpath": {
        "org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)": "/**\n* Logs a message and terminates the application with a specified status.\n* @param status exit status code\n* @param msg detailed termination message\n*/",
        "org.apache.hadoop.util.Classpath:main(java.lang.String[])": "/**\n* Main method for processing command-line options and creating a JAR file.\n* @param args command-line arguments\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer": {
        "org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run()": "/**\n* Closes all file systems automatically and logs exceptions if they occur.\n*/"
    },
    "org.apache.hadoop.fs.ftp.FtpFs": {
        "org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults()": "/**\n* Retrieves deprecated server defaults configuration.\n* @return FsServerDefaults object with settings\n*/",
        "org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves default file system server settings.\n* @param f file path (not used in this implementation)\n* @return FsServerDefaults object with configuration parameters\n*/",
        "org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FtpFs with URI and configuration.\n* @param theUri URI for the FTP file system\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.fs.local.RawLocalFs": {
        "org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path)": "/**\n* Retrieves server configuration settings for the given filesystem path.\n* @param f filesystem path\n* @return FsServerDefaults object with server configuration values\n*/",
        "org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults()": "/**\n* Retrieves default filesystem server configuration settings.\n* @return FsServerDefaults object with server configuration values\n*/",
        "org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs RawLocalFs with URI and configuration.\n* @param theUri URI for the local file system\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes RawLocalFs with configuration settings.\n* @param conf configuration settings for the file system\n*/"
    },
    "org.apache.hadoop.metrics2.filter.GlobFilter": {
        "org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String)": "/**\n* Compiles a string into a regex Pattern.\n* @param s glob pattern string to compile\n* @return compiled Pattern object\n*/"
    },
    "org.apache.hadoop.fs.shell.Delete$Rmr": {
        "org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList)": "/**\n* Modifies options for processing and invokes the superclass method.\n* @param args command-line arguments to process\n*/"
    },
    "org.apache.hadoop.fs.shell.Ls$Lsr": {
        "org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList)": "/**\n* Modifies args by adding \"-R\" and processes command-line options.\n* @param args list of command-line arguments\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.FsUsage$Dus": {
        "org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList)": "/**\n* Modifies options by adding '-s' and processes them.\n* @param args command-line arguments list\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.shell.Display$TextRecordInputStream": {
        "org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close()": "/**\n* Closes resources and invokes superclass close method.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read()": "/**\n* Reads data from input buffer, processes key-value pairs, and writes to output buffer.\n* @return int value read from input or -1 if end of data is reached\n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Writer": {
        "org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Writer with output stream and configuration settings.\n* @param fsdos output stream for writing data\n* @param minBlockSize minimum block size for data\n* @param compressName name of the compression algorithm\n* @param comparator key comparator for sorting\n* @param conf configuration settings for the writer\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean)": "/**\n* Finalizes and compresses the data block if size limit is exceeded or forced.\n* @param bForceFinish flag to force finishing the block\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int)": "/**\n* Prepares a DataOutputStream for appending a value based on length.\n* @param length size of the value; negative indicates unknown size\n* @return DataOutputStream for value appending\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock()": "/**\n* Initializes the data block appender if not already set.\n* @throws IOException if data block preparation fails\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)": "/**\n* Prepares a MetaBlock for writing.\n* @param name unique name for the MetaBlock\n* @param compressName name of the compression algorithm\n* @return DataOutputStream for the new MetaBlock\n* @throws IOException if I/O error occurs\n* @throws MetaBlockAlreadyExists if a MetaBlock with the same name exists\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:close()": "/**\n* Closes the TFile, finalizing data writing and releasing resources.\n* @throws IOException if an I/O error occurs during closing\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String)": "/**\n* Prepares a MetaBlock with a unique name.\n* @param name unique name for the MetaBlock\n* @return DataOutputStream for the new MetaBlock\n* @throws IOException if I/O error occurs\n* @throws MetaBlockAlreadyExists if a MetaBlock with the same name exists\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int)": "/**\n* Prepares a DataOutputStream for appending a key.\n* @param length the length of the key\n* @return DataOutputStream for the key\n* @throws IOException if data block initialization fails\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)": "/**\n* Appends a key-value pair to a data stream.\n* @param key byte array of the key\n* @param koff offset in key array\n* @param klen length of the key\n* @param value byte array of the value\n* @param voff offset in value array\n* @param vlen length of the value\n* @throws IOException if appending fails\n*/",
        "org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])": "/**\n* Appends a key-value pair to a data stream.\n* @param key byte array of the key\n* @param value byte array of the value\n* @throws IOException if appending fails\n*/"
    },
    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover": {
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run()": "/**\n* Manages token removal and master key updates in a background thread.\n*/"
    },
    "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder": {
        "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)": "/**\n* Constructs XORErasureEncoder with specified options.\n* @param options configuration settings for the encoder\n*/",
        "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)": "/***************\n* Prepares an ErasureCodingStep using a block group.\n* @param blockGroup the ECBlockGroup for encoding\n* @return configured ErasureCodingStep instance\n****************/"
    },
    "org.apache.hadoop.util.JvmPauseMonitor$Monitor": {
        "org.apache.hadoop.util.JvmPauseMonitor$Monitor:run()": "/**\n* Monitors JVM pauses and logs GC sleep time exceeding thresholds.\n*/"
    },
    "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream": {
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)": "/**\n* Initializes HarFSDataInputStream from a file system and path.\n* @param fs file system to read from\n* @param p path of the file\n* @param start starting byte position\n* @param length number of bytes to read\n* @param bufsize size of the buffer\n*/"
    },
    "org.apache.hadoop.util.VersionUtil": {
        "org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)": "/**\n* Compares two version strings.\n* @param version1 first version string\n* @param version2 second version string\n* @return comparison result: negative, zero, or positive\n*/"
    },
    "org.apache.hadoop.io.BloomMapFile$Reader": {
        "org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable)": "/**\n* Checks if a key probably exists in the bloom filter.\n* @param key the key to test for existence\n* @return true if the key is possibly present, false if not\n*/",
        "org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)": "/**\n* Retrieves value for the given key if it exists in the bloom filter.\n* @param key the key to seek\n* @param val Writable object to store the current value\n* @return Writable object or null if key is not present\n*/",
        "org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a BloomFilter from a file in the specified directory.\n* @param dirName directory path for the BloomFilter file\n* @param conf configuration for accessing the filesystem\n*/",
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])": "/**\n* Constructs a Reader and initializes a BloomFilter.\n* @param dir directory path for the BloomFilter file\n* @param conf configuration for accessing the filesystem\n* @param options additional options for the Reader\n*/",
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a deprecated Reader for a BloomFilter from a directory name.\n* @param fs filesystem instance\n* @param dirName directory path for the BloomFilter file\n* @param conf configuration for accessing the filesystem\n*/",
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)": "/**\n* Constructs a Reader for a BloomFilter from the specified directory.\n* @param fs filesystem for reading, dirName directory path, comparator for data comparison,\n* @param conf configuration settings, open flag to indicate immediate opening\n*/",
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs a deprecated Reader for BloomFilter from directory and comparator.\n* @param fs filesystem instance\n* @param dirName directory path for the BloomFilter file\n* @param comparator comparator for reading data\n* @param conf configuration for accessing the filesystem\n*/"
    },
    "org.apache.hadoop.io.SecureIOUtils": {
        "org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)": "/**\n* Creates a FileOutputStream and sets permissions if file doesn't exist.\n* @param f the file to create\n* @param permissions short encoding for permissions\n* @return FileOutputStream for the created file\n* @throws IOException if an I/O error occurs or file exists\n*/",
        "org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)": "/**\n* Creates a FileOutputStream for writing; uses insecure method if security is skipped.\n* @param f the file to create or overwrite\n* @param permissions new file permissions as an integer\n* @return FileOutputStream for the specified file\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Validates file owner against expected owner and group.\n* @param f file to check; @param owner actual owner; \n* @param group actual group; @param expectedOwner expected owner; \n* @param expectedGroup expected group\n* @throws IOException if owner does not match expected\n*/",
        "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Opens a file for random read and checks owner/group validity.\n* @param f file to open; @param mode access mode; \n* @param expectedOwner expected file owner; @param expectedGroup expected file group\n* @return RandomAccessFile object\n*/",
        "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)": "/**\n* Opens a secure FSDataInputStream and validates file ownership.\n* @param file the file to open; @param expectedOwner expected file owner; @param expectedGroup expected group\n* @return FSDataInputStream if successful; throws IOException on failure\n*/",
        "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)": "/************************************************************************************ \n* Opens a file securely for reading, verifying owner and group against expectations. \n* @param f file to open; @param expectedOwner expected file owner; @param expectedGroup expected group \n* @return FileInputStream for the opened file \n* @throws IOException if the file cannot be opened or checks fail \n************************************************************************************/",
        "org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)": "/**** Opens a file for random read with security checks. \n* @param f file to open; @param mode access mode; \n* @param expectedOwner expected file owner; @param expectedGroup expected file group \n* @return RandomAccessFile object \n*/",
        "org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)": "/**\n* Opens a file input stream securely or raw based on security settings.\n* @param file the file to open; @param expectedOwner expected file owner; @param expectedGroup expected group\n* @return FSDataInputStream for the file\n*/",
        "org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)": "/**\n* Opens a file for reading, ensuring security if enabled.\n* @param f file to open; @param expectedOwner expected file owner; @param expectedGroup expected group\n* @return FileInputStream for the opened file\n* @throws IOException if the file cannot be opened\n*/"
    },
    "org.apache.hadoop.io.SequenceFile$Sorter": {
        "org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)": "/**\n* Writes records to a file using a specified writer.\n* @param records iterator of key-value pairs to write\n* @param writer target Writer for output\n* @throws IOException for write errors or sync issues\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)": "/**\n* Initializes a Sorter with configuration and metadata.\n* @param fs file system instance\n* @param comparator comparator for sorting keys\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param conf configuration settings\n* @param metadata metadata for sorting\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)": "/****\n* Constructs a Sorter with file system and sorting configuration.\n* @param fs file system instance\n* @param comparator comparator for sorting keys\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Sorter with file system and key/value class types.\n* @param fs file system instance\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)": "/**\n* Clones file attributes from input to output file.\n* @param inputFile source file path\n* @param outputFile destination file path\n* @param prog progress tracker\n* @return Writer instance for output file\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)": "/**\n* Merges segments into a single output file.\n* @param segments list of segment descriptors to merge\n* @param tmpDir temporary directory for merge process\n* @return RawKeyValueIterator for merged segments\n* @throws IOException if an I/O error occurs during merging\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)": "/**\n* Merges input segments into a single output file.\n* @param inNames array of input file paths\n* @param deleteInputs flag to delete inputs after merge\n* @param factor number of segments to merge\n* @param tmpDir temporary directory for merge process\n* @return RawKeyValueIterator for merged segments\n* @throws IOException if an I/O error occurs during merging\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)": "/**\n* Merges input segments into a single output file.\n* @param inNames array of input segment paths\n* @param tempDir temporary directory for merge outputs\n* @param deleteInputs flag to delete input files after merge\n* @return RawKeyValueIterator for merged segments\n* @throws IOException if an I/O error occurs during merging\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Merges segments from input paths into a single output.\n* @param inName input file path, indexIn index file path, tmpDir temporary directory path\n* @return RawKeyValueIterator for merged segments\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean)": "/**\n* Executes a sorting pass and manages resources.\n* @param deleteInput flag to delete input files post-processing\n* @return number of segments processed\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)": "/**\n* Merges input files into a single output file.\n* @param inNames array of input file paths\n* @param deleteInputs flag to delete inputs after merge\n* @param tmpDir temporary directory for merge process\n* @return RawKeyValueIterator for merged segments\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path)": "/**\n* Executes a merge pass, writing merged records to an output file.\n* @param tmpDir temporary directory for processing\n* @return 0 upon successful completion\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)": "/**\n* Sorts input files and returns an iterator for the sorted output.\n* @param inFiles array of input file paths\n* @param tempDir temporary directory for output\n* @param deleteInput flag to delete inputs after sorting\n* @return RawKeyValueIterator for sorted segments or null if no segments\n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)": "/**** Merges input files into an output file. \n* @param inFiles array of input file paths \n* @param outFile destination output file path \n* @throws IOException if output file exists or write errors occur \n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)": "/**** Sorts input files and optionally merges output; throws IOException on existing output file.  \n* @param inFiles array of input file paths  \n* @param outFile output file path  \n* @param deleteInput flag to delete input files post-processing  \n*/",
        "org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Sorts a single input file and outputs the result.\n* @param inFile input file path\n* @param outFile output file path\n*/"
    },
    "org.apache.hadoop.util.InstrumentedReadWriteLock": {
        "org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)": "/**\n* Constructs an InstrumentedReadWriteLock with specified fairness and logging parameters.\n* @param fair enables fair locking; name for lock identification; logger for logging\n* @param minLoggingGapMs minimum gap for logging in milliseconds; lockWarningThresholdMs for warnings\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)": "/**\n* Fills a queue with EncryptedKeyVersion instances for a given key.\n* @param keyName name of the encryption key\n* @param keyQueue queue to populate with key versions\n* @param numEKVs number of key versions to generate\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream": {
        "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])": "/**\n* Constructs CryptoFSDataInputStream with encryption support.\n* @param in input stream, must be Seekable and PositionedReadable\n* @param codec encryption codec\n* @param bufferSize size of the buffer\n* @param key encryption key\n* @param iv initialization vector\n*/",
        "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])": "/**\n* Constructs CryptoFSDataInputStream with encryption.\n* @param in FSDataInputStream to read data from\n* @param codec CryptoCodec for data encryption\n* @param key encryption key\n* @param iv initialization vector\n*/"
    },
    "org.apache.hadoop.util.BasicDiskValidator": {
        "org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File)": "/**\n* Checks the status of the specified directory.\n* @param dir the directory to check\n* @throws DiskErrorException if the directory cannot be validated or created\n*/"
    },
    "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping": {
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String)": "/**\n* Executes shell command to fetch users for a netgroup.\n* @param netgroup name of the netgroup (without '@')\n* @return output of the command as a string\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String)": "/**\n* Retrieves a list of users for a specified netgroup.\n* @param netgroup name of the netgroup (without '@')\n* @return List of usernames associated with the netgroup\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)": "/**\n* Retrieves all groups for a user, including netgroups.\n* @param user the username to fetch groups for\n* @return List of group names\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List)": "/**\n* Caches netgroups by adding users if not already cached.\n* @param groups list of group names to process\n*/",
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()": "/**\n* Refreshes cached netgroup names by clearing and re-adding them.\n* @throws IOException if an I/O error occurs during processing\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable": {
        "org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin()": "/**\n* Renews user ticket and relogs from ticket cache.\n* @throws IOException if relogin fails\n*/"
    },
    "org.apache.hadoop.fs.FSLinkResolver": {
        "org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)": "/**\n* Qualifies a symlink target path based on the provided URI.\n* @param pathURI base URI for qualification\n* @param pathWithLink path containing the symlink\n* @param target target path to qualify\n* @return qualified Path object\n*/",
        "org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)": "/**\n* Resolves a path, handling symlinks up to a maximum limit.\n* @param fc FileContext for filesystem operations\n* @param path the Path object to resolve\n* @return resolved object of type T\n* @throws IOException if symlink resolution fails or is disabled\n*/"
    },
    "org.apache.hadoop.io.ArrayFile$Writer": {
        "org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable)": "/**\n* Appends a value to the map and increments the count.\n* @param value the Writable value to append\n* @throws IOException on I/O errors\n*/",
        "org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)": "/**\n* Constructs a Writer for outputting Writable objects to a specified file.\n* @param conf configuration settings\n* @param fs file system to use\n* @param file output file path\n* @param valClass class of the Writable value type\n*/",
        "org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)": "/**** Initializes a Writer for outputting data to a specified file. \n* @param conf configuration settings \n* @param fs file system to use \n* @param file output file path \n* @param valClass value type for serialization \n* @param compress compression type \n* @param progress progress tracker \n*/"
    },
    "org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister": {
        "org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close()": "/**\n* Closes the resource, updates record counts, and manages error state.\n* @throws IOException if an I/O error occurs during closing\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier": {
        "org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>()": "/**\n* Initializes a KMSDelegationTokenIdentifier with a predefined token kind.\n*/"
    },
    "org.apache.hadoop.util.ReadWriteDiskValidator": {
        "org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File)": "/**\n* Checks directory status, validates, and measures file read/write latencies.\n* @param dir directory to check\n* @throws DiskErrorException if errors occur during validation or file operations\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller": {
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run()": "/**\n* Collects and snapshots metrics while ensuring single record integrity.\n*/",
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages)": "/**\n* Initializes RatesRoller with a parent MutableRollingAverages instance.\n* @param parent the MutableRollingAverages to associate with this RatesRoller\n*/"
    },
    "org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable": {
        "org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin()": "/**\n* Relogs in using a keytab, may throw IOException on failure.\n*/"
    },
    "org.apache.hadoop.io.BloomMapFile$Writer": {
        "org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)": "/**\n* Appends key-value pair and updates Bloom filter.\n* @param key the key to append\n* @param val the value to append\n* @throws IOException on I/O errors\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a Bloom filter using configuration parameters.\n* @param conf configuration object for settings\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:close()": "/**\n* Closes the stream and writes a bloom filter to a file.\n* @throws IOException if an I/O error occurs during closing or writing\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])": "/**\n* Constructs a Writer with specified configuration and directory.\n* @param conf configuration settings for the writer\n* @param dir directory path for the output\n* @param options additional writer options\n* @throws IOException if filesystem or writer initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated Writer with specified configuration and parameters.\n* @param conf configuration settings for the writer\n* @param fs filesystem for writing\n* @param dirName output directory name\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @param compress compression type\n* @param codec compression codec\n* @param progress progressable callback\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated Writer for sequence files with specified settings.\n* @param conf configuration settings, fs filesystem, dirName output directory\n* @param keyClass class type for keys, valClass class type for values\n* @param compress compression type, progress progress indicator\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Constructs a deprecated Writer with specified configuration and directory.\n* @param conf configuration settings for the writer\n* @param fs filesystem instance\n* @param dirName output directory path\n* @param keyClass class of the key type\n* @param valClass class of the value type\n* @param compress compression type\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated Writer with specified parameters for configuration and output directory.\n* @param conf configuration settings for the writer\n* @param fs filesystem instance\n* @param dirName output directory name\n* @param comparator comparator for sorting\n* @param valClass class of the value\n* @param compress compression type\n* @param codec compression codec\n* @param progress progressable for tracking\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)": "/**\n* Constructs a deprecated Writer with specified configuration and file system.\n* @param conf configuration settings for the writer\n* @param fs file system to use\n* @param dirName output directory name\n* @param comparator key comparator\n* @param valClass value class type\n* @param compress compression type\n* @param progress progress indicator\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)": "/**\n* Constructs a deprecated Writer with configuration and filesystem settings.\n* @param conf configuration settings for the writer\n* @param fs filesystem to write to\n* @param dirName output directory path\n* @param comparator comparator for sorting\n* @param valClass class of the value being written\n* @param compress compression type for the output\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)": "/**\n* Constructs a deprecated Writer with configuration, filesystem, and directory name.\n* @param conf configuration settings for the writer\n* @param fs filesystem to use\n* @param dirName output directory name\n* @param comparator comparator for sorting\n* @param valClass class of the value type\n* @throws IOException if initialization fails\n*/",
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)": "/**\n* Constructs a deprecated Writer for specified configuration and directory.\n* @param conf configuration settings for the writer\n* @param fs filesystem to use\n* @param dirName directory path for output\n* @param keyClass class type for keys\n* @param valClass class type for values\n* @throws IOException if initialization fails\n*/"
    },
    "org.apache.hadoop.fs.FileContext$FileContextFinalizer": {
        "org.apache.hadoop.fs.FileContext$FileContextFinalizer:run()": "/**\n* Executes the run method to process exit deletions.\n*/"
    },
    "org.apache.hadoop.fs.InternalOperations": {
        "org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])": "/**\n* Renames a file or directory in the given FileSystem.\n* @param fs the FileSystem to perform the rename operation\n* @param src source Path to rename\n* @param dst destination Path\n* @param options rename options, e.g., overwrite\n* @throws IOException if an error occurs during renaming\n*/"
    },
    "org.apache.hadoop.fs.ChecksumFileSystem$FsOperation": {
        "org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path)": "/**\n* Executes an operation on a path and checks for a checksum file.\n* @param p the path to process\n* @return true if the operation succeeded, false otherwise\n*/"
    },
    "org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker": {
        "org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)": "/**\n* Handles RPC calls, verifies versions, and invokes the corresponding protocol method.\n* @param server the RPC server handling the call\n* @param protocolName the name of the protocol\n* @param rpcRequest the request containing the method and parameters\n* @param receivedTime the time the request was received\n* @return Writable response from the invoked method\n*/"
    },
    "org.apache.hadoop.io.ByteWritable$Comparator": {
        "org.apache.hadoop.io.ByteWritable$Comparator:<init>()": "/**\n* Constructs a Comparator for ByteWritable objects.\n*/"
    },
    "org.apache.hadoop.conf.ConfigurationWithLogging": {
        "org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)": "/**\n* Sets a property value and logs the action with redacted value.\n* @param name property name\n* @param value property value\n* @param source origin of the property setting\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String)": "/**\n* Retrieves property value by name and logs the redacted value.\n* @param name the property name to fetch\n* @return property value or null if not found\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)": "/**\n* Retrieves property value and logs it with redacted sensitive info.\n* @param name property name to retrieve\n* @param defaultValue value returned if property is not found\n* @return property value or null if not found\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes ConfigurationWithLogging with logging and redaction features.\n* @param conf configuration object for initialization\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)": "/**\n* Retrieves an integer property and logs the retrieval process.\n* @param name property name to fetch\n* @param defaultValue value to return if property is absent\n* @return parsed integer value\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)": "/**\n* Retrieves a long value from properties and logs the retrieval process.\n* @param name property name to fetch\n* @param defaultValue value to return if property is absent\n* @return parsed long value\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)": "/**\n* Retrieves a float property and logs the retrieval process.\n* @param name property name to fetch\n* @param defaultValue value to return if property is missing\n* @return parsed float value\n*/",
        "org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)": "/**\n* Retrieves a boolean property and logs the result.\n* @param name property name to fetch\n* @param defaultValue value to return if property is invalid\n* @return boolean value from property or defaultValue if not valid\n*/"
    },
    "org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks": {
        "org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus)": "/**\n* Reports the current service status.\n* @param status the service status to be reported\n*/"
    },
    "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory": {
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes DiskBlockFactory with buffer directory and configuration.\n* @param keyToBufferDir key for buffer directory in configuration\n* @param conf configuration object for retrieving properties\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a temporary file for writing.\n* @param pathStr desired path string\n* @param size required size for the path\n* @param conf configuration settings\n* @return temporary File object\n*/",
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)": "/**\n* Creates a DataBlock with a temporary file.\n* @param index unique block identifier\n* @param limit maximum size for the block\n* @param statistics block upload statistics\n* @return DataBlock instance\n*/"
    },
    "org.apache.hadoop.conf.ReconfigurationUtil": {
        "org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)": "/**\n* Compares two configurations and returns changed properties.\n* @param newConf new configuration to compare\n* @param oldConf old configuration for comparison\n* @return collection of PropertyChange objects\n*/",
        "org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)": "/**\n* Parses and retrieves changed properties between two configurations.\n* @param newConf the new configuration to compare\n* @param oldConf the old configuration for comparison\n* @return collection of PropertyChange objects\n*/"
    },
    "org.apache.hadoop.io.compress.bzip2.Bzip2Factory": {
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)": "/**\n* Checks and loads the native Bzip2 library.\n* @param conf configuration containing library name\n* @return true if native Bzip2 is loaded, false otherwise\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)": "/**\n* Sets the block size for bzip2 compression in the configuration.\n* @param conf configuration object to update\n* @param blockSize size of the block to set\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)": "/**\n* Sets the work factor for BZip2 compression.\n* @param conf Configuration object to modify\n* @param workFactor integer value for compression work factor\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the Bzip2 library name based on native loading status.\n* @param conf configuration for library loading check\n* @return library name as a String\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration)": "/**\n* Returns the Bzip2 compressor class based on native library availability.\n* @param conf configuration for checking native library\n* @return Compressor class type for Bzip2\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration)": "/**\n* Returns the Bzip2 decompressor class based on native library availability.\n* @param conf configuration for checking native Bzip2 library\n* @return Class of the appropriate decompressor\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration)": "/**\n* Returns a Bzip2 decompressor based on native library availability.\n* @param conf configuration for checking native library\n* @return Decompressor instance (Bzip2 or dummy)\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the block size for Bzip2 compression.\n* @param conf configuration object containing settings\n* @return block size as an integer\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves the work factor for Bzip2 compression.\n* @param conf configuration object containing settings\n* @return work factor as an integer\n*/",
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration)": "/**\n* Returns a Bzip2 compressor based on native library availability.\n* @param conf configuration for compression settings\n* @return Bzip2Compressor or BZip2DummyCompressor instance\n*/"
    },
    "org.apache.hadoop.net.TableMapping$RawTableMapping": {
        "org.apache.hadoop.net.TableMapping$RawTableMapping:load()": "/**\n* Loads key-value pairs from a file into a map.\n* @return Map of properties or null if loading fails\n*/",
        "org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List)": "/**\n* Resolves names to their corresponding values from a loaded map.\n* @param names list of names to resolve\n* @return list of resolved values or default if not found\n*/",
        "org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings()": "/**\n* Reloads cached mappings from a file; logs error if loading fails.\n*/",
        "org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List)": "/**\n* Reloads all cached mappings; specific nodes cannot be reloaded.\n* @param names list of mapping names (not used in this method)\n*/"
    },
    "org.apache.hadoop.http.lib.StaticUserWebFilter": {
        "org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves username from configuration, handling deprecated UGI format.\n* @param conf configuration object\n* @return username string or default if not found\n*/",
        "org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes a filter with a static user from configuration.\n* @param container filter container to add the filter\n* @param conf configuration object for retrieving username\n*/"
    },
    "org.apache.hadoop.security.SecurityUtil$TruststoreKeystore": {
        "org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes Truststore and Keystore from configuration settings.\n* @param conf configuration containing keystore and truststore details\n*/"
    },
    "org.apache.hadoop.crypto.CryptoCodec": {
        "org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)": "/**\n* Retrieves codec classes based on configuration and cipher suite.\n* @param conf configuration settings\n* @param cipherSuite cipher suite for codec selection\n* @return list of CryptoCodec classes or null if none found\n*/",
        "org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)": "/**\n* Retrieves a CryptoCodec instance matching the given cipher suite.\n* @param conf configuration settings\n* @param cipherSuite desired cipher suite\n* @return matching CryptoCodec or null if not found\n*/",
        "org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a CryptoCodec instance using configuration settings.\n* @param conf configuration settings\n* @return matching CryptoCodec or null if not found\n*/"
    },
    "org.apache.hadoop.security.alias.CredentialProviderFactory": {
        "org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a list of CredentialProviders from the given configuration.\n* @param conf configuration containing provider paths\n* @return List of CredentialProvider instances\n* @throws IOException if an error occurs during retrieval\n*/"
    },
    "org.apache.hadoop.security.authorize.ProxyServers": {
        "org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration)": "/**\n* Refreshes proxy server addresses from configuration.\n* @param conf configuration object containing proxy server details\n*/",
        "org.apache.hadoop.security.authorize.ProxyServers:refresh()": "/**\n* Refreshes the configuration settings to apply defaults.\n*/",
        "org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String)": "/**\n* Checks if the given address is a proxy server.\n* @param remoteAddr the address to check\n* @return true if it's a proxy server, false otherwise\n*/"
    },
    "org.apache.hadoop.metrics2.lib.MetricsAnnotations": {
        "org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object)": "/**\n* Creates a MetricsSource from the given source object.\n* @param source object to gather metrics from\n* @return MetricsSource instance\n*/",
        "org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object)": "/**\n* Creates a new MetricsSourceBuilder for the given source object.\n* @param source object to gather metrics from\n* @return MetricsSourceBuilder instance\n*/"
    },
    "org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder": {
        "org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build()": "/**\n* Builds a CompletableFuture for FSDataInputStream with file parameters.\n* @return future input stream for the file\n*/"
    },
    "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder": {
        "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build()": "/**\n* Builds and opens a file with specified parameters.\n* @return CompletableFuture of FSDataInputStream\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Initializes FSDataInputStreamBuilder with specified file system and path.\n* @param fileSystem the file system to use\n* @param path the path for initialization\n*/",
        "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)": "/**\n* Initializes FSDataInputStreamBuilder with specified file system and path handle.\n* @param fileSystem the file system to use\n* @param pathHandle the path handle for the builder\n*/"
    },
    "org.apache.hadoop.io.retry.RetryUtils": {
        "org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)": "/**\n* Retrieves a MultipleLinearRandomRetry based on configuration settings.\n* @param conf configuration object\n* @param retryPolicyEnabledKey key for enabling retry policy\n* @param defaultRetryPolicyEnabled default enabled state\n* @param retryPolicySpecKey key for retry policy specification\n* @param defaultRetryPolicySpec default retry policy specification\n* @return RetryPolicy object or null if disabled\n*/",
        "org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)": "/**\n* Retrieves default retry policy based on configuration settings.\n* @param conf configuration object\n* @param retryPolicyEnabledKey key for enabling retry policy\n* @param defaultRetryPolicyEnabled default enabled state\n* @param retryPolicySpecKey key for retry policy specification\n* @param defaultRetryPolicySpec default retry policy specification\n* @param remoteExceptionToRetry exception to retry on\n* @return RetryPolicy object for retries or a fail policy if disabled\n*/"
    },
    "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1": {
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported()": "/**\n* Checks if a feature is supported.\n* @return true if supported, false otherwise\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec()": "/**\n* Retrieves the compression codec used for data processing.\n* @return CompressionCodec object for encoding/decoding\n* @throws IOException if an error occurs during retrieval\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)": "/**\n* Creates a stream for decompressing data.\n* @param downStream input stream to decompress\n* @param decompressor decompressor algorithm to use\n* @param downStreamBufferSize buffer size for the stream\n* @return InputStream for decompressed data\n* @throws IOException if an I/O error occurs\n*/",
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)": "/**\n* Creates a compression stream for the given output stream.\n* @param downStream the output stream to compress data into\n* @param compressor the compressor to use for compression\n* @param downStreamBufferSize the buffer size for the output stream\n* @return OutputStream for compressed data\n*/"
    },
    "org.apache.hadoop.util.NativeLibraryChecker": {
        "org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[])": "/**\n* Main method to check availability of native libraries and print results.\n* @param args command-line arguments for options (-a for all, -h for help)\n*/"
    },
    "org.apache.hadoop.security.AuthenticationFilterInitializer": {
        "org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Constructs a filtered configuration map based on the given prefix.\n* @param conf Configuration object for fetching properties\n* @param prefix Prefix to filter property names\n* @return Map of filtered properties with specified prefix\n*/",
        "org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes filter with authentication settings from configuration.\n* @param container FilterContainer to add the filter\n* @param conf Configuration object for fetching properties\n*/"
    },
    "org.apache.hadoop.fs.FsUrlStreamHandlerFactory": {
        "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Initializes FsUrlStreamHandlerFactory with configuration and preloads FileSystem classes.\n* @param conf configuration settings for the handler\n*/",
        "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String)": "/**\n* Creates a URLStreamHandler for the specified protocol.\n* @param protocol the protocol to create a handler for\n* @return URLStreamHandler or null if protocol is unknown\n*/",
        "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>()": "/**\n* Constructs FsUrlStreamHandlerFactory with default configuration.\n*/"
    },
    "org.apache.hadoop.security.authorize.ProxyUsers": {
        "org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves an instance of ImpersonationProvider using the provided configuration.\n* @param conf configuration settings\n* @return ImpersonationProvider instance or null if not found\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)": "/**\n* Refreshes super user groups configuration and initializes impersonation provider.\n* @param conf configuration settings\n* @param proxyUserPrefix non-empty prefix for proxy user\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration)": "/**\n* Refreshes super user groups configuration using default proxy user prefix.\n* @param conf configuration settings\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration()": "/**\n* Refreshes super user groups configuration with default settings.\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:getSip()": "/**\n* Retrieves the ImpersonationProvider instance, refreshing config if not initialized.\n* @return ImpersonationProvider instance\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)": "/**** Authorizes user access based on remote address. \n* @param user user information for authorization \n* @param remoteAddress user's remote IP address \n* @throws AuthorizationException if authorization fails */",
        "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)": "/**\n* Authorizes user access based on their group information and remote address.\n* @param user user group information for authorization\n* @param remoteAddress address of the user attempting access\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider()": "/**\n* Retrieves the default impersonation provider instance.\n* @return DefaultImpersonationProvider cast from ImpersonationProvider\n*/",
        "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)": "/**\n* Deprecated method for user authorization with configuration.\n* @param user user information for authorization\n* @param remoteAddress user's remote IP address\n* @param conf configuration settings for authorization\n* @throws AuthorizationException if authorization fails\n*/"
    },
    "org.apache.hadoop.fs.HarFs": {
        "org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Initializes HarFs with URI and configuration.\n* @param theUri URI for the file system\n* @param conf configuration settings\n*/"
    },
    "org.apache.hadoop.security.RuleBasedLdapGroupsMapping": {
        "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String)": "/**\n* Retrieves a set of group names for a user, modifying case based on specified rule.\n* @param user the user identifier\n* @return Set of group names in specified case or original if NONE\n*/",
        "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String)": "/**\n* Retrieves user groups and applies case transformation based on the rule.\n* @param user the user identifier\n* @return List of transformed group names or original if no transformation\n*/",
        "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)": "/**\n* Sets configuration and initializes conversion rule from provided Configuration object.\n* @param conf configuration containing LDAP parameters\n*/"
    },
    "org.apache.hadoop.fs.local.LocalFs": {
        "org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration)": "/**\n* Constructs LocalFs using the provided configuration.\n* @param conf configuration settings for the filesystem\n* @throws IOException if an I/O error occurs\n* @throws URISyntaxException if the URI is invalid\n*/",
        "org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Constructs LocalFs with a URI and configuration settings.\n* @param theUri the URI for the filesystem\n* @param conf configuration settings for the filesystem\n*/"
    },
    "org.apache.hadoop.ipc.Client$Connection$1": {
        "org.apache.hadoop.ipc.Client$Connection$1:run()": "/**\n* Handles the main execution loop for processing RPC responses.\n*/"
    },
    "org.apache.hadoop.http.AdminAuthorizedServlet": {
        "org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests with admin access check.\n* @param request HTTP request object\n* @param response HTTP response object\n*/"
    },
    "org.apache.hadoop.http.HttpServer2$StackServlet": {
        "org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)": "/**\n* Handles GET requests; logs thread info if access is allowed.\n* @param request HTTP request to process\n* @param response HTTP response to send output\n*/"
    },
    "org.apache.hadoop.fs.viewfs.FsGetter": {
        "org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a new FileSystem instance from the given URI and configuration.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if an error occurs while fetching the filesystem\n*/",
        "org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Retrieves a FileSystem instance for the given URI and configuration.\n* @param uri the URI of the filesystem\n* @param conf configuration settings for the filesystem\n* @return FileSystem instance\n* @throws IOException if filesystem initialization fails\n*/"
    },
    "org.apache.hadoop.fs.FileSystemLinkResolver": {
        "org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)": "/**\n* Resolves a path, following symlinks if enabled.\n* @param filesys the FileSystem to use for resolution\n* @param path the path to resolve\n* @return resolved object of type T\n* @throws IOException if symlink resolution fails\n*/"
    },
    "org.apache.hadoop.fs.shell.Display$Text": {
        "org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData)": "/**\n* Retrieves an InputStream for the specified PathData, handling various compression types.\n* @param item PathData object to read from\n* @return InputStream for the file, possibly wrapped in a compression codec\n* @throws IOException if an I/O error occurs\n*/"
    },
    "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory": {
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)": "/**\n* Creates KMSClientProviders from given hosts and configuration.\n* @param conf configuration settings for client\n* @param origUrl original service URL\n* @param port port number for KMS service\n* @param hostsPart semicolon-separated host list\n* @return array of KMSClientProvider instances\n*/",
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)": "/**\n* Creates a KeyProvider based on the given URI and configuration.\n* @param providerUri the service URI for the KeyProvider\n* @param conf configuration settings for the provider\n* @return KeyProvider instance or null if scheme does not match\n*/"
    },
    "org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser": {
        "org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])": "/**\n* Initializes MinimalGenericOptionsParser with configuration and options.\n* @param conf configuration settings\n* @param options options to parse against\n* @param args command-line arguments\n*/"
    }
}