[
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:<init>(long,int,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:isOrderedDisjoint(java.util.List,int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:sortRangeList(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:roundDown(long,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:roundUp(long,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:sliceTo(java.nio.ByteBuffer,long,org.apache.hadoop.fs.FileRange)",
        1
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$Perms:<init>(org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.UploadHandle:toByteArray()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$LruCache:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:toUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getHarAuth(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getModificationTime()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getMasterIndexTimestamp()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getArchiveIndexTimestamp()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getHarVersion()",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:depth()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:decodeString(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:compareTo(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:getName()",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:getOffset()",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setOffset(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setLength(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getPartName()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getPartFileStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getStartIndex()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getName()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getModificationTime()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:isDir()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarStatus:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getReplication()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getAccessTime()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getPermission()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getOwner()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getGroup()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:msync()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDefaultReplication()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$1:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:add(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesRead()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesWritten()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getReadOps()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getLargeReadOps()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getWriteOps()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadLocalHost()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfOneOrTwo()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfThreeOrFour()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfFiveOrLarger()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadErasureCoded()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getRemoteReadTimeMS()",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageStatistics:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:<init>(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:isTracked(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:listXAttrs(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:getThisBuilder()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:getThisBuilder()",
        1
    ],
    [
        "org.apache.hadoop.fs.protocolPB.PBHelper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:toShort()",
        1
    ],
    [
        "org.apache.hadoop.util.StringInterner:weakIntern(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:attributes(boolean,boolean,boolean,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isSymlink()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getLen()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:hasAcl()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isEncrypted()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isErasureCoded()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isSnapshotEnabled()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableFactories:setFactory(java.lang.Class,org.apache.hadoop.io.WritableFactory)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$BlockSize:<init>(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$BufferSize:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor:<init>(short)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum:<init>(short)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam:<init>(org.apache.hadoop.fs.Options$ChecksumOpt)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$Progress:<init>(org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$CreateParent:<init>(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:getOpt(java.lang.Class,org.apache.hadoop.fs.Options$CreateOpts[])",
        1
    ],
    [
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$Rename:valueOf(byte)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:removeAcl(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FSDataInputStream,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext:openFile(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FutureDataInputStreamBuilder:withFileStatus(org.apache.hadoop.fs.FileStatus)",
        1
    ],
    [
        "org.apache.hadoop.fs.AvroFSInput:read(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:seek(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.AvroFSInput:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:withMandatoryKeys(java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:withOptionalKeys(java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:withOptions(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:withStatus(org.apache.hadoop.fs.FileStatus)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:withBufferSize(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getLastAccessTime(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:removeDomain(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ExitCodeException:getExitCode()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getOwner()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getGroup()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getMode()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:setSymlink(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:validatePosition(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:read(long,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:readFully(long,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:setReadahead(java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:setDropBehind(java.lang.Boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShell$Help:processRawArguments(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicy:getCurrentTrashDir(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:<init>(java.nio.channels.AsynchronousFileChannel,java.util.List,java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:failed(java.lang.Throwable,java.lang.Integer)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:getTrashInterval()",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:moveToTrash(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:isEnabled()",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:checkpoint()",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:expunge()",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:expungeImmediately()",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:getCurrentTrashDir()",
        1
    ],
    [
        "org.apache.hadoop.fs.Trash:getEmptier()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$8:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobExpander$StringWithOffset:<init>(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobExpander:leftmostOuterCurlyContainingSlash(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:<init>(org.apache.hadoop.fs.FileStatus[],byte[],boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.XAttrCodec:decodeValue(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$3:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getAclStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobalStorageStatistics:get(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageStatistics:getName()",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobalStorageStatistics:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobalStorageStatistics:iterator()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShell$UnknownCommandException:getMessage()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:iostatisticsStore()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$Windows:createFileOutputStreamWithMode(java.io.File,boolean,int)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,java.io.Closeable[])",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:flush()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore:incrementCounter(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSError:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:isProbeForSyncable(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsUrlStreamHandler:<init>(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.PartialListing:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$5:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageType:asList()",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageType:getNonTransientTypes()",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:toUpperCase(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.StreamCapabilitiesPolicy:unbuffer(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:setOwner(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:setGroup(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:setPath(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileStatus:validateObject()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnresolvedLinkException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnresolvedLinkException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(org.apache.hadoop.fs.BlockLocation)",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.DUHelper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.DUHelper:getFileSize(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:<init>(long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:setSamples(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:setSum(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:getSamples()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:getSum()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:mean()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:<init>(org.apache.hadoop.fs.statistics.IOStatisticsSource)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.StreamStatisticNames:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.lang.String,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:sortedMap(java.util.Map,java.util.function.Predicate)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:demandStringifyIOStatisticsSource(org.apache.hadoop.fs.statistics.IOStatisticsSource)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:demandStringifyIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:<init>(java.io.OutputStream,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:<init>(java.io.OutputStream,int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hasCapability(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hflush()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hsync()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:stubDurationTracker()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:counters()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:gauges()",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageStatistics$LongStatistic:<init>(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StubDurationTrackerFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:passthroughFn(java.io.Serializable)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(java.util.function.Function)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:addFunction(java.lang.String,java.util.function.Function)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:size()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:containsKey(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:containsValue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:get(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:put(java.lang.String,java.io.Serializable)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:remove(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:putAll(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:clear()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:keySet()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:values()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:entrySet()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:counters()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:gauges()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:minimums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:maximums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:meanStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.SourceWrappedStatistics:<init>(org.apache.hadoop.fs.statistics.IOStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getInstance()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:getInstance()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:counters()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:gauges()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:minimums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:maximums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:meanStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:<init>(org.apache.hadoop.fs.statistics.IOStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:dynamicIOStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setAtomicLong(java.util.concurrent.atomic.AtomicLong,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incAtomicLong(java.util.concurrent.atomic.AtomicLong,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:maybeUpdateMinimum(java.util.concurrent.atomic.AtomicLong,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:maybeUpdateMaximum(java.util.concurrent.atomic.AtomicLong,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:addSample(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:lookup(java.util.Map,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:lookupQuietly(java.util.Map,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMaximums(java.lang.Long,java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMinimums(java.lang.Long,java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl:<init>(java.lang.String,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl:getKey()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:<init>(org.apache.hadoop.fs.statistics.DurationTracker,org.apache.hadoop.fs.statistics.DurationTracker)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:failed()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationTracker:asDuration()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:getWrapped()",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkState(boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore:addSample(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageStatistics$LongStatistic:getName()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:copyMap(java.util.Map,java.util.Map,java.util.function.Function)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMaps(java.util.Map,java.util.Map,java.util.function.BiFunction,java.util.function.Function)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateCounters(java.lang.Long,java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateGauges(java.lang.Long,java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackJavaFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Function)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfOperation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:invokeTrackingDuration(org.apache.hadoop.fs.statistics.DurationTracker,org.apache.hadoop.util.functional.CallableRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationConsumer(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfCallable(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.concurrent.Callable)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:<init>(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:counters()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:gauges()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:minimums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:maximums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:meanStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:referenceLostContext(java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getInstance()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getThreadSpecificIOStatisticsContext(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:enableIOStatisticsContext()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:value()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getInnerStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withCounters(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withGauges(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMaximums(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMinimums(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMeanStatistics(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:clear()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:<init>(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:<init>(java.io.InputStream,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:hasCapability(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.FileSystemStatisticNames:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:<init>(org.apache.hadoop.fs.statistics.IOStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:createMaps()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:counters()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:gauges()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:maximums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:minimums()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:meanStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:writeObject(java.io.ObjectOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:readObject(java.io.ObjectInputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:requiredSerializationClasses()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:isIOStatisticsThreadLevelEnabled()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.StoreStatisticNames:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSource:getIOStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:getFileBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:getRawFs()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:getBytesPerSum()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:newCrc32()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:available()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getChecksumFilePos(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumException:<init>(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:getAlgorithmBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileEncryptionInfo:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileEncryptionInfo:toStringStable()",
        1
    ],
    [
        "org.apache.hadoop.fs.HardLink:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.HardLink:createHardLink(java.io.File,java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.fs.HardLink:createHardLinkMult(java.io.File,java.lang.String[],java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:getOutput()",
        1
    ],
    [
        "org.apache.hadoop.fs.HardLink:createIOException(java.io.File,java.lang.String,java.lang.String,int,java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:getRenewQueueLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:<init>(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:addRenewAction(org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:run()",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:removeRenewAction(org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator:<init>(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator:makeRequest()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getStatus()",
        1
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:compiled()",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:matches(java.lang.CharSequence)",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:error(java.lang.String,java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileChecksum:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileChecksum:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getFsStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.SafeMode:setSafeMode(org.apache.hadoop.fs.SafeModeAction)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:fullyDeleteOnExit(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:fullyDelete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listStatusIterator(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:listFiles(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:makeShellPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:getDU(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:addPermissions(java.util.Set,int,java.nio.file.attribute.PosixFilePermission,java.nio.file.attribute.PosixFilePermission,java.nio.file.attribute.PosixFilePermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:runCommandOnStream(java.io.InputStream,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:getCanonicalPath(java.lang.String,java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getUserAction()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getGroupAction()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getOtherAction()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsAction:implies(org.apache.hadoop.fs.permission.FsAction)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:createLocalTempFile(java.io.File,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:replaceFile(java.io.File,java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:replaceTokens(java.lang.String,java.util.regex.Pattern,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext:util()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileUtil:compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:remove()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$11:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:negate()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(java.io.File,long,long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getInitialUsed()",
        1
    ],
    [
        "org.apache.hadoop.fs.DU$DUShell:startRefresh()",
        1
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setPath(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:checkNotClosed()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:<init>(java.lang.String,int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getFromPool(org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:returnToPool(com.jcraft.jsch.ChannelSftp)",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:getHost()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getIdleCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getConnPoolSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem$1:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$4:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$2:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:close()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getBytesPerChecksum()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getChecksumSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:createWriteTraceScope()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:int2byte(int,byte[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getThreadStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:getData()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$6:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$7:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$9:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$10:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getAllThreadLocalDataSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeConsumed(long[])",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:typeQuota(long[])",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:<init>(org.apache.hadoop.fs.QuotaUsage$Builder)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:length(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:fileCount(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:directoryCount(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getFileCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getDirectoryCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:listFiles(org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$OpenFileOptions:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getCrcType()",
        1
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getCrcTypeFromAlgorithmName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:<init>(org.apache.hadoop.util.DataChecksum$Type,int)",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$Store:<init>(long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:hasWildcard()",
        1
    ],
    [
        "org.apache.hadoop.HadoopIllegalArgumentException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileAlreadyExistsException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:getChecksumType()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:getBytesPerChecksum()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDefaultPort()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getInitialWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ParentNotDirectoryException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:open(org.apache.hadoop.fs.Path,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:supportsSymlinks()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getDelegationTokens(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:<init>(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:read(long,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readFully(long,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readFully(long,byte[])",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:seek(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer$Builder:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceUtils:wrapHadoopConf(java.lang.String,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer$Builder:conf(org.apache.hadoop.tracing.TraceConfiguration)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsTracer:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:<init>(org.apache.hadoop.fs.FSInputStream,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:seek(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:getFileDescriptor()",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:hasCapability(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.PositionedReadable:minSeekForVectorReads()",
        1
    ],
    [
        "org.apache.hadoop.fs.PositionedReadable:maxReadSizeForVectorReads()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getAclBit()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getEncryptedBit()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getErasureCodedBit()",
        1
    ],
    [
        "org.apache.hadoop.fs.BBPartHandle:<init>(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.BBPartHandle:bytes()",
        1
    ],
    [
        "org.apache.hadoop.fs.BBPartHandle:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.DU$DUShell:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.DU$DUShell:getExecString()",
        1
    ],
    [
        "org.apache.hadoop.fs.DU$DUShell:parseExecResult(java.io.BufferedReader)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:<init>(java.io.InputStream,org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.FileSystem$Statistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsAction:or(org.apache.hadoop.fs.permission.FsAction)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:seek(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:getEntries()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:hasMore()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirectoryEntries:getToken()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getSnapshotLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getSnapshotFileCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getSnapshotDirectoryCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getSnapshotSpaceConsumed()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getErasureCodingPolicy()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toErasureCodingPolicy()",
        1
    ],
    [
        "org.apache.hadoop.fs.PartHandle:toByteArray()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Time:now()",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:floor(long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getRawFileSystem()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getBytesPerSum()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getChecksumFilePos(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumOffset(long,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.CombinedFileRange:getUnderlying()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:getBytesPerChecksum()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:getChecksumLength(long,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:truncate(org.apache.hadoop.fs.Path,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:exists(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.Stat:isAvailable()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:<init>(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:getPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:createOutputStreamWithMode(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$Windows:createDirectoryWithMode(java.io.File,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsStatus:<init>(long,long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:getOpt(java.lang.Class,org.apache.hadoop.fs.Options$HandleOpt[])",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt$Location:allowChange()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt$Data:allowChange()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:<init>(java.lang.String,java.util.Optional)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:areSymlinksEnabled()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.Stat:getFileStatus()",
        1
    ],
    [
        "org.apache.hadoop.fs.HardLink$LinkStats:report()",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:remove()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:addClass(java.lang.Class,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:compareTo(java.util.concurrent.Delayed)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:<init>(int,int,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:getOpt(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:isEnabled()",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:getEmptier()",
        1
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:getTimeFromCheckpoint(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.InvalidPathHandleException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:bytes()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.InvalidPathHandleException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:<init>(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:getReplication()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:getBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:bufferSize(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:replication(short)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:blockSize(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:create()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:overwrite(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:append()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FunctionsRaisingIOE:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getPathHandle()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getThisBuilder()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getMandatoryKeys()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalKeys()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Set,java.util.Collection,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.BackReference:<init>(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.BackReference:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:getOffset()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:getReference()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:flags()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:hasCapability(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:makeImmutable()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:isImmutable()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:toConfigurationString()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FSBuilderSupport:<init>(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.LogExactlyOnce:warn(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.store.LogExactlyOnce:<init>(org.slf4j.Logger)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:objectHasCapability(java.lang.Object,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Retryer:continueRetry()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:<init>(java.util.concurrent.ExecutorService)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:executeFunction(java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:executeRunnable(java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:<init>(int,java.nio.file.Path,int,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:releaseLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:getPrevious()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:getNext()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:setNext(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:setPrevious(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getSummary(java.lang.StringBuilder)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getDebugInfo()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getBlockData()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getBufferPoolSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getFuturePool()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getPrefetchingStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getFileSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getMaxBlocksCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getTrackerFactory()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:setDebug(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getLocalDirAllocator()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Retryer:updateStatus()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:stateEqualsOneOf(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getBlockNumber()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getState()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:<init>(org.apache.hadoop.fs.impl.prefetch.BufferData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager,java.time.Instant)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:getAll()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getBuffer()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getActionFuture()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:<init>(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager,java.time.Instant)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cachePut(int,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numCached()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numCachingErrors()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numReadErrors()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquireHelper(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numCreated()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getTimestamp()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.PrefetchConstants:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:append(java.lang.StringBuilder,java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getIntList(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:fromShortName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:stubDurationTrackerFactory()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:containsBlock(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:blocks()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:size()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:readFile(java.nio.file.Path,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToHeadOfLinkedList(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:writeFile(java.nio.file.Path,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getChecksum(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getStats()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteCacheFiles()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getIntList(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:get()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getBufferStr(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:getFutureStr(java.util.concurrent.Future)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkState(boolean,java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:numCreated()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:invalidate()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:resetReadStats()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:isValid()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getFS()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getThisBuilder()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:currentThreadId()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:<init>(java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:getSource()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.WeakRefMetricsSource:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.CombinedFileRange:append(org.apache.hadoop.fs.FileRange)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples:pair(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getReplication()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFlags()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getChecksumOpt()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getThisBuilder()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FsLinkResolution:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ParentNotDirectoryException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$3:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:popOptionWithArgument(java.lang.String,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isOrderReverse()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:getOpts()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$UnknownOptionException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find$2:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:addClass(java.lang.Class,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.And:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:isPass()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.And:addChildren(java.util.Deque)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Print:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:getOut()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:setCaseSensitive(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:addArguments(java.util.Deque)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:toLowerCase(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:<init>(boolean,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:isDescend()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:<init>(org.apache.hadoop.fs.shell.find.Expression)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:isExpression(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:addExpression(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setFollowLink(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setFollowArgLink(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:setRootExpression(org.apache.hadoop.fs.shell.find.Expression)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setOut(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setErr(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setIn(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:setConfiguration(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:addStop(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:isStop(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:getMaxDepth()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:isFollowLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:getErr()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:isFollowArgLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:isDepthFirst()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FindOptions:getMinDepth()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:getRootExpression()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpressionFactory()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getChildren()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getOptions()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getArguments()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getArgument(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:addChild(org.apache.hadoop.fs.shell.find.Expression)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:addArgument(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getFileSystem(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:prepare()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:apply(org.apache.hadoop.fs.shell.PathData,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:finish()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:getHelp()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:isAction()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:isOperator()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:getPrecedence()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:addChildren(java.util.Deque)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:addArguments(java.util.Deque)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.FilterExpression:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rm:expandArgument(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rm:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.util.ToolRunner:confirmPrompt(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:checkIfSchemeInferredFromPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:setStat(org.apache.hadoop.fs.FileStatus)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:representsDirectory()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:mappingRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:removeAuthority(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:findLongestDirPrefix(java.lang.String,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:normalizeWindowsPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:read()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:setRightAlign(int[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FsStatus:getCapacity()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsStatus:getUsed()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsStatus:getRemaining()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:addRow(java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystemOverloadScheme(org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:getTargetFileSystemURIs()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:getMountedOnPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:setColumnHide(int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage:setHumanReadable(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage:getUsagesTable()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isDisplayECPolicy()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isOrderTime()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isOrderSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:getOrderComparator()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isUseAtime()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isHideNonPrintable()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.PrintableString:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:maxLength(int,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:getOptValue(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.PathIOException:setOperation(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.PathIOException:setTargetPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:setThreadCount(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:setThreadPoolQueueSize(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:initThreadPoolExecutor()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:waitForCompletion()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getChildFileSystems()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:getCommandName()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:run(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:runAll()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getStorageTypeHeader(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getHeader()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getHeader(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getErasureCodingPolicyHeader()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getSnapshotHeader()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Count:isHumanReadable()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getHeaderFields()",
        1
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:getQuotaHeaderFields()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException:<init>(int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException:getMessage()",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:popOption(java.lang.String,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getXAttrs(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute:getAttribute(char)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:writeDelimiter(org.apache.hadoop.fs.FSDataOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandUtils:formatDescription(java.lang.String,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processOptions(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processOptions(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:run(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getReplacementCommand()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processOptions(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:exitCodeForError()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getListingGroupSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processPath(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:postProcessPath(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:isSorted()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getCommandField(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getStickyBit()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getAclStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:getEntries()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.ScopedAclEntries:getAccessEntries()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.ScopedAclEntries:getDefaultEntries()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclUtil:isMinimalAcl(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:getName()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:getType()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:getPermission()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:preserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:shouldPreserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:file(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:newArrayList()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:getScope()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:removeAcl(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processOptions(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$DuplicatedOptionException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:addObject(org.apache.hadoop.fs.shell.Command,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:setName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:getNames()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processOptions(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:getHosts()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:size()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getXAttrs(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt$Location:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getTypeQuota(org.apache.hadoop.fs.StorageType)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getTypeConsumed(org.apache.hadoop.fs.StorageType)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions$Chown:parseOwnerGroup(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:<init>(int,org.apache.hadoop.util.DataChecksum$Type,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getAlgorithmName()",
        1
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:<init>(long,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.Globber$GlobBuilder:build()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:<init>(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:<init>(org.apache.hadoop.fs.PathHandle)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:bytes()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:writeObject(java.io.ObjectOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:readObject(java.io.ObjectInputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:readObjectNoData()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$LruCache:removeEldestEntry(java.util.Map$Entry)",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getUsed()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:incDfsUsed(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:running()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:setUsed(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:close()",
        1
    ],
    [
        "org.apache.hadoop.io.Text:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.Text:clear()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getVersion()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext:getTracer()",
        1
    ],
    [
        "org.apache.hadoop.fs.Globber:unescapePathComponent(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.Globber:getPathComponents(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:addKVAnnotation(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DurationInfo:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.PathIOException:formatPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:getIter()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:remove()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:cacheSize()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedExceptionAction)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:fixName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getName()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:debugLogFileSystemClose(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:cancelDeleteOnExit(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:methodNotSupported()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.Globber$GlobBuilder:withPathPattern(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.Globber$GlobBuilder:withPathFiltern(org.apache.hadoop.fs.PathFilter)",
        1
    ],
    [
        "org.apache.hadoop.fs.Globber$GlobBuilder:withResolveSymlinks(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:remove(org.apache.hadoop.fs.FileSystem$Cache$Key,org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.security.AccessControlException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:resolveLink(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listXAttrs(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getAllStoragePolicies()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnsupportedFileSystemException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getAllStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:printStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createDataOutputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:getMandatoryKeys()",
        1
    ],
    [
        "org.apache.hadoop.util.LambdaUtils:eval(java.util.concurrent.CompletableFuture,java.util.concurrent.Callable)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.OpenFileParameters:getBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:fsGetter()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:isValidName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFsStatus()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getUriDefaultPort()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getAllStoragePolicies()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:supportsSymlinks()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getDelegationTokens(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$MountPoint:<init>(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:checkPathIsSlash(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getChildren()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetFileSystem()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INode:isLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INode:getLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getMyFs()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isRoot()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus$Builder:owner(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus$Builder:group(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus$Builder:addEntries(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus$Builder:stickyBit(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus$Builder:build()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NotInMountpointException:<init>(org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:get(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:getConfigName()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NotInMountpointException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:nflyStatus()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:<init>(org.apache.hadoop.fs.Path,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:<init>(org.apache.hadoop.fs.viewfs.FsGetter)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:supportAutoAddingFallbackOnNoMounts()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isInternalDir()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getHomeDirPrefixValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:stripRoot()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isLastInternalDirLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:<init>(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.Object,java.lang.String,org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:verifyRenameStrategy(java.net.URI,java.net.URI,boolean,org.apache.hadoop.fs.viewfs.ViewFileSystem$RenameStrategy)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getMyFs()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getMountPoints()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:isRootInternalDir()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:clear()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetFileSystemForClose()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getSrc()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:logInvalidFileNameFormat(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$MountPathInfo:<init>(org.apache.hadoop.fs.Path,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:canonicalizeUri(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:breakIntoPathComponents(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:osException(int,java.lang.String,java.lang.Throwable,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.Path,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getLen()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isFile()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isSymlink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getReplication()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getModificationTime()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getAccessTime()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getPermission()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getOwner()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getGroup()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getSymlink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:setSymlink(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:getBlockLocations()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INode:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addFallbackLink(org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:resolveInternal(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileAlreadyExistsException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDirLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$MountPoint:<init>(org.apache.hadoop.fs.Path,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:workSet()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getRack(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:mayThrowFileNotFound(java.util.List,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:split(java.lang.String,char)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getType()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileChecksum(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:open(org.apache.hadoop.fs.Path,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setReplication(org.apache.hadoop.fs.Path,short)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:<init>(org.apache.hadoop.fs.viewfs.InodeTree,java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getVarListInString(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getSrcPathRegex()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getSrcPattern()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getDstPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getVarInDestPathMap()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getRegexGroupValueFromMather(java.util.regex.Matcher,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkState(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:setInternalDirFs(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isInternalDir()",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:stringToURI(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:checkMntEntryKeyEqualsTarget(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:setRoot(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getLinkEntries(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getLinkType()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getTarget()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getSettings()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getUgi()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getConfig()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getInternalDirFs()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:getLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:hasFallbackLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:uriToString(java.net.URI[])",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:newInstance(java.lang.Class,java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:<init>(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:checkPathIsSlash(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:<init>(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:initialize()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptResolvedDestPathStr(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkState(boolean,java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumLength(long,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:removeAcl(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:openFile(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.BulkDeleteUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:writeChunk(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:checkClosed()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:verifyExitCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:parseOutput()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:getCapacity()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:getUsed()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:getAvailable()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:getExecString()",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:parseExecResult(java.io.BufferedReader)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:checkPathArg(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:hasWindowsDrive(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.Path:validateObject()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:set(boolean,java.util.zip.Checksum,int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:needChecksum()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:resetState()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:readFully(java.io.InputStream,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.HardLink$HardLinkCGUnix:setLinkCountCmdTemplate(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShell$Usage:processRawArguments(java.util.LinkedList)",
        1
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setInterval(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:setInitialUsed(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:readFully(long,byte[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:read(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:getFileDescriptor()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:read(long,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:readFully(long,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.MultipartUploaderBuilder:build()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:getWrappedStream()",
        1
    ],
    [
        "org.apache.hadoop.fs.InvalidRequestException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.InvalidRequestException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:resolve(java.util.function.BiFunction,org.apache.hadoop.fs.Options$HandleOpt[])",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions$Chgrp:parseOwnerGroup(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.EmptyStorageStatistics:getLongStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.PathHandle:toByteArray()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext$GlobalIterable:iterator()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:remove(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:get(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:currentThreadID()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:containsKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:currentAuditContext()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:setGlobalContextEntry(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:getGlobalContextEntry(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:removeGlobalContextEntry(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:getGlobalContextEntries()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.AuditConstants:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.audit.AuditStatisticNames:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:optLong(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:optDouble(java.lang.String,double)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:mustLong(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:mustDouble(java.lang.String,double)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seek(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:available()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getFileDescriptor()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:hasCapability(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getAsyncChannel()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics:getLongStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics:getLong(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics:isTracked(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShell:getHelp()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:setQuietMode(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsShell:getUsagePrefix()",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing:addRow(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:wrap(java.lang.String,int,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:<init>()",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:wrapWidth(int)",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:getSpan()",
        1
    ],
    [
        "org.apache.hadoop.tracing.Span:addKVAnnotation(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.BBUploadHandle:<init>(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.BBUploadHandle:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.BBUploadHandle:bytes()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setWriteChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:<init>(java.util.List,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:get(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:size()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:set(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.RawParser:getPermission()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsAction:not()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsAction:and(org.apache.hadoop.fs.permission.FsAction)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.UmaskParser:getUMask()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:validateObject()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setScope(org.apache.hadoop.fs.permission.AclEntryScope)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setType(org.apache.hadoop.fs.permission.AclEntryType)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setPermission(org.apache.hadoop.fs.permission.FsAction)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry$Builder:build()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry$Builder:setName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus$2:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsAction:getFsAction(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getUnmasked()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:getUnmasked()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntryType:toStringStable()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:<init>(org.apache.hadoop.fs.permission.AclEntryType,java.lang.String,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.AclEntryScope)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus$Builder:addEntry(org.apache.hadoop.fs.permission.AclEntry)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionParser:applyNormalPattern(java.lang.String,java.util.regex.Matcher)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionParser:applyOctalPattern(java.util.regex.Matcher)",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionParser:combineModeSegments(char,int,int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:getRaw()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",
        1
    ],
    [
        "org.apache.hadoop.util.DirectBufferPool:getBuffer(int)",
        1
    ],
    [
        "org.apache.hadoop.util.DirectBufferPool:returnBuffer(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:getOutstandingBufferCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream:getInputStream()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:<init>(long,org.apache.hadoop.fs.store.BlockUploadStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:dataSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:remainingCapacity()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:closeBlock()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.LogExactlyOnce:info(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.store.LogExactlyOnce:error(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.store.LogExactlyOnce:debug(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:<init>(int,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:verifyOpen()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:getBytes()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:verifyState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:hasData()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:innerClose()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:blockAllocated()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:blockReleased()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:build()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withAttributes(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withAttribute(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withEvaluated(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withEvaluated(java.lang.String,java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withFilter(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.AuditSpan:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:addAttribute(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:escapeToPathElement(java.lang.CharSequence)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:maybeStripWrappedQuotes(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:builder()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.CallableRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.InvocationRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.audit.AuditingFunctions:callableWithinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,java.util.concurrent.Callable)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:dataSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:innerClose()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(byte[])",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:bufferCapacityUsed()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:remainingCapacity()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:innerClose()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$Key:isEqual(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:get()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:isShutdownInProgress()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:getDiscardedInstances()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getInitialWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext:resolve(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileContext:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$Perms:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:msync()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:printStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.String,java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.fs.CommonPathCapabilities:<init>()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:writeChunk(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:checkClosed()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:newInstance(java.lang.Class,java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getBaseUri(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$BlockSize:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$BufferSize:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$Progress:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts$CreateParent:getValue()",
        1
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:getChecksumType()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:methodNotSupported()",
        1
    ],
    [
        "org.apache.hadoop.fs.UnsupportedMultipartUploaderException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:remove()",
        1
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt$Data:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageStatistics$LongStatistic:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String,org.apache.hadoop.util.DiskValidator)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:<init>(java.lang.String,org.apache.hadoop.util.DiskValidator)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:isContextValid(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:removeContext(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getCurrentDirectoryIndex()",
        1
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getFsStatus()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getServerDefaults()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getUriDefaultPort()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:toString()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:hflush()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:hsync()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:setDropBehind(java.lang.Boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:abort()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:quota(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:spaceConsumed(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:spaceQuota(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:fileAndDirectoryCount(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsStatus:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.fs.FsStatus:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:getTrimmedStrings(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.DF:getDirPath()",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker$DiskErrorException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker$DiskErrorException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.service.LoggingStateChangeListener:<init>(org.slf4j.Logger)",
        1
    ],
    [
        "org.apache.hadoop.service.LoggingStateChangeListener:stateChanged(org.apache.hadoop.service.Service)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String,org.apache.hadoop.service.Service$STATE)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:isInState(org.apache.hadoop.service.Service$STATE)",
        1
    ],
    [
        "org.apache.hadoop.service.Service$STATE:getValue()",
        1
    ],
    [
        "org.apache.hadoop.service.Service$STATE:toString()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(java.lang.Thread$UncaughtExceptionHandler)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:haltOnOutOfMemory(java.lang.OutOfMemoryError)",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler$InterruptData:<init>(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler$InterruptData:toString()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:getOwner()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:getService()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:run()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:getName()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:isSignalAlreadyReceived()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:raise()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:toString()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:getSignalCount()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.AbstractLaunchableService:bindArgs(org.apache.hadoop.conf.Configuration,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:shutdown()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:isClassnameDefined()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$ExitException:getExitCode()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:getUsageMessage()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:createOptions()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:getConfigurationsToCreate()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:getClassLoader()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:getServiceName()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:warn(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:error(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:isParseSuccessful()",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:getCommandLine()",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:getRemainingArgs()",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getHostname()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:getServiceWasShutdown()",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:add(org.apache.hadoop.service.ServiceStateChangeListener)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:remove(org.apache.hadoop.service.ServiceStateChangeListener)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:reset()",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners(org.apache.hadoop.service.Service)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:getState()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getName()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:setConfig(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:serviceStart()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:serviceStop()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:waitForServiceToStop(long)",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getLifecycleHistory()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:toString()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:putBlocker(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:removeBlocker(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getBlockers()",
        1
    ],
    [
        "org.apache.hadoop.service.CompositeService:getServices()",
        1
    ],
    [
        "org.apache.hadoop.service.CompositeService:addService(org.apache.hadoop.service.Service)",
        1
    ],
    [
        "org.apache.hadoop.service.CompositeService:removeService(org.apache.hadoop.service.Service)",
        1
    ],
    [
        "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:<init>(org.apache.hadoop.service.CompositeService)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceOperations:<init>()",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceOperations:stop(org.apache.hadoop.service.Service)",
        1
    ],
    [
        "org.apache.hadoop.util.Options$ProgressableOption:<init>(org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable:set(short)",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable:compareTo(org.apache.hadoop.io.ShortWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:define(java.lang.Class,org.apache.hadoop.io.WritableComparator)",
        1
    ],
    [
        "org.apache.hadoop.util.Options$LongOption:<init>(long)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:reset(java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:loadClass(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:extractIOEs(java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:instance()",
        1
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:loaded()",
        1
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:toJson(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.FunctionRaisingIOE:unchecked(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:fromJson(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$PureJavaComparer:compareTo(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.VIntWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.VIntWritable:set(int)",
        1
    ],
    [
        "org.apache.hadoop.io.VIntWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.VIntWritable:compareTo(org.apache.hadoop.io.VIntWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:<init>(int,long)",
        1
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:compareTo(org.apache.hadoop.io.ElasticByteBufferPool$Key)",
        1
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$IntegerOption:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:set(org.apache.hadoop.io.UTF8)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:utf8Length(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:writeChars(java.io.DataOutput,java.lang.String,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:skipFully(java.io.DataInput,int)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:highSurrogate(int)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:lowSurrogate(int)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:setProgressable(org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:preserveInput(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:doSync()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getCompressionType()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getCompressionCodec()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:getSegmentList()",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:get()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableName:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableName:setName(java.lang.Class,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableName:addName(java.lang.Class,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableName:getName(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:isBlockCompressed()",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:set(long)",
        1
    ],
    [
        "org.apache.hadoop.util.Options$ClassOption:<init>(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:<init>(org.apache.hadoop.io.compress.CompressionCodec)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:reset(java.io.DataInputStream,int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:writeCompressedBytes(java.io.DataOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.ByteWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.ByteWritable:set(byte)",
        1
    ],
    [
        "org.apache.hadoop.io.ByteWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ByteWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ByteWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.ByteWritable:compareTo(org.apache.hadoop.io.ByteWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:compareTo(org.apache.hadoop.io.LongWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.OutputBuffer:<init>(org.apache.hadoop.io.OutputBuffer$Buffer)",
        1
    ],
    [
        "org.apache.hadoop.io.OutputBuffer$Buffer:getData()",
        1
    ],
    [
        "org.apache.hadoop.io.OutputBuffer$Buffer:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.OutputBuffer$Buffer:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:forceInit(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable)",
        1
    ],
    [
        "org.apache.hadoop.io.FastByteComparisons:compareTo(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:readInt(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:readVLong(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:<init>(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:copyBytes()",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:getBytes()",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:getCapacity()",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:set(int)",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.IntWritable:compareTo(org.apache.hadoop.io.IntWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool:getBufferTree(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:isCompressed()",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:getKey()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:createValueBytes()",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:set(float)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getPassFactor(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getSegmentDescriptors(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getApproxChkSumLength(long)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer$Buffer:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer$Buffer:read(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool:resetInstance()",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool:submitReadahead(java.lang.String,java.io.FileDescriptor,long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.NullWritable:get()",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>(java.lang.String,java.io.FileDescriptor,long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getCacheManipulator()",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:decodeVIntSize(byte)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Reader:next(org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable:get()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$FSDataInputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer$ComparatorOption:<init>(org.apache.hadoop.io.WritableComparator)",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:set(double)",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:compareTo(org.apache.hadoop.io.DoubleWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.VersionedWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader$ComparatorOption:<init>(org.apache.hadoop.io.WritableComparator)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight:accessRight()",
        1
    ],
    [
        "org.apache.hadoop.util.NativeCodeLoader:isNativeCodeLoaded()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:getStateCode()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:getMessage()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIOException:getErrorCode()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIOException:<init>(java.lang.String,org.apache.hadoop.io.nativeio.Errno)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:syncFileRangeIfPossible(java.io.FileDescriptor,long,long,int)",
        1
    ],
    [
        "org.apache.hadoop.util.CleanerUtil:getCleaner()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CachedName:<init>(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:getOperatingSystemPageSize()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$CachedUid:<init>(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:<init>(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:createDescriptor(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:<init>(long,long,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:isPmem(long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:mapBlock(java.lang.String,long,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:unmapBlock(long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memCopy(byte[],long,boolean,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:isPmem()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:getAddress()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:getPmdkLibPath()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:<init>(int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:<init>(java.lang.String,java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:stripDomain(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:getShareDeleteFileDescriptor(java.io.File,long)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIOException:getErrno()",
        1
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:renameTo(java.io.File,java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIOException:<init>(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIOException:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:toStrings()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:toArray()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:resetBuffer(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:write(int)",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:write(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:reset(int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:write(java.io.DataInput,int)",
        1
    ],
    [
        "org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.TwoDArrayWritable:toArray()",
        1
    ],
    [
        "org.apache.hadoop.io.TwoDArrayWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.TwoDArrayWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.NullWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:iterator()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:size()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:set(java.util.EnumSet,java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readString(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeString(java.io.DataOutput,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:add(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:firstKey()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:lastKey()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:clear()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:containsKey(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:containsValue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:entrySet()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:get(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:keySet()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:putAll(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:remove(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:size()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:values()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:tailMap(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:headMap(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:subMap(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:readFully(java.io.InputStream,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:<init>(org.apache.hadoop.io.DataInputBuffer$Buffer)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer$Buffer:reset(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer$Buffer:getData()",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer$Buffer:getPosition()",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer$Buffer:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.VersionMismatchException:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCodeConstants:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:extractIntOption(java.lang.String,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCodecOptions:<init>(org.apache.hadoop.io.erasurecode.ECSchema)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCodecOptions:getSchema()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:getNumDataUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:getNumParityUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int,boolean,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getSchema()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:setSchema(org.apache.hadoop.io.erasurecode.ECSchema)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:updateCoders(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderNames(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoders(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCodecNames()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlock:isErased()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCodeNative:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCodeNative:checkNativeCodeLoaded()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getInstance()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(java.nio.ByteBuffer,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:<init>(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:getBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:toBytesArray()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:getParityBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:getDataBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumDataUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumParityUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumDataUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumParityUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:initPiggyBackIndexWithoutPBVec(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:initPiggyBackFullIndexVec(int,int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:allocateByteBuffer(boolean,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeByPiggyBack(java.nio.ByteBuffer[],java.nio.ByteBuffer,java.nio.ByteBuffer,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeByPiggyBack(byte[][],int[],byte[],int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:encodeWithPiggyBacks(java.nio.ByteBuffer[],java.nio.ByteBuffer[][],int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:cloneBufferData(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:performDecodeImpl(java.nio.ByteBuffer[],int[],int,int[],java.nio.ByteBuffer[],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:preferDirectBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:performEncodeImpl(java.nio.ByteBuffer[],int[],int,java.nio.ByteBuffer[],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:preferDirectBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.InvalidDecodingException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getFieldSize()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpMatrix(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getValidIndexes(java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:bytesToHex(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMul(byte,byte)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:markBuffers(java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:allocateBuffer(boolean,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:toLimits(java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:resetBuffers(java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:performEncodeImpl(java.nio.ByteBuffer[],int[],int,java.nio.ByteBuffer[],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumAllUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:allowChangeInputs()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:allowVerboseDump()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:adjustOrder(java.lang.Object[],java.lang.Object[],int[],int[],java.lang.Object[],java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:substitute(java.nio.ByteBuffer[],int,java.nio.ByteBuffer,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],java.nio.ByteBuffer[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:substitute(byte[][],int[],int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],byte[][],int[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getNullIndexes(java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:checkGetBytesArrayBuffer(byte[][],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:checkGetDirectBuffer(java.nio.ByteBuffer[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getEmptyChunk(int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:isAllZero()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:cloneAsDirectByteBuffer(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,int,byte[][],int[],byte[][],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,int,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,int,int[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,int,int[],byte[][],int[],byte[][],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:remainder(java.nio.ByteBuffer[],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:remainder(byte[][],int[],int,int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfVectMulInit(byte,byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInv(byte)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMulTab()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:<init>(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:performDecodeImpl(java.nio.ByteBuffer[],int[],int,int[],java.nio.ByteBuffer[],int[])",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlock:<init>(boolean,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.GenericWritable:set(org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.io.GenericWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.GenericWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read(byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:skip(long)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:available()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:readStreamHeader()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getProcessedByteCount()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:compress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:getBytesRead()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:getBytesWritten()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:initSymbols(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:initSymbols(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateProcessedByteCount(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:reportCRCError()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CRC:initialiseCRC()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CRC:getFinalCRC()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:makeMaps()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:hbCreateDecodeTables(int[],int[],int[],char[],int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data:initTT(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CRC:updateCRC(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:chooseBlockSize(long)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CRC:updateCRC(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:flush()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:getAllowableBlockSize(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsW(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsFinishedWithStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues0(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues2(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues6(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues7(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:hbMakeCodeLengths(byte[],int[],org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:hbAssignCodes(int[],byte[],int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:generateMTFValues()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:randomiseBlock()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSimpleSort(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:med3(byte,byte,byte)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:vswap(int[],int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:getRemaining()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(boolean,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:compress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionOutputStream:<init>(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:writeHeader(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:compress()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:resetState()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirectBuf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:compressDirectBuf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:<init>(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:resetState()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BlockCompressorStream:rawWriteInt(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRecommendedBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:populateUncompressedBuffer(byte[],int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:inflateDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getRecommendedBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionLevel(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionStrategy(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CodecConstants:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.AlreadyClosedException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:getDefaultExtension()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:createOutputStream(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:getCompressorType()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:createCompressor()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:createDecompressor()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:rawReadInt()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompressDirectBuf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:compressDirectBuf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByClassName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:removeSuffix(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionOutputStream:setTrackedCompressor(org.apache.hadoop.io.compress.Compressor)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:setTrackedDecompressor(org.apache.hadoop.io.compress.Decompressor)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:createCache(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:borrow(java.util.Map,java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:getClass(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:getLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:isNativeCodeLoaded()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:isNativeCodeLoaded()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getLibraryName()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionOutputStream:flush()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:isNativeZlibLoaded(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:isNativeZlibLoaded()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:isNativeZlibLoaded()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getLibraryName()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:compressionLevel()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionStrategy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionLevel(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:compressionStrategy()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader:windowBits()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:compress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:writeHeader(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:fillTrailer()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:writeTrailer(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:getBytesRead()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:getBytesWritten()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:windowBits()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInputFromSavedData()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:inflateDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>(int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:compress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndCopyBytesToLocal(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUShortLE(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndSkipBytes(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndSkipBytesUntilNull()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:copyBytesToLocal(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUIntLE(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:getBytesRead()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUByte(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:<init>(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createDirectDecompressor()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:seek(long)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readCompressedByteArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:displayByteArray(byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeVLong(java.io.DataOutput,long)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:isNegativeVInt(byte)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableUtils:getVIntSize(long)",
        1
    ],
    [
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBufferTree(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:release()",
        1
    ],
    [
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getCurrentBuffersCount(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.Text:copyBytes()",
        1
    ],
    [
        "org.apache.hadoop.io.Text:bytesToCodePoint(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.Text:encode(java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.Text:getBytes()",
        1
    ],
    [
        "org.apache.hadoop.io.Text:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.Text:ensureCapacity(int)",
        1
    ],
    [
        "org.apache.hadoop.io.Text:decode(java.nio.ByteBuffer,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.Text:validateUTF8(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.Text:utf8Length(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.Text$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.Text$2:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:reset(java.io.DataInputStream,int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:writeUncompressedBytes(java.io.DataOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:writeCompressedBytes(java.io.DataOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.NullWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.NullWritable:compareTo(org.apache.hadoop.io.NullWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.DoubleWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.CompressedWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.CompressedWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.CompressedWritable:ensureInflated()",
        1
    ],
    [
        "org.apache.hadoop.io.CompressedWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class,byte)",
        1
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:getClass(byte)",
        1
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:getId(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.ShortWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:<init>(java.util.TreeMap)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:get(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:set(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:getMetadata()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.MultipleIOException$Builder:add(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.io.MultipleIOException$Builder:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.io.VLongWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.VLongWritable:set(long)",
        1
    ],
    [
        "org.apache.hadoop.io.VLongWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.VLongWritable:compareTo(org.apache.hadoop.io.VLongWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.SerializationFactory:getSerialization(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.io.Serializable,java.io.Serializable)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization:getDeserializer(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization:getSerializer(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:close()",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:deserialize(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:open(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getReader(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getSchema(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getWriter(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:accept(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getReader(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getSchema(org.apache.avro.specific.SpecificRecord)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getWriter(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:close()",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:open(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:serialize(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization:accept(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization:getSerializer(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:open(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:serialize(java.io.Serializable)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:close()",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:open(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:serialize(org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:close()",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:open(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.io.Serializable)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:close()",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization:accept(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization:getDeserializer(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization:getSerializer(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:open(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:close()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption:<init>(org.apache.hadoop.fs.FileSystem)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer:<init>(org.apache.hadoop.io.DataInputByteBuffer$Buffer)",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getData()",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getPosition()",
        1
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getLength()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:set(byte[],double)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:shouldPreserveInput()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:cleanup()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(int[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(org.apache.hadoop.io.SequenceFile$ValueBytes[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",
        1
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:close()",
        1
    ],
    [
        "org.apache.hadoop.util.GenericsUtil:getClass(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.GenericsUtil:toArray(java.lang.Class,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:stream(org.apache.hadoop.fs.FSDataInputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:start(long)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:length(long)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:bufferSize(int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getKeyClassName()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getValueClassName()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:deserializeValue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:deserializeKey(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getVersion()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getSync()",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader$ComparatorOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options:prependOptions(java.lang.Object[],java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.io.InputBuffer$Buffer:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.InputBuffer:<init>(org.apache.hadoop.io.InputBuffer$Buffer)",
        1
    ],
    [
        "org.apache.hadoop.io.InputBuffer$Buffer:reset(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.InputBuffer$Buffer:getPosition()",
        1
    ],
    [
        "org.apache.hadoop.io.InputBuffer$Buffer:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:ignoreSync()",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:<init>(byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:set(org.apache.hadoop.io.MD5Hash)",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:getDigester()",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:getBytes()",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:quarterDigest()",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash:charToNibble(char)",
        1
    ],
    [
        "org.apache.hadoop.io.MD5Hash$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption:<init>(org.apache.hadoop.io.SequenceFile$Metadata)",
        1
    ],
    [
        "org.apache.hadoop.util.Options$FSDataOutputStreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.BytesWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getMetadata()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:getCodec()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:isCompressed()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:isBlockCompressed()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:flush()",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Class,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:getStaticProtobufMethod(java.lang.Class,java.lang.String,java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:readRawVarint32(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:wrappedReadForCompressedData(java.io.InputStream,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:skipFully(java.io.InputStream,long)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:writeFully(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:writeFully(java.nio.channels.FileChannel,java.nio.ByteBuffer,long)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:listDirectory(java.io.File,java.io.FilenameFilter)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:fsync(java.nio.channels.FileChannel,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:wrapWithMessage(java.io.IOException,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.IOUtils:readFullyToByteArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:clear()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:containsKey(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:containsValue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:entrySet()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:size()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:get(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:keySet()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:remove(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:values()",
        1
    ],
    [
        "org.apache.hadoop.io.MapWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$BooleanOption:<init>(boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.Options$PathOption:<init>(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:set(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:get()",
        1
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:compareTo(org.apache.hadoop.io.BooleanWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:isRunning(org.apache.hadoop.util.Daemon)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.AsyncGet$Util:wait(java.lang.Object,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:<init>(int,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:getFailoverOrRetrySleepTime(int)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:getClassName()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:calculateSleepTime(int)",
        1
    ],
    [
        "org.apache.hadoop.util.Daemon:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:<init>(org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.CallReturn:getState()",
        1
    ],
    [
        "org.apache.hadoop.util.Time:monotonicNow()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:isAsynchronousMode()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:setAsynchronousMode(boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Counters:isZeros()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.MultiException:getExceptions()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:iterator()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:<init>(java.lang.Class,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:<init>(java.lang.Object,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getWaitTime(long)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFailover()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:idempotentOrAtMostOnce(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:close()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:proxyName()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair:<init>(int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getRetryPolicy(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getFailoverCount()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getProxy()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:isRpcInvocation(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:nextCallId()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFail()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getProxyInfo()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:getFailException()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler:hasSuccessfulCall()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC:getConnectionIdForProxy(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getProxyProvider()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:isDone()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:<init>(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:searchPair(int)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parsePositiveInt(java.lang.String[],int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int,long)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:isSaslFailure(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:hasWrappedAccessControlException(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:constructReasonString(long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.MultiException:<init>(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.MultiException:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetriableException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler:setLowerLayerAsyncReturn(org.apache.hadoop.util.concurrent.AsyncGet)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:constructReasonString(int)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.MultipleIOException:<init>(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer$ComparatorOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:getKeyClass()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getName()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:<init>(short,short)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFileDumper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:getBlockRegionList()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getCompressedSize()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getRawSize()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getRegion()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getCompressionAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Magic:size()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:size()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(java.lang.String,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFileDumper$Align:calculateWidth(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getEntry(int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getOffset()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getMetaName()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Magic:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:getDefaultCompressionAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:getMetaByName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyLength()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:<init>(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:compatibleWith(org.apache.hadoop.io.file.tfile.Utils$Version)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:readVLong(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:<init>(org.apache.hadoop.io.RawComparator)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:writeVLong(java.io.DataOutput,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:isSorted()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:<init>(org.apache.hadoop.io.compress.CompressionOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(long,long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:compareTo(org.apache.hadoop.io.file.tfile.Utils$Version)",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:getBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:size()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getFirstKey()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getRecordCount()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getComparator()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:<init>(java.io.DataOutputStream,byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:close()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getComparatorString()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:entries()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:addBlockRegion(org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong:<init>(long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:incRecordCount()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getOutputStream()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getRawSize()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getStartPos()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:available()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:skip(long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:lowerBound(java.util.List,java.lang.Object,java.util.Comparator)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:write(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:<init>(java.io.OutputStream,byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flushBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:upperBound(java.util.List,java.lang.Object,java.util.Comparator)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:lowerBound(java.util.List,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:upperBound(java.util.List,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:size()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:getBlockIndex()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:getRecordIndex()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:begin()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:end()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(int,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(int,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:reset(java.io.DataInputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isClosed()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:incRecordIndex()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(byte[],int,int,long)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator:compare(org.apache.hadoop.io.file.tfile.CompareUtils$Scalar,org.apache.hadoop.io.file.tfile.CompareUtils$Scalar)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.ByteArray:buffer()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:buffer()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.ByteArray:size()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValueStream()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeKey(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValueLength()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:isValueLengthKnown()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getInputStream()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getBlockRegion()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:flush()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:close()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:<init>(java.io.DataInputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputOutputStream:<init>(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputOutputStream:write(int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputOutputStream:write(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputOutputStream:write(byte[])",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:set(float)",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:compareTo(org.apache.hadoop.io.FloatWritable)",
        1
    ],
    [
        "org.apache.hadoop.io.FloatWritable:toString()",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Merger:<init>(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:lessThanUnsigned(long,long)",
        1
    ],
    [
        "org.apache.hadoop.io.FastByteComparisons:lexicographicalComparerJavaImpl()",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:<init>(org.apache.hadoop.io.DataOutputBuffer$Buffer)",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:getData()",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:writeTo(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:getPrimitiveClass(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeBooleanArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeCharArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeByteArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeShortArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeIntArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeLongArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeFloatArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:writeDoubleArray(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readBooleanArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readCharArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readByteArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readShortArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readIntArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readLongArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readFloatArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readDoubleArray(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.UTF8$Comparator:compare(byte[],int,int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.WritableFactories:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableFactories:getFactory(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:ownStream()",
        1
    ],
    [
        "org.apache.hadoop.net.DNSDomainNameResolver:getAllByDomainName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.DNS:reverseDns(java.net.InetAddress,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:init(org.apache.hadoop.net.InnerNode$Factory)",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:getPath(org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:incrementRacks()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:normalize(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:contains(org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getNumOfLeaves()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:isSameParents(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:setRandomSeed(long)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getRandom()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:isChildScope(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getFirstHalf(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getLastHalf(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:normalizeNetworkLocationPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:countEmptyRacks()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:checkChannelValidity(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:write(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:waitForWritable()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:getChannel()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:setTimeout(int)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:createURI(java.lang.String,boolean,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getStaticResolution(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:addStaticResolution(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getAllStaticResolutions()",
        1
    ],
    [
        "org.apache.hadoop.net.ConnectTimeoutException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:normalizeHostName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:verifyHostnames(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getHostNameOfIP(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getHostPortString(java.net.InetSocketAddress)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getLocalHostname()",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getPortFromHostPortString(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:isLocalAddress(java.net.InetAddress)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:see(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:wrapWithMessage(java.io.IOException,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:quoteHost(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:isValidSubnet(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:addMatchingAddrs(java.net.NetworkInterface,org.apache.commons.net.util.SubnetUtils$SubnetInfo,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:getFreeSocketPort()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketInputStream$Reader:performIO(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.net.TableMapping:getRawMapping()",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:reloadCachedMappings()",
        1
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:toString()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:set(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:getRawMapping()",
        1
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:reference()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:getDomainSocket()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:getHandler()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:getInputStream()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:getEffectivePath(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:unreferenceCheckClosed()",
        1
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:setClosed()",
        1
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:getReferenceCount()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:toString()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:isClosed()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:<init>(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:getOutputStream()",
        1
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:toString()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo:close()",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:getNumOfChildren()",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:isRack()",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:isAncestor(org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:isParent(org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:getLoc(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:getNumOfLeaves()",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl$Factory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:getUncachedHosts(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:cacheResolvedHosts(java.util.List,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:getCachedHosts(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:getSwitchMap()",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:toString()",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:isSingleSwitch()",
        1
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:reloadCachedMappings(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:<init>(java.net.Proxy)",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:createSocket()",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:setProxy(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:createSocket()",
        1
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:getRawMapping()",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:toString()",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:getDependency(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:setTimeout(long)",
        1
    ],
    [
        "org.apache.hadoop.net.DNS:getSubinterface(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.DNS:getSubinterfaceInetAddrs(java.net.NetworkInterface)",
        1
    ],
    [
        "org.apache.hadoop.net.DNS:resolveLocalHostname()",
        1
    ],
    [
        "org.apache.hadoop.net.DNS:resolveLocalHostIPAddress()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:read(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:close()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:waitForReadable()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:timeoutExceptionString(java.nio.channels.SelectableChannel,long,int)",
        1
    ],
    [
        "org.apache.hadoop.net.DomainNameResolverFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isRack()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isNodeGroup()",
        1
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:<init>(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:getSwitchMap()",
        1
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitch()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameNodeGroup(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo:<init>(java.nio.channels.spi.SelectorProvider,java.nio.channels.Selector)",
        1
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream$Writer:performIO(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered:extraCheck(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered:extraWrite(java.lang.Object,java.lang.String,com.fasterxml.jackson.core.JsonGenerator)",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:init()",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:doTrace(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,java.lang.String,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:extraCheck(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:extraWrite(java.lang.Object,java.lang.String,com.fasterxml.jackson.core.JsonGenerator)",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getStats(int)",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:recordValues(double[])",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:hasLogged()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:setShouldLog()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:setHasLogged()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getValueCount()",
        1
    ],
    [
        "org.apache.hadoop.log.LogLevel:isValidProtocol(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:createSSLSocketFactory()",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:printGenericCommandUsage(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.util.ServletUtil:initHTML(javax.servlet.ServletResponse,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ServletUtil:getParameter(javax.servlet.ServletRequest,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.log.LogLevel$Servlet:process(org.apache.log4j.Logger,java.lang.String,java.io.PrintWriter)",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String,org.apache.hadoop.util.Timer)",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:shouldLog()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:getLogSupressionMessage(org.apache.hadoop.log.LogThrottlingHelper$LogAction)",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:reset()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:getCount()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:getStats(int)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:asyncDispatches()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:asyncRequests()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:asyncRequestsWaiting()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:asyncRequestsWaitingMax()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatched()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedActive()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedActiveMax()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeMax()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeMean()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeStdDev()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeTotal()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:expires()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requests()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requestsActive()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requestsActiveMax()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeMax()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeMean()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeStdDev()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:requestTimeTotal()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:responses1xx()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:responses2xx()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:responses3xx()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:responses4xx()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:responses5xx()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:responsesBytesTotal()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:statsOnMs()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:<init>(org.eclipse.jetty.server.handler.StatisticsHandler,int)",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet$Event:getInternalName()",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:setResponseHeader(javax.servlet.http.HttpServletResponse)",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileOutputServlet:sanitize(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.NoCacheFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:getAsyncProfilerHome()",
        1
    ],
    [
        "org.apache.hadoop.util.ProcessUtils:getPid()",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:getInteger(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.Integer)",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:getOutput(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:getLong(javax.servlet.http.HttpServletRequest,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:getMinWidth(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.util.ProcessUtils:runCmdAsync(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:needsQuoting(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.io.OutputStream,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:quoteOutputStream(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:unquoteHtmlChars(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getWebAppsPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:setContextAttributes(org.eclipse.jetty.servlet.ServletContextHandler,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:<init>()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getWebAppContext()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addListener(org.eclipse.jetty.server.ServerConnector)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:setAttribute(java.lang.String,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,boolean)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addHandlerAtFront(org.eclipse.jetty.server.Handler)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addHandlerAtEnd(org.eclipse.jetty.server.Handler)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getFilterHolder(java.lang.String,java.lang.String,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getFilterMapping(java.lang.String,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,org.eclipse.jetty.servlet.FilterHolder,org.eclipse.jetty.servlet.FilterMapping)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addFilterPathMapping(java.lang.String,org.eclipse.jetty.servlet.ServletContextHandler)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getAttribute(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getPort()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:setThreads(int,int)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:loadListeners()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:bindListener(org.eclipse.jetty.server.ServerConnector)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:constructBindException(org.eclipse.jetty.server.ServerConnector,java.io.IOException)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addMultiException(org.eclipse.jetty.util.MultiException,java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:join()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:isAlive()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getDefaultHeaders()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$XFrameOption:toString()",
        1
    ],
    [
        "org.apache.hadoop.http.IsActiveServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:initHttpHeaderMap()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:<init>(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:inferMimeType(javax.servlet.ServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpConfig$Policy:fromString(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.PrometheusServlet:getPrometheusSink()",
        1
    ],
    [
        "org.apache.hadoop.http.WebServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        1
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",
        1
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter$User:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter$User:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter$User:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:addEndpoint(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:setAuthFilterConfigurationPrefix(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:setAuthFilterConfigurationPrefixes(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:setXFrameOption(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:run()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpRequestLog:<init>()",
        1
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterNames()",
        1
    ],
    [
        "org.apache.hadoop.http.JettyUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsFilter:accepts(org.apache.hadoop.metrics2.MetricsRecord)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:info()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:<init>(org.apache.hadoop.metrics2.MetricsCollector,java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:tuple(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:info()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:<init>(long,long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:setInitialFlushTime(java.util.Date)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:updateFlushTime(java.util.Date)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:scheduleFlush(java.util.Date)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:extractId(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setUnits(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setDmax(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setTmax(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setSlope(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getGangliaConfForMetric(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_int(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:pad()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:emitToGangliaHosts()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getUnits()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getTmax()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getDmax()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:metricsEntrySet()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:getType()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:getSlope()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getSlope()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:<init>(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:isConnected()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:tooManyConnectionFailures()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:close()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:<init>(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:close()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:prometheusName(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:flush()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:parseTopMetricsTags(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.FileSink:flush()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.FileSink:close()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.RegexFilter:compile(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setIncludePattern(com.google.re2j.Pattern)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setExcludePattern(com.google.re2j.Pattern)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setIncludeTagPattern(java.lang.String,com.google.re2j.Pattern)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setExcludeTagPattern(java.lang.String,com.google.re2j.Pattern)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:iterator()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:clear()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:visit(org.apache.hadoop.metrics2.MetricsVisitor)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:<init>(java.lang.String,java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBuffer:<init>(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:enqueue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:_dequeue()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:clearConsumerLock()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:size()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:front()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:checkConsumer()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:setConsumerLock()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:inMiniClusterMode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startTimer()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopTimer()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:clearConfigs()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:info()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:start()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:source()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:sink()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getHostname()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getClassName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSource(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSourceAdapter(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSinkAdapter(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MsInfo:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:visit(org.apache.hadoop.metrics2.MetricsVisitor)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:visit(org.apache.hadoop.metrics2.MetricsVisitor)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:visit(org.apache.hadoop.metrics2.MetricsVisitor)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(long,boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:tags()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:waitTillNotified(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:notifyAnyWaiters()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:reset(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:getAttrName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:metrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBuffer:iterator()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:visit(org.apache.hadoop.metrics2.MetricsVisitor)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:visit(org.apache.hadoop.metrics2.MetricsVisitor)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:add(org.apache.hadoop.metrics2.MetricsTag)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:add(org.apache.hadoop.metrics2.AbstractMetric)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tags()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:metrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttribute(javax.management.Attribute)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttributes(javax.management.AttributeList)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:invoke(java.lang.String,java.lang.Object[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:setRecordFilter(org.apache.hadoop.metrics2.MetricsFilter)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:setMetricFilter(org.apache.hadoop.metrics2.MetricsFilter)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:tagName(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:metricName(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:<init>(org.apache.hadoop.metrics2.MetricsRecord,org.apache.hadoop.metrics2.MetricsFilter)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:timestamp()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:context()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:tags()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:metrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(int,boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(float,boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:set(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:records()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:timestamp()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:keys()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getPropertyInternal(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.UniqueNames$Count:<init>(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setInterval(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setNumInfo(org.apache.hadoop.metrics2.MetricsInfo)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:run()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:getQuantiles()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantileInfos(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:<init>(org.apache.hadoop.metrics2.util.Quantile[])",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setEstimator(org.apache.hadoop.metrics2.util.QuantileEstimator)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:getInterval()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:addQuantileInfo(int,org.apache.hadoop.metrics2.MetricsInfo)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:add(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:stop()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Quantile:<init>(double,double)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:init(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:getImpl()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdownInstance()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setImpl(org.apache.hadoop.metrics2.MetricsSystem)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeObjectName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSource(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(org.apache.hadoop.metrics2.MetricsInfo)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:get(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:getTag(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tags()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:metrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getSnapshotTimeStamp()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getCount()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getSum()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:total()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:numSamples()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:<init>(double,long,long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:close()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:set(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys:add(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getName(java.lang.reflect.Field)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getName(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric$3:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:set(float)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:compareAndSet(float,float)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:info()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:add(long,double)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:add(double)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:mean()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:min()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:max()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:lastStat()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:getDeclaredFieldsIncludingInherited(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:getDeclaredMethodsIncludingInherited(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(java.lang.Object,boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:isInt(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:isLong(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:isFloat(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:isDouble(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:nameFrom(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:value()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:getGlobalMetrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetricsInfo:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:<init>(java.lang.String,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:getThreadUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:getThreadUsageFromGroup(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:calculateMaxMemoryUsage(java.lang.management.MemoryUsage)",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:getNumGcWarnThresholdExceeded()",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:getNumGcInfoThresholdExceeded()",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:getTotalGcExtraSleepTime()",
        1
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor$GcData:getGcTimePercentage()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetricsInfo:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:<init>(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:compareTo(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:variance()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Contracts:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Contracts:checkArg(double,boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:getMbeanNameService(javax.management.ObjectName)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:getMbeanNameName(javax.management.ObjectName)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:allowableError(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem:<init>(long,int,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:getSampleCount()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:clear()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Servers:<init>()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$RecordCache:removeEldestEntry(java.util.Map$Entry)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:getTag(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:getMetric(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:getMetricInstance(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:tags()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:metrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache$Record:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:updateTotal(long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache:get(java.lang.String,java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Quantile:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Quantile:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Quantile:compareTo(org.apache.hadoop.metrics2.util.Quantile)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Quantile:toString()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsRecordBuilder:endRecord()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsSystem:register(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:<init>(org.apache.hadoop.metrics2.MetricsCollector)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:tuple(java.lang.String,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:setUserGroups(java.lang.String,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.NetgroupCache:getNetgroups(java.lang.String,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.NetgroupCache:clear()",
        1
    ],
    [
        "org.apache.hadoop.security.NetgroupCache:add(java.lang.String,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getUsersForNetgroupCommand(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolVersion(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB:refreshServiceAcl(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.security.RefreshUserMappingsProtocol)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:refreshUserToGroupsMappings(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:refreshSuperUserGroupsConfiguration(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.security.NetgroupCache:getGroups()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$LoginParams:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$LoginParams:put(org.apache.hadoop.security.UserGroupInformation$LoginParam,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.NullGroupsMapping:getGroupsSet(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.NullGroupsMapping:getGroups(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:containsUpperCase(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$QualityOfProtection:getSaslQop()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress)",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials$SerializedFormat:valueOf(int)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:<init>(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.ipc.Server$Connection)",
        1
    ],
    [
        "org.apache.hadoop.security.token.SecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:encodePassword(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroups(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.GroupMappingServiceProvider:getGroupsSet(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getPrefix()",
        1
    ],
    [
        "org.apache.hadoop.security.FastSaslServerFactory:<init>(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.FastSaslServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",
        1
    ],
    [
        "org.apache.hadoop.security.FastSaslServerFactory:getMechanismNames(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:getToken(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:isPrivateCloneOf(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:getService()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:getAllTokens()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:getTokenMap()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:numberOfTokens()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:getSecretKey(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:numberOfSecretKeys()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:addSecretKey(org.apache.hadoop.io.Text,byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:removeSecretKey(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:getAllSecretKeys()",
        1
    ],
    [
        "org.apache.hadoop.security.Credentials:getSecretKeyMap()",
        1
    ],
    [
        "org.apache.hadoop.security.HadoopKerberosName:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.User:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.User:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups$TimerToTickerAdapter:<init>(org.apache.hadoop.util.Timer)",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:noGroupsForUser(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:isNegativeCacheEnabled()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:getBackgroundRefreshSuccess()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:getBackgroundRefreshException()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:getBackgroundRefreshQueued()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:getBackgroundRefreshRunning()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:logError(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsInternal(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.KDiag:flush()",
        1
    ],
    [
        "org.apache.hadoop.security.KDiag:arg(java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.KDiag:getAndSet(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setShouldRenewImmediatelyForTests(boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:getKind()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$ExitException:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:reportDuplicateEntry(java.lang.String,java.lang.Integer,java.lang.String,java.lang.Integer,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:parseId(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:checkSupportedPlatform()",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:isInteger(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:clear()",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getName2IdCmdNIX(java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getId2NameCmdNIX(int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getName2IdCmdMac(java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getId2NameCmdMac(int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:bashQuote(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroups(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:getLoginAppName()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:getAuthMethod()",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getRelativeDistinguishedName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:lookupPosixGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:resolveCustomGroupFilterArgs(javax.naming.directory.SearchResult)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getGroupNames(javax.naming.directory.SearchResult,java.util.Collection,java.util.Collection,boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:failover(int,int)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:setConfigurations(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:extractPassword(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:<init>(java.io.OutputStream,javax.security.sasl.SaslServer)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:<init>(java.io.OutputStream,javax.security.sasl.SaslClient)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:disposeSasl()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:flush()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer$SecurityProvider:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:<init>(javax.security.auth.callback.CallbackHandler)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:evaluateResponse(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:throwIfNotComplete()",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:<init>(javax.net.ssl.SSLSocketFactory)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getPasswordCharArray(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket()",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.net.InetAddress,int)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:parseMethodsToIgnore(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:parseBrowserUserAgents(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:isBrowser(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:<init>(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedOrigins(javax.servlet.FilterConfig)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeMaxAge(javax.servlet.FilterConfig)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:destroy()",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:encodeHeader(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:isCrossOrigin(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:areOriginsAllowed(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:isMethodAllowed(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:areHeadersAllowed(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:getAllowedMethodsHeader()",
        1
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:getAllowedHeadersHeader()",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:getHeader(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:getMethod()",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:proceed()",
        1
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:sendError(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addHeader(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setHeader(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setDateHeader(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addDateHeader(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setIntHeader(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addIntHeader(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:containsHeader(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",
        1
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter:init(javax.servlet.FilterConfig)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$StandardHostResolver:getByName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UGIExceptionMessages:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:<init>(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:isLoginSuccess()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getSubjectLock()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Renew:validate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Edit:validate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Remove:validate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.SecretManager:generateSecret()",
        1
    ],
    [
        "org.apache.hadoop.security.token.SecretManager:createPassword(byte[],javax.crypto.SecretKey)",
        1
    ],
    [
        "org.apache.hadoop.security.token.SecretManager:createSecretKey(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.token.SecretManager$InvalidToken:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token$TrivialRenewer:getKind()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token$TrivialRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token$TrivialRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackDuration(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getSequenceNumber()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:<init>(long,long,long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.Daemon:<init>(java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:setCurrentKeyId(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:setDelegationTokenSeqNum(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCurrentTokensSize()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:getKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCurrentKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getAllKeys()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getDelegationKey(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getRenewDate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getMasterKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getDelegationTokenSeqNum()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[],java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:incrementCurrentKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:setExpiryDate(long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:getExpiryDate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:incrementDelegationTokenSeqNum()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setIssueDate(long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setMaxDate(long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setMasterKeyId(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setSequenceNumber(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getRealUser()",
        1
    ],
    [
        "org.apache.hadoop.util.Time:formatTime(long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getPassword()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getTrackingId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:getIdentifier()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getMaxDate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getRenewer()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:getPassword()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCandidateTokensForCleanup()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:stopThreads()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:<init>(long,long,java.util.function.Function)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:size()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:containsKey(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:containsValue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:get(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:put(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:remove(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:putAll(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:clear()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:keySet()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:values()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:entrySet()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:setAuthHandlerClass(java.util.Properties)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:setHandlerAuthMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:setCurator(org.apache.curator.framework.CuratorFramework)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getDoAs(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getHttpUserGroupInformationInContext()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:<init>(org.apache.hadoop.security.authentication.server.AuthenticationHandler)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:getTokenTypes()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:setConnectionConfigurator(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:augmentURL(java.net.URL,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:useQueryStringForDelegationToken()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:setService(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:<init>(org.apache.hadoop.security.authentication.client.Authenticator)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initJsonFactory(java.util.Properties)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.ServletUtils:getParameter(javax.servlet.http.HttpServletRequest,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:getHttpMethod()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:requiresKerberosCredentials()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getType()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token:getDelegationToken()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token:setDelegationToken(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:mapReader()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationTokenSeqNum()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:setDelegationTokenSeqNum(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:incrementDelegationTokenSeqNum()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCurrentKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:setCurrentKeyId(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:incrementCurrentKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:<init>(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getCurator()",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrSharedCount(org.apache.curator.framework.recipes.shared.SharedCount,int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createPersistentNode(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyRemoved(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationTokenSeqNum()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:setDelegationTokenSeqNum(int)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getCurrentKeyId()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getNodePath(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromMemory(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:toStringStable()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:isPrivate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:getClassForIdentifier(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:addBinaryBuffer(java.lang.StringBuilder,byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:getRenewer()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DelegationTokenIssuer:getAdditionalTokenIssuers()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:stripPrefix(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:formatDate(long)",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Get:isGenericUrl()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Print:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Get:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Edit:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Append:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Remove:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Renew:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Import:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:readNextRpcPacket()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:getNegotiatedProperty(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:getMechanismName()",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:remaining()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:createSaslReply(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:saslEvaluateToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto,boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:useWrap()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:dispose()",
        1
    ],
    [
        "org.apache.hadoop.security.FastSaslClientFactory:<init>(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.IngressPortBasedResolver:getServerProperties(java.net.InetAddress,int)",
        1
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:nestURIForLocalJavaKeyStoreProvider(java.net.URI)",
        1
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:locatePassword(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:noPasswordInstruction(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.FastSaslClientFactory:getMechanismNames(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.FastSaslClientFactory:createSaslClient(java.lang.String[],java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:unsignedBytesToInt(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:disposeSasl()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:<init>(java.io.InputStream,javax.security.sasl.SaslServer)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:<init>(java.io.InputStream,javax.security.sasl.SaslClient)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:skip(long)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:available()",
        1
    ],
    [
        "org.apache.hadoop.security.AccessControlException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.AccessControlException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:encodeIdentifier(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[])",
        1
    ],
    [
        "org.apache.hadoop.security.AnnotatedSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.AnnotatedSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream:write(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isInitialized()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setLoginUser(org.apache.hadoop.security.UserGroupInformation)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getOsPrincipalClass()",
        1
    ],
    [
        "org.apache.hadoop.security.User:getLogin()",
        1
    ],
    [
        "org.apache.hadoop.security.User:setLogin(javax.security.auth.login.LoginContext)",
        1
    ],
    [
        "org.apache.hadoop.security.User:setLastLogin(long)",
        1
    ],
    [
        "org.apache.hadoop.security.User:getName()",
        1
    ],
    [
        "org.apache.hadoop.security.User:getAuthenticationMethod()",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:trimLoginMethod(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getConfiguration()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getParameters()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getRefreshTime(javax.security.auth.kerberos.KerberosTicket)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:executeAutoRenewalTask(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable)",
        1
    ],
    [
        "org.apache.hadoop.security.User:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:setUser(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:setKeytabFile(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getSubject()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getAppName()",
        1
    ],
    [
        "org.apache.hadoop.security.User:getLastLogin()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$RealUser:<init>(org.apache.hadoop.security.UserGroupInformation)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$RealUser:getRealUser()",
        1
    ],
    [
        "org.apache.hadoop.security.User:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getTokenIdentifiers()",
        1
    ],
    [
        "org.apache.hadoop.security.User:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedAction)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:<init>(org.apache.hadoop.security.UserGroupInformation$LoginParams)",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:setPrincipal(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:setTicketCacheFile(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getOSLoginModuleName()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.lang.String[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates:getCNs(java.security.cert.X509Certificate)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates:getDNSSubjectAlts(java.security.cert.X509Certificate)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isIP4Address(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:acceptableCountryWildcard(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:countDots(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String[],java.lang.String[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String[],java.lang.String[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadTrustManager(java.nio.file.Path)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:checkClientTrusted(java.security.cert.X509Certificate[],java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:checkServerTrusted(java.security.cert.X509Certificate[],java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:getAcceptedIssuers()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:destroy()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String[],java.lang.String[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadKeyManager(java.nio.file.Path)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseEngineClientAlias(java.lang.String[],java.security.Principal[],javax.net.ssl.SSLEngine)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseEngineServerAlias(java.lang.String,java.security.Principal[],javax.net.ssl.SSLEngine)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getClientAliases(java.lang.String,java.security.Principal[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseClientAlias(java.lang.String[],java.security.Principal[],java.net.Socket)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getServerAliases(java.lang.String,java.security.Principal[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseServerAlias(java.lang.String,java.security.Principal[],java.net.Socket)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getCertificateChain(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getPrivateKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getResource(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:destroy()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:disableExcludedCiphers(javax.net.ssl.SSLEngine)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:createSSLServerSocketFactory()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:resetDefaultFactory()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:alterCipherList(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:bindToOpenSSLProvider()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:getDefaultCipherSuites()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:getSupportedCipherSuites()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:configureSocket(java.net.Socket)",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String[],java.lang.String[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.conf.Configured:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:getExecString()",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolveFullGroupNames(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler:handle(javax.security.auth.callback.Callback[])",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:valueOf(byte)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:getMechanismNames(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:<init>(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getInetAddressByName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:setSearchDomains(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:prependFileAuthority(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:getGroups(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:isNonEmpty()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:getCanonicalUser(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:login()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:logout()",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer:curThreadTracer()",
        1
    ],
    [
        "org.apache.hadoop.security.Groups$GroupCacheLoader:reload(java.lang.String,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getHostKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.Service:getServiceKey()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.Service:getProtocol()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsWithAcls()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsAcls(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsBlockedAcls(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsWithMachineLists()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsMachineList(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsBlockedMachineList(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:isWildCardACLValue(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:isAllAllowed()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:getString(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(java.io.PrintWriter)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.Service:<init>(java.lang.String,java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getAclKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserIpConfKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.MachineList:includes(java.net.InetAddress)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserUserConfKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserGroupConfKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:getGroups()",
        1
    ],
    [
        "org.apache.hadoop.util.MachineList:getCollection()",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:get(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$ListCommand:validate()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$ListCommand:execute()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:<init>(java.lang.String,char[])",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider:needsPassword()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider:noPasswordError()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider:noPasswordWarning()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell:getPasswordReader()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$PasswordReader:readPassword(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:getCredential()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPath()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:bytesToChars(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:deleteCredentialEntry(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:flush()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:toString()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell:getCommandUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$PasswordReader:format(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProvider:isTransient()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$Command:printProviderWritten()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$Command:doHelp()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:execute()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:getOutputStreamForKeystore()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:keystoreExists()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:getInputStreamForFile()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:modeToPosixFilePermission(int)",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setTokenServiceUseIp(boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal)",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getComponents(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ZKUtil:resolveConfIndirection(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:validateSslConfiguration(org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:getMessage()",
        1
    ],
    [
        "org.apache.hadoop.security.WhitelistBasedResolver:getSaslProperties(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.util.CombinedIPWhiteList:isIn(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:decodeIdentifier(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:splitKerberosName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher$AlgMode:get(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:getLoadingFailureReason()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:getCipherSuite()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoStreamUtils:getInputStreamOffset(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getDecryptor()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getTmpBuf()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getBuffer()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:returnBuffer(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:returnDecryptor(org.apache.hadoop.crypto.Decryptor)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:releaseBuffer(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:setReadahead(java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:setDropBehind(java.lang.Boolean)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getFileDescriptor()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:cleanDecryptorPool()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:checkStream()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:getTmpBuf()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:setDropBehind(java.lang.Boolean)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher$Padding:get(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:getName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:init(int,byte[],byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:<init>()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:getCipherSuite()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:generateSecureRandom(byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoProtocolVersion:getVersion()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoProtocolVersion:setUnknownValue(int)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoProtocolVersion:getUnknownValue()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CryptoProtocolVersion:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:setProvider(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:generateSecureRandom(byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:setUnknownValue(int)",
        1
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:getUnknownValue()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:<init>()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:getCipherSuite()",
        1
    ],
    [
        "org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite,byte[],byte[],byte[],byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher$Transform:<init>(java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:<init>(long,int,int,long)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:clean()",
        1
    ],
    [
        "org.apache.hadoop.crypto.random.OpensslSecureRandom:<init>()",
        1
    ],
    [
        "org.apache.hadoop.crypto.random.OpensslSecureRandom:nextBytes(byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:<init>()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:getCipherSuite()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:<init>(java.lang.String,java.lang.String,byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:warmUpEncryptedKeys(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:generateEncryptedKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:drain(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:reencryptEncryptedKeys(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyProvider()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:close()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:<init>(byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getBitLength()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getCipher()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getDescription()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:getAttributes()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:serialize()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getVersions()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getBitLength()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:addVersion()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getAttributes()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setBitLength(int)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setCipher(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setDescription(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:setAttributes(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell:getCommandUsage()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell:prettifyException(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:getKeysMetadata(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:isBadorWrongPassword(java.io.IOException)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:renameOrFail(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeys()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getMetadata(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getCipher()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:buildVersionName(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:getAlgorithm(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:getBaseName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:findProvider(java.util.List,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:getCurrentKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:getKeyVersion(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:deleteKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:invalidateCache(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:getMetadata(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:getConf()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getMaterial()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:deriveIV(byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getVersionName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$KeyVersion)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptionKeyName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptedKeyVersion()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptedKeyIv()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptionKeyVersionName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getCanonicalServiceName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getDelegationToken(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:needsPassword()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:noPasswordError()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:noPasswordWarning()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$WrapperException:<init>(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:writer()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator:<init>(int,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createURL(java.lang.String,java.lang.String,java.lang.String,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeySets(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.KMSUtil:checkNotNull(java.lang.Object,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:initializeQueuesForKeys(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createAuthenticatedURL()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:shutdown()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKMSUrl()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:put(java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:take()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:poll(long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:cancel()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:indexFor(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSDelegationToken:<init>()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:shuffle(org.apache.hadoop.crypto.key.kms.KMSClientProvider[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:setClientTokenProvider(org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getProviders()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:nextIdx()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:flush()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:isCanceled()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator:configure(java.net.HttpURLConnection)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:isTransient()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyVersion(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeys()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyVersions(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getMetadata(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:deleteKey(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String,byte[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:flush()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:toString()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$RollCommand:validate()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$Command:printProviderWritten()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderFactory:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:validate()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:<init>(org.apache.hadoop.crypto.key.KeyProvider$Metadata)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:isValid(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:checkFieldSeparator(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:setSignature(byte[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:getContext()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:build()",
        1
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:<init>(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:setSenderName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB)",
        1
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.RefreshCallQueueProtocol)",
        1
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB:refreshCallQueue(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.GenericRefreshProtocol)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:getReturnCode()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:getMessage()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:getSenderName()",
        1
    ],
    [
        "org.apache.hadoop.ipc.StandbyException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ClientId:getClientId()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ClientId:getMsb(byte[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.ClientId:getLsb(byte[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.ClientId:toBytes(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:createScheduler(java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:createCallQueueInstance(java.lang.Class,int,int,java.lang.String,int[],org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:<init>(java.util.concurrent.BlockingQueue,org.apache.hadoop.ipc.RpcScheduler,boolean,boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:isServerFailOverEnabled()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:shouldBackOff(org.apache.hadoop.ipc.Schedulable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:isClientBackoffEnabled()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:throwBackoff()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:offer(org.apache.hadoop.ipc.Schedulable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:peek()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:poll()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:poll(long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:take()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:size()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:remainingCapacity()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:getDefaultQueueCapacityWeights(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:queueIsReallyEmpty(java.util.concurrent.BlockingQueue)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:stringRepr(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:drainTo(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:drainTo(java.util.Collection,int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:iterator()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC:getSuperInterfaces(java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolName(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolProxy:getProxy()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getAddress()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:<init>(org.apache.hadoop.thirdparty.protobuf.Message)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:readFrom(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:getSignature()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:getSignature()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:isContextValid()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:getCurrent()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:setCurrent(org.apache.hadoop.ipc.CallerContext)",
        1
    ],
    [
        "org.apache.hadoop.ipc.IpcException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:sendRequest(byte[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:flush()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcKindMapValue:<init>(java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>()",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:getDeclaredClass()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getProtocolVersion()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ExternalCall:waitForCompletion()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ExternalCall:isDone()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ExternalCall:run()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ExternalCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",
        1
    ],
    [
        "org.apache.hadoop.util.Time:monotonicNowNanos()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:isCallCoordinated()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getClientStateId()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:isResponseDeferred()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getDetailedMetricsName()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getRemoteUser()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getProcessingDetails()",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer:activateSpan(org.apache.hadoop.tracing.Span)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Span:addTimelineAnnotation(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:run()",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:addTimelineAnnotation(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcServerException:getRpcStatusProto()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Schedulable:getCallerContext()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:run()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature$ProtocolSigFingerprint:<init>(org.apache.hadoop.ipc.ProtocolSignature,int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException:<init>(java.io.IOException,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException:getCause()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetriableException:<init>(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:<init>(java.net.InetSocketAddress,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:<init>(long,int[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getMethods()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:methodToTraceString(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:toTraceName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshRegistry:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshRegistry:register(java.lang.String,org.apache.hadoop.ipc.RefreshHandler)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshRegistry:unregister(java.lang.String,org.apache.hadoop.ipc.RefreshHandler)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshRegistry:unregisterAll(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshRegistry:handlerName(org.apache.hadoop.ipc.RefreshHandler)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$AuthProtocol:valueOf(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:getErrorCode()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:instantiateException(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:getQueueName(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:getProcessingName(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:getCacheName()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getServerName()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getNumInProcessHandler()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getTotalRequests()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getTotalRequestsPerSecond()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getRpcVersion()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:<init>(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:getProtocolImplMap(org.apache.hadoop.ipc.RPC$RpcKind)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getMethodName()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getParameterClasses()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:setDetailedMetricsName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getParameters()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:<init>(java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:add(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:setDelegate(org.apache.hadoop.ipc.DecayRpcScheduler)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:removeInstance(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getSchedulingDecisionSummary()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getUniqueIdentityCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalCallVolume()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getAverageResponseTime()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getResponseTimeCountInLastWindow()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getPort()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:add(org.apache.hadoop.ipc.Server$Connection)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:remove(org.apache.hadoop.ipc.Server$Connection)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:incrUserConnections(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:decrUserConnections(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:getDroppedConnections()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:size()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:toArray()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getLastContact()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:scheduleIdleScanTask()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:stopIdleScan()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(int[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:resetCache()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:signalNotEmpty()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:setDelegate(org.apache.hadoop.ipc.FairCallQueue)",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:peek()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:size()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:iterator()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection,int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:remainingCapacity()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:getQueueSizes()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:getOverflowedCalls()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:addTerseLoggingExceptions(java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:addSuppressedLoggingExceptions(java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:isTerseLog(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ExceptionsHandler:isSuppressedLog(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:serverNameFromClass(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl:<init>(java.lang.Class,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl:<init>(long,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.RPC$Server)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:call(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String,org.apache.hadoop.io.Writable,long)",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:get()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequestHeader(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getAsyncRpcResponse()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnProtoType(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:setCapacity(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:setSize(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getDefaultThresholds(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getDefaultBackOffResponseTimeThresholds(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:isServiceUser(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:updateAverageResponseTime(boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getIdentity(org.apache.hadoop.ipc.Schedulable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:newSchedulable(org.apache.hadoop.security.UserGroupInformation)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getCallCostSnapshot()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalCallSnapshot()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalRawCallVolume()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalServiceUserCallVolume()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalServiceUserRawCallVolume()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:getName()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:getValue()",
        1
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getDecayedCallCosts()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:<init>(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:readFrom(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner:combine(java.lang.Class,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(com.google.protobuf.ServiceException)",
        1
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getByteString(byte[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:isRpcInvocation()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:lock()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:unlock()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:completed(boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload,boolean,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection$2:run()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:getRequestHeader()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:writeTo(org.apache.hadoop.ipc.ResponseBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DefaultRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getTicket()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getRpcTimeout()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getRetryPolicy()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolProxy:<init>(java.lang.Class,java.lang.Object,boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:setCallIdAndRetryCountUnprotected(java.lang.Integer,int,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getRetryCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getExternalHandler()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:incCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:decAndGetCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:createCall(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:stop()",
        1
    ],
    [
        "org.apache.hadoop.ipc.AsyncCallLimitExceededException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:setAlignmentContext(org.apache.hadoop.ipc.AlignmentContext)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getConnection(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.ipc.Client$Call,int,java.util.concurrent.atomic.AtomicBoolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:releaseAsyncCall()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getAsyncCallCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:getRemoteAddress()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:getRpcResponse()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getConnectionIds()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:getSelector()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:closeCurrentConnection(java.nio.channels.SelectionKey,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:getAddress()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:getReader()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener$Reader:addConnection(org.apache.hadoop.ipc.Server$Connection)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:setLastContact(long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:shouldClose()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener$Reader:shutdown()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client:getSocketFactory()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ClientCache:clearCache()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:isOpen()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getHostInetAddress()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getRemotePort()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:setResponseFields(org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:setReturnStatus(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcServerException:getRpcErrorCodeProto()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:sendDeferedResponse()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getServer()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getMaxIdleTime()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getMaxRetriesOnSasl()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getMaxRetriesOnSocketTimeouts()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getTcpNoDelay()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getTcpLowLatency()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getDoPing()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getPingInterval()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:addCall(org.apache.hadoop.ipc.Client$Call)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getProtocol()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:setAddress(java.net.InetSocketAddress)",
        1
    ],
    [
        "org.apache.hadoop.net.NetUtils:bindToLocalAddress(java.net.InetAddress,boolean)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer:getCurrentSpan()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:writeConnectionHeader(org.apache.hadoop.ipc.Client$IpcStreams)",
        1
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:getAuthMethod()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:markClosed(java.io.IOException)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:closeConnection()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:interruptConnectingThread()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:setInputStream(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:setOutputStream(java.io.OutputStream)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getCallQueue()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getVersion()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:<init>(org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:readFrom(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:isUnshadedProtobufMessage(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:readFrom(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcConstants:<init>()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:isIdle()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:decRpcCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:incRpcCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getTrueCause(java.io.IOException)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:disposeSasl()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getHostAddress()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:setServiceClass(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:setResponse(java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setTracer(org.apache.hadoop.tracing.Tracer)",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceUtils:byteStringToSpanContext(org.apache.hadoop.thirdparty.protobuf.ByteString)",
        1
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:convert(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:getRequestHeader()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getMaxIdleTime()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getAsyncReturnMessage()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getDefaultQueueWeights(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:moveToNextQueue()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getCurrentIndex()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:<init>(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:writeTo(org.apache.hadoop.ipc.ResponseBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:<init>(java.lang.Class,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:<init>(org.apache.hadoop.ipc.RPC$Server,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:deferResponse()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:getServer()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:getMethodName()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:setDeferredResponse(org.apache.hadoop.io.Writable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:setDeferredError(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequestHeader(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnProtoType(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:capacity()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl:isShadedPBImpl()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:callComplete()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getAsyncReturnMessage()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection$PingInputStream:handleTimeout(java.net.SocketTimeoutException,int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getHostInetAddress()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:postponeResponse()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:log(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:get()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getCallId()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getCallRetryCount()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getRemotePort()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:isOnAuxiliaryPort()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getEstablishedQOP()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getClientId()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getProtocol()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getPriorityLevel()",
        1
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setPurgeIntervalNanos(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getLogSlowRPCThresholdTime()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:isLogSlowRPC()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:getReturnStatus()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:isEmpty()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getHandlers()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getQueueClassPrefix()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:convertQueueClass(java.lang.Class,java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:convertSchedulerClass(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:setClientBackoffEnabled(boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setLogSlowRPC(boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:setIsAuxiliary()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:setShouldClose()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:getMessage()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getDelimitedLength(org.apache.hadoop.thirdparty.protobuf.Message)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:start()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:shutdownMetricsUpdaterExecutor()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:join()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:call(org.apache.hadoop.io.Writable,long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getConf()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:getUserToConnectionsMap()",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabled()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:channelIO(java.nio.channels.ReadableByteChannel,java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:waitPending()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:doPurge(org.apache.hadoop.ipc.Server$RpcCall,long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:incPending()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:decPending()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:isEqual(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:toString()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$FatalRpcServerException:toString()",
        1
    ],
    [
        "org.apache.hadoop.tracing.Tracer:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Span:<init>()",
        1
    ],
    [
        "org.apache.hadoop.tracing.SpanContext:<init>()",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:<init>(org.apache.hadoop.tracing.Span)",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceConfiguration:<init>()",
        1
    ],
    [
        "org.apache.hadoop.tracing.Span:close()",
        1
    ],
    [
        "org.apache.hadoop.util.MachineList$InetAddressFactory:getByName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ConfTest:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ConfTest:parseConf(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.util.ConfTest$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ConfTest:terminate(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getCurrentTime()",
        1
    ],
    [
        "org.apache.hadoop.util.CpuTimeTracker:<init>(long)",
        1
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:safeParseLong(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readProcCpuInfoFile()",
        1
    ],
    [
        "org.apache.hadoop.util.CpuTimeTracker:updateElapsedJiffies(java.math.BigInteger,long)",
        1
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readProcNetInfoFile()",
        1
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readDiskBlockInformation(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.util.CpuTimeTracker:getCumulativeCpuTime()",
        1
    ],
    [
        "org.apache.hadoop.util.CpuTimeTracker:getCpuTrackerUsagePercent()",
        1
    ],
    [
        "org.apache.hadoop.util.DurationInfo:getFormattedText()",
        1
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:putInternal(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:getElementIndex(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:visitAll(org.apache.hadoop.util.IdentityHashStore$Visitor)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:nextNonemptyEntry()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:convert(org.apache.hadoop.util.LightWeightGSet$LinkedElement)",
        1
    ],
    [
        "org.apache.hadoop.util.MergeSort:swap(int[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.XMLUtils:newSecureDocumentBuilderFactory()",
        1
    ],
    [
        "org.apache.hadoop.util.XMLUtils:newSecureSAXParserFactory()",
        1
    ],
    [
        "org.apache.hadoop.util.XMLUtils:bestEffortSetAttribute(javax.xml.transform.TransformerFactory,java.util.concurrent.atomic.AtomicBoolean,java.lang.String,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.Waitable:<init>(java.util.concurrent.locks.Condition)",
        1
    ],
    [
        "org.apache.hadoop.util.Waitable:await()",
        1
    ],
    [
        "org.apache.hadoop.util.Waitable:provide(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:valueOf(char)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:format(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.util.HeapSort:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.HeapSort:downHeap(org.apache.hadoop.util.IndexedSortable,int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:<init>(java.lang.management.GarbageCollectorMXBean)",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:<init>(long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.CombinedIPList:isIn(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum$ChecksumNull:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:terminateCalled()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:haltCalled()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:getFirstExitException()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:getFirstHaltException()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:resetFirstExitException()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:resetFirstHaltException()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil:addSuppressed(java.lang.Throwable,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$HaltException:getExitCode()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ConfigurationHelper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ExitCodeException:<init>(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ExitCodeException:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$ListItem:isNull()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$ListItem:normalize()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$ListItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$ListItem:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.ThreadUtil:joinUninterruptibly(java.lang.Thread)",
        1
    ],
    [
        "org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.ClassLoader,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:newCondition()",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:getSuppressedCount()",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:getMaxSuppressedWait()",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:getStackTrace(java.lang.Thread)",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock$SuppressedStats:incrementSuppressed(long)",
        1
    ],
    [
        "org.apache.hadoop.util.QuickSort:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.QuickSort:fix(org.apache.hadoop.util.IndexedSortable,int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.QuickSort:getMaxDepth(int)",
        1
    ],
    [
        "org.apache.hadoop.util.ProcessUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,int)",
        1
    ],
    [
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,byte[])",
        1
    ],
    [
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,int,byte[])",
        1
    ],
    [
        "org.apache.hadoop.util.LineReader:close()",
        1
    ],
    [
        "org.apache.hadoop.util.LineReader:fillBuffer(java.io.InputStream,byte[],boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.LineReader:unsetNeedAdditionalRecordAfterSplit()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:isTimedOut()",
        1
    ],
    [
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:getNamedThreadFactory(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:getActiveCount()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:size()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:values()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:resize(int)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:isNull()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting:acquire(int)",
        1
    ],
    [
        "org.apache.hadoop.util.KMSUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:cast(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.util.Iterator)",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:checkNonnegative(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:newLinkedList()",
        1
    ],
    [
        "org.apache.hadoop.util.Lists:saturatedCast(long)",
        1
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:isSystemClass(java.lang.String,java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:remove()",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor$CallableWithPermitRelease:call()",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:getName()",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:fromJsonStream(java.io.InputStream)",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:load(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:fromResource(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:toBytes(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.StopWatch:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.OperationDuration:time()",
        1
    ],
    [
        "org.apache.hadoop.util.OperationDuration:value()",
        1
    ],
    [
        "org.apache.hadoop.util.OperationDuration:humanTime(long)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:updateRecommendedLength(int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:isExpired(org.apache.hadoop.util.LightWeightCache$Entry,long)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:iterator()",
        1
    ],
    [
        "org.apache.hadoop.util.Daemon:<init>(java.lang.ThreadGroup,java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.util.CrcUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.CrcUtil:galoisFieldMultiply(int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.CrcUtil:writeInt(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.CrcUtil:readInt(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.util.RunJar:ensureDirectory(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.RunJar:skipUnjar()",
        1
    ],
    [
        "org.apache.hadoop.util.RunJar:useClientClassLoader()",
        1
    ],
    [
        "org.apache.hadoop.util.RunJar:getHadoopClasspath()",
        1
    ],
    [
        "org.apache.hadoop.util.RunJar:getSystemClasses()",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadExecutor()",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadExecutor(java.util.concurrent.ThreadFactory)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.ThreadFactory)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadScheduledExecutor()",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadScheduledExecutor(java.util.concurrent.ThreadFactory)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.ExecutorHelper:logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.ExecutorHelper:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:<init>(org.apache.hadoop.util.concurrent.AsyncGet)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:callAsyncGet(long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.RejectedExecutionHandler)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.ThreadFactory,java.util.concurrent.RejectedExecutionHandler)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.RejectedExecutionHandler)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory,java.util.concurrent.RejectedExecutionHandler)",
        1
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.util.StopWatch:<init>(org.apache.hadoop.util.Timer)",
        1
    ],
    [
        "org.apache.hadoop.util.LambdaUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.UTF8ByteArrayUtils:findByte(byte[],int,int,byte)",
        1
    ],
    [
        "org.apache.hadoop.util.RateLimitingFactory$NoRateLimiting:acquire(int)",
        1
    ],
    [
        "org.apache.hadoop.util.CacheableIPList:updateCacheExpiryTime()",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:size()",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:clear()",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:lookup(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:resolve(java.lang.ref.WeakReference)",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:noteLost(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:getReferenceLostCount()",
        1
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:getEntriesCreatedCount()",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:<init>(long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.NativeCrc32:verifyChunkedSums(int,int,java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.util.NativeCrc32:verifyChunkedSumsByteArray(int,int,byte[],int,byte[],int,int,java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.util.NativeCrc32:calculateChunkedSums(int,int,java.nio.ByteBuffer,java.nio.ByteBuffer)",
        1
    ],
    [
        "org.apache.hadoop.util.NativeCrc32:calculateChunkedSumsByteArray(int,int,byte[],int,byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.DirectBufferPool:countBuffersOfSize(int)",
        1
    ],
    [
        "org.apache.hadoop.util.NativeCodeLoader:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.StringInterner:strongIntern(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:simpleHostname(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:arrayToString(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:hexStringToByte(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:formatTime(long)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:formatTimeSortable(long)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:getTrimmedStringsSplitByEquals(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:findNext(java.lang.String,char,char,int,java.lang.StringBuilder)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:hasChar(char[],char)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:toStartupShutdownString(java.lang.String,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:escapeHTML(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:popFirstNonOption(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.util.StringUtils:isAlpha(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:parseExecResult(java.io.BufferedReader)",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getVersion()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getRevision()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getBranch()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getDate()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getUser()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getUrl()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getSrcChecksum()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getProtocVersion()",
        1
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getCompilePlatform()",
        1
    ],
    [
        "org.apache.hadoop.util.DiskValidatorFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:iterator()",
        1
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:clear()",
        1
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:getNumChunks()",
        1
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:getMaxChunkSize()",
        1
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:get(int)",
        1
    ],
    [
        "org.apache.hadoop.util.PrintJarMainClass:main(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:fetch()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.BiFunctionRaisingIOE:unchecked(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object,boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:close()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:<init>(long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:<init>(java.util.concurrent.Callable)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:get()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:eval()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:isClosed()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:close()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.CallableRaisingIOE:unchecked()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:<init>(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:<init>(java.util.concurrent.ExecutorService)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:close()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:submit(java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.ConsumerRaisingIOE:andThen(org.apache.hadoop.util.functional.ConsumerRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:waitFor(java.util.Collection,int)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:<init>(org.apache.hadoop.fs.RemoteIterator)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:castAndThrow(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:<init>(java.util.Iterator)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:isSet()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getSource()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromSingleton(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromIterator(java.util.Iterator)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromIterable(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromArray(java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:typeCastingRemoteIterator(org.apache.hadoop.fs.RemoteIterator)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:filteringRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:closingRemoteIterator(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:haltableRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:rangeExcludingIterator(long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:runSingleThreaded(org.apache.hadoop.util.functional.TaskPool$Task)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.FunctionalIO:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.FunctionalIO:extractIOExceptions(java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples$Tuple:<init>(java.lang.Object,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples$Tuple:setValue(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples$Tuple:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples$Tuple:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.functional.Tuples$Tuple:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:unwrapInnerException(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getClassByNameOrNull(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:setContentionTracing(boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:getTaskName(long,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:clearCache()",
        1
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:getCacheSize()",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:buildGeneralOptions(org.apache.commons.cli.Options)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getClassLoader()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:setClassLoader(java.lang.ClassLoader)",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:matchesCurrentDirectory(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:preProcessForWindows(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.HttpExceptionUtils:getOneLineMessage(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.HttpExceptionUtils:throwException(java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:sourceName(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkState(boolean,java.util.function.Supplier)",
        1
    ],
    [
        "org.apache.hadoop.util.InvalidChecksumSizeException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.PureJavaCrc32C:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:<init>(int,float)",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:size()",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:contains(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:get(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:put(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:remove(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:iterator()",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:clear()",
        1
    ],
    [
        "org.apache.hadoop.util.GSetByHashMap:values()",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:removeElement(org.apache.hadoop.util.IntrusiveCollection$Element)",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:iterator()",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:contains(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:addFirst(org.apache.hadoop.util.IntrusiveCollection$Element)",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:addAll(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:add(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.Time:getUtcTime()",
        1
    ],
    [
        "org.apache.hadoop.util.CleanerUtil:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.CleanerUtil:newBufferCleaner(java.lang.Class,java.lang.invoke.MethodHandle)",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader$HostDetails:<init>(java.lang.String,java.util.Set,java.lang.String,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader$HostDetails:getExcludedHosts()",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker:mkdirsWithExistsCheck(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker:getFileNameForDiskIoCheck(java.io.File,int)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker:replaceFileOutputStreamProvider(org.apache.hadoop.util.DiskChecker$FileIoProvider)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker:getFileOutputStreamProvider()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$StringOption:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getCrcPolynomialForType(org.apache.hadoop.util.DataChecksum$Type)",
        1
    ],
    [
        "org.apache.hadoop.util.CrcComposer:<init>(int,int,long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getOSType()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getWinUtilsPath()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:appendScriptExtension(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:checkHadoopHomeInner(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:addOsText(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:fileNotFoundException(java.lang.String,java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:setEnvironment(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:joinThread(java.lang.Thread)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getEnvironment(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:setTimedOut()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getProcess()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getAllShells()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getMemlockLimit(java.lang.Long)",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread,long)",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService,long)",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:setStatus(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:setParent(org.apache.hadoop.util.Progress)",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:getProgressWeightage(int)",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:phase()",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:startNextPhase()",
        1
    ],
    [
        "org.apache.hadoop.util.Progress:getParent()",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:isJaasConfigurationSet(org.apache.zookeeper.client.ZKClientConfig)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:<init>(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:close()",
        1
    ],
    [
        "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:getAuth()",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getACL(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getData(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getData(java.lang.String,org.apache.zookeeper.data.Stat)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getChildren(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:exists(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getNodePath(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:createTransaction(java.util.List,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:create(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:commit()",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:delete(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:setData(java.lang.String,byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.util.ProgramDriver$ProgramDescription:<init>(java.lang.Class,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.ProgramDriver$ProgramDescription:invoke(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:<init>(java.lang.String,byte[])",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:<init>(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.Class,java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:buildChecked()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:build()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods:throwIfInstance(java.lang.Throwable,java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors:formatProblems(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors:methodName(java.lang.Class,java.lang.Class[])",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:<init>(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:orNoop()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:<init>(java.lang.reflect.Method,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible:<init>(java.lang.reflect.Method)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:build()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:bind(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:asStatic()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible:run()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible:<init>(java.lang.reflect.Constructor)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible:run()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:loadClassSafely(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:loadClass(java.lang.ClassLoader,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:isNoop()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:isStatic()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:bind(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:<init>(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod,java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.ClassUtil:findContainingResource(java.lang.ClassLoader,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum$Java9Crc32CFactory:createChecksum()",
        1
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:convert(org.apache.hadoop.ipc.RPC$RpcKind)",
        1
    ],
    [
        "org.apache.hadoop.tracing.Span:getContext()",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceUtils:spanContextToByteString(org.apache.hadoop.tracing.SpanContext)",
        1
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:getContext()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:<init>(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,int)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getChecksumHeaderSize()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum$Type:valueOf(int)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:writeHeader(java.io.DataOutputStream)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:compare(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:update(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:update(int)",
        1
    ],
    [
        "org.apache.hadoop.util.DataChecksum:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:isJavaVersionAtLeast(int)",
        1
    ],
    [
        "org.apache.hadoop.util.RateLimitingFactory:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.RateLimitingFactory:unlimitedRate()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCodeNative:getLoadingFailureReason()",
        1
    ],
    [
        "org.apache.hadoop.util.SignalLogger$Handler:<init>(java.lang.String,org.slf4j.Logger)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$StringItem:<init>(java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$1:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:actualArrayLength(int)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:getIndex(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:printDetails(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:clear()",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAll(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAll(java.util.Collection,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAny(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAny(java.util.Collection,long,java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor$RunnableWithPermitRelease:run()",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getAvailablePermits()",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getWaitingCount()",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getPermitCount()",
        1
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:delegate()",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:readFileToSetWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:readFirstTagValue(org.w3c.dom.Element,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:finishRefresh()",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader$HostDetails:getIncludedHosts()",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader$HostDetails:getExcludedMap()",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:getHostDetails()",
        1
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:getLazyLoadedHostDetails()",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.util.ExitUtil$HaltException:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.hash.JenkinsHash:rot(long,int)",
        1
    ],
    [
        "org.apache.hadoop.util.hash.Hash:parseHashType(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.hash.JenkinsHash:getInstance()",
        1
    ],
    [
        "org.apache.hadoop.util.hash.MurmurHash:getInstance()",
        1
    ],
    [
        "org.apache.hadoop.util.hash.Hash:hash(byte[])",
        1
    ],
    [
        "org.apache.hadoop.util.hash.Hash:hash(byte[],int)",
        1
    ],
    [
        "org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.SignalLogger$Handler:handle(sun.misc.Signal)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:buckets2words(int)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.HashFunction:clear()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:not()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:xor(org.apache.hadoop.util.bloom.Filter)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:and(org.apache.hadoop.util.bloom.Filter)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:not()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:or(org.apache.hadoop.util.bloom.Filter)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:xor(org.apache.hadoop.util.bloom.Filter)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:getNBytes()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:getBytes()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:getActiveStandardBF()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:add(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:add(java.util.Collection)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:add(org.apache.hadoop.util.bloom.Key[])",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:createVector()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:randomRemove()",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.Key:getWeight()",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:intersection(java.util.Set,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:getGcTimes()",
        1
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:available()",
        1
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:mark(int)",
        1
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:read()",
        1
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:read(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:skip(long)",
        1
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:<init>(java.util.concurrent.locks.Lock)",
        1
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:acquire()",
        1
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:release()",
        1
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:tryLock()",
        1
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:isLocked()",
        1
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:newCondition()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$StringItem:comparableQualifier(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.FindClass:err(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.util.FindClass:out(java.lang.String,java.lang.Object[])",
        1
    ],
    [
        "org.apache.hadoop.util.FastNumberFormat:format(java.lang.StringBuilder,long,int)",
        1
    ],
    [
        "org.apache.hadoop.util.FileBasedIPList:readLines(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor$TsAndData:setValues(long,long)",
        1
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor$GcData:clone()",
        1
    ],
    [
        "org.apache.hadoop.util.PureJavaCrc32:reset()",
        1
    ],
    [
        "org.apache.hadoop.util.CpuTimeTracker:toString()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:getShutdownHooksInOrder()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getHook()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getTimeout()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getTimeUnit()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:clearShutdownHooks()",
        1
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:upHeap()",
        1
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:top()",
        1
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:downHeap()",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:newHashSet()",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:newTreeSet()",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:cast(java.lang.Iterable)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:addAll(java.util.Collection,java.util.Iterator)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:capacity(int)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:union(java.util.Set,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:difference(java.util.Set,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:differenceInTreeSets(java.util.Set,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:symmetricDifference(java.util.Set,java.util.Set)",
        1
    ],
    [
        "org.apache.hadoop.util.Sets:newConcurrentHashSet()",
        1
    ],
    [
        "org.apache.hadoop.util.AsyncDiskService:<init>(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.util.AsyncDiskService:execute(java.lang.String,java.lang.Runnable)",
        1
    ],
    [
        "org.apache.hadoop.util.AsyncDiskService:shutdown()",
        1
    ],
    [
        "org.apache.hadoop.util.AsyncDiskService:shutdownNow()",
        1
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor$GcData:update(long,long,long,long,int)",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:<init>(long)",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:getCurrentValue()",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:setCurrentValue(long)",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:setIfGreater(long)",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:nextValue()",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:equals(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.util.ProgramDriver:<init>()",
        1
    ],
    [
        "org.apache.hadoop.util.ProgramDriver$ProgramDescription:getDescription()",
        1
    ],
    [
        "org.apache.hadoop.util.ServletUtil:parseLongParam(javax.servlet.ServletRequest,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider:get(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider:write(java.io.FileOutputStream,byte[])",
        1
    ],
    [
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB)",
        1
    ],
    [
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.tools.GetUserMappingsProtocol)",
        1
    ],
    [
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB:getGroupsForUser(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:printShellUsage()",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell$SubCommand:validate()",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:printException(java.lang.Exception)",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing$Column:addRow(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing$Column:setWrapWidth(int)",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing$Column:getMaxWidth()",
        1
    ],
    [
        "org.apache.hadoop.tools.TableListing:<init>(org.apache.hadoop.tools.TableListing$Column[],boolean,int)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationContext:getDeprecatedKeyMap()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationContext:getReverseDeprecatedKeyMap()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getOverlay()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getAndSetAccessed()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:logDeprecation(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getQuietMode()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:reloadConfiguration()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:isParserRestricted()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:findSubVariable(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getenv(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getProperty(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:putIntoUpdatingResource(java.lang.String,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getHexDigits(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration:unitFor(java.util.concurrent.TimeUnit)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration:unitFor(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageSize:getValue()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageSize:getUnit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:convertStorageUnit(double,org.apache.hadoop.conf.StorageUnit,org.apache.hadoop.conf.StorageUnit)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getFinalParameters()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:parse(java.io.InputStream,java.lang.String,boolean)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:addTags(java.util.Properties)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:getResource()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:getName()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:overlay(java.util.Properties,java.util.Properties)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:parse()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:readTagFromConfig(java.lang.String,java.lang.String,java.lang.String,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:checkForOverride(java.util.Properties,java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:toString(java.util.List,java.lang.StringBuilder)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:getAllPropertiesByTag(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:isPropertyTag(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange:<init>(java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.ConfigRedactor:configIsSensitive(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:init()",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:getReconfigurable(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:printHeader(java.io.PrintWriter,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:printFooter(java.io.PrintWriter)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:getParams(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationTaskStatus:<init>(long,long,java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.conf.ConfServlet:getConfFromContext()",
        1
    ],
    [
        "org.apache.hadoop.conf.ConfServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)",
        1
    ],
    [
        "org.apache.hadoop.conf.ConfServlet$BadFormatException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit:divide(double,double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit:multiply(double,double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit:toString()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:<init>(org.apache.hadoop.conf.ReconfigurableBase)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:shutdownReconfigurationTask()",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:isPropertyReconfigurable(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:getKey()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:getNewKeys()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:getCustomMessage()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:<init>(java.lang.String[],java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationException:constructMessage(java.lang.String,java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationException:<init>()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:clearAccessed()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedItem:<init>(java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:<init>(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:hasNext()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:next()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:remove()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:<init>()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:convertToInt(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:isIncluded(int)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:toString()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:getRangeStart()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageSize:<init>(org.apache.hadoop.conf.StorageUnit,double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageSize:checkState(boolean,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toKBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toMBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toGBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toTBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toPBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:toEBs(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.ha.StreamPumper:pump()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:terminateConnection()",
        1
    ],
    [
        "org.apache.hadoop.ha.StreamPumper:<init>(org.slf4j.Logger,java.lang.String,java.io.InputStream,org.apache.hadoop.ha.StreamPumper$StreamType)",
        1
    ],
    [
        "org.apache.hadoop.ha.StreamPumper:join()",
        1
    ],
    [
        "org.apache.hadoop.ha.StreamPumper:start()",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ha.ZKFCProtocol)",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:cedeActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:gracefulFailover(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceStatus:<init>(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceStatus:setReadyToBecomeActive()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceStatus:setNotReadyToBecomeActive(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo:getSource()",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ha.HAServiceProtocol)",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:monitorHealth(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo:<init>(org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceStatus:getState()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceStatus:isReadyToBecomeActive()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceStatus:getNotReadyReason()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:setProtocol(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:setInstance(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:setBindAddress(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:setPort(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:setNumHandlers(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:setVerbose(boolean)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFCRpcServer:start()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFCRpcServer:getAddress()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFCRpcServer:stopAndJoin()",
        1
    ],
    [
        "org.apache.hadoop.ha.BadFencingConfigurationException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.BadFencingConfigurationException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:getTargetIds(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:getUsageString()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:isAutoFailoverEnabled()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:addTransitionToActiveCliOpts(org.apache.commons.cli.Options)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAAdmin$UsageInfo:<init>(java.lang.String,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:cleanup(com.jcraft.jsch.ChannelExec)",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthCheckFailedException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthCheckFailedException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:isEnabled(int)",
        1
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.PowerShellFencer:checkArgs(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:setLastHealthState(org.apache.hadoop.ha.HealthMonitor$State)",
        1
    ],
    [
        "org.apache.hadoop.ha.FailoverFailedException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.FailoverFailedException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:mainLoop()",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:shutdown()",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:join()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:printUsage()",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:addCallback(org.apache.hadoop.ha.HealthMonitor$Callback)",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback)",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:start()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:fatalError(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord:<init>(boolean,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:recordActiveAttempt(org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord)",
        1
    ],
    [
        "org.apache.hadoop.ha.ServiceFailedException:<init>(java.lang.String,java.lang.Throwable)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:waitForActiveAttempt(int,long)",
        1
    ],
    [
        "org.apache.hadoop.ha.ServiceFailedException:<init>(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:getLastHealthState()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:scheduleRecheck(long)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeActive()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeStandby()",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:notifyFatalError(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:fenceOldActive(byte[])",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:toString()",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:enterState(org.apache.hadoop.ha.HealthMonitor$State)",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:setLastServiceStatus(org.apache.hadoop.ha.HAServiceStatus)",
        1
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:isAlive()",
        1
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:waitForZKConnectionEvent(int)",
        1
    ],
    [
        "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg:<init>(org.apache.hadoop.ha.FenceMethod,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg:toString()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getTransitionTargetHAStatus()",
        1
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:parseArgs(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState,java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:tryGetPid(java.lang.Process)",
        1
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:abbreviate(java.lang.String,int)",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorAddress()",
        1
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:addFencingParameters(java.util.Map)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:isNodeExists(org.apache.zookeeper.KeeperException$Code)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:isNodeDoesNotExist(org.apache.zookeeper.KeeperException$Code)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:isSuccess(org.apache.zookeeper.KeeperException$Code)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:becomeStandby()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:createLockNodeAsync()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:isSessionExpired(org.apache.zookeeper.KeeperException$Code)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:enterNeutralMode()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:monitorLockNodeAsync()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:initiateZookeeper(org.apache.zookeeper.client.ZKClientConfig)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:sleepFor(int)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:preventSessionReestablishmentForTests()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:allowSessionReestablishmentForTests()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:getZKSessionIdForTests()",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code,org.apache.zookeeper.KeeperException$Code)",
        1
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:getHAZookeeperConnectionState()",
        1
    ],
    [
        "org.apache.hadoop.io.ByteBufferPool:release()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:setAllowNullValueProperties(boolean)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:setRestrictSystemProperties(boolean)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration:setRestrictSystemProps(boolean)",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:suffix()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:unit()",
        1
    ],
    [
        "org.apache.hadoop.conf.Configured:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDelegationToken(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getChildFileSystems()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setWriteChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getRawFileSystem()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getWorkingDirectory()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setWriteChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getUri()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getCommandFactory()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getDepth()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:isRecursive()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Command:setRecursive(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:setDirectWrite(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:setLazyPersist(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:setOverwrite(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:setWriteChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getExecutor()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getThreadCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getThreadPoolQueueSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:getListingGroupSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isDirRecurse()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isHumanReadable()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isPathOnly()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOptions()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:preferDirectBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:release()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getNumDataUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getNumParityUnits()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOptions()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:preferDirectBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:release()",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:getErr()",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:getOut()",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:setErr(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:setOut(java.io.PrintStream)",
        1
    ],
    [
        "org.apache.hadoop.tools.CommandShell:setSubCommand(org.apache.hadoop.tools.CommandShell$SubCommand)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:fromBytes(double)",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:getLongName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:getShortName()",
        1
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:getSuffixChar()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:close()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:getLogger()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:getProvider()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:getLogger()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getEngineId()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:getLogger()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getRandom()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setEngineId(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setRandom(java.util.Random)",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:getLogger()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getExtension()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getCreated()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getDescription()",
        1
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getMyFs()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getUriDefaultPort()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setVerifyChecksum(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getUriDefaultPort()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getDirPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getJitter()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:getRefreshInterval()",
        1
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:setShouldFirstRefresh(boolean)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:getWrappedStream()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getChunkPosition(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:mark(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:markSupported()",
        1
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getChunkPosition(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:getPos()",
        1
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:seekToNewSource(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:getBufferedDataSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:getDataChecksum()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:hasCapability(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getLength()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.EtagChecksum:getAlgorithmName()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileChecksum:getChecksumOpt()",
        1
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:setBlockLocations(org.apache.hadoop.fs.BlockLocation[])",
        1
    ],
    [
        "org.apache.hadoop.fs.PathIOException:withFullyQualifiedPath(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getFileAndDirectoryCount()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getQuota()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getSpaceConsumed()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getSpaceQuota()",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:setQuota(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:setSpaceConsumed(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:setSpaceQuota(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.EmptyStorageStatistics:getLong(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.StorageStatistics:getScheme()",
        1
    ],
    [
        "org.apache.hadoop.fs.EmptyStorageStatistics:isTracked(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.EmptyStorageStatistics:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalPath()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalPathHandle()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptions()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getBlockSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getChecksumOpt()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFlags()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getProgress()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getReplication()",
        1
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:isRecursive()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getBufferSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:withFileStatus(org.apache.hadoop.fs.FileStatus)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:getBasePath()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:getData()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:setData(java.util.concurrent.CompletableFuture)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:setLength(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:setOffset(long)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:getBlockData()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getBlockNumber()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getKind()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getMasked()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:getGroupName()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:getPermission()",
        1
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:getUserName()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getHelp()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getPath(org.apache.hadoop.fs.shell.PathData)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:setHelp(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:setUsage(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque)",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getPrecedence()",
        1
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:isOperator()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:getKeyToBufferDir()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getIndex()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getState()",
        1
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getStatistics()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getFallbackLink()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:isInternalDir()",
        1
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:getNewClasses()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:get()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:getComponentType()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:getDeclaredComponentType()",
        1
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:isDeclaredComponentType(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:getIndexInterval()",
        1
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:setIndexInterval(int)",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:getCompressionCodec()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:getKeyClass()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:getValueClass()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.WritableComparator:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:mark(int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:markSupported()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:getAdjustedEnd()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:getAdjustedStart()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:resetState()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:setEnd(long)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:setStart(long)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:resetState()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:getDefaultExtension()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:getRemaining()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getCodecOptions()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getCoderOptions()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCoderOptions(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getInputBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getOutputBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getSubPacketSize()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:preferDirectBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:preferDirectBuffer()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:isSupported()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:getCodec()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:isSupported()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getCallId()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getCounters()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:calculateSleepTime(int)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:calculateSleepTime(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:getExpirationTime()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:getNext()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:isSuccess()",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:setExpirationTime(long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:setNext(org.apache.hadoop.util.LightWeightGSet$LinkedElement)",
        1
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:getByteBuffer()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getCallQueue()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getMaxQueueSize()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getNumReaders()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getPurgeIntervalNanos()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getRpcDetailedMetrics()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getRpcMetrics()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:getServiceAuthorizationManager()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setAlignmentContext(org.apache.hadoop.ipc.AlignmentContext)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setCallQueue(org.apache.hadoop.ipc.CallQueueManager)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setRpcRequestClass(java.lang.Class)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server:setSocketSendBufSize(int)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getCallerContext()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getFederatedNamespaceState()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getTimestampNanos()",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:markCallCoordinated(boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:setClientStateId(long)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:setFederatedNamespaceState(org.apache.hadoop.thirdparty.protobuf.ByteString)",
        1
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:setPriorityLevel(int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:type()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:type()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:type()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:type()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:type()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:type()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:parent()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:parent()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:parent()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:postStop()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:preStart()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:preStop()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetric:changed()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetric:clearChanged()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetric:setChanged()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounter:info()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGauge:info()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:getEstimator()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:getSnapshotTimeStamp()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:setExtended(boolean)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:setUpdateTimeStamp(boolean)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:flush()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getDatagramSocket()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getHostName()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getMetricsServers()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:isSupportSparseMetrics()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:resetBuffer()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:setDatagramSocket(java.net.DatagramSocket)",
        1
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:getConf()",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:reloadCachedMappings()",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:reloadCachedMappings(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:isSingleSwitch()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getNumOfNonEmptyRacks()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getNumOfRacks()",
        1
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:hasClusterEverBeenMultiRack()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:getLevel()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:getName()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:getNetworkLocation()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:getParent()",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:setLevel(int)",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:setNetworkLocation(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.net.NodeBase:setParent(org.apache.hadoop.net.Node)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:getFs()",
        1
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:getChildren()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:close()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:getChannel()",
        1
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:setTimeout(long)",
        1
    ],
    [
        "org.apache.hadoop.security.Groups:getNegativeCache()",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getLdapUrls()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getDefaultProperties()",
        1
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:setRunRenewalLoop(boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getKeyStore()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPassword()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getReadLock()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getUri()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getWriteLock()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:isChanged()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setChanged(boolean)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setPassword(char[])",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setPath(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setReadLock(java.util.concurrent.locks.Lock)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setWriteLock(java.util.concurrent.locks.Lock)",
        1
    ],
    [
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getKeyStoreType()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getSchemeName()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getKeyStoreType()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getSchemeName()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getKeyStoreType()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getSchemeName()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getAlgorithm()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getKeyStoreType()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getSchemeName()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.PolicyProvider$1:getServices()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String[],java.lang.String[],java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.security.token.SecretManager:checkAvailableForRead()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getMetrics()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRenewInterval()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:isRunning()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:isTokenWatcherEnabled()",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:setID(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:setKind(org.apache.hadoop.io.Text)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token:setPassword(byte[])",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getIssueDate()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:getKind()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getOwner()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:hashCode()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.security.token.Token$TrivialRenewer:isManaged(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getAuthHandler()",
        1
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getTokenManager()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getConfig()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getFailureCause()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getFailureState()",
        1
    ],
    [
        "org.apache.hadoop.service.AbstractService:getStartTime()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$ListCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$RollCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$ListCommand:getUsage()",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:addKVAnnotation(java.lang.String,java.lang.Number)",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:detach()",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:reattach()",
        1
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:span()",
        1
    ],
    [
        "org.apache.hadoop.util.Daemon:getRunnable()",
        1
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:getConfiguration()",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:getLock()",
        1
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:getTimer()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$BooleanOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$ClassOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$FSDataInputStreamOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$FSDataOutputStreamOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$IntegerOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$LongOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$PathOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.Options$ProgressableOption:getValue()",
        1
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:clear()",
        1
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:initialize(int)",
        1
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:size()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getExitCode()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:getWaitingThread()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell:setWorkingDirectory(java.io.File)",
        1
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:getVectorSize()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:getConstructor()",
        1
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:getReference()",
        1
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:isContextReset()",
        1
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:isContextReset()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:drain(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:warmUpEncryptedKeys(java.lang.String[])",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:cancelDelegationToken(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getCanonicalServiceName()",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getDelegationToken(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:renewDelegationToken(org.apache.hadoop.security.token.Token)",
        1
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:selectDelegationToken(org.apache.hadoop.security.Credentials)",
        1
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:getConf()",
        1
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:hasMore()",
        1
    ],
    [
        "org.apache.hadoop.fs.GlobFilter$1:accept(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:basePath()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:pageSize()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockAddedToFileCache()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockEvictedFromFileCache()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockRemovedFromFileCache()",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:executorAcquired(java.time.Duration)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:memoryAllocated(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:memoryFreed(int)",
        1
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationCompleted()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getID()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMaximumSample(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMinimumSample(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addTimedOperation(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getCounterReference(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getGaugeReference(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMaximumReference(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMeanStatistic(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMinimumReference(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementCounter(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementGauge(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMaximum(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMinimum(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:reset()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setCounter(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setGauge(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMaximum(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMinimum(java.lang.String,long)",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getAggregator()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getID()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:failed()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:failed()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:getType()",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptRemainingPath(org.apache.hadoop.fs.Path)",
        1
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptSource(java.lang.String)",
        1
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:enterNeutralMode()",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getUnderlyingProxyObject()",
        1
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:getUnderlyingProxyObject()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.GenericWritable:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.GenericWritable:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.NullWritable:readFields(java.io.DataInput)",
        1
    ],
    [
        "org.apache.hadoop.io.NullWritable:write(java.io.DataOutput)",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:cancel()",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:getLength()",
        1
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:getOffset()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:getSize()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getKey()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getProgress()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getValue()",
        1
    ],
    [
        "org.apache.hadoop.io.SequenceFile$UncompressedBytes:getSize()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:getDefaultExtension()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:getCompressorType()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:getDecompressorType()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:getDefaultExtension()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:getDecompressorType()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:decompress(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:getRemaining()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsInput()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:setInput(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:getCompressorType()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:getDecompressorType()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:getDefaultExtension()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getConf()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getDefaultExtension()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:reinit(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:reset()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:getBytesRead()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:getBytesWritten()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:getRemaining()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsDictionary()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:end()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:getBytesRead()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:getBytesWritten()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setDictionary(byte[],int,int)",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:finished()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:getRemaining()",
        1
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:getInputBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:getOutputBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:finish()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:getInputBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:getOutputBlocks()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCoderName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCoderName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCoderName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCoderName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCoderName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCodecName()",
        1
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCoderName()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:magnitude()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.ByteArray:offset()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong:magnitude()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:offset()",
        1
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:size()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getInterface()",
        1
    ],
    [
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:performFailover(java.lang.Object)",
        1
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryForever:shouldRetry(java.lang.Exception,int,int,boolean)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DefaultCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DefaultCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DefaultRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DefaultRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)",
        1
    ],
    [
        "org.apache.hadoop.ipc.DefaultRpcScheduler:stop()",
        1
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getRevision()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getConnectionId()",
        1
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getConnectionId()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getConf()",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:getConnectionId()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getCount()",
        1
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:shouldLog()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:postStart()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.impl.MsInfo:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:description()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:name()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:init(org.apache.commons.configuration2.SubsetConfiguration)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink:flush()",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",
        1
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:getCount()",
        1
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.NullGroupsMapping:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.NullGroupsMapping:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:cacheGroupsAdd(java.util.List)",
        1
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:cacheGroupsRefresh()",
        1
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getConf()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getKeyManagers()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getTrustManagers()",
        1
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:setConf(org.apache.hadoop.conf.Configuration)",
        1
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:getExitCode()",
        1
    ],
    [
        "org.apache.hadoop.service.launcher.AbstractLaunchableService:execute()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$IntegerItem:getType()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$ListItem:getType()",
        1
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$StringItem:getType()",
        1
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:close()",
        1
    ],
    [
        "org.apache.hadoop.fs.FileRange:createFileRange(long,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal()",
        2
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed()",
        2
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:processDeleteOnExit()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime()",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[])",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions()",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup()",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:msync()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:msync()",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getDefaultReplication()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp()",
        2
    ],
    [
        "org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)",
        2
    ],
    [
        "org.apache.hadoop.util.Lists:partition(java.util.List,int)",
        2
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",
        2
    ],
    [
        "org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder()",
        2
    ],
    [
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:toExtendedShort()",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:toOctal()",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:handleStartProperty()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isDir()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileStatus:isFile()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileStatus:getSymlink()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen()",
        2
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        2
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:blockSize(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:repFac(short)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:createParent()",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent()",
        2
    ],
    [
        "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.AvroFSInput:seek(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:seek(long)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.AvroFSInput:tell()",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:readRecordLength()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getPosition()",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getUri()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read()",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long)",
        2
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)",
        2
    ],
    [
        "org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",
        2
    ],
    [
        "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)",
        2
    ],
    [
        "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:clearStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close()",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable)",
        2
    ],
    [
        "org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[])",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop()",
        2
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close()",
        2
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:close()",
        2
    ],
    [
        "org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush()",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getConnectorAddress(int)",
        2
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)",
        2
    ],
    [
        "org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.StorageType:getMovableTypes()",
        2
    ],
    [
        "org.apache.hadoop.fs.StorageType:getTypesSupportingQuota()",
        2
    ],
    [
        "org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:unbuffer()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileStatus:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileStatus:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.DUHelper:check(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:checkMutable()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:bind()",
        2
    ],
    [
        "org.apache.hadoop.util.CloseableReferenceCount:unreference()",
        2
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run()",
        2
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled()",
        2
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getCounter(long)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getPadding(long)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor()",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer:getInstance()",
        2
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded()",
        2
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",
        2
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",
        2
    ],
    [
        "org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:set(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileUtil:permissionsFromMode(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:execute()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:toString()",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.DU:refresh()",
        2
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:getChecksumSize()",
        2
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getChecksumSize(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[])",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[])",
        2
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage$Builder:build()",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder)",
        2
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName()",
        2
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt()",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled()",
        2
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt()",
        2
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.fs.GlobFilter:hasPattern()",
        2
    ],
    [
        "org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet)",
        2
    ],
    [
        "org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)",
        2
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[])",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)",
        2
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks()",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:getUri()",
        2
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:getUri()",
        2
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:skip(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads()",
        2
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads()",
        2
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads()",
        2
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads()",
        2
    ],
    [
        "org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient)",
        2
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem$1:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext()",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:touch()",
        2
    ],
    [
        "org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long)",
        2
    ],
    [
        "org.apache.hadoop.util.Timer:now()",
        2
    ],
    [
        "org.apache.hadoop.util.AsyncDiskService:awaitTermination(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable()",
        2
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus()",
        2
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileRangeImpl:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:pathCapabilities()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer()",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",
        2
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setDone()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt()",
        2
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Print:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:<init>(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:getConfigSuffix()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:negate()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Result:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name$Iname:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Print$Print0:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:createOptions()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:prepare()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:finish()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:isAction()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)",
        2
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate()",
        2
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:confirmForceManual()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:isSorted()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Command:isDeprecated()",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getName()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:toString()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:<init>(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.fs.RawPathHandle:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text)",
        2
    ],
    [
        "org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",
        2
    ],
    [
        "org.apache.hadoop.security.token.Token:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        2
    ],
    [
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.PathIOException:getMessage()",
        2
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext()",
        2
    ],
    [
        "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next()",
        2
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",
        2
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:getAllStoragePolicies()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getAllStoragePolicies()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:supportsSymlinks()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getRootDir()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getRootLink()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)",
        2
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:checkState()",
        2
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix()",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.CallReturn:getReturnValue()",
        2
    ],
    [
        "org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel()",
        2
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.DF:getFilesystem()",
        2
    ],
    [
        "org.apache.hadoop.fs.DF:getMount()",
        2
    ],
    [
        "org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh()",
        2
    ],
    [
        "org.apache.hadoop.fs.DF:getPercentUsed()",
        2
    ],
    [
        "org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.Path:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:seek(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:init()",
        2
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>()",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)",
        2
    ],
    [
        "org.apache.hadoop.tools.TableListing$Column:getRow(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:fromShort(short)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:toStringStable()",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntryType:toString()",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)",
        2
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize()",
        2
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:msync()",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:msync()",
        2
    ],
    [
        "org.apache.hadoop.fs.FileContext:printStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getStatistics()",
        2
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:getPos()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:syncFs()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:hflush()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:hsync()",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:quota(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long)",
        2
    ],
    [
        "org.apache.hadoop.fs.ContentSummary$Builder:build()",
        2
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.LoggingStateChangeListener:<init>()",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:toString()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:getService()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:run()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:toString()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException)",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions()",
        2
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses()",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:resetGlobalListeners()",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:notifyListeners()",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:getServiceState()",
        2
    ],
    [
        "org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:serviceStop()",
        2
    ],
    [
        "org.apache.hadoop.service.CompositeService:serviceStart()",
        2
    ],
    [
        "org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)",
        2
    ],
    [
        "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.io.ShortWritable:<init>(short)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long)",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer$Buffer:read()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[])",
        2
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable()",
        2
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:fromBytes(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.VIntWritable:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8)",
        2
    ],
    [
        "org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.UTF8:skip(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)",
        2
    ],
    [
        "org.apache.hadoop.io.SetFile:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayFile:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.LongWritable:<init>(long)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Reader:seek(long)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.ByteWritable:<init>(byte)",
        2
    ],
    [
        "org.apache.hadoop.io.OutputBuffer:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.OutputBuffer:getData()",
        2
    ],
    [
        "org.apache.hadoop.io.OutputBuffer:getLength()",
        2
    ],
    [
        "org.apache.hadoop.io.OutputBuffer:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableComparator:readLong(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.BytesWritable:<init>(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.BytesWritable:get()",
        2
    ],
    [
        "org.apache.hadoop.io.BytesWritable:getSize()",
        2
    ],
    [
        "org.apache.hadoop.io.BytesWritable:setCapacity(int)",
        2
    ],
    [
        "org.apache.hadoop.io.IntWritable:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)",
        2
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.io.ElasticByteBufferPool:size(boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long)",
        2
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)",
        2
    ],
    [
        "org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable)",
        2
    ],
    [
        "org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable)",
        2
    ],
    [
        "org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Reader:key()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator)",
        2
    ],
    [
        "org.apache.hadoop.io.DoubleWritable:<init>(double)",
        2
    ],
    [
        "org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable()",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:isAvailable()",
        2
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.NativeCrc32:isAvailable()",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage()",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable()",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize()",
        2
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion)",
        2
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])",
        2
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)",
        2
    ],
    [
        "org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])",
        2
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)",
        2
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int)",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:getData()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:getPosition()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputBuffer:getLength()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState()",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])",
        2
    ],
    [
        "org.apache.hadoop.io.erasurecode.ECBlock:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CRC:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:finish()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:getCompressedData()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:available()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:resetState()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BlockCompressorStream:compress()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:getCompressorType()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:getDecompressorType()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState()",
        2
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader()",
        2
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)",
        2
    ],
    [
        "org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)",
        2
    ],
    [
        "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:charAt(int)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:set(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:encode(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.token.Token:buildCacheKey()",
        2
    ],
    [
        "org.apache.hadoop.io.Text:set(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:append(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:decode(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.Text:decode(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.Text:validateUTF8(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.VLongWritable:<init>(long)",
        2
    ],
    [
        "org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem)",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer:getData()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer:getPosition()",
        2
    ],
    [
        "org.apache.hadoop.io.DataInputByteBuffer:getLength()",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.Key:<init>(byte[],double)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",
        2
    ],
    [
        "org.apache.hadoop.util.GenericsUtil:toArray(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.io.InputBuffer:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.InputBuffer:reset(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.InputBuffer:getPosition()",
        2
    ],
    [
        "org.apache.hadoop.io.InputBuffer:getLength()",
        2
    ],
    [
        "org.apache.hadoop.io.MD5Hash:read(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.MD5Hash:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream)",
        2
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.IOUtils:fsync(java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.io.MapWritable:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.MapWritable:putAll(java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:<init>(boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.BooleanWritable:toString()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty()",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps()",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:isExpired()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:now()",
        2
    ],
    [
        "org.apache.hadoop.util.Timer:monotonicNow()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:close()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client:getCallId()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:close()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState)",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos()",
        2
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[])",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.io.FloatWritable:<init>(float)",
        2
    ],
    [
        "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:<init>()",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:getData()",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:getLength()",
        2
    ],
    [
        "org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)",
        2
    ],
    [
        "org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:toString()",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:toString()",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)",
        2
    ],
    [
        "org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection)",
        2
    ],
    [
        "org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)",
        2
    ],
    [
        "org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int)",
        2
    ],
    [
        "org.apache.hadoop.net.TableMapping:getConf()",
        2
    ],
    [
        "org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>()",
        2
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)",
        2
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:getConf()",
        2
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:toString()",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int)",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read()",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available()",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:isOpen()",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:close()",
        2
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long)",
        2
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:isLeafParent()",
        2
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString()",
        2
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketInputWrapper:setTimeout(long)",
        2
    ],
    [
        "org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)",
        2
    ],
    [
        "org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)",
        2
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology()",
        2
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)",
        2
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        2
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)",
        2
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)",
        2
    ],
    [
        "org.apache.hadoop.log.LogLevel:printUsage()",
        2
    ],
    [
        "org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream)",
        2
    ],
    [
        "org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])",
        2
    ],
    [
        "org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        2
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:<init>()",
        2
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2:toString()",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",
        2
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsTag:toString()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink:close()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink:close()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:dequeue()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:clear()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:waitForData()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:incr()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:incr()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long)",
        2
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long)",
        2
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:toString()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:add(double)",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:min()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:max()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:reset()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:stddev()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:compress()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:query(double)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache:<init>()",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",
        2
    ],
    [
        "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",
        2
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.security.NetgroupCache:getNetgroupNames()",
        2
    ],
    [
        "org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults()",
        2
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier)",
        2
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal()",
        2
    ],
    [
        "org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)",
        2
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.Groups:refresh()",
        2
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.KDiag:close()",
        2
    ],
    [
        "org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.security.KDiag:usage()",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        2
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:close()",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:getAuthorizationID()",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction)",
        2
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        2
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[])",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        2
    ],
    [
        "org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable)",
        2
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:serviceStart()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId()",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        2
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.security.token.Token:isManaged()",
        2
    ],
    [
        "org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Get:validate()",
        2
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell:getCommandUsage()",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:disposeSasl()",
        2
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:readMoreData()",
        2
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:close()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AuthorizationException:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reset()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getLogin()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isLoginSuccess()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setLastLogin(long)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getUserName()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getRealUser()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getShortUserName()",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path)",
        2
    ],
    [
        "org.apache.hadoop.util.FindClass:getResource(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine()",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket()",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)",
        2
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:toString()",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput)",
        2
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry()",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:getUsersString()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:getGroupsString()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups()",
        2
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])",
        2
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell:promptForCredential()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider()",
        2
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket)",
        2
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)",
        2
    ],
    [
        "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress)",
        2
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec)",
        2
    ],
    [
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>()",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getPos()",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:available()",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion)",
        2
    ],
    [
        "org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite)",
        2
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:finalize()",
        2
    ],
    [
        "org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute()",
        2
    ],
    [
        "org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)",
        2
    ],
    [
        "org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:close()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient()",
        2
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider()",
        2
    ],
    [
        "org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RefreshResponse:successResponse()",
        2
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto)",
        2
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ClientId:toString(byte[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:isClientBackoffEnabled()",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getCallQueueLen()",
        2
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder)",
        2
    ],
    [
        "org.apache.hadoop.ipc.CallerContext:toString()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:sendPing()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ExternalCall:get()",
        2
    ],
    [
        "org.apache.hadoop.util.Timer:monotonicNowNanos()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getUserGroupInformation()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getRemoteUser()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:unwrapRemoteException()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests()",
        2
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:toString()",
        2
    ],
    [
        "org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary()",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount()",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume()",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime()",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getNumDroppedConnections()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:isFull()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getNumOpenConnections()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getConnections()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan()",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer()",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary()",
        2
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException)",
        2
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[])",
        2
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client:close()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client:checkAsyncCall()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getListenerAddress()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:doStop()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:getRemotePort()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:toString()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:waitForWork()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)",
        2
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes()",
        2
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:switchToSimple()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:close()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:checkDataLength(int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString()",
        2
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode()",
        2
    ],
    [
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:capacity()",
        2
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection$PingInputStream:read()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:getHostAddress()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getRemoteIp()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getRemotePort()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getProtocol()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getPriorityLevel()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:addAuxiliaryListener(int)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server:isServerFailOverEnabled()",
        2
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)",
        2
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.tracing.Tracer$Builder:build()",
        2
    ],
    [
        "org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)",
        2
    ],
    [
        "org.apache.hadoop.tracing.NullTraceScope:<init>()",
        2
    ],
    [
        "org.apache.hadoop.tracing.TraceScope:close()",
        2
    ],
    [
        "org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)",
        2
    ],
    [
        "org.apache.hadoop.util.MachineList:includes(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.util.ConfTest:listFiles(java.io.File)",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getNumProcessors()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getNumCores()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getCpuFrequency()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readProcStatFile()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten()",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile()",
        2
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:realloc(int)",
        2
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext()",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:formatPercent(double,int)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:limitDecimalTo2(double)",
        2
    ],
    [
        "org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",
        2
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes)",
        2
    ],
    [
        "org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException)",
        2
    ],
    [
        "org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException)",
        2
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable)",
        2
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable)",
        2
    ],
    [
        "org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion)",
        2
    ],
    [
        "org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",
        2
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",
        2
    ],
    [
        "org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)",
        2
    ],
    [
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell$1:run()",
        2
    ],
    [
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:size()",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary()",
        2
    ],
    [
        "org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator)",
        2
    ],
    [
        "org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.util.Lists:newArrayListWithCapacity(int)",
        2
    ],
    [
        "org.apache.hadoop.util.Lists:computeArrayListCapacity(int)",
        2
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)",
        2
    ],
    [
        "org.apache.hadoop.util.OperationDuration:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.OperationDuration:finished()",
        2
    ],
    [
        "org.apache.hadoop.util.OperationDuration:asDuration()",
        2
    ],
    [
        "org.apache.hadoop.util.OperationDuration:getDurationString()",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:iterator()",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$Values:iterator()",
        2
    ],
    [
        "org.apache.hadoop.util.CrcUtil:getMonomial(long,int)",
        2
    ],
    [
        "org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)",
        2
    ],
    [
        "org.apache.hadoop.util.CrcUtil:intToBytes(int)",
        2
    ],
    [
        "org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[])",
        2
    ],
    [
        "org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[])",
        2
    ],
    [
        "org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)",
        2
    ],
    [
        "org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:get()",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone()",
        2
    ],
    [
        "org.apache.hadoop.util.StopWatch:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)",
        2
    ],
    [
        "org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:toString()",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:prune()",
        2
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot()",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])",
        2
    ],
    [
        "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getVersion()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getRevision()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getBranch()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getDate()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getUser()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getUrl()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:_getBuildVersion()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getSrcChecksum()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getProtocVersion()",
        2
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getCompilePlatform()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:apply()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions()",
        2
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException)",
        2
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException)",
        2
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:<init>(boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.util.PureJavaCrc32C:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:toArray()",
        2
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection)",
        2
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:clear()",
        2
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection)",
        2
    ],
    [
        "org.apache.hadoop.util.CleanerUtil:unmapHackImpl()",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:getExcludedHosts()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getGetPermissionCommand()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:checkHadoopHome()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getHadoopHomeDir()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:getWinUtilsFile()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell:destroyAllShellProcesses()",
        2
    ],
    [
        "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run()",
        2
    ],
    [
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread)",
        2
    ],
    [
        "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService)",
        2
    ],
    [
        "org.apache.hadoop.util.Progress:addNewPhase()",
        2
    ],
    [
        "org.apache.hadoop.util.Progress:addPhase(float)",
        2
    ],
    [
        "org.apache.hadoop.util.Progress:getInternal()",
        2
    ],
    [
        "org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder)",
        2
    ],
    [
        "org.apache.hadoop.util.Progress:complete()",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[])",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic()",
        2
    ],
    [
        "org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)",
        2
    ],
    [
        "org.apache.hadoop.util.DataChecksum:getHeader()",
        2
    ],
    [
        "org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int)",
        2
    ],
    [
        "org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)",
        2
    ],
    [
        "org.apache.hadoop.util.RateLimitingFactory:create(int)",
        2
    ],
    [
        "org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger)",
        2
    ],
    [
        "org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:<init>(int)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$Values:clear()",
        2
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString()",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:getHosts()",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)",
        2
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.util.hash.Hash:getInstance(int)",
        2
    ],
    [
        "org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:not()",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key)",
        2
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.AutoCloseableLock:close()",
        2
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$StringItem:isNull()",
        2
    ],
    [
        "org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)",
        2
    ],
    [
        "org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])",
        2
    ],
    [
        "org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)",
        2
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval()",
        2
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor:getLatestGcData()",
        2
    ],
    [
        "org.apache.hadoop.util.PureJavaCrc32:<init>()",
        2
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:executeShutdown()",
        2
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:put(java.lang.Object)",
        2
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:pop()",
        2
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:adjustTop()",
        2
    ],
    [
        "org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)",
        2
    ],
    [
        "org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator)",
        2
    ],
    [
        "org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int)",
        2
    ],
    [
        "org.apache.hadoop.util.SequentialNumber:skipTo(long)",
        2
    ],
    [
        "org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.tools.CommandShell:run(java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",
        2
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:build()",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$7:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$3:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:reloadExistingConfigurations()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:handleInclude()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:toString()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List)",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$6:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$2:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus()",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$4:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])",
        2
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)",
        2
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:handleEndProperty()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:iterator()",
        2
    ],
    [
        "org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageSize:parse(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$5:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.conf.StorageUnit$1:getDefault(double)",
        2
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:reset()",
        2
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        2
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto)",
        2
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:createReqInfo()",
        2
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:createReqInfo()",
        2
    ],
    [
        "org.apache.hadoop.ha.FailoverController:createReqInfo()",
        2
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)",
        2
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:startRPC()",
        2
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)",
        2
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)",
        2
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State)",
        2
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String)",
        2
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover()",
        2
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget)",
        2
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getFencingParameters()",
        2
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus()",
        2
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)",
        2
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)",
        3
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)",
        3
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)",
        3
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:available()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:close()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run()",
        3
    ],
    [
        "org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getStorageStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive()",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:create()",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:append()",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileStatus:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink()",
        3
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)",
        3
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",
        3
    ],
    [
        "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList)",
        3
    ],
    [
        "org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:sync(long)",
        3
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:reset()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[])",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        3
    ],
    [
        "org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)",
        3
    ],
    [
        "org.apache.hadoop.fs.GlobExpander:expand(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore()",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",
        3
    ],
    [
        "org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext)",
        3
    ],
    [
        "org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS()",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt)",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor)",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil)",
        3
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)",
        3
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])",
        3
    ],
    [
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:close()",
        3
    ],
    [
        "org.apache.hadoop.util.VersionInfo:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks()",
        3
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:finalize()",
        3
    ],
    [
        "org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State)",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",
        3
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor$Builder:build()",
        3
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet()",
        3
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable()",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:clear()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:unreference(boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.util.LineReader:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics()",
        3
    ],
    [
        "org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:encrypt()",
        3
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator:hasNext()",
        3
    ],
    [
        "org.apache.hadoop.fs.BatchedRemoteIterator:next()",
        3
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>()",
        3
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>()",
        3
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)",
        3
    ],
    [
        "org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp)",
        3
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown()",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:read()",
        3
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:read()",
        3
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read()",
        3
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getReadOps()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getData()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:reset()",
        3
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)",
        3
    ],
    [
        "org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet)",
        3
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        3
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class)",
        3
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close()",
        3
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close()",
        3
    ],
    [
        "org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort()",
        3
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getCanonicalUri()",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI)",
        3
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem)",
        3
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getStatus()",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.CombinedFileRange:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:close()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",
        3
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:data()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:relative()",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext()",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext)",
        3
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:<init>()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:getOptions()",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getDescription()",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Command:getUsage()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo)",
        3
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:<init>()",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream)",
        3
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:moved(boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:changed(boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.DF:<init>(java.io.File,long)",
        3
    ],
    [
        "org.apache.hadoop.util.Shell:<init>()",
        3
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)",
        3
    ],
    [
        "org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem()",
        3
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall)",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.DF:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.Path:isUriPathAbsolute()",
        3
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build()",
        3
    ],
    [
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus)",
        3
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.Stat:getExecString()",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)",
        3
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString()",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:skip(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:reset()",
        3
    ],
    [
        "org.apache.hadoop.fs.audit.CommonAuditContext:createInstance()",
        3
    ],
    [
        "org.apache.hadoop.tools.TableListing:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>(short)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:toString()",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:getStrings(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",
        3
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:available()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:position()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:reset()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush()",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI)",
        3
    ],
    [
        "org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex()",
        3
    ],
    [
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos()",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:sync()",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:getLength()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)",
        3
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE)",
        3
    ],
    [
        "org.apache.hadoop.service.AbstractService:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:toString()",
        3
    ],
    [
        "org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception)",
        3
    ],
    [
        "org.apache.hadoop.service.AbstractService:recordLifecycleEvent()",
        3
    ],
    [
        "org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.service.CompositeService:stop(int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:blockSize(long)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:replication(short)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int)",
        3
    ],
    [
        "org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class)",
        3
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class)",
        3
    ],
    [
        "org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8)",
        3
    ],
    [
        "org.apache.hadoop.io.UTF8:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash)",
        3
    ],
    [
        "org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable)",
        3
    ],
    [
        "org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.UTF8:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.io.BinaryComparable:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.security.token.Token:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)",
        3
    ],
    [
        "org.apache.hadoop.io.BytesWritable:setSize(int)",
        3
    ],
    [
        "org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:canRead(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:canWrite(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileUtil:canExecute(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded()",
        3
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool:getInstance()",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock()",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason()",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit()",
        3
    ],
    [
        "org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)",
        3
    ],
    [
        "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run()",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool()",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers()",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor)",
        3
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet)",
        3
    ],
    [
        "org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)",
        3
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:next(int)",
        3
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream()",
        3
    ],
    [
        "org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream)",
        3
    ],
    [
        "org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[])",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:write(int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.BlockCompressorStream:finish()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:getCompressorType()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset()",
        3
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState()",
        3
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:write(java.io.DataOutput,int)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:find(java.lang.String,int)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:set(byte[])",
        3
    ],
    [
        "org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text)",
        3
    ],
    [
        "org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)",
        3
    ],
    [
        "org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.Text:toString()",
        3
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.MapWritable:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.MapWritable:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.Key:<init>(byte[])",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType)",
        3
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",
        3
    ],
    [
        "org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer)",
        3
    ],
    [
        "org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.io.MD5Hash:digest(byte[])",
        3
    ],
    [
        "org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8)",
        3
    ],
    [
        "org.apache.hadoop.io.MD5Hash:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon)",
        3
    ],
    [
        "org.apache.hadoop.security.Groups$TimerToTickerAdapter:read()",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedWriteLock:unlock()",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming()",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:startLockTiming()",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedReadLock:unlock()",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedReadLock:startLockTiming()",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.io.MultipleIOException$Builder:build()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone()",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        3
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[])",
        3
    ],
    [
        "org.apache.hadoop.io.UTF8:set(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[])",
        3
    ],
    [
        "org.apache.hadoop.io.UTF8:getBytes(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer)",
        3
    ],
    [
        "org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable)",
        3
    ],
    [
        "org.apache.hadoop.security.token.TokenIdentifier:getBytes()",
        3
    ],
    [
        "org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable)",
        3
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData()",
        3
    ],
    [
        "org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node)",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node)",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)",
        3
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node)",
        3
    ],
    [
        "org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)",
        3
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)",
        3
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:write(int)",
        3
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)",
        3
    ],
    [
        "org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)",
        3
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>()",
        3
    ],
    [
        "org.apache.hadoop.net.TableMapping:<init>()",
        3
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping)",
        3
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)",
        3
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:kick()",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket)",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:socketpair()",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen()",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close()",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close()",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:close()",
        3
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close()",
        3
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel)",
        3
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo)",
        3
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)",
        3
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node)",
        3
    ],
    [
        "org.apache.hadoop.net.DNS:getIPs(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:read()",
        3
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)",
        3
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:<init>(long)",
        3
    ],
    [
        "org.apache.hadoop.log.LogThrottlingHelper:record(double[])",
        3
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest)",
        3
    ],
    [
        "org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap()",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL()",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName()",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler)",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)",
        3
    ],
    [
        "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink:flush()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)",
        3
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed()",
        3
    ],
    [
        "org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown()",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName)",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:remove()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:add(long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:toString()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev()",
        3
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long)",
        3
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot()",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int)",
        3
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getTokens()",
        3
    ],
    [
        "org.apache.hadoop.security.User:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.Groups:getGroups(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:println()",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:printSysprop(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:printEnv(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:dump(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap()",
        3
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        3
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:write(int)",
        3
    ],
    [
        "org.apache.hadoop.security.SaslOutputStream:write(byte[])",
        3
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers()",
        3
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers()",
        3
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",
        3
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",
        3
    ],
    [
        "org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:getKey()",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>()",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy()",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey()",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:<init>()",
        3
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew()",
        3
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel()",
        3
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read()",
        3
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[])",
        3
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient)",
        3
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning()",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning()",
        3
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError()",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError()",
        3
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:read()",
        3
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getKeytab()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isHadoopLogin()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$RealUser:getName()",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:toString()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation)",
        3
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection)",
        3
    ],
    [
        "org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable)",
        3
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)",
        3
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)",
        3
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)",
        3
    ],
    [
        "org.apache.hadoop.util.FindClass:loadResource(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:<init>()",
        3
    ],
    [
        "org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.ObjectWritable$NullInstance:<init>()",
        3
    ],
    [
        "org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configured:<init>()",
        3
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:getAclString()",
        3
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])",
        3
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute()",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getTGT()",
        3
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",
        3
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",
        3
    ],
    [
        "org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor()",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor()",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor()",
        3
    ],
    [
        "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor()",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])",
        3
    ],
    [
        "org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])",
        3
    ],
    [
        "org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close()",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto)",
        3
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue()",
        3
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable)",
        3
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable)",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength()",
        3
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:initialize()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine()",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)",
        3
    ],
    [
        "org.apache.hadoop.util.StopWatch:start()",
        3
    ],
    [
        "org.apache.hadoop.util.StopWatch:stop()",
        3
    ],
    [
        "org.apache.hadoop.util.StopWatch:now()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:sendResponse()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[])",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:readResponse()",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception)",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception)",
        3
    ],
    [
        "org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.fs.PartialListing:get()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",
        3
    ],
    [
        "org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections()",
        3
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)",
        3
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:reset()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:toByteArray()",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache()",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",
        3
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)",
        3
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl()",
        3
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings()",
        3
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration()",
        3
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue()",
        3
    ],
    [
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover()",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth()",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus()",
        3
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod()",
        3
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close()",
        3
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode()",
        3
    ],
    [
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse()",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2()",
        3
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer)",
        3
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer)",
        3
    ],
    [
        "org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:cleanupCalls()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server:getRemoteAddress()",
        3
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser()",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall)",
        3
    ],
    [
        "org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)",
        3
    ],
    [
        "org.apache.hadoop.util.MachineList:<init>(java.util.Collection)",
        3
    ],
    [
        "org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:<init>()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead()",
        3
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten()",
        3
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:<init>(int)",
        3
    ],
    [
        "org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext()",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:next()",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int)",
        3
    ],
    [
        "org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory()",
        3
    ],
    [
        "org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory()",
        3
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:formatSize(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage:formatSize(long)",
        3
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:humanReadableInt(long)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:byteDesc(long)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)",
        3
    ],
    [
        "org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:byteToHexString(byte[])",
        3
    ],
    [
        "org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)",
        3
    ],
    [
        "org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister()",
        3
    ],
    [
        "org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:<init>()",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:<init>(int)",
        3
    ],
    [
        "org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable)",
        3
    ],
    [
        "org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable)",
        3
    ],
    [
        "org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:addChunk(int)",
        3
    ],
    [
        "org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int)",
        3
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.OperationDuration:toString()",
        3
    ],
    [
        "org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)",
        3
    ],
    [
        "org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)",
        3
    ],
    [
        "org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes()",
        3
    ],
    [
        "org.apache.hadoop.util.CrcComposer:digest()",
        3
    ],
    [
        "org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)",
        3
    ],
    [
        "org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)",
        3
    ],
    [
        "org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:split(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:camelize(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)",
        3
    ],
    [
        "org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.util.VersionInfo:getBuildVersion()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.functional.LazyAtomicReference:get()",
        3
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable)",
        3
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException)",
        3
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future)",
        3
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException)",
        3
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        3
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)",
        3
    ],
    [
        "org.apache.hadoop.util.FindClass:getClass(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)",
        3
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration:<init>()",
        3
    ],
    [
        "org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)",
        3
    ],
    [
        "org.apache.hadoop.util.DataChecksum:newCrc32C()",
        3
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection)",
        3
    ],
    [
        "org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.Shell:getHadoopHome()",
        3
    ],
    [
        "org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File)",
        3
    ],
    [
        "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell)",
        3
    ],
    [
        "org.apache.hadoop.util.Progress:addPhase()",
        3
    ],
    [
        "org.apache.hadoop.util.Progress:addPhases(int)",
        3
    ],
    [
        "org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)",
        3
    ],
    [
        "org.apache.hadoop.util.Progress:get()",
        3
    ],
    [
        "org.apache.hadoop.util.Progress:getProgress()",
        3
    ],
    [
        "org.apache.hadoop.util.Progress:toString()",
        3
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable()",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available()",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available()",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available()",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream)",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable()",
        3
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable()",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked()",
        3
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic()",
        3
    ],
    [
        "org.apache.hadoop.fs.FileSystem:loadFileSystems()",
        3
    ],
    [
        "org.apache.hadoop.util.VersionInfo:main(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])",
        3
    ],
    [
        "org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString()",
        3
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>()",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.Key:equals(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[])",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[])",
        3
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio()",
        3
    ],
    [
        "org.apache.hadoop.util.FindClass:dumpResource(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.util.FindClass:usage(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.util.GcTimeMonitor:run()",
        3
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor)",
        3
    ],
    [
        "org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object)",
        3
    ],
    [
        "org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable)",
        3
    ],
    [
        "org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable)",
        3
    ],
    [
        "org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[])",
        3
    ],
    [
        "org.apache.hadoop.util.ProgramDriver:run(java.lang.String[])",
        3
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:handleStartElement()",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[])",
        3
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:handleEndElement()",
        3
    ],
    [
        "org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)",
        3
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)",
        3
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)",
        3
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)",
        3
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)",
        3
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String)",
        3
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction)",
        3
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)",
        4
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:copy()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)",
        4
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:close()",
        4
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:close()",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll()",
        4
    ],
    [
        "org.apache.hadoop.fs.FsShell:close()",
        4
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:close()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus)",
        4
    ],
    [
        "org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])",
        4
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString()",
        4
    ],
    [
        "org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults()",
        4
    ],
    [
        "org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem$DirListingIterator:next()",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData)",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[])",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",
        4
    ],
    [
        "org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext)",
        4
    ],
    [
        "org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone()",
        4
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn()",
        4
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",
        4
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke()",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode()",
        4
    ],
    [
        "org.apache.hadoop.fs.DUHelper:main(java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh()",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString()",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString()",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString()",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)",
        4
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable)",
        4
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable)",
        4
    ],
    [
        "org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator()",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling()",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:accept()",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:getAttribute(int)",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:shutdown()",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long)",
        4
    ],
    [
        "org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long)",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:flush()",
        4
    ],
    [
        "org.apache.hadoop.fs.GlobPattern:compile(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.find.Name:prepare()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:close()",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        4
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize()",
        4
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystemStorageStatistics:reset()",
        4
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:clearStatistics()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",
        4
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        4
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getCanonicalUri()",
        4
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri()",
        4
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus()",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters)",
        4
    ],
    [
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext()",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Command:displayError(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",
        4
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:exact()",
        4
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:content()",
        4
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:path()",
        4
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:reference()",
        4
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean)",
        4
    ],
    [
        "org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies()",
        4
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)",
        4
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors()",
        4
    ],
    [
        "org.apache.hadoop.fs.DF:main(java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.Path:checkNotSchemeWithRelative()",
        4
    ],
    [
        "org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull()",
        4
    ],
    [
        "org.apache.hadoop.fs.Path:isAbsolute()",
        4
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases()",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long)",
        4
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission)",
        4
    ],
    [
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto)",
        4
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry)",
        4
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getDefault()",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getDirDefault()",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getFileDefault()",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault()",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList)",
        4
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:read()",
        4
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long)",
        4
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload()",
        4
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload()",
        4
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload()",
        4
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DataBlock:close()",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize()",
        4
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength()",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[])",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.service.CompositeService:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:<init>()",
        4
    ],
    [
        "org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.service.CompositeService:serviceStop()",
        4
    ],
    [
        "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run()",
        4
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable)",
        4
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class)",
        4
    ],
    [
        "org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.io.BytesWritable:hashCode()",
        4
    ],
    [
        "org.apache.hadoop.io.Text:hashCode()",
        4
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode()",
        4
    ],
    [
        "org.apache.hadoop.io.BytesWritable:set(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable)",
        4
    ],
    [
        "org.apache.hadoop.fs.FileUtil:list(java.io.File)",
        4
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File)",
        4
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)",
        4
    ],
    [
        "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit()",
        4
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:freeBuffers()",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:unbuffer()",
        4
    ],
    [
        "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int)",
        4
    ],
    [
        "org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[])",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.CompressionOutputStream:close()",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:close()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.CompressionInputStream:close()",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:close()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createCompressor()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createDecompressor()",
        4
    ],
    [
        "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput)",
        4
    ],
    [
        "org.apache.hadoop.security.token.Token:write(java.io.DataOutput)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)",
        4
    ],
    [
        "org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:skip(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:readString(java.io.DataInput,int)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])",
        4
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI)",
        4
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:find(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:<init>(byte[])",
        4
    ],
    [
        "org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text)",
        4
    ],
    [
        "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.Text:getTextLength()",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:toString()",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)",
        4
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider:getAliases()",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName()",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName()",
        4
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable)",
        4
    ],
    [
        "org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable)",
        4
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType)",
        4
    ],
    [
        "org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>()",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run()",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:tryLock()",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo()",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush()",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close()",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable)",
        4
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close()",
        4
    ],
    [
        "org.apache.hadoop.io.UTF8:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes()",
        4
    ],
    [
        "org.apache.hadoop.io.MD5Hash:digest(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync()",
        4
    ],
    [
        "org.apache.hadoop.security.token.TokenIdentifier:getTrackingId()",
        4
    ],
    [
        "org.apache.hadoop.security.token.Token:encodeToUrlString()",
        4
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode()",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node)",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node)",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node)",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",
        4
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)",
        4
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)",
        4
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:<init>()",
        4
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>()",
        4
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode)",
        4
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket)",
        4
    ],
    [
        "org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)",
        4
    ],
    [
        "org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.net.DNS:getHosts(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)",
        4
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream)",
        4
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)",
        4
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)",
        4
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:toString()",
        4
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)",
        4
    ],
    [
        "org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue()",
        4
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords()",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)",
        4
    ],
    [
        "org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server:stop()",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans()",
        4
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)",
        4
    ],
    [
        "org.apache.hadoop.http.HttpServer2:stop()",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>()",
        4
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:toString()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)",
        4
    ],
    [
        "org.apache.hadoop.metrics2.util.SampleQuantiles:toString()",
        4
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder)",
        4
    ],
    [
        "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.security.KDiag:endln()",
        4
    ],
    [
        "org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        4
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault()",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads()",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey()",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>()",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator)",
        4
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",
        4
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:read(byte[])",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isFromKeytab()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isFromTicket()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:shouldRelogin()",
        4
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$Key:toString()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$RealUser:toString()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$ConnectionManager:closeAll()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection)",
        4
    ],
    [
        "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)",
        4
    ],
    [
        "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:<init>()",
        4
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:<init>()",
        4
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem)",
        4
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:<init>()",
        4
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem)",
        4
    ],
    [
        "org.apache.hadoop.fs.FsShell:<init>()",
        4
    ],
    [
        "org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.Command:<init>()",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        4
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",
        4
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.security.KDiag:<init>()",
        4
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:<init>()",
        4
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping()",
        4
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput)",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper()",
        4
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)",
        4
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",
        4
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>()",
        4
    ],
    [
        "org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute()",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute()",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)",
        4
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)",
        4
    ],
    [
        "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)",
        4
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)",
        4
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized()",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:get(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.util.StopWatch:close()",
        4
    ],
    [
        "org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit)",
        4
    ],
    [
        "org.apache.hadoop.util.StopWatch:toString()",
        4
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)",
        4
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:doHealthChecks()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$Call:<init>()",
        4
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey)",
        4
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable)",
        4
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:run()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:<init>(int)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run()",
        4
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts()",
        4
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)",
        4
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",
        4
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke()",
        4
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message)",
        4
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",
        4
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:removeNextElement()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:close()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:doRunLoop()",
        4
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall)",
        4
    ],
    [
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        4
    ],
    [
        "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",
        4
    ],
    [
        "org.apache.hadoop.util.MachineList:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.SysInfo:newInstance()",
        4
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize()",
        4
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize()",
        4
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream)",
        4
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$SetIterator:remove()",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:evict()",
        4
    ],
    [
        "org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)",
        4
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean)",
        4
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:fill()",
        4
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int)",
        4
    ],
    [
        "org.apache.hadoop.io.UTF8:toString()",
        4
    ],
    [
        "org.apache.hadoop.io.UTF8:toStringChecked()",
        4
    ],
    [
        "org.apache.hadoop.io.UTF8:fromBytes(byte[])",
        4
    ],
    [
        "org.apache.hadoop.io.UTF8:readString(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto)",
        4
    ],
    [
        "org.apache.hadoop.ipc.Client:toString()",
        4
    ],
    [
        "org.apache.hadoop.util.StringUtils:byteToHexString(byte)",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:toString()",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage()",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.ExitUtil:terminate(int)",
        4
    ],
    [
        "org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData)",
        4
    ],
    [
        "org.apache.hadoop.util.ExitUtil:halt(int)",
        4
    ],
    [
        "org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)",
        4
    ],
    [
        "org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)",
        4
    ],
    [
        "org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture)",
        4
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture)",
        4
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString()",
        4
    ],
    [
        "org.apache.hadoop.util.DurationInfo:toString()",
        4
    ],
    [
        "org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)",
        4
    ],
    [
        "org.apache.hadoop.util.CrcComposer:update(int,long)",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:lock()",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:lockInterruptibly()",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)",
        4
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:unlock()",
        4
    ],
    [
        "org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)",
        4
    ],
    [
        "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext()",
        4
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext()",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future)",
        4
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",
        4
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)",
        4
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)",
        4
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getKeyClass()",
        4
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getValueClass()",
        4
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.util.FindClass:loadClass(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle)",
        4
    ],
    [
        "org.apache.hadoop.fs.FsUrlStreamHandler:<init>()",
        4
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration()",
        4
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf()",
        4
    ],
    [
        "org.apache.hadoop.util.FindClass:<init>()",
        4
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:<init>()",
        4
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration)",
        4
    ],
    [
        "org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)",
        4
    ],
    [
        "org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.Shell:runCommand()",
        4
    ],
    [
        "org.apache.hadoop.util.Progress:addPhase(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[])",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",
        4
    ],
    [
        "org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled()",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString()",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable()",
        4
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable()",
        4
    ],
    [
        "org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)",
        4
    ],
    [
        "org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object)",
        4
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)",
        4
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[])",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int)",
        4
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[])",
        4
    ],
    [
        "org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)",
        4
    ],
    [
        "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)",
        4
    ],
    [
        "org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)",
        4
    ],
    [
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.conf.Configuration$Parser:parseNext()",
        4
    ],
    [
        "org.apache.hadoop.http.HttpServer2:openListeners()",
        4
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String)",
        4
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode()",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive()",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)",
        4
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)",
        4
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString()",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber()",
        5
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run()",
        5
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:closeAll()",
        5
    ],
    [
        "org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation)",
        5
    ],
    [
        "org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])",
        5
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)",
        5
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])",
        5
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",
        5
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",
        5
    ],
    [
        "org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults()",
        5
    ],
    [
        "org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults()",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults()",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults()",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)",
        5
    ],
    [
        "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls()",
        5
    ],
    [
        "org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:copy()",
        5
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString()",
        5
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.fs.FSInputStream:toString()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",
        5
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)",
        5
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:seek(long)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:skip(long)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:write(int)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:close()",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:hflush()",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:hsync()",
        5
    ],
    [
        "org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileContext:getAllStatistics()",
        5
    ],
    [
        "org.apache.hadoop.fs.FileContext:clearStatistics()",
        5
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)",
        5
    ],
    [
        "org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        5
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable()",
        5
    ],
    [
        "org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext()",
        5
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext()",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",
        5
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)",
        5
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:init()",
        5
    ],
    [
        "org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",
        5
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        5
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize()",
        5
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:toString()",
        5
    ],
    [
        "org.apache.hadoop.fs.Path:checkNotRelative()",
        5
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",
        5
    ],
    [
        "org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",
        5
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO()",
        5
    ],
    [
        "org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)",
        5
    ],
    [
        "org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission)",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission()",
        5
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        5
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission()",
        5
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:createImmutable(short)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData)",
        5
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize()",
        5
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long)",
        5
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",
        5
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.service.AbstractService:start()",
        5
    ],
    [
        "org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE)",
        5
    ],
    [
        "org.apache.hadoop.security.KDiag:printDefaultRealm()",
        5
    ],
    [
        "org.apache.hadoop.io.BytesWritable:equals(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.Text:equals(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.security.token.Token$PrivateToken:hashCode()",
        5
    ],
    [
        "org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable)",
        5
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File)",
        5
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",
        5
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:close()",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])",
        5
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:read()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:skip(long)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.CompressorStream:close()",
        5
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:close()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DecompressorStream:close()",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close()",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close()",
        5
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:close()",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createCompressor()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor()",
        5
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createDecompressor()",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader()",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.security.Credentials:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.security.token.Token:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:readBlock()",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue()",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[])",
        5
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.io.Text:readString(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[])",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)",
        5
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)",
        5
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto)",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:getKeys()",
        5
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput)",
        5
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text)",
        5
    ],
    [
        "org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token)",
        5
    ],
    [
        "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)",
        5
    ],
    [
        "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",
        5
    ],
    [
        "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int)",
        5
    ],
    [
        "org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",
        5
    ],
    [
        "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)",
        5
    ],
    [
        "org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",
        5
    ],
    [
        "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)",
        5
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int)",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush()",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[])",
        5
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)",
        5
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close()",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",
        5
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)",
        5
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",
        5
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",
        5
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)",
        5
    ],
    [
        "org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket)",
        5
    ],
    [
        "org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:<init>()",
        5
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node)",
        5
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node)",
        5
    ],
    [
        "org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node)",
        5
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)",
        5
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)",
        5
    ],
    [
        "org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int)",
        5
    ],
    [
        "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:help(java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)",
        5
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reattachMetrics()",
        5
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans()",
        5
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop()",
        5
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:stop()",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create()",
        5
    ],
    [
        "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)",
        5
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)",
        5
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        5
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",
        5
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",
        5
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.security.KDiag:validateKrb5File()",
        5
    ],
    [
        "org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])",
        5
    ],
    [
        "org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])",
        5
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init()",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads()",
        5
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run()",
        5
    ],
    [
        "org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer)",
        5
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab()",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem)",
        5
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem)",
        5
    ],
    [
        "org.apache.hadoop.fs.FsShell:newShellInstance()",
        5
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:<init>()",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        5
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class)",
        5
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMaps()",
        5
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)",
        5
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)",
        5
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper()",
        5
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)",
        5
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean)",
        5
    ],
    [
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",
        5
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",
        5
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor()",
        5
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor()",
        5
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)",
        5
    ],
    [
        "org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call)",
        5
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor$Monitor:run()",
        5
    ],
    [
        "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)",
        5
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",
        5
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:run()",
        5
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ResponseBuffer:<init>()",
        5
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay()",
        5
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",
        5
    ],
    [
        "org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)",
        5
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",
        5
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:take()",
        5
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)",
        5
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:poll()",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Responder:run()",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int)",
        5
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse()",
        5
    ],
    [
        "org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)",
        5
    ],
    [
        "org.apache.hadoop.util.FileBasedIPList:reload()",
        5
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize()",
        5
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)",
        5
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",
        5
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)",
        5
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:evictExpiredEntries()",
        5
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:evictEntries()",
        5
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)",
        5
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)",
        5
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)",
        5
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:read()",
        5
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)",
        5
    ],
    [
        "org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List)",
        5
    ],
    [
        "org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture)",
        5
    ],
    [
        "org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)",
        5
    ],
    [
        "org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)",
        5
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext()",
        5
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        5
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:getKeyClass()",
        5
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:getValueClass()",
        5
    ],
    [
        "org.apache.hadoop.util.FindClass:run(java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)",
        5
    ],
    [
        "org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream)",
        5
    ],
    [
        "org.apache.hadoop.util.Shell:run()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot()",
        5
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object)",
        5
    ],
    [
        "org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)",
        5
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)",
        5
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)",
        5
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)",
        5
    ],
    [
        "org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput)",
        5
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable)",
        5
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)",
        5
    ],
    [
        "org.apache.hadoop.fs.FsShell:createOptionTableListing()",
        5
    ],
    [
        "org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)",
        5
    ],
    [
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])",
        5
    ],
    [
        "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)",
        5
    ],
    [
        "org.apache.hadoop.http.HttpServer2:start()",
        5
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat)",
        5
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode()",
        5
    ],
    [
        "org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches()",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int)",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock()",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.FilePosition:toString()",
        6
    ],
    [
        "org.apache.hadoop.fs.FileSystem:closeAll()",
        6
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])",
        6
    ],
    [
        "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)",
        6
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.MeanStatistic:clone()",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>()",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics)",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator)",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",
        6
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build()",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:read()",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer)",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",
        6
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        6
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int)",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable()",
        6
    ],
    [
        "org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",
        6
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)",
        6
    ],
    [
        "org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)",
        6
    ],
    [
        "org.apache.hadoop.security.Credentials:readProto(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)",
        6
    ],
    [
        "org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)",
        6
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry)",
        6
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory()",
        6
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getHomeDirectory()",
        6
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp)",
        6
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem)",
        6
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory()",
        6
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory()",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.PathIOException:getPath()",
        6
    ],
    [
        "org.apache.hadoop.fs.PathIOException:getTargetPath()",
        6
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getUsed()",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory()",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints()",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory()",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints()",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink()",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)",
        6
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File)",
        6
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI)",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[])",
        6
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.Path:getParentUtil()",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)",
        6
    ],
    [
        "org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        6
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path)",
        6
    ],
    [
        "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto)",
        6
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])",
        6
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission()",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)",
        6
    ],
    [
        "org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration)",
        6
    ],
    [
        "org.apache.hadoop.service.AbstractService:stop()",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata)",
        6
    ],
    [
        "org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)",
        6
    ],
    [
        "org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.security.token.Token:equals(java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkDir(java.io.File)",
        6
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File)",
        6
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:flushBuffer()",
        6
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:flush()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)",
        6
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock()",
        6
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset()",
        6
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun()",
        6
    ],
    [
        "org.apache.hadoop.io.MapFile$Merger:close()",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close()",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup()",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close()",
        6
    ],
    [
        "org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream)",
        6
    ],
    [
        "org.apache.hadoop.security.Credentials:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[])",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[])",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer)",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable)",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)",
        6
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        6
    ],
    [
        "org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)",
        6
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int)",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput)",
        6
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token)",
        6
    ],
    [
        "org.apache.hadoop.security.token.Token:copyToken()",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>()",
        6
    ],
    [
        "org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)",
        6
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce()",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey()",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable)",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream)",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[])",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close()",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[])",
        6
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>()",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache()",
        6
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources()",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",
        6
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>()",
        6
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[])",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class)",
        6
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int)",
        6
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)",
        6
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)",
        6
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",
        6
    ],
    [
        "org.apache.hadoop.security.KDiag:validateKeyLength()",
        6
    ],
    [
        "org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        6
    ],
    [
        "org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.KDiag:validateShortName()",
        6
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser()",
        6
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto)",
        6
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:<init>()",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Ls:<init>()",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Count:<init>()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder()",
        6
    ],
    [
        "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder()",
        6
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps()",
        6
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:createConnection()",
        6
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab()",
        6
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean)",
        6
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache()",
        6
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache()",
        6
    ],
    [
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor()",
        6
    ],
    [
        "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor()",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call)",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse()",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall)",
        6
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int)",
        6
    ],
    [
        "org.apache.hadoop.util.CacheableIPList:reset()",
        6
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[])",
        6
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)",
        6
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)",
        6
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.util.LightWeightCache:put(java.lang.Object)",
        6
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData)",
        6
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:toString(boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        6
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)",
        6
    ],
    [
        "org.apache.hadoop.io.WritableComparator:newKey()",
        6
    ],
    [
        "org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable)",
        6
    ],
    [
        "org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        6
    ],
    [
        "org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.security.token.Token:decodeIdentifier()",
        6
    ],
    [
        "org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class)",
        6
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>()",
        6
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>()",
        6
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)",
        6
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)",
        6
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow()",
        6
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)",
        6
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput)",
        6
    ],
    [
        "org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",
        6
    ],
    [
        "org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)",
        6
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:becomeActive()",
        6
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean)",
        6
    ],
    [
        "org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close()",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)",
        7
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)",
        7
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)",
        7
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics)",
        7
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)",
        7
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])",
        7
    ],
    [
        "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",
        7
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        7
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)",
        7
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput)",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int)",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int)",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)",
        7
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)",
        7
    ],
    [
        "org.apache.hadoop.util.Shell:execCommand(java.lang.String[])",
        7
    ],
    [
        "org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])",
        7
    ],
    [
        "org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials)",
        7
    ],
    [
        "org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials)",
        7
    ],
    [
        "org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)",
        7
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token)",
        7
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)",
        7
    ],
    [
        "org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory()",
        7
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp)",
        7
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:<init>()",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:refreshStatus()",
        7
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getUsed()",
        7
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getUsed()",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance()",
        7
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",
        7
    ],
    [
        "org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date)",
        7
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)",
        7
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI)",
        7
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink()",
        7
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        7
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",
        7
    ],
    [
        "org.apache.hadoop.fs.Path:getParent()",
        7
    ],
    [
        "org.apache.hadoop.fs.Path:getOptionalParentPath()",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator()",
        7
    ],
    [
        "org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput)",
        7
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        7
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])",
        7
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)",
        7
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",
        7
    ],
    [
        "org.apache.hadoop.service.AbstractService:close()",
        7
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object)",
        7
    ],
    [
        "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object)",
        7
    ],
    [
        "org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object)",
        7
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials)",
        7
    ],
    [
        "org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File)",
        7
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:write(int)",
        7
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)",
        7
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",
        7
    ],
    [
        "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        7
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        7
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA()",
        7
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA()",
        7
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish()",
        7
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int)",
        7
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean)",
        7
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close()",
        7
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd()",
        7
    ],
    [
        "org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup()",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",
        7
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        7
    ],
    [
        "org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput)",
        7
    ],
    [
        "org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput)",
        7
    ],
    [
        "org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text)",
        7
    ],
    [
        "org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",
        7
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry()",
        7
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable)",
        7
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)",
        7
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",
        7
    ],
    [
        "org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket)",
        7
    ],
    [
        "org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)",
        7
    ],
    [
        "org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics()",
        7
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[])",
        7
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo()",
        7
    ],
    [
        "org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop()",
        7
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates()",
        7
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int)",
        7
    ],
    [
        "org.apache.hadoop.ipc.Server$Handler:run()",
        7
    ],
    [
        "org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File)",
        7
    ],
    [
        "org.apache.hadoop.security.KDiag:validateJAAS(boolean)",
        7
    ],
    [
        "org.apache.hadoop.security.KDiag:validateNTPConf()",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        7
    ],
    [
        "org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto)",
        7
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode()",
        7
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:getActiveData()",
        7
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession()",
        7
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab()",
        7
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab()",
        7
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)",
        7
    ],
    [
        "org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress)",
        7
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:updateAddress()",
        7
    ],
    [
        "org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)",
        7
    ],
    [
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        7
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:run()",
        7
    ],
    [
        "org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry)",
        7
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)",
        7
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)",
        7
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toString(boolean)",
        7
    ],
    [
        "org.apache.hadoop.fs.QuotaUsage:toString()",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class)",
        7
    ],
    [
        "org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",
        7
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)",
        7
    ],
    [
        "org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class)",
        7
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)",
        7
    ],
    [
        "org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder)",
        7
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)",
        7
    ],
    [
        "org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)",
        7
    ],
    [
        "org.apache.hadoop.util.HostsFileReader:refresh()",
        7
    ],
    [
        "org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key)",
        7
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource)",
        7
    ],
    [
        "org.apache.hadoop.conf.Configuration:getProps()",
        7
    ],
    [
        "org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)",
        8
    ],
    [
        "org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)",
        8
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData)",
        8
    ],
    [
        "org.apache.hadoop.fs.BlockLocation:<init>()",
        8
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",
        8
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot()",
        8
    ],
    [
        "org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics)",
        8
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object)",
        8
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)",
        8
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        8
    ],
    [
        "org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput)",
        8
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput)",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData)",
        8
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[])",
        8
    ],
    [
        "org.apache.hadoop.fs.FileUtil:readLink(java.io.File)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])",
        8
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        8
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin()",
        8
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions()",
        8
    ],
    [
        "org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials)",
        8
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials)",
        8
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:toFile()",
        8
    ],
    [
        "org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        8
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        8
    ],
    [
        "org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext:processDeleteOnExit()",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:<init>()",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList)",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.Truncate:waitForRecovery()",
        8
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])",
        8
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next()",
        8
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",
        8
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI)",
        8
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:parentExists()",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",
        8
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.Path:isRoot()",
        8
    ],
    [
        "org.apache.hadoop.fs.Path:suffix(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",
        8
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus)",
        8
    ],
    [
        "org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader)",
        8
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus()",
        8
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])",
        8
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])",
        8
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build()",
        8
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable)",
        8
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials)",
        8
    ],
    [
        "org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)",
        8
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC()",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock()",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC()",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize()",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close()",
        8
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish()",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int)",
        8
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)",
        8
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close()",
        8
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd()",
        8
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close()",
        8
    ],
    [
        "org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",
        8
    ],
    [
        "org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier()",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier()",
        8
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>()",
        8
    ],
    [
        "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",
        8
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",
        8
    ],
    [
        "org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)",
        8
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent()",
        8
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow()",
        8
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean()",
        8
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans()",
        8
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",
        8
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown()",
        8
    ],
    [
        "org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File)",
        8
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class)",
        8
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run()",
        8
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates()",
        8
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)",
        8
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        8
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:getCurrentActive()",
        8
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",
        8
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal()",
        8
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin()",
        8
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server)",
        8
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation)",
        8
    ],
    [
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)",
        8
    ],
    [
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        8
    ],
    [
        "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)",
        8
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)",
        8
    ],
    [
        "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)",
        8
    ],
    [
        "org.apache.hadoop.fs.ContentSummary:toString()",
        8
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        8
    ],
    [
        "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class)",
        8
    ],
    [
        "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)",
        8
    ],
    [
        "org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput)",
        8
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput)",
        8
    ],
    [
        "org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput)",
        8
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token)",
        8
    ],
    [
        "org.apache.hadoop.security.token.Token:toString()",
        8
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:setDeprecatedProperties()",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:size()",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:clear()",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:iterator()",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:write(java.io.DataOutput)",
        8
    ],
    [
        "org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String)",
        8
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)",
        9
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int)",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.SetReplication:waitForReplication()",
        9
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        9
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",
        9
    ],
    [
        "org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create()",
        9
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object)",
        9
    ],
    [
        "org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList)",
        9
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List)",
        9
    ],
    [
        "org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)",
        9
    ],
    [
        "org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell()",
        9
    ],
    [
        "org.apache.hadoop.util.Shell:checkIsBashSupported()",
        9
    ],
    [
        "org.apache.hadoop.util.Shell:isSetsidSupported()",
        9
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO()",
        9
    ],
    [
        "org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",
        9
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getCredentials()",
        9
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider:flush()",
        9
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:flush()",
        9
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        9
    ],
    [
        "org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileContext$FileContextFinalizer:run()",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList)",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList)",
        9
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList)",
        9
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",
        9
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",
        9
    ],
    [
        "org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        9
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)",
        9
    ],
    [
        "org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",
        9
    ],
    [
        "org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        9
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus)",
        9
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)",
        9
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>()",
        9
    ],
    [
        "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)",
        9
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB()",
        9
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock()",
        9
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init()",
        9
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB()",
        9
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close()",
        9
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int)",
        9
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)",
        9
    ],
    [
        "org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)",
        9
    ],
    [
        "org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)",
        9
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans()",
        9
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start()",
        9
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",
        9
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",
        9
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",
        9
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",
        9
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages)",
        9
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",
        9
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)",
        9
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats()",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken()",
        9
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",
        9
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[])",
        9
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int)",
        9
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",
        9
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",
        9
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque)",
        9
    ],
    [
        "org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream)",
        9
    ],
    [
        "org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream)",
        9
    ],
    [
        "org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.io.WritableComparator:get(java.lang.Class)",
        9
    ],
    [
        "org.apache.hadoop.io.ByteWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.IntWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.WritableComparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.Text$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.NullWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.LongWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.DoubleWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.MD5Hash$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.ShortWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.FloatWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.BytesWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.BooleanWritable$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.io.UTF8$Comparator:<init>()",
        9
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        9
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map)",
        9
    ],
    [
        "org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration)",
        9
    ],
    [
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)",
        9
    ],
    [
        "org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList)",
        10
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",
        10
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)",
        10
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded()",
        10
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo()",
        10
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",
        10
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation)",
        10
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        10
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation)",
        10
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        10
    ],
    [
        "org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[])",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed()",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next()",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",
        10
    ],
    [
        "org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)",
        10
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.LocatedFileStatus:<init>()",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.Globber:doGlob()",
        10
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",
        10
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)",
        10
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0()",
        10
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Print:execute()",
        10
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream)",
        10
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>()",
        10
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",
        10
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)",
        10
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)",
        10
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",
        10
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks()",
        10
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:recheckElectability()",
        10
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive()",
        10
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)",
        10
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)",
        10
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList)",
        10
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        10
    ],
    [
        "org.apache.hadoop.conf.Configuration:handleDeprecation()",
        10
    ],
    [
        "org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.conf.Configuration:getRaw(java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.conf.Configuration:unset(java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",
        10
    ],
    [
        "org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File)",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getNumProcessors()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getCpuFrequency()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead()",
        11
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten()",
        11
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission()",
        11
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner()",
        11
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup()",
        11
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput)",
        11
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush()",
        11
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus()",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)",
        11
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList)",
        11
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        11
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        11
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.Globber:glob()",
        11
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",
        11
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",
        11
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream)",
        11
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)",
        11
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",
        11
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)",
        11
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)",
        11
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)",
        11
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)",
        11
    ],
    [
        "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent)",
        11
    ],
    [
        "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String)",
        11
    ],
    [
        "org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)",
        11
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)",
        11
    ],
    [
        "org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",
        11
    ],
    [
        "org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)",
        11
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)",
        11
    ],
    [
        "org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)",
        11
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)",
        11
    ],
    [
        "org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)",
        12
    ],
    [
        "org.apache.hadoop.util.SysInfoWindows:getNumCores()",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",
        12
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",
        12
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        12
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)",
        12
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        12
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",
        12
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset()",
        12
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream)",
        12
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read()",
        12
    ],
    [
        "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",
        12
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus)",
        12
    ],
    [
        "org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token)",
        12
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI)",
        12
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getCanonicalServiceName()",
        12
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName()",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:get(java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        12
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)",
        12
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)",
        12
    ],
    [
        "org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)",
        12
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)",
        12
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)",
        12
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String)",
        12
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[])",
        12
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)",
        12
    ],
    [
        "org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput)",
        12
    ],
    [
        "org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File)",
        13
    ],
    [
        "org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)",
        13
    ],
    [
        "org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path)",
        13
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",
        13
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)",
        13
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)",
        13
    ],
    [
        "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration)",
        13
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName()",
        13
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getCanonicalServiceName()",
        13
    ],
    [
        "org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy()",
        13
    ],
    [
        "org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore()",
        13
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword()",
        13
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword()",
        13
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getStrings(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",
        13
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.net.TableMapping$RawTableMapping:load()",
        13
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory)",
        13
    ],
    [
        "org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getDirContext()",
        13
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean)",
        13
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",
        13
    ],
    [
        "org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration)",
        13
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:getParentZnode()",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)",
        13
    ],
    [
        "org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)",
        13
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])",
        13
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)",
        13
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)",
        13
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)",
        13
    ],
    [
        "org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)",
        13
    ],
    [
        "org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)",
        13
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])",
        13
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])",
        13
    ],
    [
        "org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)",
        13
    ],
    [
        "org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)",
        13
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        14
    ],
    [
        "org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        14
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read()",
        14
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)",
        14
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",
        14
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        14
    ],
    [
        "org.apache.hadoop.fs.FileContext:getUMask()",
        14
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",
        14
    ],
    [
        "org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",
        14
    ],
    [
        "org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages()",
        14
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)",
        14
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles()",
        14
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getInts(java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",
        14
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        14
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",
        14
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)",
        14
    ],
    [
        "org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List)",
        14
    ],
    [
        "org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings()",
        14
    ],
    [
        "org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)",
        14
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:init()",
        14
    ],
    [
        "org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",
        14
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider)",
        14
    ],
    [
        "org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",
        14
    ],
    [
        "org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)",
        14
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration)",
        14
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:confirmFormat()",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)",
        14
    ],
    [
        "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)",
        14
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",
        14
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)",
        15
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object)",
        15
    ],
    [
        "org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object)",
        15
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File)",
        15
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",
        15
    ],
    [
        "org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[])",
        15
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)",
        15
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",
        15
    ],
    [
        "org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.KDiag:validateKinitExecutable()",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)",
        15
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build()",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build()",
        15
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:connect()",
        15
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)",
        15
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS()",
        15
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)",
        15
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:createDecompressor()",
        15
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createCompressor()",
        15
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.SnappyCodec:createDecompressor()",
        15
    ],
    [
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)",
        15
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",
        15
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)",
        15
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])",
        15
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)",
        15
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout()",
        15
    ],
    [
        "org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)",
        15
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDefaultBlockSize()",
        15
    ],
    [
        "org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval()",
        15
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter()",
        15
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized()",
        15
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)",
        15
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",
        15
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)",
        15
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",
        15
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)",
        15
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        15
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",
        15
    ],
    [
        "org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList)",
        15
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem)",
        15
    ],
    [
        "org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)",
        15
    ],
    [
        "org.apache.hadoop.io.compress.Lz4Codec:createCompressor()",
        15
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException)",
        15
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported()",
        15
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)",
        15
    ],
    [
        "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate()",
        15
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders()",
        15
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)",
        15
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)",
        15
    ],
    [
        "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider()",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider()",
        15
    ],
    [
        "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class)",
        15
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        15
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        15
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)",
        15
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyServers:refresh()",
        15
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)",
        15
    ],
    [
        "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)",
        15
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)",
        15
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",
        15
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)",
        15
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",
        15
    ],
    [
        "org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        15
    ],
    [
        "org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[])",
        15
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:getCompressorType()",
        15
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType()",
        15
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createDecompressor()",
        15
    ],
    [
        "org.apache.hadoop.net.TableMapping:reloadCachedMappings()",
        15
    ],
    [
        "org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List)",
        15
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String)",
        15
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)",
        15
    ],
    [
        "org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[])",
        15
    ],
    [
        "org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration)",
        15
    ],
    [
        "org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)",
        15
    ],
    [
        "org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)",
        15
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)",
        15
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource()",
        16
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)",
        16
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        16
    ],
    [
        "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build()",
        16
    ],
    [
        "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig)",
        16
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)",
        16
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)",
        16
    ],
    [
        "org.apache.hadoop.fs.FutureDataInputStreamBuilder:build()",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory()",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        16
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)",
        16
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",
        16
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile()",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])",
        16
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem)",
        16
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",
        16
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:init(boolean)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",
        16
    ],
    [
        "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)",
        16
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys()",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[])",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",
        16
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)",
        16
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData()",
        16
    ],
    [
        "org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",
        16
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:connect()",
        16
    ],
    [
        "org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)",
        16
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize()",
        16
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getServerDefaults()",
        16
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize()",
        16
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",
        16
    ],
    [
        "org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)",
        16
    ],
    [
        "org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)",
        16
    ],
    [
        "org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)",
        16
    ],
    [
        "org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor)",
        16
    ],
    [
        "org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:initHM()",
        16
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:close()",
        16
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer)",
        16
    ],
    [
        "org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec()",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",
        16
    ],
    [
        "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",
        16
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)",
        16
    ],
    [
        "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)",
        16
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.RPC$Builder:build()",
        16
    ],
    [
        "org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        16
    ],
    [
        "org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",
        16
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass()",
        16
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)",
        16
    ],
    [
        "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)",
        16
    ],
    [
        "org.apache.hadoop.security.KDiag:validateSasl(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)",
        16
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.conf.Configuration:getPassword(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        16
    ],
    [
        "org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",
        16
    ],
    [
        "org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])",
        16
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",
        16
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",
        16
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",
        16
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",
        16
    ],
    [
        "org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)",
        16
    ],
    [
        "org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run()",
        16
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List)",
        16
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        16
    ],
    [
        "org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer)",
        16
    ],
    [
        "org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)",
        16
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources()",
        17
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",
        17
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        17
    ],
    [
        "org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)",
        17
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)",
        17
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.FsShell:getFS()",
        17
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",
        17
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)",
        17
    ],
    [
        "org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory()",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",
        17
    ],
    [
        "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)",
        17
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",
        17
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createCompressor()",
        17
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",
        17
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor()",
        17
    ],
    [
        "org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor()",
        17
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",
        17
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)",
        17
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock()",
        17
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[])",
        17
    ],
    [
        "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",
        17
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)",
        17
    ],
    [
        "org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory()",
        17
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getServerDefaults()",
        17
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults()",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getServerDefaults()",
        17
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)",
        17
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)",
        17
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        17
    ],
    [
        "org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)",
        17
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable)",
        17
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object)",
        17
    ],
    [
        "org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",
        17
    ],
    [
        "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        17
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",
        17
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>()",
        17
    ],
    [
        "org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)",
        17
    ],
    [
        "org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.fs.GetSpaceUsed$Builder:build()",
        17
    ],
    [
        "org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)",
        17
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder()",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder()",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder()",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder()",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder()",
        17
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder()",
        17
    ],
    [
        "org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)",
        17
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        17
    ],
    [
        "org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream)",
        17
    ],
    [
        "org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)",
        17
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        18
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        18
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        18
    ],
    [
        "org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)",
        18
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",
        18
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)",
        18
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List)",
        18
    ],
    [
        "org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)",
        18
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",
        18
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.io.compress.BZip2Codec:createCompressor()",
        18
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int)",
        18
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:close()",
        18
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock()",
        18
    ],
    [
        "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",
        18
    ],
    [
        "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",
        18
    ],
    [
        "org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory()",
        18
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:close()",
        18
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)",
        18
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",
        18
    ],
    [
        "org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read()",
        18
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        18
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",
        18
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        18
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        18
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",
        18
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        18
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        18
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:initRPC()",
        18
    ],
    [
        "org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.DU:main(java.lang.String[])",
        18
    ],
    [
        "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups)",
        18
    ],
    [
        "org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig)",
        18
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig)",
        18
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path)",
        18
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int)",
        18
    ],
    [
        "org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)",
        18
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration()",
        18
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)",
        18
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)",
        18
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration)",
        18
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:initZK()",
        18
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        18
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        18
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        18
    ],
    [
        "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)",
        18
    ],
    [
        "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String)",
        18
    ],
    [
        "org.apache.hadoop.conf.Configuration:main(java.lang.String[])",
        18
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start()",
        19
    ],
    [
        "org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder)",
        19
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",
        19
    ],
    [
        "org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams)",
        19
    ],
    [
        "org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration)",
        19
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:getDirectoryContents()",
        19
    ],
    [
        "org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)",
        19
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle)",
        19
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore()",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex()",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int)",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:close()",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int)",
        19
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)",
        19
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",
        19
    ],
    [
        "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",
        19
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData)",
        19
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData)",
        19
    ],
    [
        "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        19
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",
        19
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir()",
        19
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:readIndex()",
        19
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",
        19
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        19
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        19
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        19
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList)",
        19
    ],
    [
        "org.apache.hadoop.fs.FsShell:getTrash()",
        19
    ],
    [
        "org.apache.hadoop.security.Groups:getUserToGroupsMappingService()",
        19
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)",
        19
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:<init>()",
        19
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration()",
        19
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",
        19
    ],
    [
        "org.apache.hadoop.http.HttpServer2$Builder:build()",
        19
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers()",
        19
    ],
    [
        "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode)",
        19
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)",
        19
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])",
        19
    ],
    [
        "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String)",
        19
    ],
    [
        "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties)",
        20
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams)",
        20
    ],
    [
        "org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData)",
        20
    ],
    [
        "org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize()",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication()",
        20
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",
        20
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)",
        20
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)",
        20
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey()",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey()",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int)",
        20
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)",
        20
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults()",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData)",
        20
    ],
    [
        "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)",
        20
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore()",
        20
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:midKey()",
        20
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable)",
        20
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)",
        20
    ],
    [
        "org.apache.hadoop.io.MapFile$Merger:mergePass()",
        20
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",
        20
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",
        20
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",
        20
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",
        20
    ],
    [
        "org.apache.hadoop.fs.FsShell:getCurrentTrashDir()",
        20
    ],
    [
        "org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path)",
        20
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:ensureInitialized()",
        20
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration)",
        20
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",
        20
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String)",
        20
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:getSip()",
        20
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",
        20
    ],
    [
        "org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)",
        20
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List)",
        20
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties)",
        21
    ],
    [
        "org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        21
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)",
        21
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData)",
        21
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles()",
        21
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit()",
        21
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)",
        21
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",
        21
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)",
        21
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",
        21
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:openForSequentialIO()",
        21
    ],
    [
        "org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData)",
        21
    ],
    [
        "org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)",
        21
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path)",
        21
    ],
    [
        "org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        21
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long)",
        21
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum()",
        21
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        21
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",
        21
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance()",
        21
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])",
        21
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush()",
        21
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable)",
        21
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)",
        21
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",
        21
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled()",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor()",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getGroups()",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getGroupsSet()",
        21
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)",
        21
    ],
    [
        "org.apache.hadoop.fs.FsShell:init()",
        21
    ],
    [
        "org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",
        21
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider()",
        21
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",
        21
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",
        21
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider()",
        21
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)",
        21
    ],
    [
        "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)",
        21
    ],
    [
        "org.apache.hadoop.util.curator.ZKCuratorManager:start()",
        21
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData)",
        22
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close()",
        22
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",
        22
    ],
    [
        "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",
        22
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData)",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData)",
        22
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner()",
        22
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)",
        22
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)",
        22
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",
        22
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind()",
        22
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable)",
        22
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",
        22
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled()",
        22
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab()",
        22
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",
        22
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName()",
        22
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getGroupNames()",
        22
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation)",
        22
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject)",
        22
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject)",
        22
    ],
    [
        "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        22
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        22
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        22
    ],
    [
        "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)",
        22
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:authorizeConnection()",
        22
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        23
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",
        23
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData)",
        23
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)",
        23
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",
        23
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)",
        23
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)",
        23
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)",
        23
    ],
    [
        "org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable)",
        23
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",
        23
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit()",
        23
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",
        23
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)",
        23
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",
        23
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean)",
        23
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",
        23
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",
        23
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)",
        23
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)",
        23
    ],
    [
        "org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",
        23
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:print()",
        23
    ],
    [
        "org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation)",
        23
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getLoginUser()",
        23
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject)",
        23
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer)",
        23
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData)",
        24
    ],
    [
        "org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        24
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",
        24
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[])",
        24
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[])",
        24
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[])",
        24
    ],
    [
        "org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable)",
        24
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",
        24
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        24
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",
        24
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)",
        24
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",
        24
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",
        24
    ],
    [
        "org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)",
        24
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path)",
        24
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path)",
        24
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path)",
        24
    ],
    [
        "org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)",
        24
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)",
        24
    ],
    [
        "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",
        24
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getCurrentUser()",
        24
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased()",
        24
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased()",
        24
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction)",
        24
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction)",
        24
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:cedeActive(int)",
        24
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou()",
        24
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])",
        25
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",
        25
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        25
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",
        25
    ],
    [
        "org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        25
    ],
    [
        "org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)",
        25
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getHomeDirectory()",
        25
    ],
    [
        "org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)",
        25
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>()",
        25
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        25
    ],
    [
        "org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)",
        25
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)",
        25
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory()",
        25
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",
        25
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",
        25
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)",
        25
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)",
        25
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)",
        25
    ],
    [
        "org.apache.hadoop.security.alias.UserProvider:<init>()",
        25
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction)",
        25
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        25
    ],
    [
        "org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)",
        25
    ],
    [
        "org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration)",
        25
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser()",
        25
    ],
    [
        "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)",
        25
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb()",
        25
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object)",
        25
    ],
    [
        "org.apache.hadoop.ipc.Client$Connection$1:run()",
        25
    ],
    [
        "org.apache.hadoop.ipc.Server:doKerberosRelogin()",
        25
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[])",
        25
    ],
    [
        "org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int)",
        25
    ],
    [
        "org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover()",
        25
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])",
        26
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        26
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        26
    ],
    [
        "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",
        26
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",
        26
    ],
    [
        "org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        26
    ],
    [
        "org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        26
    ],
    [
        "org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        26
    ],
    [
        "org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory()",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path)",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean)",
        26
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory()",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        26
    ],
    [
        "org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        26
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        26
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>()",
        26
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration)",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)",
        26
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory()",
        26
    ],
    [
        "org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",
        26
    ],
    [
        "org.apache.hadoop.fs.FileContext:getHomeDirectory()",
        26
    ],
    [
        "org.apache.hadoop.fs.FilterFs:getHomeDirectory()",
        26
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",
        26
    ],
    [
        "org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",
        26
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)",
        26
    ],
    [
        "org.apache.hadoop.security.KDiag:loginFromKeytab()",
        26
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)",
        26
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation)",
        26
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi()",
        26
    ],
    [
        "org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List)",
        26
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod)",
        26
    ],
    [
        "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",
        26
    ],
    [
        "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)",
        26
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)",
        26
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",
        27
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",
        27
    ],
    [
        "org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol()",
        27
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)",
        27
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)",
        27
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        27
    ],
    [
        "org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        27
    ],
    [
        "org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        27
    ],
    [
        "org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        27
    ],
    [
        "org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        27
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path)",
        27
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir()",
        27
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path)",
        27
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)",
        27
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)",
        27
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run()",
        27
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date)",
        27
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean)",
        27
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean)",
        27
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean)",
        27
    ],
    [
        "org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)",
        27
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        27
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        27
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        27
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        27
    ],
    [
        "org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        27
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration)",
        27
    ],
    [
        "org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",
        27
    ],
    [
        "org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",
        27
    ],
    [
        "org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        27
    ],
    [
        "org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",
        27
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",
        27
    ],
    [
        "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",
        27
    ],
    [
        "org.apache.hadoop.security.KDiag:execute()",
        27
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[])",
        27
    ],
    [
        "org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[])",
        27
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)",
        27
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)",
        27
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String)",
        27
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token)",
        27
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token)",
        27
    ],
    [
        "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",
        27
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse()",
        27
    ],
    [
        "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",
        27
    ],
    [
        "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",
        27
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)",
        27
    ],
    [
        "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object)",
        27
    ],
    [
        "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[])",
        28
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget)",
        28
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)",
        28
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)",
        28
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)",
        28
    ],
    [
        "org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",
        28
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint()",
        28
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint()",
        28
    ],
    [
        "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately()",
        28
    ],
    [
        "org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData)",
        28
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",
        28
    ],
    [
        "org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink()",
        28
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem)",
        28
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.security.KDiag:run(java.lang.String[])",
        28
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[])",
        28
    ],
    [
        "org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",
        28
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)",
        28
    ],
    [
        "org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",
        28
    ],
    [
        "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)",
        28
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",
        28
    ],
    [
        "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",
        28
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.lang.String)",
        28
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.net.URL)",
        28
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path)",
        28
    ],
    [
        "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream)",
        28
    ],
    [
        "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover()",
        29
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:createProxy()",
        29
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)",
        29
    ],
    [
        "org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)",
        29
    ],
    [
        "org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.FsUrlConnection:connect()",
        29
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem()",
        29
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path)",
        29
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI)",
        29
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration)",
        29
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)",
        29
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",
        29
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",
        29
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",
        29
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)",
        29
    ],
    [
        "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",
        29
    ],
    [
        "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)",
        29
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:tryConnect()",
        30
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)",
        30
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine)",
        30
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine)",
        30
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine)",
        30
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:getAllServiceState()",
        30
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:becomeActive()",
        30
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:becomeStandby()",
        30
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int)",
        30
    ],
    [
        "org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)",
        30
    ],
    [
        "org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget)",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData)",
        30
    ],
    [
        "org.apache.hadoop.fs.FsUrlConnection:getInputStream()",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData)",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData)",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",
        30
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        30
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        30
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",
        30
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)",
        30
    ],
    [
        "org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs()",
        30
    ],
    [
        "org.apache.hadoop.fs.FileContext:getLocalFSFileContext()",
        30
    ],
    [
        "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus)",
        30
    ],
    [
        "org.apache.hadoop.fs.FileContext:getFileContext()",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys()",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[])",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",
        30
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",
        30
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",
        30
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)",
        30
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer)",
        30
    ],
    [
        "org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)",
        30
    ],
    [
        "org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",
        30
    ],
    [
        "org.apache.hadoop.ha.HealthMonitor:loopUntilConnected()",
        31
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine)",
        31
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget)",
        31
    ],
    [
        "org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList)",
        31
    ],
    [
        "org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",
        31
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",
        31
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        31
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        31
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        31
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",
        31
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.Command:runAll()",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList)",
        31
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",
        31
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey()",
        31
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",
        31
    ],
    [
        "org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[])",
        31
    ],
    [
        "org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded()",
        31
    ],
    [
        "org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)",
        31
    ],
    [
        "org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData)",
        31
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",
        31
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",
        31
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])",
        31
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String)",
        31
    ],
    [
        "org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",
        31
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",
        31
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)",
        31
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",
        31
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL)",
        31
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",
        31
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[])",
        32
    ],
    [
        "org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[])",
        32
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        32
    ],
    [
        "org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        32
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        32
    ],
    [
        "org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        32
    ],
    [
        "org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        32
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        32
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",
        32
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)",
        32
    ],
    [
        "org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)",
        32
    ],
    [
        "org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList)",
        32
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",
        32
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor)",
        32
    ],
    [
        "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",
        32
    ],
    [
        "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",
        32
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)",
        32
    ],
    [
        "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)",
        32
    ],
    [
        "org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String)",
        32
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",
        32
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String)",
        32
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])",
        32
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String)",
        32
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])",
        32
    ],
    [
        "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String)",
        32
    ],
    [
        "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",
        32
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer)",
        32
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String)",
        32
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)",
        32
    ],
    [
        "org.apache.hadoop.ha.HAAdmin:run(java.lang.String[])",
        33
    ],
    [
        "org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        33
    ],
    [
        "org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",
        33
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",
        33
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",
        33
    ],
    [
        "org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList)",
        33
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",
        33
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next()",
        33
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",
        33
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Get:execute()",
        33
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Edit:execute()",
        33
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Append:execute()",
        33
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Remove:execute()",
        33
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Renew:execute()",
        33
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell$Import:execute()",
        33
    ],
    [
        "org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)",
        33
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String)",
        33
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)",
        33
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:readAndProcess()",
        33
    ],
    [
        "org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[])",
        33
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:doGetLevel()",
        33
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:doSetLevel()",
        33
    ],
    [
        "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",
        33
    ],
    [
        "org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        34
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.fs.shell.Command:run(java.lang.String[])",
        34
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",
        34
    ],
    [
        "org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)",
        34
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",
        34
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",
        34
    ],
    [
        "org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)",
        34
    ],
    [
        "org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)",
        34
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String)",
        34
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey)",
        34
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest()",
        34
    ],
    [
        "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",
        35
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",
        35
    ],
    [
        "org.apache.hadoop.fs.FsShell:run(java.lang.String[])",
        35
    ],
    [
        "org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",
        35
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",
        35
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)",
        35
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",
        35
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge()",
        35
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean)",
        35
    ],
    [
        "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)",
        35
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",
        35
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",
        35
    ],
    [
        "org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",
        35
    ],
    [
        "org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",
        35
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",
        35
    ],
    [
        "org.apache.hadoop.util.Classpath:main(java.lang.String[])",
        35
    ],
    [
        "org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)",
        35
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine)",
        35
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop()",
        35
    ],
    [
        "org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[])",
        35
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",
        36
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",
        36
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)",
        36
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)",
        36
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",
        36
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        36
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean)",
        36
    ],
    [
        "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",
        36
    ],
    [
        "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",
        36
    ],
    [
        "org.apache.hadoop.io.MapFile:main(java.lang.String[])",
        36
    ],
    [
        "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",
        36
    ],
    [
        "org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",
        36
    ],
    [
        "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",
        36
    ],
    [
        "org.apache.hadoop.util.RunJar:run(java.lang.String[])",
        36
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])",
        36
    ],
    [
        "org.apache.hadoop.ipc.Server$Listener$Reader:run()",
        36
    ],
    [
        "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",
        37
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",
        37
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path)",
        37
    ],
    [
        "org.apache.hadoop.util.RunJar:main(java.lang.String[])",
        37
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",
        37
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",
        38
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",
        38
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",
        38
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",
        38
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])",
        38
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[])",
        38
    ],
    [
        "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])",
        38
    ],
    [
        "org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",
        39
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])",
        39
    ],
    [
        "org.apache.hadoop.util.ConfTest:main(java.lang.String[])",
        39
    ],
    [
        "org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])",
        39
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)",
        40
    ],
    [
        "org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])",
        40
    ],
    [
        "org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[])",
        40
    ],
    [
        "org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[])",
        40
    ],
    [
        "org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[])",
        40
    ],
    [
        "org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])",
        40
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)",
        41
    ],
    [
        "org.apache.hadoop.security.KDiag:main(java.lang.String[])",
        41
    ],
    [
        "org.apache.hadoop.fs.FsShell:main(java.lang.String[])",
        41
    ],
    [
        "org.apache.hadoop.log.LogLevel:main(java.lang.String[])",
        41
    ],
    [
        "org.apache.hadoop.util.FindClass:main(java.lang.String[])",
        41
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List)",
        42
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List)",
        43
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[])",
        44
    ],
    [
        "org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[])",
        44
    ]
]